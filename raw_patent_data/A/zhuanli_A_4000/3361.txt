标题title
基于云端计算的盲人智能导航眼镜系统及装置
摘要abst
本发明公开了基于云端计算的盲人智能导航眼镜系统及装置，包括采用摄像头采集场景视频信息，使用超声波传感器测量障碍物的距离，将采集的场景视频信息传输到云端；通过部署在云端的目标检测模型和测距避障模型对上传的视频图像信息进行分析，完成目标识别、场景判定、人脸识别、测距定位任务；最后将信息分析结果和障碍物距离信息进行语音合成播报给盲人。本发明使得盲人智能导航眼镜运行更加流畅，使用更加方便，具备更高的检测速度和精度。
权利要求书clms
1.基于云端计算的盲人智能导航眼镜系统，其特征在于，包括数据采集模块、数据分析模块、数据处理模块；数据采集模块：使用单目摄像头采集当前场景视频信息，使用超声波传感器测量当前场景无法识别的障碍物距离，将场景视频信息通过ZeroMQ消息传输协议上传至云端服务器；数据分析模块：通过部署在云端服务器的目标检测算法和测距避障算法对上传的视频图像进行分析，完成目标检测、人脸识别、场景判定、测距定位任务；所述目标检测算法包含基于改进YOLOv3的目标检测算法和基于opencv人脸检测算法；基于改进YOLOv3的目标检测算法，使用K-means聚类算法对数据集的先验框进行聚类分析，选出适合本数据集的先验框数目以及大小，同时改进网络结构，增加一个目标检测层，再采用多尺度训练；采用1-IOU作为K-means算法的距离度量公式，基于交并比的距离度量公式如下：d＝1-IOU式中，centroid是聚类中心，box则是标注框，d是每一个标注框与每一个聚类中心之间的距离，IOU是标注框与聚类中心的交并比；数据处理模块：将视频图像分析结果通过ZeroMQ消息传输协议返回至主板，通过对视频图像分析结果和障碍物距离信息进行关键词提取、匹配中文字符、追加单位、添加字句，将分析结果解析成一段完整的语句，完成播报语句的生成，最后调用语音合成技术将播报语句转换为语音播报给盲人。2.根据权利要求1所述的基于云端计算的盲人智能导航眼镜系统，其特征在于，所述改进网络结构为，YOLOv3算法使用Darknet-53网络进行特征提取，网络输出三种尺寸的特征层进行目标检测，特征层的大小分别为13×13、26×26、52×52，对52×52特征层进行2倍上采样，获取104×104尺寸的特征层，然后将上采样得到的104×104特征层与原有网络中4倍下采样得到的浅层104×104尺寸的卷积层进行融合，通过卷积操作后，组成4倍下采样的特征融合目标检测层。3.根据权利要求1所述的基于云端计算的盲人智能导航眼镜系统，其特征在于，所述数据处理模块中分析结果解析包括以下步骤：目标检测，对检测出来的目标进行中文字符匹配；判断检测出来的目标是否包含“person”目标，若包含便进行人脸识别，将识别出来的身份提取；从检测出来的目标中选择具有场景代表性的目标判定盲人当前所在的场景，将判定出来的场景提取；对检测出来的目标进行测距、定位，提取场景、距离、目标中文字符；对无法识别的障碍物使用超声波模块进行测距，并提取障碍物测距信息；以上提取关键词的顺序为：场景、距离、目标中文字符、障碍物距离信息，对提取的关键词进行匹配中文字符、追加单位、添加字句，使其成为一段完整的语句。4.根据权利要求1所述的基于云端计算的盲人智能导航眼镜系统，其特征在于，所述测距避障算法是基于线性回归的单目测距与避障算法，在测距方面，该算法将采集的目标距离与该目标在图像中的垂直高度进行回归拟合，通过调节自由度，获取最佳测距回归模型；在定向方面，该算法通过计算目标在图像中的重心，并根据重心落入图像的位置判断目标所在方向。5.根据权利要求1所述的基于云端计算的盲人智能导航眼镜系统，其特征在于，所述基于opencv人脸检测算法，利用Haar特征完成人脸检测，使用积分图对Haar-like特征求值，同时使用AdaBoost算法训练出强分类器将人脸和非人脸区分开来，最后将强分类器级联。6.基于云端计算的盲人智能导航眼镜装置，其特征在于，包括：主板、超声波传感器、摄像头、移动电源、耳机、眼镜外壳、云端计算平台；主板发送，接受数据，并处理语音信息；超声波传感器采集障碍物距离数据；摄像头采集场景视频数据；移动电源给开发板供电；耳机传输音频数据；主板、超声波传感器、摄像头、移动电源、耳机安装在眼镜外壳中；云端计算平台通过ZeroMQ消息传输协议完成与主板之间的数据传输；该装置使用摄像头采集当前场景视频信息，使用超声波传感器测量当前场景无法识别的障碍物距离，同时将场景视频信息通过ZeroMQ消息传输协议上传至云端计算平台；利用部署在云端的目标检测算法和测距避障算法对上传的视频图像进行分析，完成目标检测、人脸识别、场景判定、测距定位任务；数据处理模块将视频图像分析结果通过ZeroMQ消息传输协议返回至主板，通过对视频图像分析结果和障碍物距离信息进行关键词提取、匹配中文字符、追加单位、添加字句，将分析结果解析成一段完整的语句，完成播报语句的生成，最后调用语音合成技术将播报语句转换为语音通过耳机播报给盲人。7.根据权利要求6所述的基于云端计算的盲人智能导航眼镜装置，其特征在于，所述目标检测算法包含基于改进YOLOv3的目标检测算法和基于opencv人脸检测算法；所述基于改进YOLOv3的目标检测算法，使用K-means聚类算法对数据集的先验框进行聚类分析，选出适合本数据集的先验框数目以及大小，同时改进网络结构，增加一个目标检测层，再采用多尺度训练；采用1-IOU作为K-means算法的距离度量公式，基于交并比的距离度量公式如下：d＝1-IOU式中，centroid是聚类中心，box则是标注框，d是每一个标注框与每一个聚类中心之间的距离，IOU是标注框与聚类中心的交并比；所述基于opencv人脸检测算法，利用Haar特征完成人脸检测，使用积分图对Haar-like特征求值，同时使用AdaBoost算法训练出强分类器将人脸和非人脸区分开来，最后将强分类器级联。8.根据权利要求7所述的基于云端计算的盲人智能导航眼镜装置，其特征在于，所述改进网络结构为，YOLOv3算法使用Darknet-53网络进行特征提取，网络输出三种尺寸的特征层进行目标检测，特征层的大小分别为13×13、26×26、52×52，对52×52特征层进行2倍上采样，获取104×104尺寸的特征层，然后将上采样得到的104×104特征层与原有网络中4倍下采样得到的浅层104×104尺寸的卷积层进行融合，通过卷积操作后，组成4倍下采样的特征融合目标检测层。9.根据权利要求6所述的基于云端计算的盲人智能导航眼镜装置，其特征在于，数据处理模块中分析结果解析包括以下步骤：目标检测，对检测出来的目标进行中文字符匹配；判断检测出来的目标是否包含“person”目标，若包含便进行人脸识别，将识别出来的身份提取；从检测出来的目标中选择具有场景代表性的目标判定盲人当前所在的场景，将判定出来的场景提取；对检测出来的目标进行测距、定位，提取场景、距离、目标中文字符；对无法识别的障碍物使用超声波模块进行测距，并提取障碍物测距信息；以上提取关键词的顺序为：场景、距离、目标中文字符、障碍物距离信息，对提取的关键词进行匹配中文字符、追加单位、添加字句，使其成为一段完整的语句。10.根据权利要求6所述的基于云端计算的盲人智能导航眼镜装置，其特征在于，所述测距避障算法是基于线性回归的单目测距与避障算法，在测距方面，该算法将采集的目标距离与该目标在图像中的垂直高度进行回归拟合，通过调节自由度，获取最佳测距回归模型；在定向方面，该算法通过计算目标在图像中的重心，并根据重心落入图像的位置判断目标所在方向。
说明书desc
技术领域本发明涉及盲人导航技术领域领域，尤其涉及基于云端计算的盲人智能导航眼镜系统及装置。背景技术盲人由于失去视力导致他们的生活范围大大缩小，其大部分时间都将待在室内活动，考虑到室内起居生活是盲人生活必不可少的一部分，如何帮助盲人完成自己生活的起居，成为解决盲人生活困难，提升盲人生活质量的重要内容。近些年，室内导盲装置的研究逐渐成为众多学者和机构组织的研究方向，大量的研究成果即室内导盲装置涌现，总结目前比较流行的导盲方式主要分以下几种：传感器导盲、射频识别导盲、动物导盲等。传感器导盲主要使用超声波、红外线等传感器探测障碍物。使用传感器设计出来的导盲设备具有体积小、测距精度高等特点，但传感器的测距范围有限，且容易受环境因素的影响。采用射频识别技术的RFID导盲方式是将RFID阅读器嵌入导盲装置中，并通过预先铺设在盲道上的射频识别标签实现导盲。然而由于RFID电子标签价格比较高，且地面铺设范围大，成本高，难以推广实现。同时RFID技术也存在着电子标签信息被非法修改的情况，其安全性还有待提高。动物导盲主要利用导盲犬，通过它们的嗅觉、视觉为盲人导航。但目前我国导盲犬专业训练人员比较少，同时导盲犬的训练难度较大，训练时间较长，面对我国庞大的盲人数量，实行导盲犬助盲较为困难。针对室内环境，盲人导航眼镜是一种不错的选择。但由于硬件条件限制，以及功能较为单一，现有的导盲眼镜还在不断的研究探索中。近几年来，随着计算机硬件技术快速发展，盲人导航系统的硬件环境得到了持续改善，同时深度学习、图像处理、语音合成、云端计算等技术日益成熟，并逐渐应用到人工智能的各个领域中。当前盲人导航系统对这些技术的应用较为单一，并没有将它们有机结合起来实现更丰富的功能。发明内容发明目的：为了解决目前盲人导航设备受硬件性能限制，计算能力弱且功能单一，导盲效果不理想的问题，本发明提出一种基于云端计算的盲人智能导航眼镜系统及装置。技术方案：基于云端计算的盲人智能导航眼镜系统，其特征在于，包括数据采集模块、数据分析模块、数据处理模块；数据采集模块：使用单目摄像头采集当前场景视频信息，使用超声波传感器测量当前场景无法识别的障碍物距离，将场景视频信息通过ZeroMQ消息传输协议上传至云端服务器；数据分析模块：通过部署在云端服务器的目标检测算法和测距避障算法对上传的视频图像进行分析，完成目标检测、人脸识别、场景判定、测距定位任务；所述目标检测算法包含基于改进YOLOv3的目标检测算法和基于opencv人脸检测算法；基于改进YOLOv3的目标检测算法，使用K-means聚类算法对数据集的先验框进行聚类分析，选出适合本数据集的先验框数目以及大小，同时改进网络结构，增加一个目标检测层，再采用多尺度训练；采用1-IOU作为K-means算法的距离度量公式，基于交并比的距离度量公式如下：d＝1-IOU式中，centroid是聚类中心，box则是标注框，d是每一个标注框与每一个聚类中心之间的距离，IOU是标注框与聚类中心的交并比；数据处理模块：将视频图像分析结果通过ZeroMQ消息传输协议返回至主板，通过对视频图像分析结果和障碍物距离信息进行关键词提取、匹配中文字符、追加单位、添加字句，将分析结果解析成一段完整的语句，完成播报语句的生成，最后调用语音合成技术将播报语句转换为语音播报给盲人。所述改进网络结构为，YOLOv3算法使用Darknet-53网络进行特征提取，网络输出三种尺寸的特征层进行目标检测，特征层的大小分别为13×13、26×26、52×52，对52×52特征层进行2倍上采样，获取104×104尺寸的特征层，然后将上采样得到的104×104特征层与原有网络中4倍下采样得到的浅层104×104尺寸的卷积层进行融合，通过卷积操作后，组成4倍下采样的特征融合目标检测层。所述数据处理模块中分析结果解析包括以下步骤：目标检测，对检测出来的目标进行中文字符匹配，比如检测出来“person”这个目标，就匹配“人”这个中文字符，同理，“bed”匹配成“床”。；判断检测出来的目标是否包含“person”目标，若包含便进行人脸识别，将识别出来的身份提取；从检测出来的目标中选择具有场景代表性的目标判定盲人当前所在的场景，将判定出来的场景提取；对检测出来的目标进行测距、定位，提取场景、距离、目标中文字符，如提取的结果为“、5.00、人”，可解析为“在你的正前方5m处有3个人”；对无法识别的障碍物使用超声波模块进行测距，并提取障碍物测距信息；以上提取关键词的顺序为：场景、距离、目标中文字符、障碍物距离信息，对提取的关键词进行匹配中文字符、追加单位、添加字句，使其成为一段完整的语句。若分析结果的关键词提取为“客厅、、5.00、沙发、、3.50、人、”，则其可以解析为“你目前在客厅，在你的左前方5m处有1个沙发，在你的正前方3.5m处有2个人，他们分别是张三、李四”。所述基于opencv人脸检测算法，利用Haar特征完成人脸检测，使用积分图对Haar-like特征求值，同时使用AdaBoost算法训练出强分类器将人脸和非人脸区分开来，最后将强分类器级联。所述测距避障算法是基于线性回归的单目测距与避障算法，在测距方面，该算法将采集的目标距离与该目标在图像中的垂直高度进行回归拟合，通过调节自由度，获取最佳测距回归模型；在定向方面，该算法通过计算目标在图像中的重心，并根据重心落入图像的位置判断目标所在方向。基于云端计算的盲人智能导航眼镜装置，包括：主板、超声波传感器、摄像头、移动电源、耳机、眼镜外壳、云端计算平台；主板发送，接受数据，并处理语音信息；超声波传感器采集障碍物距离数据；摄像头采集场景视频数据；移动电源给开发板供电；耳机传输音频数据；主板、超声波传感器、摄像头、移动电源、耳机安装在眼镜外壳中；云端计算平台通过ZeroMQ消息传输协议完成与主板之间的数据传输；该装置使用摄像头采集当前场景视频信息，使用超声波传感器测量当前场景无法识别的障碍物距离，同时将场景视频信息通过ZeroMQ消息传输协议上传至云端计算平台；利用部署在云端的目标检测算法和测距避障算法对上传的视频图像进行分析，完成目标检测、人脸识别、场景判定、测距定位任务；数据处理模块将视频图像分析结果通过ZeroMQ消息传输协议返回至主板，通过对视频图像分析结果和障碍物距离信息进行关键词提取、匹配中文字符、追加单位、添加字句，将分析结果解析成解析成一段完整的语句，完成播报语句的生成，最后调用语音合成技术将播报语句转换为语音通过耳机播报给盲人。所述目标检测算法包含基于改进YOLOv3的目标检测算法和基于opencv人脸检测算法；所述基于改进YOLOv3的目标检测算法，使用K-means聚类算法对数据集的先验框进行聚类分析，选出适合本数据集的先验框数目以及大小，同时改进网络结构增加一个104×104尺寸目标检测层，再采用多尺度训练；采用1-IOU作为K-means算法的距离度量公式，基于交并比的距离度量公式如下：d＝1-IOU式中，centroid是聚类中心，box则是标注框，d是每一个标注框与每一个聚类中心之间的距离，IOU是标注框与聚类中心的交并比；所述基于opencv人脸检测算法，利用Haar特征完成人脸检测，使用积分图对Haar-like特征求值，同时使用AdaBoost算法训练出强分类器将人脸和非人脸区分开来，最后将强分类器级联。所述数据处理模块中分析结果解析包括以下步骤：目标检测，对检测出来的目标进行中文字符匹配，比如检测出来“person”这个目标，就匹配“人”这个中文字符，同理，“bed”匹配成“床”。；判断检测出来的目标是否包含“person”目标，若包含便进行人脸识别，将识别出来的身份提取；从检测出来的目标中选择具有场景代表性的目标判定盲人当前所在的场景，将判定出来的场景提取；对检测出来的目标进行测距、定位，提取场景、距离、目标中文字符，如提取的结果为“、5.00、人”，可解析为“在你的正前方5m处有3个人”；对无法识别的障碍物使用超声波模块进行测距，并提取障碍物测距信息；以上提取关键词的顺序为：场景、距离、目标中文字符、障碍物距离信息，对提取的关键词进行匹配中文字符、追加单位、添加字句，使其成为一段完整的语句。若分析结果的关键词提取为“客厅、、5.00、沙发、、3.50、人、”，则其可以解析为“你目前在客厅，在你的左前方5m处有1个沙发，在你的正前方3.5m处有2个人，他们分别是张三、李四”。所述测距避障算法是基于线性回归的单目测距与避障算法，在测距方面，该算法将采集的目标距离与该目标在图像中的垂直高度进行回归拟合，通过调节自由度，获取最佳测距回归模型；在定向方面，该算法通过计算目标在图像中的重心，并根据重心落入图像的位置判断目标所在方向。有益效果：与现有技术相比，本发明具有如下显著优点：1、计算速度快，功能可拓展性强，本发明通过将复杂的数据计算迁移到云端，克服了盲人导航设备受硬件性能限制的问题；2、具备更高的检测精度和更快的检测速度，本发明提出的基于改进YOLOv3的目标检测算法实现对小目标、遮挡目标、相邻目标的准确检测3、实现目标有效定位，本发明在测距的基础上增加了定向功能。附图说明图1是本发明的系统设计框图；图2是本发明的云端计算流程图；图3是本发明的图像数据采集传输流程图；图4是本发明的场景目标检测流程图；图5是本发明的人脸识别流程图；图6是本发明的测距与避障算法流程图；图7是本发明的语音合成流程图；图8是本发明的改进后的YOLOv3网络结构图；图9是本发明的多尺度训练流程图。具体实施方式下面结合附图对本发明的技术方案作进一步说明。本发明的系统设计框图如图1所示，盲人智能导航眼镜主要由三大部分组成：硬件平台P1、ZeroMQ消息传输协议P2、软件平台P3。其中，硬件平台P1是以主板1为核心，在其基础上集成超声波传感器2、摄像头3、语音合成4、语音播报5、预警提醒6。主板1作为硬件平台的中央处理器，负责数据的采集、发送、接收以及语音信息的播报；超声波传感器2负责采集4m以内无法识别障碍物距离信息；摄像头3像素为800万，可以捕捉3280×2464像素静态图片和30FPS 1080P的视频，负责采集场景视频信息；语音合成4模块采用云端TTS服务将文本信息转化成语音信息；语音播报5模块通过3.5mm插头耳机将语音信息播报给盲人；预警提醒6模块将障碍物距离信息通过语音播报5模块播报给盲人，从而实现预警提醒。ZeroMQ消息传输协议P2是一个更小、更快、更简单的传输层协议，它是由TCP传输控制协议、socket通信以及消息队列组成，主要完成硬件平台与软件平台的数据传输，该协议可以实现多个硬件平台连接一个软件平台，完成异步并发操作。软件平台P3以云端模型9为核心，对上传的视频图像8进行信息分析，实现目标检测&amp;人脸识别10、测距定位11、场景判定12功能。云端模型9是依托云端计算平台部署目标检测模型和测距避障模型，通过模型对硬件平台P1上传的视频图像8进行目标检测与人脸识别10、测距定位11、场景判定12，将该过程的数据分析结果通过ZeroMQ消息传输协议P2返回到硬件平台P1的语音合成4模块，该模块对数据分析结果进行语音合成，最终通过主板音频输出接口将语音播报给盲人，从而实现盲人导航。图2为本发明的云端计算流程图。本发明包括硬件平台和软件平台。硬件平台是一款可佩戴的盲人眼镜，包括主板、超声波传感器、摄像头、移动电源、耳机、眼镜外壳，需要安装开发语言python 3以及opencv、zmq、picamera等调用库；软件平台是部署目标检测模型和测距避障模型的云端计算平台，需要配置模型运行所需要的环境，下载安装Anaconda3项目库和tensorflow-gpu 1.14.0、Keras 2.1.5、CUDA 10.0、cudnn 10.0等框架；硬件平台和软件平台之间的数据传输由ZeroMQ消息传输协议完成。图3为本发明的图像数据采集传输流程图。本发明图像数据采集工作是通过视频实时拍摄完成，通过调用opencv的VideoCapture对象来捕获硬件平台中摄像头的视频。接下来通过ZeroMQ消息传输协议将视频中的每一帧图像传输至云端，然而ZeroMQ的发送和接收都是字节流，因此需要调用opencv的imencode方法对图像进行编码，之后将编码的图像数据转换成矩阵形式。同理，云端在接收图像时，需要将矩阵形式的数据转换为图像数据，接着调用opencv的imdecode方法对图像进行解码。图4为本发明的场景目标检测流程图。考虑到盲人智能导航眼镜系统对目标检测速度有较高的要求，所以选用基于改进YOLOv3的目标检测算法对室内场景目标进行检测。相较于室外场景目标检测，室内场景目标检测更加困难与复杂，因为室内场景中不同类别的目标物体具有很多相似的特征元素，比如沙发、椅子和床，这些目标之间的差异非常微小，很容易导致误检的情况。同时，由于室内空间的限制，室内的目标物体放置比较紧凑，经常出现目标物体被遮挡的情况，再加上拍摄角度、光线强弱、尺寸变化等因素的影响，导致漏检情况的发生。综合以上情况，本发明对YOLOv3算法作了进一步改进，首先使用K-means算法聚类出最优Anchor框的数目和大小；接着对YOLOv3网络结构进行了改进；最后完成了多尺度训练。改进后的YOLOv3算法对小目标、遮挡目标、低像素目标、相似目标有更好的识别准确度。盲人智能导航眼镜在进行场景目标检测时，首先启动摄像头采集当前场景图像；接下来将图像传送到改进后的YOLOv3算法中进行目标识别；最后从所识别目标中选取最具代表性的目标判定盲人当前所处的场景，例如，在一个场景中识别出“床”这个目标物体，那么可以判定盲人所处的场景是卧室，同样，场景中识别出“马桶”这个目标物体，就判定该场景为卫生间。在场景目标检测模块中，既实现了室内场景的判定，又完成目标检测的功能，为盲人室内场景导航提供更加详实的信息，助力盲人顺利完成日常生活的操作。图5为本发明的人脸识别流程图。为了使盲人能够在知道他人身份的情况下顺利与他人沟通，本发明加入了人脸检测识别模块。首先判断检测目标中是否包含人，若包含就进行人脸检测，并将检测到的人脸提取出来；然后将提取的人脸图像放入人脸比对库中进行人脸比对；最后确定人物身份；人脸检测采用了基于opencv的人脸检测技术，利用Haar特征完成人脸检测，通过使用积分图对Haar-like特征求值加速检测的进程，同时使用AdaBoost算法训练出强分类器将人脸和非人脸区分开来，最后将强分类器级联起来提高检测准确率。opencv中的Haar级联人脸检测算法计算量小，计算速度快。图6为本发明的测距与避障算法流程图。本发明所采用的测距与避障算法主要包括测距和定向两部分。对于测距，采用了基于线性回归的单目测距算法，其主要任务是将目标物体的实际距离和目标物体在图像中的垂直高度之间的非线性关系成功拟合出来，整个测距算法的框架由两部分构成，第一部分是统计不同距离下的图像中目标物体的垂直高度值，通过对统计出来的数据进行回归拟合，获取测距模型。第二部分则是使用训练出来的测距模型进行测距，首先需要对场景中的目标物体进行检测，然后返回所检测目标物体在图像中的垂直高度值，最后测距模型根据垂直高度值进行测距。测距的准确度一方面与测距模型回归拟合的好坏相关，另一方面也与检测过程中能否非常贴切地框出目标的高度相关。对于定向，其首先计算图像中目标的重心，根据重心落入图像的位置，确定目标相对于盲人所处的方向。图7为本发明的语音合成流程图。首先盲人通过智能导航眼镜采集当前场景信息，并将场景信息传送至云端进行信息分析；然后对分析结果进行关键词提取、匹配中文字符、追加单位、添加字句等操作，将分析结果解析成一段连贯而又容易理解的语句；最后将解析语句转化为语音播报给盲人，对盲人进行语音导航。上述场景目标检测流程中的基于改进YOLOv3的目标检测算法如下：选择最优先验框YOLOv3算法是通过先验框对边界框进行预测，初始化的先验框具有固定的宽高值。在预测过程中，通过对先验框的偏移量进行微调得到边界框的位置，所以，先验框的选择决定了目标物体的检测精度和速度。YOLOv3本身先验框的选取是依据VOC数据集和COCO数据集中目标的尺寸，与盲人导航眼镜所面向的室内场景目标大小具有较大的差距。因此，为了提高目标检测的精度和速度，根据实际的数据集选择优化先验框的设置是非常必要的。本发明提出通过K-means聚类算法对数据集的目标框进行聚类分析，获得室内场景中比较典型的目标尺寸，作为先验框尺寸的设置依据，获得最优的先验框。如果采用传统K-means算法的衡量标准去对室内场景数据集的标注框进行聚类分析，那么K-means算法只会对标注框的宽和高进行聚类。由于K-means算法会将距离聚类中心更小的标注框归属为一类，因此使用欧式距离作为标注框与聚类中心相似性的度量指标将会导致聚类出的先验框尺寸偏小，从而无法生成合理的先验框。为此，本发明提出采用交并比作为评价标注框与聚类中心相似性的度量指标。交并比计算的是标注框与聚类中心的交集和并集的比值，其大小与标注框尺寸无关，所以可以有效避免由于标注框尺寸大小在聚类过程中造成的误差。IOU表示标注框与聚类中心的重合程度，IOU越大，二者的重合度越高，二者相似性越高。因为K-means算法将距离聚类中心更小的标注框归属为一类，所以IOU不能直接作为距离度量公式，为此，本发明采用1-IOU作为K-means算法的距离度量公式。综上，基于交并比的距离度量公式定义如下：d＝1-IOU 式中，centroid是聚类中心，box则是标注框，d是每一个标注框与每一个聚类中心之间的距离，IOU是标注框与聚类中心的交并比；从式中可以看出，标注框与聚类中心的距离随IOU值的增大而减小，聚类的主要任务就是保证标注框与聚类中心有更大的交并比。改进网络结构YOLOv3算法使用Darknet-53网络进行特征提取，网络最终输出三种尺寸的特征层进行目标检测，特征层的大小分别为13×13、26×26、52×52。YOLOv3算法通过使用Darknet-53网络进行特征提取以及多尺度预测，获得了不错的检测精度和速度。但是，YOLOv3算法是在Darknet-53网络较深网络层进行的特征提取，虽然能够获得更丰富的语义信息，但也忽略了遮挡相邻的目标特征。其次，YOLOv3算法虽然在小目标的检测效果上有了进一步提升，但是很多情况下，对小目标的检测还是会出现特征信息丢失，产生漏检、误检、重复检测的问题。针对以上问题，需要对YOLOv3算法的网络结构进行一些改进，以提升网络对小目标、遮挡目标、相邻目标检测的速度和准确度。常规YOLOv3算法通过对Darknet-53网络提取的特征进行特征融合，输出三个尺寸的特征层。52×52特征层是YOLOv3网络通过8倍下采样得到的检测最小和最浅目标的特征层，为了从目标中获得更细粒度的特征和位置信息，本发明在原有YOLOv3网络结构基础上，对52×52特征层进行2倍上采样，获取104×104尺寸的特征层。接下来将上采样得到的104×104特征层与原有网络中4倍下采样得到的浅层104×104尺寸的卷积层进行融合，通过若干卷积层的卷积操作之后，组成了4倍下采样的特征融合目标检测层，实现了在104×104尺寸的特征层上进行小目标、浅特征目标以及相邻遮挡目标的检测。改进后的YOLOv3网络结构图如图8所示。多尺度训练由于多尺度训练能够显著提高模型对不同尺寸目标的适应能力，在深度学习网络训练中是一种重要的训练方法。多尺度训练只对全卷积网络有效，一般在训练时设置几种不同尺度的图片，每个epoch从中随机选取一个尺度进行训练，这样训练出来的模型鲁棒性强。原YOLOv3算法只采取单一尺度训练的方法，这将导致网络模型对图像尺寸的泛化能力下降，对于以前没有遇到过的图像大小，其检测准确性会降低。为了提高YOLOv3作为全卷积网络对物体大小的鲁棒性，使得全卷积网络可以接收任何尺寸的输入图像，在训练过程中也可以随意地改变输入图像的大小，为此本发明采用多尺度训练对YOLOv3进行训练。YOLOv3网络一共包含五个残差网络，网络输出图像是输入图像的1/32，所以在进行多尺度训练时，只需要采用32的整倍尺寸作为输入尺寸，本发明中预先定义的尺度分别为：416×416、480×352、608×384、576×416。多尺度训练流程图如图9所示。上述测距与避障算法流程图中的基于线性回归的单目测距算法如下：物体实际距离y与物体在图像中的垂直高度x呈光滑的非线性关系，因此可以通过采用多元线性回归模型，将非线性关系转化为线性关系，从而实现对数据的回归拟合。本发明所创建的非线性回归模型为：y＝α0+α1x+α2x2+...+αmxm+ε 式中，y为物体实际距离，x为物体在图像中的垂直高度，α0，α1，…，αm为未知参数，ε是随机误差项。将非线性回归模型转化为多元线性回归模型，如公式所示：y＝α0+α1x1+α2x2+...+αmxm+ε 假设ε～N，则公式对应的多元线性回归方程为：y＝α0+α1x1+α2x2+...+αmxm 式中，y为点处的回归值，是对未知参数的估计值。方程即为创建的回归模型，通过调节不同的自由度对创建的回归模型进行训练，求得α0，α1，α2，…，αm，获得表示目标在图像中的垂直高度x与目标实际距离y线性关系的回归方程。
