标题title
一种动态环境下移动机器人控制方法及系统
摘要abst
本发明公开了一种动态环境下移动机器人控制方法及系统，包括：基于scLTL逻辑规范构建编码器，将移动机器人的多维任务状态编码成一维状态；基于历史数据和风险模拟对移动机器人作业环境中不确定因素的风险进行预测和评估；融合编码器编码和强化学习，并考虑预测风险构建融合模型，通过融合模型得到移动机器人的多任务作业进行决策和路径规划。本发明采用逻辑规范进行多任务控制，考虑动态环境中不确定因素的风险，提出编码式强化学习融合算法，进行安全路径规划，规避环境中的风险，为移动机器人执行多任务作业生成最优安全路径，提前规避环境中的潜在风险，实现复杂问题简单化，并压缩探索空间，降低计算成本，从而提高求解速度。
权利要求书clms
1.一种动态环境下移动机器人控制方法，其特征在于，包括如下步骤：步骤1：基于scLTL逻辑规范构建编码器，将移动机器人的多维任务状态编码成一维状态；步骤2：基于历史数据和风险模拟对移动机器人作业环境中不确定因素的风险进行预测和评估；步骤3：融合编码器编码和强化学习，并考虑环境中的预测风险构建融合模型，通过融合模型得到移动机器人的多任务作业进行决策和路径规划。2.根据权利要求1所述的动态环境下移动机器人控制方法，其特征在于，所述基于scLTL逻辑规范构建编码器具体包括：多任务原子命题化，根据每个单任务状态τi，将I项任务进行原子命题化，并写入构建的scLTL逻辑规范任务模型φ；将任务模型φ转化为有限状态自动机Aφ；基于有限状态自动机Aφ与机器人多任务状态，构建编码器。3.根据根据权利要求2所述的动态环境下移动机器人控制方法，其特征在于，所述scLTL逻辑规范任务模型φ为：其中，T表示布尔运算符真，prp是原子命题集，φ、φ1、φ2代表原子命题，X表示下一步，Xφ表示φ下一个状态为真，F表示未来，Fφ表示φ未来是真，φ1Uφ2表示φ1为真直到φ2满足。4.根据根据权利要求2所述的动态环境下移动机器人控制方法，其特征在于，所述有限状态自动机Aφ为：其中，Q表示有限状态集，表示原子命题的有限集合，δ表示/＞的状态转移函数，q0表示初始状态，qF表示接受状态的有限集合。5.根据根据权利要求1所述的动态环境下移动机器人控制方法，其特征在于，所述编码器为：m＝Encoder＝Encoder其中，m∈M，为编码后的机器人多任务状态，其维度为一维，多任务状态ST＝。6.根据根据权利要求1所述的动态环境下移动机器人控制方法，其特征在于，所述基于历史数据和风险模拟对移动机器人作业环境中不确定因素的风险进行预测和评估具体包括：基于历史风险数据构建历史风险模型，对历史风险模型迭代学习直至满足要求，通过历史风险模型对对未来一段时间T0内的风险进行预测；针对现实环境中可能发生的突发事件进行模拟，构建随机模拟模型，基于统计方法，计算每一条道路在未来时间段T0的风险。7.根据根据权利要求6所述的动态环境下移动机器人控制方法，其特征在于，所述历史风险模型为：其中，为时间段T的历史风险数据集，c由核方程计算得到；/＞为模型输出值，α、β、ε为模型学习参数，/＞α、β、ε通过多次迭代学习确定，k为路径编号；该模型的迭代学习过程通过以下最小化公式进行：其中C1、C2为核函数计算所得矩阵，γ为常数，K为训练数据的维数，tr为计算矩阵的迹，J为数据长度，d为通过学习构建模型的维数，为训练数据集；未来一段时间T0内的风险为：其中Yt为时刻t的风险模型值，为所构建的风险模型数据集合的转置。8.根据根据权利要求6所述的动态环境下移动机器人控制方法，其特征在于，所述随机模拟模型为：其中，为时刻t时路段k的风险信息，ef为突发事件信息；未来时间段T0内的随机风险未来时间段T0内风险为：其中，α'、β'为平衡参数，高斯噪声。9.根据根据权利要求1所述的动态环境下移动机器人控制方法，其特征在于，所述融合模型其中，/＞是编码后的状态集，x,y为机器人环境坐标值，/＞是有限动作集，/＞为状态转移概率函数，/＞为初始状态，/＞为奖励函数，PrP为原子命题集，L为标签函数，为有限接受状态集合，Risk为风险信息；所述融合模型采用Q-Learning进行策略学习，利用贝尔曼方程进行Q值更新，直到收敛，所述贝尔曼方程为：其中，Q是状态s和动作a的Q值，α”为学习率，γ'为折扣因子，r为报酬，s'为更新后的状态，a'为更新后的动作。10.一种动态环境下移动机器人控制系统，其特征在于，包括多任务编码器模块、风险预测评估模块和路径规划模块，其中：所述多任务编码器模块基于scLTL逻辑规范构建编码器，将移动机器人的多维任务状态编码成一维状态；所述风险预测评估模块基于历史数据和风险模拟对移动机器人作业环境中不确定因素的风险进行预测和评估；所述路径规划模块融合编码器编码和强化学习，并考虑环境中的预测风险构建融合模型，通过融合模型得到移动机器人的多任务作业进行决策和路径规划。
说明书desc
技术领域本发明涉及物流机器人作业，尤其涉及一种动态环境下移动机器人控制方法及系统。背景技术随着人工智能技术的进步，机器人执行多任务的需求也日益增加，然而，机器人在执行多任务时的控制技术，尚有待提高，尤其是不确定风险情况下，多任务的控制以及安全路径规划。其中一个典型的应用场景，就是高校各校区间的材料物品配送。当前，当前诸多高校普遍拥有多个校区，分布在城市各角落，高校校区之间的距离会造成校区之间材料传递不便，尤其是疫情防控期间等特殊情况，多个校区间的文件、材料、物品运输极度依靠人工传递，费时费力，影响教职工的办公效率以及学生的日常生活。开发一个适用于执行多任务的机器人控制系统，实现跨校区机器人自动配送，可以给高校师生提供极大的便利。机器人在执行多任务时，面临的主要难点主要是多任务控制、安全路径规划。现有研究表明，单纯的强化学习算法，可以有效处理单任务情况下机器人的路径规划问题。但无法处理多任务控制问题，对于多任务控制，需要将作业任务分为多个阶段，定义多个奖惩函数，较为复杂。Linear temporal logic逻辑规范可以有效的进行多任务控制，并与强化学习进行融合，可以有效解决任务控制与路径规划。但现有的Linear temporal logic与强化学习的融合方式主要是乘积型，计算成本较大，并且未考虑环境中不确定因素所带来的风险问题。发明内容本发明的目的在于提供一种动态环境下移动机器人控制方法及系统，提前规避环境中的潜在风险，实现复杂问题简单化，并压缩探索空间，降低计算成本，提高了求解速度。实现本发明目的的技术方案为：一种动态环境下移动机器人控制方法，包括步骤：基于scLTL逻辑规范构建编码器，将移动机器人的多维任务状态编码成一维状态；基于历史数据和风险模拟对移动机器人作业环境中不确定因素的风险进行预测和评估；融合编码器编码和强化学习，并考虑环境中的预测风险构建融合模型，通过融合模型得到移动机器人的多任务作业进行决策和路径规划。进一步地，基于scLTL逻辑规范构建编码器具体包括：多任务原子命题化：根据每个单任务状态τi，将I项任务进行原子命题化，并写入构建的scLTL逻辑规范任务模型φ；将任务模型φ转化为有限状态自动机Aφ；基于有限状态自动机Aφ与机器人多任务状态，构建编码器。进一步地，所述scLTL逻辑规范任务模型φ为：其中，T表示布尔运算符真，prp是原子命题集，φ、φ1、φ2代表原子命题，X表示下一步，Xφ表示φ下一个状态为真，F表示未来，Fφ表示φ未来是真，φ1Uφ2表示φ1为真直到φ2满足。进一步地，所述有限状态自动机Aφ为：Aφ＝＜Q,2PrP,δ,q0,qF＞其中，Q表示有限状态集，2PrP表示原子命题的有限集合，δ表示Q×2PrP→Q的状态转移函数，q0表示初始状态，qF表示接受状态的有限集合。进一步地，所述编码器为：m＝Encoder＝Encoder其中，m∈M，为编码后的机器人多任务状态，其维度为一维，多任务状态ST＝。进一步地，所述基于历史数据和风险模拟对移动机器人作业环境中不确定因素的风险进行预测和评估具体包括：基于历史风险数据构建历史风险模型，对历史风险模型迭代学习直至满足要求，通过历史风险模型对对未来一段时间T0内的风险进行预测；针对现实环境中可能发生的突发事件进行模拟，构建随机模拟模型，基于统计方法，计算每一条道路在未来时间段T0的风险。进一步地，所述历史风险模型为：其中，为时间段T的历史风险数据集，c由核方程计算得到；/＞为模型输出值，α、β、ε为模型学习参数，/＞α、β、ε通过多次迭代学习确定，k为路径编号；该模型的迭代学习过程通过以下最小化公式进行：其中C1、C2为核函数计算所得矩阵，γ为常数，K为训练数据的维数，tr为计算矩阵的迹，J为数据长度，d为通过学习构建模型的维数，为训练数据集；未来一段时间T0内的风险为：其中Yt为时刻t的风险模型值，为所构建的风险模型数据集合的转置。进一步地，所述随机模拟模型为：其中，为时刻t时路段k的风险信息，ef为突发事件信息；未来时间段T0内的随机风险未来时间段T0内风险为：其中，α'、β'为平衡参数，高斯噪声。进一步地，所述融合模型其中，是编码后的状态集，/＞x,y为机器人环境坐标值，/＞是有限动作集，/＞为状态转移概率函数，/＞为初始状态，/＞为奖励函数，PrP为原子命题集，L为标签函数，/＞为有限接受状态集合，Risk为风险信息。进一步地，所述融合模型采用Q-Learning进行策略学习，利用贝尔曼方程进行Q值更新，直到收敛，所述贝尔曼方程为：其中，Q是状态s和动作a的Q值，α”为学习率，γ'为折扣因子，r为报酬，s'为更新后的状态，a'为更新后的动作。一种动态环境下移动机器人控制系统，包括多任务编码器模块、风险预测评估模块和路径规划模块，其中：所述多任务编码器模块基于scLTL逻辑规范构建编码器，将移动机器人的多维任务状态编码成一维状态；所述风险预测评估模块基于历史数据和风险模拟对移动机器人作业环境中不确定因素的风险进行预测和评估；所述路径规划模块融合编码器编码和强化学习，并考虑环境中的预测风险构建融合模型，通过融合模型得到移动机器人的多任务作业进行决策和路径规划。和既有的移动机器人控制相比，本发明具有以下有益效果：使用scLTL逻辑公式进行多任务管理，构建机器人多任务状态编码器，将机器人多任务状态转化为一维的单一状态，压缩了状态空间，任务数量越多，状态空间压缩越明显；考虑环境中的不确定因素所带来的风险，在规划层面上为机器人规划避免风险的最佳作业路径，提高机器人的安全性保障，这是传统MDP算法或scLTL与MDP算法所无法解决的问题；提出了scLTL与强化学习的编码式融合算法，与传统MDP与scLTL的算法相比，该算法架构更加有效，计算成本更低。附图说明图1为本发明控制系统架构图。图2为多任务控制模块图。图3为风险预测评估模块图。图4为路径规划模块图。图5为本发明控制方法流程图。图6为扬州大学扬子津校区、荷花池校区的简易地图。图7为校内交通通畅情况下的路径规划图，图7中的为扬子津校区路径规划图，图7中的为荷花池校区路径规划图。图8为考虑了校内交通拥堵风险图，图8中的为作业路径示意图，图8中的为报酬与学习次数关系示意图。图9为24×60环境实验结果图。图10为学习进程与状态压缩结果图。具体实施方式本发明为一种考虑动态环境中不确定因素风险的移动机器人控制方法，提出了一种不确定环境下考虑环境风险的移动机器人进行多任务作业的移动控制系统。该系统采用syntactically co-safe linear temporal logic逻辑规范进行多任务控制，考虑动态环境中不确定因素的风险，提出了编码式强化学习的融合算法，进行安全路径规划，在规划层面规避环境中的风险，为移动机器人执行多任务作业生成最优安全路径，提前规避环境中的潜在风险。所提出算法，首先利用syntactically co-safe linear temporallogic逻辑规范对多任务进行原子命题化，采用一个公式，进行多任务管理；其次，构建任务编码器，将多维度多任务状态编码成一维的单状态，并提出了面向多任务的编码式强化学习算法，将复杂的多任务路径规划问题转化为求解一个满足scLTL公式的最优策略问题，实现复杂问题简单化，并压缩探索空间，降低计算成本，从而提高求解速度；重中之重，所提出算法中，考虑了动态环境中不确定因素所带来的风险，构建了面向动态环境风险的安全策略求解算法，解决传统马尔可夫决策过程无法适用于动态环境的问题，在规划层面上提前规避风险，为移动机器人作业提供最优安全控制策略以及最优作业路径。如图1所示，本实施例提供的一种考虑风险的移动机器人控制系统该控制系统由以下3部分组成：S1:多任务编码器；S2：风险预测评估；S3：路径规划。本发明各项关键技术原理如下：S1:多任务编码控制器如图1中S1部分所示，本项目主要通过scLTL逻辑规范进行机器人的多任务控制，进而构建编码器，将多维任务状态编码成一维状态，从而压缩状态空间，技术原理如下：首先，将多任务原子命题化，利用公式进行整理表达，其中，T表示布尔运算符“真”；prp是原子命题集；φ，φ1，φ2代表原子命题；X表示下一步；Xφ表示φ下一个状态为真；F表示未来；Fφ＝TUφ表示φ未来是真；U：φ1Uφ2表示φ1为真直到φ2满足。通过以上逻辑表达方式，可以将所有任务的原子命题，写入公式，从而进行统一控制管理。其次，构建有限状态自动机Aφ，将scLTL公式φ转化为有限状态自动机FSA Aφ，其中，Q:有限状态集，原子命题的有限集合，δ：/＞状态转移函数，q0：初始状态，qF：接受状态的有限集合。通过有限状态自动机FSAAφ的状态转移。机器人执行I项任务，多任务状态为即机器人的任务状态由高维的ST定义，维度为I。每个单任务状态的数量由决定。多任务状态空间大小为I，整个状态空间随任务数量I增长，呈指数性增长，加大机器人的最优控制策略求解难度。如图2所示，S1模块主要由多任务原子命题化、有限状态自动机构建和所构建的编码器Encoder组成。多任务原子命题化:将I项任务，根据每个单任务状态τi，进行原子命题化，并写入公式φ；构建有限状态自动机:将任务公式φ转化为有限状态自动机Aφ；构建编码器:基于有限状态自动机Aφ知识与机器人多任务状态ST＝，构建编码器，由公式定义。m＝Encoder＝Encoder……,其中，为编码后的机器人多任务状态，其维度为一维，从而实现将高维度的机器人多任务状态转化为一维状态，压缩状态空间。|M|＜I，随着多任务数量增多，空间压缩效果更明显。S1模块的输入和输出如下：输入：机器人多任务状态ST＝；输出：机器人多任务编码状态m。S2：风险预测评估如图1中S2部分所示，该模块主要对环境中不确定因素的风险进行预测和评估，如交通拥堵风险等。如图3所示，S2模块主要由历史风险模型、风险预测、随机仿真模拟机制、风险模拟、风险计算模块组成。历史风险模型是基于历史风险数据构建。为时间段T的历史风险数据集，其中，/＞为编号为k的路段在时间段T内的交通风险信息。基于历史数据的风险模型/＞构建方法如下：①初始化模型数据以及模型构建参数α，β，ε；②学习模型由公式-定义如下，其中，c可由核方程计算得到；α，β，ε通过多次迭代学习可得到；③整个学习过程，通过最小化公式进行，公式如下，其中C1,C2为核函数计算所得矩阵，K为训练数据的维数，tr为计算矩阵的迹，J为数据长度，d为通过学习构建模型的维数，为形式整合后的数据集，γ为常数；④迭代学习L次，从而构建基于历史数据的风险模型y；风险预测是基于历史风险模型对风险进行预测，通过公式，可以对未来一段时间T0内的风险进行预测，公式如下，其中Yt为时刻t的风险模型值，为低维空间所构建的风险模型数据集合的转置。利用历史数据，构建模型，从而进行风险预测的最大缺点就是过度基于历史经验，但现实状况具有非常大的随机性，基于历史数据模型进行的风险预测，不能够很好的处理随机问题。本发明中，为克服该缺点，提出了新的风险计算方法。通过，随机方针模拟机制和风险模拟，很好的解决了该问题。随机方针模拟机制：该机制通过定义随机模拟模型，针对现实环境中可能发生的突发事件进行模拟，随机模拟模型由公式定义。其中，为时刻t时路段k的风险信息，ef为突发事件信息。风险模拟：基于随机方针模拟机制与当前状态，进行大量随机风险模拟，利用统计方法，计算每一条道路在未来时间段T0的风险，未来时间段T0内的随机风险/＞风险计算：通过公式计算未来时间段T0内风险，公式如下，其中，α'，β'为平衡参数，高斯噪声。注意，/＞是基于历史数据所预测的风险，侧重于过往历史数据，缺乏应对突发不确定性，/＞是基于随机模拟所计算的随机风险，侧重于现实状况的随机性与不确定性，弥补/＞的不足。综上所述，S2模块的输入和输出如下：输入：环境信息Xt；输出：风险信息Risk。以交通拥堵风险为例，其输入为当前t时刻的交通环境信息，输出为未来一段时间内各路段、路网的交通拥堵风险概率。S3：路径规划如图1中S3部分所示，该模块融合scLTL与强化学习，并考虑环境中的风险，形成新的融合算法，为移动机器人的多任务作业进行决策和路径规划。如图4所示，编码式scLTL与强化学习融合算法由有限状态自动机和MDP融合编码而来。S3模块的输入和输出如下：输入：任务信息、风险信息、有限状态自动机FSA Aφ，输出：多任务作业策略、路径。主要技术原理如下：所提出融合算法模型由定义，其中，/＞是编码后的状态集，/＞x,y为机器人环境坐标值，/＞是有限动作集，/＞为状态转移概率函数，/＞为初始状态，/＞为奖励函数，PrP为原子命题集，/＞为标签函数，/＞为有限接受状态集合,Risk为风险信息。传统MDP或scLTL与MDP的融合算法是乘积型算法，整个探索空间是增大的，并且无法处理环境风险问题。与传统算法相比，本发明所提出的算法中构建了机器人多任务状态编码器，从而将高维的机器人多任务状态转化为一维单一状态，实现空间压缩，并基于环境风险Risk，求解可应对动态环境风险的最优策略。本项目使用Q-Learning进行策略学习，利用贝尔曼方程进行Q值更新，直到收敛，贝尔曼方程如下，其中，Q是状态s和动作a的Q值，α”为学习率，γ'为折扣因子，r为报酬。算法伪代码如下：本发明所研发系统的流程图如图5所示，包括：第一步：初始化各参数；第二步：多任务设定；第三步：利用scLTL进行多任务原子命题化，构建所对应的有限状态自动机，进行多任务管理；第四步：构建编码器第五步：利用所提出路径规划算法，综合考虑环境信息、不确定风险，进行路径规划，生成多任务作业策略以及路径；第六步：机器人执行多任务作业；第七步：机器人执行过程中如果遇到作业中断，返回第二步，重新进行路径规划；第八步：作业结束。本发明旨在考虑环境中不确定因素所带来的风险，为机器人执行多任务作业提供最优控制策略，规划安全作业路径。本发明一方面利用scLTL逻辑规范，将所有任务命题化，形成一个逻辑公式，构建编码器，对多任务进行有效控制和统一管理，将多维任务状态编码成一维状态，从而压缩状态空间，降低计算成本。另一方面，本发明考虑环境中的不确定风险，提出了基于编码型的scLTL-MDP算法，利用历史数据模型并融合仿真模拟的方法，计算环境中不确定因素所带来的风险分布，在不确定性环境下生成移动机器人执行多任务的最优安全策略，提前规避风险，为机器人实现多任务作业提供最优安全路径。本发明可应用于机器人多任务作业，主要包括自动化领域、物流运输领域，如高校各校区的文件材料配送、工厂自动化配送、以及物流配送。实施例以扬州大学各校区间机器人自动配送作业控制为应用案例进行说明。图6所示为扬州大学扬子津校区、荷花池校区的简易地图。机器人在扬子津校区取件后，搭乘校车前往荷花池校区，进行配送服务。如图6，机器人在扬子津校区从s0出发，前往取件点西公寓C1、建工办C2、笃行楼C3取件。取件完成后，前往东门搭乘校车前往荷花池校区，在荷花池校区s1点下车，前往行政楼D1点进行配送，所有文件均在行政楼进行配送。上述所有任务原子命题化，由具体实施方式中公式控制。任务原子命题定义如下：原子命题ci:表示成功完成取件点Ci的取件任务,例如，c1表示成功完成西公寓的取件任务；原子命题b:表示成功搭乘校车前往下一个校区，例如，在东门成功搭乘校车前往荷花池校区；原子命题di:表示成功在Di点完成配送任务，例如，d1表示成功在D1行政楼完成配送任务，注意所举例子中，荷花池所有收取的文件材料均配送到荷花池校区行政楼，即d1,d2,d3三个命题所对应的配送点实际为1个配送点。将上述命题，写入scLTL公式φ，如下所示，其中，F ci表示机器人在未来会成功完成取件点Ci的取件任务，Fb表示机器人在未来会成功搭乘校车前往下一个校区，表示机器人在完成取件任务前不会前往下一个校区，F di表示机器人在未来会成功在Di点完成配送任务，/＞表示机器人不会执行Di点配送任务直到机器人搭乘校车前往下一个校区，/＞表示机器人如果没有收取Ci点的文件材料则不会执行Di点配送任务。所提出算法的奖励函数，具体定义如下表，表1奖励函数定义表报酬定义方式r＝-0.1基本步长成本r＝+1.0scLTL公式φr＝-con交通拥堵风险其中，con为常量，由交通拥堵风险分布决定。本项目算法源代码采用python编写，算法求解生成的作业路径如图7、图8所示。其中，图7为校内交通通畅情况下的路径规划，图8考虑了校内交通拥堵风险。图7与图6相对应，深蓝色格子为学院等建筑物或其他不可通行区域，浅蓝色为机器人的移动作业路径，绿色点为出发位置，紫色点为机器人执行配送任务的位置。在对校内无交通拥堵风险情况仿真实验后，我们设计了扬子津校区存在交通拥堵风险的仿真实验，如图8，红色格子代表交通拥堵风险路段，仿真结果表明，所提出算法可以有效的处理交通拥堵风险条件下的路径规划问题，可以避开拥堵风险较高的路段，为机器人提供安全高效的作业路径。图8为所提出算法学习过程中报酬与学习次数间的变化图，可以看出当学习200次后，报酬不再增加，达到收敛状态，即已学习到最优策略。状态空间压缩状况：如图9所示，以24×60环境为例，任务数为6，红色路段由浅及深，风险级别越高。图10所示为学习收敛以及状态空间大小，状态空间减小76.6％.。试验结果表明，本项目所提出的算法可以有效的对机器人的多任务进行控制，在风险存在的环境中，为机器人作业生成有效的安全作业路径。
