标题title
一种唇语识别方法、装置、芯片及终端
摘要abst
本发明实施例公开了一种唇语识别方法、装置、芯片及终端，该方法包括获取第一人脸图像；按照预设模糊算法对第一人脸图像进行模糊处理，得到第二人脸图像；基于模糊处理后的各第二人脸图像，计算各自的人脸模糊度；基于相邻的第二人脸图像，计算人脸模糊度变化率；筛选满足预设变化率要求的第二人脸图像，作为第三人脸图像；提取第三人脸图像的梅尔频谱图；将梅尔频谱图输入WaveNet声码器，利用WaveNet声码器合成视觉语音，实现唇语识别。本方案通过在检测人脸阶段引入人脸模糊检测与人脸模糊度，作为判断该图像帧是否可进行唇语识别建模的训练数据的参数，筛选剔除视频中的模糊帧的图像，从而改善了视觉中文语音合成的效果。
权利要求书clms
1.一种唇语识别方法，其特征在于，包括：获取第一人脸图像；按照预设模糊算法对第一人脸图像进行模糊处理，得到第二人脸图像；基于模糊处理后的各第二人脸图像，计算各自的人脸模糊度；基于相邻的第二人脸图像，计算人脸模糊度变化率；筛选满足预设变化率要求的第二人脸图像，作为第三人脸图像；提取第三人脸图像的梅尔频谱图；将梅尔频谱图输入WaveNet声码器，利用WaveNet声码器合成视觉语音，实现唇语识别。2.如权利要求1所述的唇语识别方法，其特征在于，所述按照预设模糊算法对第一人脸图像进行模糊处理包括：采用如下公式对第一人脸图像进行模糊处理：其中，表示模糊图片的像素点坐标，η表示加性噪声，表示卷积过程，/＞表示原始输入图像，/＞表示模糊函数，/＞表示模糊图像。3.如权利要求2所述的唇语识别方法，其特征在于，所述计算各自的人脸模糊度包括：采用如下公式计算各自的人脸模糊度：CI表示人脸模糊度，J表示分辨率水平上限，PQj表示在分辨率水平j下的模糊质量因数；其中，PQj由如下公式定义：其中，PNBj表示在分辨率j下时，像素点的模糊状态；表示图像模糊像素总数；PNEj表示在分辨率j下时，像素点的清晰状态；表示图像清晰像素总数。4.如权利要求3所述的唇语识别方法，其特征在于，所述计算人脸模糊度变化率包括：采用如下公式计算人脸模糊度变化率：其中，表示计算人脸模糊度变化率的初始图像序号，/＞表示计算人脸模糊度变化率的结束图片序号，ξ表示可察觉边缘闯值常数，/＞表示循环变量，CJ表示人脸模糊度变化率，CI表示人脸模糊度。5.如权利要求4所述的唇语识别方法，其特征在于，所述筛选满足预设变化率要求的第二人脸图像，作为第三人脸图像包括：将第二人脸图像的人脸模糊度变化率与预设变化率阈值范围进行比较；其中预设变化率阈值范围包括；若第二人脸图像的人脸模糊度变化率处于范围内，判定满足预设变化率要求，以将该第二人脸图像作为第三人脸图像；否则，判定不满足预设变化率要求，以将该第二人脸图像删除。6.如权利要求1-5任意一项所述的唇语识别方法，其特征在于，所述提取第三人脸图像的梅尔频谱图包括：采用基于LSTM单元和Attention单元的文本语音合成模型Tacotron2改进为视觉语音合成模型，提取第三人脸图像的梅尔频谱图；其中，所述视觉语音合成模型包括多通道注意力的视觉特征提取模块、双向LSTM单元的时序特征提取模块、位置敏感注意力的语意特征提取模块、梅尔频谱解码器以及WaveNet声码器。7.如权利要求1-5任一项所述的唇语识别方法，其特征在于，所述唇语识别方法还包括：将WaveNet声码器输出的视觉语音输入到视觉语音识别模型，以利用视觉语音识别模型将视觉语音转换为文本输出；其中，视觉语音识别模型由VGG网络改进的梅尔频谱编码器和CTC算法连接而成。8.一种唇语识别装置，其特征在于，包括：获取模块，用于获取第一人脸图像；模糊处理模块，用于按照预设模糊算法对第一人脸图像进行模糊处理，得到第二人脸图像；第一计算模块，用于基于模糊处理后的各第二人脸图像，计算各自的人脸模糊度；第二计算模块，用于基于相邻的第二人脸图像，计算人脸模糊度变化率；筛选模块，用于筛选满足预设变化率要求的第二人脸图像，作为第三人脸图像；特征提取模块，用于提取第三人脸图像的梅尔频谱图；识别模块，用于将梅尔频谱图输入WaveNet声码器，利用WaveNet声码器合成视觉语音，实现唇语识别。9.一种芯片，其特征在于，包括：第一处理器，用于从存储器中调用并运行计算机程序，使得安装有所述芯片的设备执行如权利要求1至7任一项所述的唇语识别方法的各个步骤。10.一种终端，其特征在于，包括：存储器、第二处理器以及存储在所述存储器中并可在所述第二处理器上运行的计算机程序，其特征在于，所述第二处理器执行所述计算机程序时实现如权利要求1至7中任一项所述的唇语识别方法的步骤。
说明书desc
技术领域本发明涉及计算机机器学习与人工智能技术领域，特别是涉及一种唇语识别方法、装置、芯片及终端。背景技术唇语在人类交流和言语理解中起着至关重要的作用，据研究表明，人类的唇读能力很差，而听力受损的人只能得到低于30％的准确率。因此，良好的唇语识别技术可用于改进助听器，改善无声、安全、嘈杂环境中的语言信息的获取等，具有巨大的实用性，因此成为日益受到关注的领域。但是现有的唇语识别技术中会存在相同的口型由于发音者的面部嘴唇区域特征不同，以及视频录制中可能产生的动态模糊，导致唇语识别效果不佳的问题。发明内容基于此，本发明提供一种唇语识别方法、装置、芯片及终端，主要解决针对中文唇语识别不佳的问题，并通过实验验证了其有效性和可行性。第一方面，提供一种唇语识别方法，包括：获取第一人脸图像；按照预设模糊算法对第一人脸图像进行模糊处理，得到第二人脸图像；基于模糊处理后的各第二人脸图像，计算各自的人脸模糊度；基于相邻的第二人脸图像，计算人脸模糊度变化率；筛选满足预设变化率要求的第二人脸图像，作为第三人脸图像；提取第三人脸图像的梅尔频谱图；将梅尔频谱图输入WaveNet声码器，利用WaveNet声码器合成视觉语音，实现唇语识别。可选的，所述按照预设模糊算法对第一人脸图像进行模糊处理包括：采用如下公式对第一人脸图像进行模糊处理：其中，表示模糊图片的像素点坐标，η表示加性噪声，表示卷积过程，/＞表示原始输入图像，/＞表示模糊函数，/＞表示模糊图像。可选的，所述计算各自的人脸模糊度包括：采用如下公式计算各自的人脸模糊度：CI表示人脸模糊度，J表示分辨率水平上限，PQj表示在分辨率水平j下的模糊质量因数；其中，PQj可由如下公式定义：其中，PNBj表示在分辨率j下时，像素点的模糊状态；表示图像模糊像素总数；PNEj表示在分辨率j下时，像素点的清晰状态；表示图像清晰像素总数。可选的，所述计算人脸模糊度变化率包括：采用如下公式计算人脸模糊度变化率：其中，表示计算人脸模糊度变化率的初始图像序号，/＞表示计算人脸模糊度变化率的结束图片序号，ξ表示可察觉边缘闯值常数，取值为0.63，CJ表示人脸模糊度变化率，CI表示人脸模糊度。可选的，所述筛选满足预设变化率要求的第二人脸图像，作为第三人脸图像包括：将第二人脸图像的人脸模糊度变化率与预设变化率阈值范围进行比较；其中预设变化率阈值范围包括；若第二人脸图像的人脸模糊度变化率处于范围内，判定满足预设变化率要求，以将该第二人脸图像作为第三人脸图像；否则，判定不满足预设变化率要求，以将该第二人脸图像删除。可选的，所述提取第三人脸图像的梅尔频谱图包括：采用基于LSTM单元和Attention单元的文本语音合成模型Tacotron2改进为视觉语音合成模型，提取第三人脸图像的梅尔频谱图；其中，视觉语音合成模型包括多通道注意力的视觉特征提取模块、双向LSTM单元的时序特征提取模块、位置敏感注意力的语意特征提取模块、梅尔频谱解码器以及WaveNet声码器。可选的，所述唇语识别方法还包括：将WaveNet声码器输出的视觉语音输入到视觉语音识别模型，以利用视觉语音识别模型将视觉语音转换为文本输出；其中，视觉语音识别模型由VGG网络改进的梅尔频谱编码器和CTC算法连接而成。第二方面，提供一种唇语识别装置，包括：获取模块，用于获取第一人脸图像；模糊处理模块，用于按照预设模糊算法对第一人脸图像进行模糊处理，得到第二人脸图像；第一计算模块，用于基于模糊处理后的各第二人脸图像，计算各自的人脸模糊度；第二计算模块，用于基于相邻的第二人脸图像，计算人脸模糊度变化率；筛选模块，用于筛选满足预设变化率要求的第二人脸图像，作为第三人脸图像；特征提取模块，用于提取第三人脸图像的梅尔频谱图；识别模块，用于将梅尔频谱图输入WaveNet声码器，利用WaveNet声码器合成视觉语音，实现唇语识别。第三方面，提供一种芯片，包括第一处理器，用于从存储器中调用并运行计算机程序，使得安装有所述芯片的设备执行如上任一项所述的唇语识别方法的各个步骤。第四方面，提供一种终端，包括存储器、第二处理器以及存储在所述存储器中并可在所述第二处理器上运行的计算机程序，第二处理器执行所述计算机程序时实现如上介绍的唇语识别方法的各个步骤。有益效果：上述唇语识别方法、装置、芯片及终端，该方法包括获取第一人脸图像；按照预设模糊算法对第一人脸图像进行模糊处理，得到第二人脸图像；基于模糊处理后的各第二人脸图像，计算各自的人脸模糊度；基于相邻的第二人脸图像，计算人脸模糊度变化率；筛选满足预设变化率要求的第二人脸图像，作为第三人脸图像；提取第三人脸图像的梅尔频谱图；将梅尔频谱图输入WaveNet声码器，利用WaveNet声码器合成视觉语音，实现唇语识别。本方案通过在检测人脸阶段引入人脸模糊检测与人脸模糊度，作为判断该图像帧是否可进行唇语识别建模的训练数据的参数，筛选剔除视频中的模糊帧的图像，从而改善了视觉中文语音合成的效果。附图说明为了更清楚地说明本发明实施例中的技术方案，下面将对实施例描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本发明的一些实施例，对于本领域技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。图1为本发明实施例中一种唇语识别方法的基本流程示意图；图2为本发明实施例中图像模糊示意图；图3为本发明实施例中视觉语音合成模型的基本结构框图；图4为本发明实施例中3D-SENet模块的结构示意图；图5为本发明实施例中L2W网络模型的编码器结构示意图；图6为本发明实施例中视觉语音识别模型的基本结构框图；图7为本发明实施例中唇语身份认证系统的基本结构框图；图8为本发明实施例中中文字幕生成系统的基本结构框图；图9为本发明实施例中唇语识别装置的基本结构框图；图10为本发明实施例中芯片基本结构框图。具体实施方式为了使本技术领域的人员更好地理解本发明方案，下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述。在本发明的说明书和权利要求书及上述附图中的描述的一些流程中，包含了按照特定顺序出现的多个操作，但是应该清楚了解，这些操作可以不按照其在本文中出现的顺序来执行或并行执行，操作的序号如101、102等，仅仅是用于区分开各个不同的操作，序号本身不代表任何的执行顺序。另外，这些流程可以包括更多或更少的操作，并且这些操作可以按顺序执行或并行执行。需要说明的是，本文中的“第一”、“第二”等描述，是用于区分不同的消息、设备、模块等，不代表先后顺序，也不限定“第一”和“第二”是不同的类型。下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例仅仅是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域技术人员在没有付出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。本申请实施例可以基于人工智能技术对相关的数据进行获取和处理。其中，人工智能是利用数字计算机或者数字计算机控制的机器模拟、延伸和扩展人的智能,感知环境、获取知识并使用知识获得最佳结果的理论、方法、技术及应用系统。人工智能基础技术一般包括如传感器、专用人工智能芯片、云计算、分布式存储、大数据处理技术、操作/交互系统、机电一体化等技术。人工智能软件技术主要包括计算机视觉技术、机器人技术、生物识别技术、语音处理技术、自然语言处理技术以及机器学习/深度学习等几大方向。具体地请参阅图1，图1为本实施例唇语识别方法的基本流程示意图。如图1所示，一种唇语识别方法，包括：S11、获取第一人脸图像；在本申请实施例中，可使用GRID和CMLR数据库获取相关人脸图像，作为训练数据集，以用于模型训练。首先采集数据集并进行预处理。数据集包括中文句子级唇语公开数据集CMLR和英文句子级唇语公开数据集GRID。CMLR数据集是一个大型的中文句子级别的视听双模态语料库，该数据集由浙江大学视觉智能与模式分析组收集，旨在促进视觉语音识别的研究。数据集是由2009年6月至2018年6月的国家新闻节目“新闻广播”制作而成，其包含高质量的讲话视频、音频以及汉字级别的标注。GRID数据集是一个大型的英文句子级别的视听双模态语料库，该数据集由Cooke等人于2006年发表于美国声学学会杂志。数据集当时收集了1000个以“单词+颜色+介词+字母+数字+副词“为结构的句子级讲话视频、音频以及单词级标注。然后可采用如下方式进行数据预处理：GRID和CMLR数据集都是25帧率的原始视频，为了方便后续操作，本申请实施例使用ffmpeg工具将视频进行合并、剪裁和拆帧等操作。首先将原始视频合并为大于3s的完整视频段，然后将过长的视频进行剪裁为每段大于3秒的视频段，最后通过拆帧处理将每个数据都处理为大于75张图片的图片数据集。另一方面，数据集的视频都是原始的视频，人物录制场景为实验室或者播音室。因此，本申请实施例首先使用face_recognition库所带的人脸检测器，对录制场景进行去除，剪裁人物面部区域作为感兴趣区域。然后，将定位的68个人脸关键点通过仿射变换调整人脸位姿。最后，通过归一化将人脸区域处理为预先设置的区域大小，通常设置为48×48或者96x96。S12、按照预设模糊算法对第一人脸图像进行模糊处理，得到第二人脸图像；模糊图片可以理解为由清晰图片和模糊函数卷积并加上加性噪声得来的，模糊图片往往难以区分边缘区域，可以采用如下公式对第一人脸图像进行模糊处理：其中，表示模糊图片的像素点坐标，η是加性噪声，是卷积过程/＞是原始输入图像，/＞是模糊函数，/＞是模糊图像。对于不同的使用场景，模糊函数/＞也不一样，在动态人脸识别中，由于目标对象经常处于运动中，模糊主要体现为运动模糊和对焦模糊，人脸模糊度变化率主要对这两种模糊加以讨论。S13、基于模糊处理后的各第二人脸图像，计算各自的人脸模糊度；具体的，首先进行人脸模糊度模型构建：根据全局对抗网络的损失函数，构造优化判别器和优化生成器。全局对抗网络的损失函数为：其中，xg是真实样本，Pr是真实数据分布，Pg是生成数据分布，Gen是生成器，Dix是判别器，是损失函数，Dix是判别器的输出结果，/＞为判别器的最大输出结果，/＞为生成器的最小输出结果。此公式是一个最小最大优化问题，优化此目标函数的过程存在两个步骤：先优化判别器，再优化生成器，也就能把损失函数拆解成：优化判别器：优化生成器：S14、基于相邻的第二人脸图像，计算人脸模糊度变化率；本申请可选实施例中，通过引入人脸模糊度变化率CJ，根据CJ变化率范围决定处理的方式，参见如下表1所示：表1人脸模糊度变化率变化范围处理结果0≤|CJ|＜2％进入下一步处理2％≤|CJ|＜30％需要对模糊帧进行删除处理30％≤|CJ|≤1进入下一步处理人脸模糊度变化率CJ可用如下公式表述：其中，表示计算模糊度变化率的初始图片序号，/＞表示计算模糊度变化率的结束图片序号，ξ表示可察觉边缘闯值常数，可以取值为0.63，CJ表示人脸模糊度变化率，/＞表示循环变量。CI表示人脸模糊度，CI可用如下公式定义：J表示分辨率水平上限，根据上式计算出人脸图像模糊度得到的分数范围是。PQj表示在分辨率水平下的模糊质量因数，可由如下公式定义：其中，PNBj表示在分辨率j下时，像素点的模糊状态，边缘像素公式如下：PNEj＝1-PNBj其中，PNEj表示在分辨率j下时，像素点的清晰状态，先计算模糊像素总数PNBj和边缘像素总数PNEj。最后，对采集到的连续帧进行人脸模糊度变化率判断，如果存在连续帧的2％≤|CJ|＜30％，则将该连续帧片段剔除。由于该帧多处于数据集中视频的开头，因此在处理效率上不会造成太多影响。S15、筛选满足预设变化率要求的第二人脸图像，作为第三人脸图像；针对处理后的数据集，通过引入人脸模糊度检测，剔除数据集中的模糊帧片段。S16、提取第三人脸图像的梅尔频谱图；在步骤S16中，提取第三人脸图像的梅尔频谱图主要可包括如下步骤：目前大部分基于深度学习的唇语识别算法，比如WLAS、CSSMCM、CHLipNet和CHSLP-VP等都是以LSTM单元和Attention单元为基本单元搭建的算法模型，其中LSTM单元可以提取长时序的上下文本信息特征，从而更好地构建唇动信息和语音内容之间的对应关系；Attention单元能够使解码器更加关注应该关注的唇动部分特征减弱视觉混淆对模型的影响。S17、将梅尔频谱图输入WaveNet声码器，利用WaveNet声码器合成视觉语音，实现唇语识别。WaveNet声码器为一个能够生成原始音频波形的深度神经网络，是一个完全的概率自回归模型，可以基于之前已经生成的所有样本，来预测当前音频样本的概率分布。本申请实施例将基于LSTM单元和Attention单元的文本语音合成模型Tacotron2改进为视觉语音合成模型，实现人类“视觉”模态到“听觉”模态的转换，其中视觉语音合成模型的结构图，如图3所示：视觉语音合成模型主要包括五大模块，分别是多通道注意力的视觉特征提取模块、双向LSTM单元的时序特征提取模块、位置敏感注意力的语意特征提取模块、梅尔频谱解码器以及WaveNet声码器。首先将筛选后的人脸图像分为n个片段送入ResNet网络结构和注意力机制相结合的三维改进精分类模块进行短时序的视觉特征提取；其次，把短时序的视觉特征送入双向LSTM单元的时序特征提取模块进行长时序的上下文本特征编码，同时与位置注意力机制相结合实现内容信息和位置信息相结合的混合注意力机制；然后，将文本特征和混合注意力权重送入梅尔频谱解码器进行解码，通过逐帧解码序列的合成得到梅尔频谱图；最后把梅尔频谱图送入WaveNet声码器合成视觉语音。继续参考图3，唇语识别过程中只考虑唇部信息会过多缺失人脸部的背景信息，而整张人脸的加入无疑会增加计算量和网络负担，因此本申请实施例在视觉语音合成模型中基于SE模块设计了一种短时视觉特征提取模块3D-SENet。通过SE模块的通道注意力机制使得短时序视频帧具有多通道的注意力，使得模型能够在相邻帧之间聚焦于更应该关注的部分区域，帮助卷积神经网络注意到不同特征信息的重要程度，从而可以从人脸视频中筛选出有用的信息。视觉特征提取模块3D-SENet的具体结构图如图4所示，主要包含3个重要步骤，分别为三维的特征压缩、特征激励和特征重映射。该模块中将人脸视频分为m组，每组有连续的n帧人脸图像。首先通过特征压缩操作将通过3D卷积将得到的n×W×H×C特征压缩到空间n×1×1×C当中，然后将压缩后的特征图进行两层全连接层的特征激励操作，最后经过Sigmoid激活函数之后，再与原始的特征矩阵进行逐像素相乘，过程可以由以下公式表示：E1＝Conv1D)E2＝SigmoidE3＝E2·F上式中：W1、W2表达两个全连接层，F＝{F1，F2，...，Fn}是原始的相邻帧的特征矩阵，将F经过压缩操作得到相邻帧共同的全局特征图E1，再将通过E1激励操作抓取相邻帧的关系得到激活值E2，最后将E2乘以F得到具有相邻帧权重系数的特征图E3。如图4所示，视觉特征提取模块3D-SENet的三个重要步骤：特征压缩、特征激励和特征重映射的介绍如下：特征压缩：假设原始特征维度为W×H×C，其中W为宽度、H为高度、C为通道数。特征压缩的主要任务是将每个二维的通道矩阵压缩为一个一维的特征向量，使得一维向量获得之前二维矩阵全局的视野，增强感受野的感知区域。一般特征压缩操作是使用全局平均池化实现的，从而达到将一个通道上整个空间特征编码为一个全局特征的效果，具体运算公式如下所示：其中，Sc是压缩操作后得到的特征向量，Fsq代表平均池化函数、fc代表特征图F在坐标处的取值。特征激励：该操作与RNN中的门限功能相似，通过全连接层的参数为每个特征通道生成权重，从而使模型学习特征通道间的关系。将特征压缩生成的特征向量S送入2层全连接层和一个ReLU层，然后通过Sigmoid激活函数将其转化为一个归一化权值，具体运算过程如下公式所示：其中，为了降低模型复杂度以及提升模型泛化能力，使用双层全连接层结构，第一层W1起到降维的作用，通过ReLU层再由第二层W2恢复到原始维度。特征重映射：该操作类将学习到的各通道归一化权值重新映射到特征图上，通过特征激励生成的通道权重E与特征图逐通道的相乘完成映射过程，运算过程如下公式所示：式中，xc为最后输出的特征图，其大小仍然为W×H×C，未产生维度上的变化。基于3D-SENet的视觉语音合成模型主要由视频编码器和梅尔频谱图解码器两部分组成，其中多通道注意力的视觉特征提取模块和双向LSTM单元的时序特征提取模块相结合构建了觉语音合成模型L2W的视频编码器部分，该部分的具体流程图如图5所示，左侧框部分为新增加的基于多通道注意力的人脸空间视觉特征提取模块，即增加了基于多通道注意力的视觉特征提取模块。图5中右侧部分为双向LSTM单元的时序特征提取模块。假设一组有m个片段，每个片段有n帧人脸图像的人脸视频序列视频编码器部分首先将人脸图像序列I输入到3D-Conv BN-Relu中提取视觉特征，然后将特征图送入到多层3D-SENet模块进行多通道注意力权重的学习，最后将通过若干层3D-SENet模块输出的特征向量F＝{F1，F2，...，Fm}送入双向的LSTM单元中提取上下文本特征，整个编码器计算过程如下公式所示：其中，表示图像I经过3D-Conv BN-Relu和多层3D-SENet提取到的短时序算计视觉特征，/＞表示Bi-LSTM编码器生成的状态向量。实验表明，3D-SENet模块的加入能够使模型更好的关注应该关注的通道及区域，有助提取视觉特征，从而提升图像到声音波纹的对应效果。为了适用声音波纹的复杂变化，本申请实施例使用的位置敏感注意力机制通过右上角标标注所属的视频或语音片段来计算不同片段不同时刻的注意权值向量，从而使解码器更好的注意到需要注意的特征，具体过程由如下公式表示：其中，是视觉语音合成过程中解码器的状态向量，/＞为PreNet生成的梅尔频谱图，/＞为双向LSTM单元生成的编码器器状态向量，fi，j是之前的注意力权重经过累加后与F进行卷积而得到的位置特征。/＞W、V、U和b，均为待训练参数。位置敏感注意力为不同时刻编码的特征提供权重，从而使解码器预测音频时生成的梅尔频谱图更接近真实的梅尔频谱图。位置敏感注意力为不同时刻编码的特征提供权重，从而使解码器预测音频时生成的梅尔频谱图更接近真实的梅尔频谱图。本申请实施例中，将筛选后的数据进行打乱并按8：1：1的比例分为训练、测试和验证集。为保持配置相同，训练环境均在显存为16G的单卡V100上进行训练，优化器默认选用Adam优化器，初始学习率为10-3。由于GRID数据集视频较短，为防止过拟合通常将隐藏层减半，批处理数量设置为32时，至少进行3万次的迭代训练模型合成相似的梅尔频谱图，GRID数据集训练5万次。本申请实施例选用了三种客观的评估方法，分别是基于语音失真和噪声影响的语音质量感知评价算法、基于人类听觉感知系统的短期目标可理解性以及基于语谱图相似性的梅尔倒谱距离。另一方面，本申请实施例为验证其合成视觉语音的机器可识别性，将合成的视觉语音通过科大讯飞和百度的语音识别引擎上进行识别验证，其对比结果用单词正确率进行评价。本申请实施例所采用的视觉语音合成模型，在感知语音质量评价方面效果一定的情况下，其他维度的评价指标均处于最优。基于上述视觉语音合成模型网络结构设计，实现了从视觉模态到听觉模态的转换，在本申请可选实施例中，还可将“听觉”模态转换到文本模态，实现文本输出，以用于字幕输出等应用场景。在本申请可选实施例中，将WaveNet声码器输出的视觉语音输入到视觉语音识别模型W2T，以利用视觉语音识别模型将视觉语音转换为文本输出；其中，视觉语音识别模型由VGG网络改进的梅尔频谱编码器和CTC算法连接而成。VGG网络是现有神经网络，其基本思想是重复使用简单基础的网络模块来构建深度学习模型。在本申请实施例中，神经网络的输入端是255×255的RGB图像，网络前端有五个单元，每个单元由二至三个卷积层连接一个池化层构成，网络的末端则由三个全连接层和交叉熵损失构成。模型的前四个池化总共经过了十三个卷积层，再加上最后的三个全连接层总共有十六层所以又称为VGG16神经网络，VGG16神经网络架构属于规模非常大的深度卷积神经网络。在本申请实施例中将梅尔频谱图作为视觉语音识别模型L2W的直接输入，有助于将视觉语音合成模型L2W和视觉语音识别模型W2T的独立训练。W2T在模型结构上编码器部分借鉴了图像识别效果最好的网络模型VGG结构，解码器部分采用了语音识别中常用的CTC损失，整个模型通过深度卷积的特征提取到CTC损失的对应到标签空间，实现了梅尔频谱图直接转录为中文汉字的端到端结构。W2T模型的结构图如图6所示，其中虚线框内是基于VGG网络改进的梅尔频谱编码器，将梅尔频谱图作为视觉语音识别模型的特征输入，通过梅尔频谱编码器提取图像多尺度信息得到合成的梅尔频谱图序列的深度特征序列然后将特征序列MF送入到CTC算法模型中寻找与字典Y最准确的映射关系。CTC全称Connectionist temporal classification，是一种常用在语音识别、文本识别等领域的算法，用来解决输入和输出序列长度不一、无法对齐的问题。CTC可以在不知道输入和输出之间关系的情况下寻找最佳对齐方式的算法。CTC通过最大化Y关于的后验概率来求取MF与字典Y准确的映射关系，其中后验概率公式如下公式所示。式中P表示后验概率，Y是输出字典，例如标签“你好”对应的输出字典Y就是{‘niε’，‘haoε’}，t代表时序，实际还需要输入输出的序列长度，用以在计算损失时消除填充的影响。基于上述实施例提供的唇语识别方法，可以将其应用于唇语身份认证系统，参考图7所示：唇语身份认证系统主要包含视频输入、视觉语音合成、梅尔倒谱距离计算和身份认证预测四大部分。其中视觉语音合成占唇语身份认证系统的主导地位，为唇语身份认证系统提供重要依据。梅尔倒谱距离计算具体采用上述方式，在此不再赘述。身份认证预测通过比对预测的梅尔频谱图与原始的梅尔频谱图质检的梅尔倒谱距离，判断两者之间的距离差是否小于设定阈值，如是，可认定两者匹配，也即属于相同的身份；相反，若两者之间的距离差是否不小于设定阈值，则认定两者不匹配，也即属于不同的身份。从而实现唇语身份认证功能。在本申请的可选实施例中，还可基于上述实施例提供的唇语识别方法，将其应用于中文字幕生成系统，参考图8所示：基于中文唇语识别的中文字幕生成系统，主要包含视频输入、视觉语音合成、视觉语音识别和字幕生成四大部分。其中视觉语音合成和视觉语音识别占字幕生成系统的主导地位。为解决上述技术问题，本发明实施例还提供一种唇语识别装置。具体请参阅图9，图9为本实施基于深度学习的唇语识别装置的基本结构框图，包括：获取模块81，用于获取第一人脸图像；模糊处理模块82，用于按照预设模糊算法对第一人脸图像进行模糊处理，得到第二人脸图像；第一计算模块83，用于基于模糊处理后的各第二人脸图像，计算各自的人脸模糊度；第二计算模块84，用于基于相邻的第二人脸图像，计算人脸模糊度变化率；筛选模块85，用于筛选满足预设变化率要求的第二人脸图像，作为第三人脸图像；特征提取模块86，用于提取第三人脸图像的梅尔频谱图；识别模块87，用于将梅尔频谱图输入WaveNet声码器，利用WaveNet声码器合成视觉语音，实现唇语识别。各模块的具体功能可参见上述唇语识别方法的相关描述，基于上述唇语识别方法的记载，本领域技术人员可灵活确定相关模块的具体实现功能以及实现原理，在此不再赘述。为解决上述技术问题，本发明实施例还提供一种芯片，该芯片可以为通用处理器，也可以为专用处理器。该芯片包括第一处理器，第一处理器用于支持终端执行上述相关步骤，例如从存储器中调用并运行计算机程序，使得安装有所述芯片的设备执行，以实现上述各个实施例中的唇语识别方法。可选的在一些示例下，该芯片还包括收发器，收发器用于接受处理器的控制，用于支持终端执行上述相关步骤，以实现上述各个实施例中的唇语识别方法。可选的，该芯片还可以包括存储介质。需要说明的是，该芯片可以使用下述电路或者器件来实现：一个或多个现场可编程门阵列、可编程逻辑器件、控制器、状态机、门逻辑、分立硬件部件、任何其他适合的电路、或者能够执行本申请通篇所描述的各种功能的电路的任意组合。本发明还提供一种终端，包括存储器、第二处理器以及存储在所述存储器中并可在所述第二处理器上运行的计算机程序，第二处理器执行所述计算机程序时实现如上实施例中所述的唇语识别方法的步骤。具体请参阅图10，图10为示出的一种终端的基本结构框图，该终端包括通过系统总线连接的处理器、非易失性存储介质、存储器和网络接口。其中，该终端的非易失性存储介质存储有操作系统、数据库和计算机可读指令，数据库中可存储有控件信息序列，该计算机可读指令被处理器执行时，可使得处理器实现一种唇语识别方法。该终端的处理器用于提供计算和控制能力，支撑整个终端的运行。该终端的存储器中可存储有计算机可读指令，该计算机可读指令被处理器执行时，可使得处理器执行一种唇语识别方法。该终端的网络接口用于与终端连接通信。本领域技术人员可以理解，图中示出的结构，仅仅是与本申请方案相关的部分结构的框图，并不构成对本申请方案所应用于其上的终端的限定，具体的终端可以包括比图中所示更多或更少的部件，或者组合某些部件，或者具有不同的部件布置。本技术领域技术人员可以理解，这里所使用的“终端”、“终端设备”既包括无线信号接收器的设备，其仅具备无发射能力的无线信号接收器的设备，又包括接收和发射硬件的设备，其具有能够在双向通信链路上，执行双向通信的接收和发射硬件的电子设备。这种电子设备可以包括：蜂窝或其他通信设备，其具有单线路显示器或多线路显示器或没有多线路显示器的蜂窝或其他通信设备；PCS，其可以组合语音、数据处理、传真和/或数据通信能力；PDA，其可以包括射频接收器、寻呼机、互联网/内联网访问、网络浏览器、记事本、日历和/或GPS接收器；常规膝上型和/或掌上型计算机或其他设备，其具有和/或包括射频接收器的常规膝上型和/或掌上型计算机或其他设备。这里所使用的“终端”、“终端设备”可以是便携式、可运输、安装在交通工具中的，或者适合于和/或配置为在本地运行，和/或以分布形式，运行在地球和/或空间的任何其他位置运行。这里所使用的“终端”、“终端设备”还可以是通信终端、上网终端、音乐/视频播放终端，例如可以是PDA、MID和/或具有音乐/视频播放功能的移动电话，也可以是智能电视、机顶盒等设备。本发明还提供一种存储有计算机可读指令的存储介质，所述计算机可读指令被一个或多个处理器执行时，使得一个或多个处理器执行上述任一实施例所述唇语识别方法的步骤。本实施例还提供了一种计算机程序，该计算机程序可以分布在计算机可读介质上，由可计算装置来执行，以实现上述介绍的唇语识别方法的至少一个步骤；并且在某些情况下，可以采用不同于上述实施例所描述的顺序执行所示出或描述的至少一个步骤。本实施例还提供了一种计算机程序产品，包括计算机可读装置，该计算机可读装置上存储有如上所示的计算机程序。本实施例中该计算机可读装置可包括如上所示的计算机可读存储介质。本领域普通技术人员可以理解实现上述实施例方法中的全部或部分流程，是可以通过计算机程序来指令相关的硬件来完成，该计算机程序可存储于一计算机可读取存储介质中，该程序在执行时，可包括如上述各方法的实施例的流程。其中，前述的存储介质可为磁碟、光盘、只读存储记忆体等非易失性存储介质，或随机存储记忆体等。应该理解的是，虽然附图的流程图中的各个步骤按照箭头的指示依次显示，但是这些步骤并不是必然按照箭头指示的顺序依次执行。除非本文中有明确的说明，这些步骤的执行并没有严格的顺序限制，其可以以其他的顺序执行。而且，附图的流程图中的至少一部分步骤可以包括多个子步骤或者多个阶段，这些子步骤或者阶段并不必然是在同一时刻执行完成，而是可以在不同的时刻执行，其执行顺序也不必然是依次进行，而是可以与其他步骤或者其他步骤的子步骤或者阶段的至少一部分轮流或者交替地执行。以上所述实施例的各技术特征可以进行任意的组合，为使描述简洁，未对上述实施例中的各个技术特征所有可能的组合都进行描述，然而，只要这些技术特征的组合不存在矛盾，都应当认为是本说明书记载的范围。以上所述实施例仅表达了本发明的几种实施方式，其描述较为具体和详细，但并不能因此而理解为对本发明专利范围的限制。应当指出的是，对于本领域的普通技术人员来说，在不脱离本发明构思的前提下，还可以做出若干变形和改进，这些都属于本发明的保护范围。因此，本发明专利的保护范围应以所附权利要求为准。
