标题title
一种含智能软开关的配电网三相不平衡治理方法及系统
摘要abst
本发明公开一种含智能软开关的配电网三相不平衡治理方法及系统，涉及电力数据分析技术领域，方法包括：基于目标配电网模型获取当前配电状态数据集；基于DDPG智能体，根据当前配电状态数据集，确定当前配电动作；基于当前配电动作对目标配电网模型进行治理，以得到对应的奖励值及下一配电状态数据集；当前配电状态数据集、当前配电动作及对应的奖励值、下一配电状态数据集构成经验四元组；从配电治理经验池中选取多个经验四元组作为训练样本对Actor‑Critic网络进行训练，以得到最优的Actor‑Critic网络，用于根据目标配电网的配电状态数据集，确定对应的最优配电动作。本发明实现三相不平衡实时在线治理。
权利要求书clms
1.一种含智能软开关的配电网三相不平衡治理方法，其特征在于，方法包括：构建目标配电网模型；所述目标配电网模型内包括目标配电网、光伏发电站及多个智能软开关；所述目标配电网中包括PV节点和PQ节点；基于所述目标配电网模型，获取当前配电状态数据集；所述当前配电状态数据集包括光伏发电站的有功功率出力、PQ节点有功负荷、PQ节点无功负荷、PV节点有功负荷、PV节点电压幅值及PV节点电压相角；基于DDPG智能体，根据所述当前配电状态数据集，确定当前配电动作；所述当前配电动作包括任一智能软开关的有功出力和无功出力；所述DDPG智能体包括Actor-Critic网络；基于所述当前配电动作，对所述目标配电网模型进行治理，以得到对应的奖励值及下一配电状态数据集；所述当前配电动作对应的奖励值表征经由当前配电动作治理后目标配电网的三相不平衡度的大小程度；所述当前配电状态数据集、所述当前配电动作及对应的奖励值、所述下一配电状态数据集构成经验四元组；多个经验四元组构成配电治理经验池；从所述配电治理经验池中选取多个经验四元组作为训练样本，对所述Actor-Critic网络进行训练，以得到最优的Actor-Critic网络；所述最优的Actor-Critic网络用于根据目标配电网的配电状态数据集，确定对应的最优配电动作。2.根据权利要求1所述的含智能软开关的配电网三相不平衡治理方法，其特征在于，基于所述当前配电动作，对所述目标配电网模型进行治理，以得到对应的奖励值及下一配电状态数据集，具体包括：基于所述目标配电网模型，分别构建智能软开关出力约束、光伏出力约束、线路电流约束、节点电压约束及支路潮流约束；基于所述当前配电动作及所述智能软开关出力约束，确定实际配电动作；基于所述实际配电动作、所述光伏出力约束、所述线路电流约束、所述节点电压约束及所述支路潮流约束，以三相不平衡度最小为目标，建立三相不平衡优化模型；对所述三相不平衡优化模型求解，以得到目标配电网的三相不平衡优化结果及对应的三相不平衡度最小值；所述目标配电网的三相不平衡优化结果包括光伏发电站的有功功率出力结果、PQ节点有功负荷结果、PQ节点无功负荷结果、PV节点有功负荷结果、PV节点电压幅值结果及PV节点电压相角结果；基于所述三相不平衡度最小值，计算所述当前配电动作对应的奖励值；基于所述目标配电网的三相不平衡优化结果，确定下一配电状态数据集。3.根据权利要求2所述的含智能软开关的配电网三相不平衡治理方法，其特征在于，所述智能软开关出力约束为：；；；其中，和/＞分别表示节点i所连智能软开关在时刻t流过的第φ相有功出力和发出的无功出力，/＞和/＞表示节点j所连智能软开关在时刻t流过的第φ相有功出力和发出的无功出力，/＞表示时刻t节点i智能软开关第φ相的复功率，/＞表示时刻t节点j智能软开关第φ相的复功率；所述光伏出力约束为：；其中，表示光伏有功出力下限，/＞表示光伏电站在目标配电网模型的节点i、时刻t的第φ相有功出力，/＞表示光伏有功出力上限；所述线路电流约束为：；其中，分别表示支路流过的第φ相电流幅值的最小值和最大值，/＞表示目标配电网中在时刻t、节点i与节点j构成的支路流过的第φ相电流幅值；所述节点电压约束为：；其中，，/＞分别表示节点第φ相电压幅值的最小值和最大值，/＞表示目标配电网中在时刻t、节点i第φ相电压幅值；所述支路潮流约束为：；；；；；；其中，代表时刻t节点j注入的第φ相有功功率，/＞表示时刻t流经节点j和节点k之间的支路的第φ相有功功率，/＞表示与j节点相连的所有节点的集合k，/＞表示与j节点相连的所有节点的集合i，/＞代表时刻t流经节点i和节点j之间的支路的第φ相有功功率，/＞和/＞分别表示节点i和节点j之间的支路的第φ相电阻和电抗，表示时刻t节点i和节点j之间的支路流过的第φ相电流；/＞代表时刻t节点j注入的第φ相无功功率，/＞表示时刻t流经节点j和节点k之间的支路的第φ相无功功率，代表时刻t流经节点i和节点j之间的支路的第φ相无功功率；/＞表示目标配电网中在时刻t，节点j第φ相电压幅值；/＞表示目标配电网中在时刻t，节点i第φ相电压幅值，/＞表示与节点i相邻的所有节点的集合；/＞代表时刻t节点i注入的第φ相有功功率，/＞代表时刻t节点i注入的第φ相无功功率，/＞和/＞分别是时刻t节点i连接的发电机所发出的第φ相有功功率和无功功率，/＞和/＞分别是时刻t节点i的第φ相有功负荷和无功负荷。4.根据权利要求2所述的含智能软开关的配电网三相不平衡治理方法，其特征在于，所述三相不平衡优化模型中的目标函数为：；其中，f表示三相不平衡度，指时刻t节点i的第φ相电压，φ相电压包括A相电压、B相电压及C相电压，/＞指时刻t节点i的第A相电压，/＞指时刻t节点i的第B相电压，指时刻t节点i的第C相电压。5.根据权利要求4所述的含智能软开关的配电网三相不平衡治理方法，其特征在于，所述当前配电动作对应的奖励值的计算公式为：；其中，表示当前配电动作对应的奖励值，/＞表示预设值，/＞指节点i的第φ相电压，指节点i的第A相电压，/＞指节点i的第B相电压，/＞指节点i的第C相电压。6.根据权利要求1所述的含智能软开关的配电网三相不平衡治理方法，其特征在于，从所述配电治理经验池中选取多个经验四元组作为训练样本，对所述Actor-Critic网络进行训练，以得到最优的Actor-Critic网络，具体包括：随机初始化Actor网络参数和Critic网络参数；初始化目标Actor网络参数和目标Critic网络参数；从所述配电治理经验池中进行随机采样，以将得到的经验四元组作为训练样本；采用所述训练样本，以损失函数最小为目标，对初始化后的Critic网络参数进行更新；采用所述训练样本，以策略梯度值最小为目标，对初始化后的Actor网络参数进行更新；基于更新后的Critic网络参数对所述目标Critic网络参数进行更新，基于更新后的Actor网络参数对所述目标Actor网络参数进行更新；基于更新后的目标Critic网络参数、更新后的目标Actor网络参数、更新后的Critic网络参数及更新后的Actor网络参数，确定最优的Actor-Critic网络。7.根据权利要求6所述的含智能软开关的配电网三相不平衡治理方法，其特征在于，采用所述训练样本，以损失函数最小为目标，对初始化后的Critic网络参数进行更新时，采用如下函数公式：；；；其中，表示目标动作回报值，/＞为当前配电动作对应的奖励值，/＞表示折扣因子，表示使用目标Critic网络和目标Actor网络估计的未来动作价值，L表示损失函数的值，/＞表示训练样本，/＞表示使用初始化后的Critic网络参数估计的动作价值，/＞表示学习率，/＞表示训练样本中的经验四元组，表示/＞对参数/＞的梯度，/＞为当前配电状态数据集，/＞为当前配电动作，/＞表示初始化后的Critic网络参数，/＞为下一配电状态数据集，/＞表示根据和/＞确定的行为策略，/＞表示初始化后的目标Critic网络参数，/＞表示初始化后的目标Actor网络参数。8.一种含智能软开关的配电网三相不平衡治理系统，其特征在于，系统包括：配电网模型构建模块，用于构建目标配电网模型；所述目标配电网模型内包括目标配电网、光伏发电站及多个智能软开关；所述目标配电网中包括PV节点和PQ节点；配电状态数据集获取模块，用于基于所述目标配电网模型，获取当前配电状态数据集；所述当前配电状态数据集包括光伏发电站的有功功率出力、PQ节点有功负荷、PQ节点无功负荷、PV节点有功负荷、PV节点电压幅值及PV节点电压相角；配电动作确定模块，用于基于DDPG智能体，根据所述当前配电状态数据集，确定当前配电动作；所述当前配电动作包括任一智能软开关的有功出力和无功出力；所述DDPG智能体包括Actor-Critic网络；配电治理经验池构建模块，用于基于所述当前配电动作，对所述目标配电网模型进行治理，以得到对应的奖励值及下一配电状态数据集；所述当前配电动作对应的奖励值表征经由当前配电动作治理后目标配电网的三相不平衡度的大小程度；所述当前配电状态数据集、所述当前配电动作及对应的奖励值、所述下一配电状态数据集构成经验四元组；多个经验四元组构成配电治理经验池；最优网络确定模块，用于从所述配电治理经验池中选取多个经验四元组作为训练样本，对所述Actor-Critic网络进行训练，以得到最优的Actor-Critic网络；所述最优的Actor-Critic网络用于根据目标配电网的配电状态数据集，确定对应的最优配电动作。
说明书desc
技术领域本发明涉及电力数据分析技术领域，特别是涉及一种含智能软开关的配电网三相不平衡治理方法及系统。背景技术由于配电网结构不合理，配电变压器配置不足，单相负荷大量存在，导致配电台区三相不平衡普遍存在。配电网三相不平衡危害配电网供电可靠性，降低变压器效率，增加线路损耗，减少电动机输出功率；严重时造成设备烧毁，甚至引发火灾，威胁供电安全。国际上通常用三相电压不平衡度衡量电压三相不平衡的程度。三相不平衡主要有三个常见治理手段：配置换相开关，改变负荷接入的相序；增设无功补偿装置，使无功功率在相间转移；配电网重构，通过改变分段开关和联络开关的状态重构网络拓扑。但是，随着高比例强间歇性可再生能源的大规模并网，风光出力的不确定性和波动性增加，进一步增强了三相不平衡度，对配电网优化的实时性提出了更高要求。传统的三相不平衡治理手段已难以满足现阶段及未来配电网优化的要求，主要表现在：1）传统三相不平衡治理手段大都受到0-1开关特性的限制，开关开断次数有限，不能适应连续快速变化的潮流。2）配电网络重构通过改变网络拓扑结构优化供电质量，无需投入新设备。但是仅仅通过改变拓扑结构治理三相不平衡效果有限，并且开关动作对设备造成的损耗具有不可逆性。智能软开关是由两个背靠背电压源逆变器组成的电力电子器件，通常用于中压配电网中替代联络开关。不同于传统开关只有断开和闭合两种状态，面对分布式电源出力的随机性，SOP能够实现对无功功率的连续快速调节，实时在线改善线路的电压分布，抑制三相不平衡，增加新能源的消纳。同时降低了开关的动作成本，使配电网运行更加柔性经济。面对光伏出力的随机性和不确定性，传统建模方法有离线优化方法如鲁棒优化和随机优化，以及在线优化方法如李雅普诺夫优化、模型预测控制、分布式优化、动态规划优化等。但是这些方法存在的普遍问题都是求解效率低、很难满足配电网优化实时性的要求，在决策的最优性方面存在不足。考虑三相不平衡的配电网优化模型是非线性非凸模型，传统求解方法通常使用锥松弛和转化方法将其凸化线性化进行求解。但是转化模型的同时也导致求解精度下降。随着配电网历史运行数据的持续积累和配电网运营商设备算力的逐渐提升，基于数据驱动的人工智能方法在配电网运行优化领域迅速发展，有助于突破传统优化方法的局限性。将机器学习应用到配电网优化中，不依赖配电网各单元的内在联系，而是通过提取和训练配电网历史运行和决策数据，构造出模拟逼近配电网运行状态与优化决策之间关系的数学模型，在实际配电网优化任务中直接根据配电网的运行状态映射出总体优化方案。然而，传统模型驱动的优化策略在决策的最优性、应对新能源的不确定性等方面仍然存在不足，具体体现在：1）考虑到配电网三相线路的耦合关系，传统模型建模复杂，求解维度难度大。2）依赖系统单元的内在联系，需要依据网络拓扑结构和电网运行方式建模，对网络拓扑的变化敏感，对新型电力设备接入的适应性不强。3）配电网运营商在运行过程中积累的数据蕴含着丰富的相关关系，对未来的决策控制具有指导意义，传统模型驱动方法缺乏对历史决策数据信息的挖掘和利用。4）配电网优化问题本质上是一种非线性非凸的复杂系统优化问题，在精度和效率之间存在矛盾，通过简化模型来提高决策效率往往会导致精度下降。发明内容本发明的目的是提供一种含智能软开关的配电网三相不平衡治理方法及系统，实现配电网三相不平衡的实时在线治理，提高新能源消纳。为实现上述目的，本发明提供了如下方案：第一方面，本发明提供一种含智能软开关的配电网三相不平衡治理方法，包括：构建目标配电网模型；所述目标配电网模型内包括目标配电网、光伏发电站及多个智能软开关；所述目标配电网中包括PV节点和PQ节点。基于所述目标配电网模型，获取当前配电状态数据集；所述当前配电状态数据集包括光伏发电站的有功功率出力、PQ节点有功负荷、PQ节点无功负荷、PV节点有功负荷、PV节点电压幅值及PV节点电压相角。基于DDPG智能体，根据所述当前配电状态数据集，确定当前配电动作；所述当前配电动作包括任一智能软开关的有功出力和无功出力；所述DDPG智能体包括Actor-Critic网络。基于所述当前配电动作，对所述目标配电网模型进行治理，以得到对应的奖励值及下一配电状态数据集；所述当前配电动作对应的奖励值表征经由当前配电动作治理后目标配电网的三相不平衡度的大小程度；所述当前配电状态数据集、所述当前配电动作及对应的奖励值、所述下一配电状态数据集构成经验四元组；多个经验四元组构成配电治理经验池。从所述配电治理经验池中选取多个经验四元组作为训练样本，对所述Actor-Critic网络进行训练，以得到最优的Actor-Critic网络；所述最优的Actor-Critic网络用于根据目标配电网的配电状态数据集，确定对应的最优配电动作。第二方面，本发明提供一种含智能软开关的配电网三相不平衡治理系统，包括：配电网模型构建模块，用于构建目标配电网模型；所述目标配电网模型内包括目标配电网、光伏发电站及多个智能软开关；所述目标配电网中包括PV节点和PQ节点。配电状态数据集获取模块，用于基于所述目标配电网模型，获取当前配电状态数据集；所述当前配电状态数据集包括光伏发电站的有功功率出力、PQ节点有功负荷、PQ节点无功负荷、PV节点有功负荷、PV节点电压幅值及PV节点电压相角。配电动作确定模块，用于基于DDPG智能体，根据所述当前配电状态数据集，确定当前配电动作；所述当前配电动作包括任一智能软开关的有功出力和无功出力；所述DDPG智能体包括Actor-Critic网络。配电治理经验池构建模块，用于基于所述当前配电动作，对所述目标配电网模型进行治理，以得到对应的奖励值及下一配电状态数据集；所述当前配电动作对应的奖励值表征经由当前配电动作治理后目标配电网的三相不平衡度的大小程度；所述当前配电状态数据集、所述当前配电动作及对应的奖励值、所述下一配电状态数据集构成经验四元组；多个经验四元组构成配电治理经验池。最优网络确定模块，用于从所述配电治理经验池中选取多个经验四元组作为训练样本，对所述Actor-Critic网络进行训练，以得到最优的Actor-Critic网络；所述最优的Actor-Critic网络用于根据目标配电网的配电状态数据集，确定对应的最优配电动作。根据本发明提供的具体实施例，本发明公开了以下技术效果：本发明公开一种含智能软开关的配电网三相不平衡治理方法及系统，基于目标配电网模型，获取当前配电状态数据集，包括光伏发电站的有功功率出力、PQ节点有功负荷、PQ节点无功负荷、PV节点有功负荷、PV节点电压幅值及PV节点电压相角；基于DDPG智能体，根据当前配电状态数据集，确定当前配电动作，包括任一智能软开关的有功出力和无功出力；基于当前配电动作，对目标配电网模型进行治理，以得到对应的奖励值及下一配电状态数据集；由当前配电状态数据集、当前配电动作及对应的奖励值、下一配电状态数据集构成经验四元组；从配电治理经验池中选取多个经验四元组作为训练样本，对DDPG智能体内Actor-Critic网络进行训练，以得到最优的Actor-Critic网络，用于根据目标配电网的配电状态数据集，确定对应的最优配电动作。综上，本发明考虑到配电网内部分布式电源运行状态、SOP出力等均为连续值，同时系统运行数据存在很强的相关性，因此建立独立的目标网络与经验回放集合作为缓冲。其次，深度神经网络有逐层处理、有特征的内部变化具有足够的模型复杂度。由于深度确定性策略梯度算法综合了Actor-Critic网络架构，具有处理高维数据的优势，因此能够有效解决配电网优化决策的连续性问题。同时，仅需要通过系统观测进行相应的网络参数调整，适用于配电网实时优化场景的实际应用，能够实现配电网三相不平衡的实时在线治理，提高新能源消纳。附图说明为了更清楚地说明本发明实施例或现有技术中的技术方案，下面将对实施例中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本发明的一些实施例，对于本领域普通技术人员来讲，在不付出创造性劳动性的前提下，还可以根据这些附图获得其他的附图。图1为本发明含智能软开关的配电网三相不平衡治理方法的流程示意图。图2为本发明含智能软开关的配电网的结构示意图。图3为本发明Actor-Critic网络训练过程示意图。图4为本发明含智能软开关的配电网三相不平衡治理系统的示意图。具体实施方式下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例仅仅是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有做出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。本发明提供了一种含智能软开关的配电网三相不平衡治理方法及系统，采用了基于机器学习的人工智能方法优化配电网的三相不平衡度。在机器学习方法中，强化学习缺乏较强的表征性能，无法对感知问题进行很好的求解，使其应用范围局限于有限的观察空间与离散的动作空间；而深度神经网络具备的特征表示和函数逼近特性，为处理复杂、高维场景下的强化学习任务提供了可能。深度强化学习以此为出发点，通过有机融合深度学习和强化学习，使得智能体同时具备了极强的感知优势和决策优势，很大程度地降低了求解任务的复杂度和学习难度，适用于具有高维度观察空间和连续型动作空间的配电网优化任务。为使本发明的上述目的、特征和优点能够更加明显易懂，下面结合附图和具体实施方式对本发明作进一步详细的说明。与传统模型驱动方法相比，基于数据驱动的人工智能优化方法通过对配电网历史数据的积累实现对配电网优化模型的持续性修正，赋予配电网以自学习和更新的能力，并在实际应用过程中不断提升优化策略的精度与效率，在面对考虑不同运行场景下的配电网优化问题中适用性更好。目前机器学习在配电网方面的应用方法主要是基于DQN算法框架，但该算法的系统状态和输出决策动作仍为离散形式，不可避免的引入了误差，难以应对配电网高维、连续的动作和状态空间。深度强化学习方法主要可以分为基于值函数的DQN算法和基于策略梯度的DDPG算法。虽然DQN算法可以表征高维度的观察空间，但依旧局限于低维和离散的动作空间，无法处理具有高维和连续动作空间的任务。DQN的另一个缺陷是采取随机的策略，其输出的动作只是服从概率分布，导致行为具有不确定性，使神经网络参数更新的方向偏离梯度最优方向。基于DQN算法的缺陷，能够解决高维连续型动作空间的确定性策略梯度算法被提出，该算法在动作探索过程中仍然采取随机策略，而在训练学习过程中采取确定性策略。与随机策略同时整合动作和状态空间不同，确定性策略仅仅对状态空间进行整合，给定相应的状态和参数后只输出确定性的具体动作，其需要采样的数据更少，算法的效率更高。实施例一如图1所示，本发明提供一种含智能软开关的配电网三相不平衡治理方法，包括：步骤100，构建目标配电网模型；所述目标配电网模型内包括目标配电网、光伏发电站及多个智能软开关；所述目标配电网中包括PV节点和PQ节点。步骤200，基于所述目标配电网模型，获取当前配电状态数据集；所述当前配电状态数据集包括光伏发电站的有功功率出力、PQ节点有功负荷、PQ节点无功负荷、PV节点有功负荷、PV节点电压幅值及PV节点电压相角。当前配电状态数据集中PQ节点有功负荷、PQ节点无功负荷、PV节点有功负荷、PV节点电压幅值及PV节点电压相角均为连续变量，且配电网的运行潮流数据之间存在着很强的相关性。DDPG结合了用于动作探索的随机策略以及用于动作学习的确定性策略，并引入了Actor-Critic框架，可以有效表征高维的观察空间和准确处理连续高维的动作空间，非常适合解决SOP最优功率调节问题。因此着手设计基于DDPG算法的SOP最优功率调节策略，具体如下步骤所示。步骤300，基于DDPG智能体，根据所述当前配电状态数据集，确定当前配电动作；所述当前配电动作包括任一智能软开关的有功出力和无功出力；所述DDPG智能体包括Actor-Critic网络。在一个具体实例中，如图2所示，目标配电网中存在0节点-32节点，其中，11节点与21节点之间设置一个SOP，在24节点与28节点之间设置一个SOP，那么对应的当前配电动作为：。其中，和/＞是时刻t位于11节点与21节点之间SOP的有功出力和无功出力，/＞和/＞是时刻t位于24节点与28节点之间SOP的有功出力和无功出力。步骤400，基于所述当前配电动作，对所述目标配电网模型进行治理，以得到对应的奖励值及下一配电状态数据集，具体地，对应的奖励值及下一配电状态数据集进一步在DDPG智能体中用于评估当前动作并在下一次迭代中做出更好的决策。所述当前配电动作对应的奖励值表征经由当前配电动作治理后目标配电网的三相不平衡度的大小程度；所述当前配电状态数据集、所述当前配电动作及对应的奖励值、所述下一配电状态数据集构成经验四元组；多个经验四元组构成配电治理经验池。步骤400，具体包括：基于所述目标配电网模型，分别构建智能软开关出力约束、光伏出力约束、线路电流约束、节点电压约束及支路潮流约束。其中，SOP由两个电压源型变流器背靠背连接组成，分别为VSC1和VSC2，目标配电网正常运行时，VSC1的控制变量为直流电压和无功功率，VSC2的控制变量为有功功率和无功功率，智能软开关SOP采用VdcQ—PQ控制模式。SOP两端的有功输出相等，无功输出因为中间的直流隔离相互独立。SOP在运行时会产生一定损耗，但与系统运行损耗相比十分微小，故可以忽略不计。基于此，智能软开关出力约束为：。。。其中，和/＞分别表示节点i所连智能软开关在时刻t流过的第φ相有功出力和发出的无功出力，/＞和/＞表示节点j所连智能软开关在时刻t流过的第φ相有功出力和发出的无功出力，/＞表示时刻t节点i智能软开关第φ相的复功率，/＞表示时刻t节点j智能软开关第φ相的复功率。所述光伏出力约束为：。其中，表示光伏有功出力下限，/＞表示光伏电站在目标配电网模型的节点i、时刻t的第φ相有功出力，/＞表示光伏有功出力上限。所述线路电流约束为：。其中，分别表示支路流过的第φ相电流幅值的最小值和最大值，表示目标配电网中在时刻t、节点i与节点j构成的支路流过的第φ相电流幅值。所述节点电压约束为：。其中，，/＞分别表示节点第φ相电压幅值的最小值和最大值，/＞表示目标配电网中在时刻t、节点i第φ相电压幅值。Distflow潮流模型是适用于辐射状网络的支路潮流模型，被广泛应用于配电网最优潮流计算领域。将潮流方程约束式转化为Distflow潮流模型，能更好地实现配电网动态重构模型的二阶锥转化。对于配电网中的任意一条支路，支路潮流约束为：。。。。。。其中，代表时刻t节点j注入的第φ相有功功率，/＞表示时刻t流经节点j和节点k之间的支路的第φ相有功功率，/＞表示与j节点相连的所有节点的集合k，表示与j节点相连的所有节点的集合i，/＞代表时刻t流经节点i和节点j之间的支路的第φ相有功功率，/＞和/＞分别表示节点i和节点j之间的支路的第φ相电阻和电抗，/＞表示时刻t节点i和节点j之间的支路流过的第φ相电流；/＞代表时刻t节点j注入的第φ相无功功率，/＞表示时刻t流经节点j和节点k之间的支路的第φ相无功功率，代表时刻t流经节点i和节点j之间的支路的第φ相无功功率；/＞表示目标配电网中在时刻t，节点j第φ相电压幅值；/＞表示目标配电网中在时刻t，节点i第φ相电压幅值，/＞表示与节点i相邻的所有节点的集合；/＞代表时刻t节点i注入的第φ相有功功率，/＞代表时刻t节点i注入的第φ相无功功率，/＞和/＞分别是时刻t节点i连接的发电机所发出的第φ相有功功率和无功功率，/＞和/＞分别是时刻t节点i的第φ相有功负荷和无功负荷。基于所述当前配电动作及所述智能软开关出力约束，确定实际配电动作。具体地，若当前配电动作满足智能软开关出力约束，则将当前配电动作确定为实际配电动作；若当前配电动作不满足智能软开关约束，则将所述当前配电动作对应的上限值或下限值，确定为实际配电动作。基于所述实际配电动作、所述光伏出力约束、所述线路电流约束、所述节点电压约束及所述支路潮流约束，以三相不平衡度最小为目标，建立三相不平衡优化模型。所述三相不平衡优化模型中的目标函数为：。其中，f表示三相不平衡度，指时刻t节点i的第φ相电压，φ相电压包括A相电压、B相电压及C相电压，/＞指时刻t节点i的第A相电压，/＞指时刻t节点i的第B相电压，/＞指时刻t节点i的第C相电压。所建立的三相不平衡优化模型通常有网络损耗最小，运行成本最小，电压偏差最小，供电可靠性最大，光伏消纳能力最大等优势。对所述三相不平衡优化模型求解，以得到目标配电网的三相不平衡优化结果及对应的三相不平衡度最小值；所述目标配电网的三相不平衡优化结果包括光伏发电站的有功功率出力结果、PQ节点有功负荷结果、PQ节点无功负荷结果、PV节点有功负荷结果、PV节点电压幅值结果及PV节点电压相角结果。基于所述三相不平衡度最小值，计算所述当前配电动作对应的奖励值。所述当前配电动作对应的奖励值的计算公式为：。其中，表示当前配电动作对应的奖励值，/＞表示预设值，为极小值；/＞指节点i的第φ相电压，/＞指节点i的第A相电压，/＞指节点i的第B相电压，/＞指节点i的第C相电压。基于所述目标配电网的三相不平衡优化结果，确定下一配电状态数据集。基于上述步骤400能够将三相不平衡优化模型与深度强化学习算法相结合，即配电网三相不平衡的治理是通过调节SOP的有功/无功功率实现的。本发明将SOP最优功率调节问题描述为离散时间步长的马尔可夫决策过程，给出包括状态、动作、状态转移、奖励、动作价值函数的具体表达。进一步来说，其中的从时间t到时间t+1的状态转换可由如下公式表示：。其中，状态转换过程由当前时间t的系统动作和不确定性/＞决定。SOP的状态转换由操作/＞确定；光伏发电站的有功功率出力/＞的状态转换结果由不确定性/＞确定，这是因为配电网中光伏在时刻t的有功出力无法提前得知，存在一定的随机性与不确定性。确定系统状态后，可以使用动作值函数来评估系统的实时最佳调度动作的质量，该函数的计算公式为：。其中，表示操作策略，它是从状态s到动作/＞的线性映射，/＞是折扣因子，K是优化范围，rk表示第k步的奖励函数值。由上可知，目标配电网向DDPG智能体提供包含光伏出力、PQ节点的三相有功和无功负荷、PV节点的三相有功功率和电压幅值相角的系统状态的观测，DDPG智能体根据状态/＞与环境持续交互，给出包含SOP调节的行为/＞，具体包括配电网中各SOP的有功和无功出力。DDPG智能体获得由环境提供的即时回报作为行为的反馈，通过探索行为空间，找到一个近似最优策略/＞，通过使得操作策略/＞近似最优策略/＞，以最大化整个周期的累计回报值，最终获得每个时段最优的SOP有功/无功出力，实现配电网的三相不平衡治理。步骤500，从所述配电治理经验池中选取多个经验四元组作为训练样本，对所述Actor-Critic网络进行训练，以得到最优的Actor-Critic网络；所述最优的Actor-Critic网络用于根据目标配电网的配电状态数据集，确定对应的最优配电动作。在基于深度强化学习的配电网SOP功率优化过程中，首先需要对DDPG智能体进行离线训练。DDPG智能体引入了Actor-Critic框架，将控制决策与价值评估分开来，并对于策略函数和价值函数均使用了双重神经网络模型构架，如图3所示。图3中，从经验回放池D中取出当前时间t的系统动作、状态st、奖励rt及状态st+1。步骤500，具体包括：随机初始化Actor网络参数和Critic网络参数；具体地，分别建立在线策略网络和在线价值网络，网络参数和/＞可以设置为任何值。初始化目标Actor网络参数和目标Critic网络参数；具体地，按照与在线策略网络和在线价值网络相同的网络结构为策略网络和价值网络建立目标网络，而目标网络的参数和/＞与在线网络的参数初始化为相同值。从所述配电治理经验池中进行随机采样，以将得到的经验四元组作为训练样本。在经验四元组中，存在：。式中，是探索噪声，用于随机探索动作，可以设置为正态分布随机噪声，/＞表示根据/＞和/＞确定的行为策略，然后，环境执行动作A，并观察到奖励R和下一个新状态/＞，将历史状态转换数据/＞存储在经验回放池中，即设置在配电治理经验池中。在更新阶段，首先从经验回放池中随机采样小批量的训练样本，然后训练在线价值网络来更加准确地评估动作值函数。采用所述训练样本，以损失函数最小为目标，对初始化后的Critic网络参数进行更新；具体地，采用如下函数公式：。。。其中，表示目标动作回报值，/＞为当前配电动作对应的奖励值，/＞表示折扣因子，表示使用目标Critic网络和目标Actor网络估计的未来动作价值，L表示损失函数的值，/＞表示训练样本，/＞表示使用初始化后的Critic网络参数估计的动作价值，/＞表示学习率，/＞表示训练样本中的经验四元组，表示/＞对参数/＞的梯度，/＞为当前配电状态数据集，/＞为当前配电动作，/＞表示初始化后的Critic网络参数，/＞为下一配电状态数据集，/＞表示根据和/＞确定的行为策略，/＞表示初始化后的目标Critic网络参数，/＞表示初始化后的目标Actor网络参数。采用所述训练样本，以策略梯度值最小为目标，对初始化后的Actor网络参数进行更新；具体地，采用如下函数公式训练在线策略网络以更新神经网络参数：。其中，表示神经网络参数/＞的学习率，/＞表示/＞对动作/＞的梯度，其更新目标是最小化以下功能：。其中，表示使用在线Critic网络和在线Actor网络估计的动作价值，/＞是确定的行为策略，/＞是采样策略梯度值，其计算公式为：。基于更新后的Critic网络参数对所述目标Critic网络参数进行更新，基于更新后的Actor网络参数对所述目标Actor网络参数进行更新；具体地，根据设置的学习速率和/＞对目标网络的参数/＞和/＞进行更新，为了使学习过程稳定，DDPG算法采用了软更新的方法，如下：。。基于更新后的目标Critic网络参数、更新后的目标Actor网络参数、更新后的Critic网络参数及更新后的Actor网络参数，确定最优的Actor-Critic网络。在一个具体实例中，对DDPG智能体的离线训练流程进行了详细介绍，如下所示：输入：环境状态信息。输出：最优动作策略。参数：学习率和/＞；折扣因子/＞；目标网络学习率/＞。随机初始化Actor网络参数和Critic网络参数/＞。初始化目标Actor网络参数和目标Critic网络参数/＞。初始化经验回放池D。执行一次第一循环，直至执行M次后，第一循环结束。初始化动作探索噪声。得到初始化状态S。在t=1时开始第二循环，直至t=T时，第二循环结束。从上文内容可知，t对应t时刻，需要基于t时刻的状态、动作等进行数据处理。根据在线Actor网络和探索噪声选择动作。执行动作A，从环境中得到回报R和下一状态。把存储到经验回放池中。从经验回放池中随机采样含有N个经验的训练样本。得到目标值。通过最小化损失函数更新网络：。更新Critic网络参数：。更新Actor网络参数：。根据学习率更新目标网络的参数：，。第二循环结束。第一循环结束。当DDPG智能体的离线训练过程结束时，算法参数将被固定并直接用于配电网SOP的有功/无功功率在线优化。基于DDPG的SOP最优功率在线优化模型的输入为目标配电网的光伏发电站的有功功率出力、PQ节点有功负荷、PQ节点无功负荷、PV节点有功负荷、PV节点电压幅值及PV节点电压相角；模型的输出为日内24时段的决策结果，具体包括各SOP的有功/无功出力。当三相不平衡治理任务来临时，在每个时段根据当前系统状态，利用训练好的DDPG智能体选择调度动作，然后执行动作并且进入下一个环境状态，同时获得反馈的即时奖励，继而采集时段t+1系统的状态信息作为新的样本，并进行这个时段的决策，最终得到日内24时段的SOP最优有功/无功功率调节结果，依据结果在配电网中调度SOP实现三相不平衡的治理。综上，本发明为解决三相不平衡治理方法和配电网优化传统求解方法存在的缺陷与难点，结合了SOP连续调节三相无功出力的优势，以及强化学习利用历史数据进行决策和应对新能源出力不确定性的优势。本发明在配电网中引入SOP，建立三相不平衡优化模型；将SOP的功率优化调节问题转化为带有马尔可夫决策过程的深度强化学习任务；最后构建基于深度确定性策略梯度算法的SOP功率最优调节框架及其实现方法，用于确定各SOP的最优有功/无功出力，从而实现配电网三相不平衡的实时在线治理，提高了新能源消纳。进一步来说，传统三相不平衡治理手段大都受到0-1开关特性的限制，开关开断次数有限，不能适应连续快速变化的潮流。应对该缺陷，本发明建立基于SOP的配电网三相不平衡优化模型，不同于传统开关只有断开和闭合两种状态，面对分布式电源出力的随机性和随机性，SOP能够实现对无功功率的连续快速调节，实时在线改善线路的电压分布，抑制三相不平衡，增加新能源的消纳。另外，传统模型驱动控制方法建模复杂，适应拓扑变化能力弱，缺乏对历史数据信息的利用，求解的精度和效率间存在矛盾。本发明融合了人工智能方法在大数据智能化分析处理方面的优势，设计了基于深度强化学习的配电网实时优化策略，选用深度确定性策略梯度算法，引入了经验回放机制和双网络构架，使用深度神经网络逼近策略函数和价值函数并利用策略梯度方法求得最优策略。该方法直接构建配电网运行状态与优化结果的映射关系，对配电网各元件的物理模型不敏感，避免了复杂非线性、非凸优化模型求解效率低的问题。提供系统实时优化策略，以1h为单位时间步长对SOP进行调节，自适应可再生能源出力和负荷使用情况的不确定性。本方法充分发掘和利用了配电网历史运行数据和决策数据，通过对配电网历史数据的积累实现模型的持续性修正，赋予决策以自学习和更新的能力，在实际应用过程中不断提升决策精度或效率；同时基于对系统历史数据的认知和利用，配电网运营商直接使用离线训练好的最优优化策略进行快速在线决策，计算和响应速度更快。实施例二如图4所示，为了实现实施例一中的技术方案，以达到相应的功能和技术效果，本实施例还提供了一种含智能软开关的配电网三相不平衡治理系统，包括：配电网模型构建模块，用于构建目标配电网模型；所述目标配电网模型内包括目标配电网、光伏发电站及多个智能软开关；所述目标配电网中包括PV节点和PQ节点。配电状态数据集获取模块，用于基于所述目标配电网模型，获取当前配电状态数据集；所述当前配电状态数据集包括光伏发电站的有功功率出力、PQ节点有功负荷、PQ节点无功负荷、PV节点有功负荷、PV节点电压幅值及PV节点电压相角。配电动作确定模块，用于基于DDPG智能体，根据所述当前配电状态数据集，确定当前配电动作；所述当前配电动作包括任一智能软开关的有功出力和无功出力；所述DDPG智能体包括Actor-Critic网络。配电治理经验池构建模块，用于基于所述当前配电动作，对所述目标配电网模型进行治理，以得到对应的奖励值及下一配电状态数据集；所述当前配电动作对应的奖励值表征经由当前配电动作治理后目标配电网的三相不平衡度的大小程度；所述当前配电状态数据集、所述当前配电动作及对应的奖励值、所述下一配电状态数据集构成经验四元组；多个经验四元组构成配电治理经验池。最优网络确定模块，用于从所述配电治理经验池中选取多个经验四元组作为训练样本，对所述Actor-Critic网络进行训练，以得到最优的Actor-Critic网络；所述最优的Actor-Critic网络用于根据目标配电网的配电状态数据集，确定对应的最优配电动作。实施例三本实施例提供一种电子设备，包括存储器及处理器，存储器用于存储计算机程序，处理器运行计算机程序以使电子设备执行实施例一的含智能软开关的配电网三相不平衡治理方法。可选地，上述电子设备可以是服务器。另外，本发明实施例还提供一种计算机可读存储介质，其存储有计算机程序，该计算机程序被处理器执行时实现实施例一的含智能软开关的配电网三相不平衡治理方法。相对于现有技术，本发明还具有如下优点：1）本发明实现对配电网三相无功功率的快速连续调节，从而抑制三相不平衡，提升电压质量，同时使得配电网运行更加柔性经济。2）本发明充分利用挖掘历史决策数据信息形成决策优势，同时良好应对新能源出力的不确定性，实现对配电网三相不平衡的实时在线调节。3）本发明基于Actor-Critic框架的微电网控制器人工智能模型，通过海量历史数据的训练，构建了微电网系统运行工况与调度决策结果之间的映射关系；设计了基于DDPG算法的学习框架，通过离线训练和在线决策获得最优协调控制策略，实现配电网三相不平衡的实时在线治理，提高新能源消纳。本说明书中各个实施例采用递进的方式描述，每个实施例重点说明的都是与其他实施例的不同之处，各个实施例之间相同相似部分互相参见即可。对于实施例公开的系统而言，由于其与实施例公开的方法相对应，所以描述的比较简单，相关之处参见方法部分说明即可。本文中应用了具体个例对本发明的原理及实施方式进行了阐述，以上实施例的说明只是用于帮助理解本发明的方法及其核心思想；同时，对于本领域的一般技术人员，依据本发明的思想，在具体实施方式及应用范围上均会有改变之处。综上所述，本说明书内容不应理解为对本发明的限制。
