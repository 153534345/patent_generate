标题title
一种面向无人机集群的智能路径规划方法
摘要abst
本发明公开一种面向无人机集群的智能路径规划方法，将障碍物的位置信息、无人机集群的当前位置坐标和终点位置输入预先训练获得的环境探索任务模型，预测输出临时目标点；将无人机集群的当前位置坐标和临时目标点输入预先训练获得的飞行路径生成模型，预测输出无人机集群的角速度和无人机集群的线速度；按照预测输出的无人机集群的角速度和无人机集群的线速度，无人机集群移动到预测输出的临时目标点处；判断环境探索任务模型中输出的临时目标点是否为终点位置，若是则结束运行，临时目标点按照时间顺序排列组合获得路径轨迹。本发明使得无人机集群完成动态探索任务，为每架无人机规划避开障碍物以及其他无人机的安全飞行路径。
权利要求书clms
1.一种面向无人机集群的智能路径规划方法，其特征在于，具体包括如下步骤：步骤1，获取地图信息、障碍物的位置信息、起点位置和终点位置；步骤2，若无人机集群位于起点位置，将起点位置作为无人机集群的当前位置坐标；步骤3，将障碍物的位置信息、无人机集群的当前位置坐标和终点位置输入预先训练获得的环境探索任务模型，预测输出临时目标点；步骤4，将无人机集群的当前位置坐标和临时目标点输入预先训练获得的飞行路径生成模型，预测输出无人机集群的角速度和无人机集群的线速度；步骤5，按照预测输出的无人机集群的角速度和无人机集群的线速度，无人机集群移动到预测输出的临时目标点处；步骤6，判断环境探索任务模型中输出的临时目标点是否为终点位置，若是则结束运行，临时目标点按照时间顺序排列组合获得路径轨迹，否则进入步骤3。2.根据权利要求1所述的一种面向无人机集群的智能路径规划方法，其特征在于，预先训练获得飞行路径生成模型，通过以下步骤实现：利用深度确定性策略梯度算法中的策略训练网络actor1和动作评估网络critic1，构建初始的飞行路径生成模型；利用预先获取的训练数据迭代更新初始的飞行路径生成模型，获得训练完成的飞行路径生成模型。3.根据权利要求2所述的一种面向无人机集群的智能路径规划方法，其特征在于，利用预先获取的飞行路径训练数据迭代更新初始的飞行路径生成模型，获得训练完成的飞行路径生成模型，通过以下步骤实现：步骤11，获取包括历史起点位置、历史终点位置、历史无人机集群距离障碍物的距离信息、历史临时目标点、历史无人机集群线速度和历史无人机集群角速度的训练数据；将历史起点位置信息作为无人机集群的0时刻的历史临时目标点，t≥1；定义马尔可夫决策过程为一个马尔可夫决策序列集合E1：，，式中，表示0时刻飞行路径生成模型的状态空间，表示1时刻飞行路径生成模型的状态空间，表示2时刻飞行路径生成模型的状态空间，表示t-1时刻飞行路径生成模型的状态空间，表示t时刻飞行路径生成模型的状态空间，表示0时刻飞行路径生成模型的动作空间，表示1时刻飞行路径生成模型的动作空间，表示t-1时刻飞行路径生成模型的动作空间，为0时刻的奖励函数，为1时刻的奖励函数，为t-1时刻的奖励函数，为t+1时刻的奖励函数，为t+2时刻的奖励函数，为t+3时刻的奖励函数，为t+τ+1时刻的奖励函数，γ为学习折扣因子，τ为学习率；状态空间、状态空间、动作空间和动作空间分别为：，，，，式中，Dt表示t时刻可探索范围内的历史无人机集群距与障碍物的距离，表示t-1时刻环境探索任务模型的动作空间，ρt表示t-1时刻的历史临时目标点与t时刻的历史临时目标点的方位，dt表示t-1时刻的历史临时目标点与t时刻的历史临时目标点的距离，表示t-1时刻飞行路径生成模型的动作空间，St为t时刻无人机集群的已探索区域值，Gt为t时刻其他无人机集群的已探索区域，Xt-1表示t-1时刻的历史临时目标点，vt-1为t-1时刻的历史无人机集群线速度，ωt-1为t-1时刻的历史无人机集群角速度；步骤12，构建策略训练网络actor1中的在线网络μ、动作评估网络critic1中的在线网络Q、在线网络μ对应的目标网络μ'、在线网络Q对应的目标网络Q'；初始化当前状态空间、学习率τ、在线网络μ对应的网络参数θ、在线网络Q对应的网络参数φ、目标网络对应的网络参数和目标网络对应的网络参数；构建经验回放池并初始化；步骤13，获取t时刻飞行路径生成模型的状态空间和飞行路径生成模型的动作空间，θμ为t时刻的网络参数θ，π为在线网络μ对应的策略，用于判断t时刻无人机集群是否到达t-1时刻的历史临时目标点；无人机集群执行飞行路径生成模型的动作空间，获得奖励rt；基于已知的t+1时刻可探索范围内的历史无人机集群距与障碍物的距离Dt+1、已知的环境探索任务模型的动作空间、已知的t时刻的历史临时目标点与t+1时刻的历史临时目标点的方位ρt+1和已知的t时刻的历史临时目标点与t+1时刻的历史临时目标点的距离dt+1，得到t+1时刻状态空间；步骤14，将放入经验回放池中，为t时刻的奖励函数；步骤15，从经验回放池中随机选取个N样本输入策略训练网络actor1中，i∈；根据折扣因子γ计算在线网络Q在样本i+1下的评估值，并通过梯度下降方法迭代更新训练网络参数θ和网络参数φ：，φ的值更新为，θ的值更新为，式中，为在线网络Q在样本i下的评估值，为样本i的奖励函数，为目标网络在样本i下的评估值，为在样本i下的评估值在网络参数φ下的梯度值，为在样本i下的评估值在动作空间下的梯度值，为动作空间在网络参数θ下的梯度值；步骤16，的值更新为，的值更新为；步骤17，若无人机集群到达历史终点位置，则结束运行，获得训练完成的飞行路径生成模型，否则t的数值增加1，进入步骤13。4.根据权利要求3所述的一种面向无人机集群的智能路径规划方法，其特征在于，的表达式为：,，，，，式中，Xt-1为t-1时刻的历史临时目标点，Xt表示t时刻的历史临时目标点，为可变常数权重，时分配给设定的较大值，时分配给设定的较小值，ΔU为t+1时刻的历史临时目标点与t时刻的历史临时目标点之间的势场力差，为常数固定权重，为无人机集群的方向变化差，和ε是两个微小常数权重，为势场引力，为引力因子，表示为t-1时刻的历史临时目标点与t时刻的历史临时目标点的距离，为斥力因子，表示为无人机当前位置坐标与障碍物位置坐标的距离，为障碍物的影响范围，为势场引力与势场斥力之和。5.根据权利要求4所述的一种面向无人机集群的智能路径规划方法，其特征在于，预先训练获得环境探索任务模型，通过以下步骤实现：利用深度确定性策略梯度算法中的策略训练网络actor2和动作评估网络critic2，构建初始的环境探索任务模型；利用预先获取的训练数据迭代更新初始的环境探索任务模型，获得训练完成的环境探索任务模型。6.根据权利要求5所述的一种面向无人机集群的智能路径规划方法，其特征在于，利用预先获取的训练数据迭代更新初始的环境探索任务模型，获得训练完成的环境探索任务模型，通过以下步骤实现：步骤21，获取包括历史起点位置、历史终点位置、历史无人机集群距离障碍物的距离信息、历史临时目标点、历史无人机集群线速度和历史无人机集群角速度的训练数据；将历史起点位置信息作为无人机集群的0时刻的历史临时目标点，t≥1；定义所述的马尔可夫决策过程为一个马尔可夫决策序列集合E2：,,式中，表示0时刻环境探索任务模型的状态空间，表示1时刻环境探索任务模型的状态空间，表示2时刻环境探索任务模型的状态空间，表示t-1时刻环境探索任务模型的状态空间，表示t时刻环境探索任务模型的状态空间，表示0时刻环境探索任务模型的动作空间，表示1时刻环境探索任务模型的动作空间，表示t-1时刻环境探索任务模型的动作空间，为0时刻的奖励函数，为1时刻的奖励函数，为t-1时刻的奖励函数，为t+1时刻的奖励函数，为t+2时刻的奖励函数，为t+3时刻的奖励函数，为t+τ2+1时刻的奖励函数，γ2为环境探索任务模型的学习折扣因子，τ2为环境探索任务模型的学习率；步骤22，构建策略训练网络actor2中的在线网络μ2、动作评估网络critic2中的在线网络Q2、在线网络μ2目对应的目标网络、在线网络Q2对应的目标网络；初始化当前状态空间、学习率τ2、在线网络μ2对应的网络参数θ2、在线网络Q2对应的网络参数φ2、目标网络对应的网络参数和目标网络对应的网络参数；构建第二经验回放池并初始化；步骤23，获取t时刻环境探索任务模型的状态空间和环境探索任务模型的动作空间，为t时刻的网络参数θ2，π为在线网络μ2对应的策略，用于判断t时刻无人机集群是否到达t-1时刻的临时目标点；无人机集群执行飞行路径生成模型的动作空间，获得t时刻的奖励函数；步骤24，基于已知的t+1时刻可探索范围内的历史无人机集群距与障碍物的距离Dt+1、已知的t时刻飞行路径生成模型的动作空间、已知的t+1时刻无人机集群的已探索区域值St+1和已知的t+1时刻其他无人机集群的已探索区域Gt+1，获得t+1时刻状态空间；将放入第二经验回放池中；步骤25，从第二经验回放池中随机选取个M个样本输入策略训练网络actor2中，j∈；根据折扣因子γ2计算在线网络Q2在j+1时刻的评估值，并通过梯度下降方法训练迭代更新网络参数θ2和网络参数φ2：，φ2的值更新为，θ2的值更新为，式中，为在线网络Q2在样本j下的评估值，rj为样本j的奖励函数，为目标网络在样本j下的评估值，为在样本j下的评估值在网络参数φ2下的梯度值，为在样本j下的评估值在动作空间下的梯度值，为动作空间在网络参数θ2下的梯度值；步骤26，的值更新为，的值更新为；步骤27，若无人机集群到达历史终点位置，则结束运行，获得训练完成的环境探索任务模型，否则t的数值增加1，进入步骤23。7.根据权利要求6所述的一种面向无人机集群的智能路径规划方法，其特征在于，t时刻的奖励函数的表达式为：，式中，α为确定性增益权重，Δregion为有效探索面积增加值。8.一种电子设备，包括存储器、处理器及存储在存储器上并可在处理器上运行的计算机程序，其特征在于，所述处理器执行所述程序时实现权利要求1至7中任一项所述方法的步骤。9.一种计算机可读存储介质，其上存储有计算机程序，其特征在于，该计算机程序被处理器执行时实现权利要求1至7中任一项所述方法的步骤。
说明书desc
技术领域本发明涉及一种面向无人机集群的智能路径规划方法，属于路径规划以及智能计算技术领域。背景技术面向无人机集群的智能路径规划方法是一个协同规划问题，在未知的环境中，常有人力所不能探测到信息的区域，为排除危险因素，减少人力成本以及降低人身风险，通过携带监测探索仪器的无人机集群进行分布式的未知环境探索具有一定的可行性，并通过对无人机集群的飞行进行协同路径规划，能够满足在对环境进行探测的同时，增强无人机集群的飞行效率。对于单个无人机来讲，一般的全局规划方法如基于几何搜索的路径规划算法，基于采样的路径搜索算法，往往面临在复杂环境中规划效率不高，高维空间的低容错率等问题，同时大量的计算时间和成本的消耗往往使得这类方法难以做到实际应用。随着人工智能在无人机路径规划领域的发展，越来越多的智能算法结构被高效应用，如一些群体智能方法、遗传算法和蚁群算法，强化学习的路径规划方法。而在这些方法中，群体智能的方法往往适用于对特定问题的解决，并且更适合于单无人机的应用场景，并且在环境中如果可行飞行路径过多的情况下，算法容易出现寻找路径过程中陷入局部最优的问题而导致路径规划失败，出现无人机的飞行安全问题。在现实应用场景中，单架无人机的性能往往是难以完成大区域的环境探索任务的，因此对多无人机协同探索的研究值得进行与深入，并且对于全局的路径规划方法，庞大的计算量也需要得到控制。发明内容本发明所要解决的技术问题是克服现有技术的缺陷，提供一种面向无人机集群的智能路径规划方法，根据在具体的环境地图模型下，协同规划多架无人机避开环境中的障碍物，快速合理地做出决策，完成环境探索任务，在不同的环境场景下，对于每架无人机而言，均能够规划出安全可靠的飞行路径，具有良好的适应性。为达到上述目的，本发明提供一种面向无人机集群的智能路径规划方法，具体包括如下步骤：步骤1，获取地图信息、障碍物的位置信息、起点位置和终点位置；步骤2，若无人机集群位于起点位置，将起点位置作为无人机集群的当前位置坐标；步骤3，将障碍物的位置信息、无人机集群的当前位置坐标和终点位置输入预先训练获得的环境探索任务模型，预测输出临时目标点；步骤4，将无人机集群的当前位置坐标和临时目标点输入预先训练获得的飞行路径生成模型，预测输出无人机集群的角速度和无人机集群的线速度；步骤5，按照预测输出的无人机集群的角速度和无人机集群的线速度，无人机集群移动到预测输出的临时目标点处；步骤6，判断环境探索任务模型中输出的临时目标点是否为终点位置，若是则结束运行，临时目标点按照时间顺序排列组合获得路径轨迹，否则进入步骤3。优先地，预先训练获得飞行路径生成模型，通过以下步骤实现：利用深度确定性策略梯度算法中的策略训练网络actor1和动作评估网络critic1，构建初始的飞行路径生成模型；利用预先获取的训练数据迭代更新初始的飞行路径生成模型，获得训练完成的飞行路径生成模型。优先地，利用预先获取的飞行路径训练数据迭代更新初始的飞行路径生成模型，获得训练完成的飞行路径生成模型，通过以下步骤实现：步骤11，获取包括历史起点位置、历史终点位置、历史无人机集群距离障碍物的距离信息、历史临时目标点、历史无人机集群线速度和历史无人机集群角速度的训练数据；将历史起点位置信息作为无人机集群的0时刻的历史临时目标点，t≥1；定义马尔可夫决策过程为一个马尔可夫决策序列集合E1：，，式中，表示0时刻飞行路径生成模型的状态空间，表示1时刻飞行路径生成模型的状态空间，表示2时刻飞行路径生成模型的状态空间，表示t-1时刻飞行路径生成模型的状态空间，表示t时刻飞行路径生成模型的状态空间，表示0时刻飞行路径生成模型的动作空间，表示1时刻飞行路径生成模型的动作空间，表示t-1时刻飞行路径生成模型的动作空间，为0时刻的奖励函数，为1时刻的奖励函数，为t-1时刻的奖励函数，为t+1时刻的奖励函数，为t+2时刻的奖励函数，为t+3时刻的奖励函数，为t+τ+1时刻的奖励函数，γ为学习折扣因子，τ为学习率；状态空间、状态空间、动作空间和动作空间分别为：，，，，式中，Dt表示t时刻可探索范围内的历史无人机集群距与障碍物的距离，表示t-1时刻环境探索任务模型的动作空间，ρt表示t-1时刻的历史临时目标点与t时刻的历史临时目标点的方位，dt表示t-1时刻的历史临时目标点与t时刻的历史临时目标点的距离，表示t-1时刻飞行路径生成模型的动作空间，St为t时刻无人机集群的已探索区域值，Gt为t时刻其他无人机集群的已探索区域，Xt-1表示t-1时刻的历史临时目标点，vt-1为t-1时刻的历史无人机集群线速度，ωt-1为t-1时刻的历史无人机集群角速度；步骤12，构建策略训练网络actor1中的在线网络μ、动作评估网络critic1中的在线网络Q、在线网络μ对应的目标网络μ'、在线网络Q对应的目标网络Q'；初始化当前状态空间、学习率τ、在线网络μ对应的网络参数θ、在线网络Q对应的网络参数φ、目标网络对应的网络参数和目标网络对应的网络参数；构建经验回放池并初始化；步骤13，获取t时刻飞行路径生成模型的状态空间和飞行路径生成模型的动作空间，θμ为t时刻的网络参数θ，π为在线网络μ对应的策略，用于判断t时刻无人机集群是否到达t-1时刻的历史临时目标点；无人机集群执行飞行路径生成模型的动作空间，获得奖励rt；基于已知的t+1时刻可探索范围内的历史无人机集群距与障碍物的距离Dt+1、已知的环境探索任务模型的动作空间、已知的t时刻的历史临时目标点与t+1时刻的历史临时目标点的方位ρt+1和已知的t时刻的历史临时目标点与t+1时刻的历史临时目标点的距离dt+1，得到t+1时刻状态空间；步骤14，将放入经验回放池中，为t时刻的奖励函数；步骤15，从经验回放池中随机选取个N样本输入策略训练网络actor1中，i∈；根据折扣因子γ计算在线网络Q在样本i+1下的评估值，并通过梯度下降方法迭代更新训练网络参数θ和网络参数φ：，φ的值更新为，θ的值更新为，式中，为在线网络Q在样本i下的评估值，为样本i的奖励函数，为目标网络在样本i下的评估值，为在样本i下的评估值在网络参数φ下的梯度值，为在样本i下的评估值在动作空间下的梯度值，为动作空间在网络参数θ下的梯度值；步骤16，的值更新为，的值更新为；步骤17，若无人机集群到达历史终点位置，则结束运行，获得训练完成的飞行路径生成模型，否则t的数值增加1，进入步骤13。优先地，的表达式为：,，，，，式中，Xt-1为t-1时刻的历史临时目标点，Xt表示t时刻的历史临时目标点，为可变常数权重，时分配给设定的较大值，时分配给设定的较小值，ΔU为t+1时刻的历史临时目标点与t时刻的历史临时目标点之间的势场力差，为常数固定权重，为无人机集群的方向变化差，和ε是两个微小常数权重，为势场引力，为引力因子，表示为t-1时刻的历史临时目标点与t时刻的历史临时目标点的距离，为斥力因子，表示为无人机当前位置坐标与障碍物位置坐标的距离，为障碍物的影响范围，为势场引力与势场斥力之和。优先地，预先训练获得环境探索任务模型，通过以下步骤实现：利用深度确定性策略梯度算法中的策略训练网络actor2和动作评估网络critic2，构建初始的环境探索任务模型；利用预先获取的训练数据迭代更新初始的环境探索任务模型，获得训练完成的环境探索任务模型。优先地，利用预先获取的训练数据迭代更新初始的环境探索任务模型，获得训练完成的环境探索任务模型，通过以下步骤实现：步骤21，获取包括历史起点位置、历史终点位置、历史无人机集群距离障碍物的距离信息、历史临时目标点、历史无人机集群线速度和历史无人机集群角速度的训练数据；将历史起点位置信息作为无人机集群的0时刻的历史临时目标点，t≥1；定义所述的马尔可夫决策过程为一个马尔可夫决策序列集合E2：,,式中，表示0时刻环境探索任务模型的状态空间，表示1时刻环境探索任务模型的状态空间，表示2时刻环境探索任务模型的状态空间，表示t-1时刻环境探索任务模型的状态空间，表示t时刻环境探索任务模型的状态空间，表示0时刻环境探索任务模型的动作空间，表示1时刻环境探索任务模型的动作空间，表示t-1时刻环境探索任务模型的动作空间，为0时刻的奖励函数，为1时刻的奖励函数，为t-1时刻的奖励函数，为t+1时刻的奖励函数，为t+2时刻的奖励函数，为t+3时刻的奖励函数，为t+τ2+1时刻的奖励函数，γ2为环境探索任务模型的学习折扣因子，τ2为环境探索任务模型的学习率；步骤22，构建策略训练网络actor2中的在线网络μ2、动作评估网络critic2中的在线网络Q2、在线网络μ2目对应的目标网络、在线网络Q2对应的目标网络；初始化当前状态空间、学习率τ2、在线网络μ2对应的网络参数θ2、在线网络Q2对应的网络参数φ2、目标网络对应的网络参数和目标网络对应的网络参数；构建第二经验回放池并初始化；步骤23，获取t时刻环境探索任务模型的状态空间和环境探索任务模型的动作空间，为t时刻的网络参数θ2，π为在线网络μ2对应的策略，用于判断t时刻无人机集群是否到达t-1时刻的临时目标点；无人机集群执行飞行路径生成模型的动作空间，获得t时刻的奖励函数；步骤24，基于已知的t+1时刻可探索范围内的历史无人机集群距与障碍物的距离Dt+1、已知的t时刻飞行路径生成模型的动作空间、已知的t+1时刻无人机集群的已探索区域值St+1和已知的t+1时刻其他无人机集群的已探索区域Gt+1，获得t+1时刻状态空间；将放入第二经验回放池中；步骤25，从第二经验回放池中随机选取个M个样本输入策略训练网络actor2中，j∈；根据折扣因子γ2计算在线网络Q2在j+1时刻的评估值，并通过梯度下降方法训练迭代更新网络参数θ2和网络参数φ2：，φ2的值更新为，θ2的值更新为，式中，为在线网络Q2在样本j下的评估值，rj为样本j的奖励函数，为目标网络在样本j下的评估值，为在样本j下的评估值在网络参数φ2下的梯度值，为在样本j下的评估值在动作空间下的梯度值，为动作空间在网络参数θ2下的梯度值；步骤26，的值更新为，的值更新为；步骤27，若无人机集群到达历史终点位置，则结束运行，获得训练完成的环境探索任务模型，否则t的数值增加1，进入步骤23。优先地，t时刻的奖励函数的表达式为：，式中，α为确定性增益权重，Δregion为有效探索面积增加值。一种电子设备，包括存储器、处理器及存储在存储器上并可在处理器上运行的计算机程序，所述处理器执行所述程序时实现上述任一项所述方法的步骤。一种计算机可读存储介质，其上存储有计算机程序，该计算机程序被处理器执行时实现上述任一项所述方法的步骤。本发明所达到的有益效果：1、本发明能够在障碍物不同的模拟地图模型中进行无人机集群的协同规划，无人机集群在完成探索任务的同时，规划出安全有效的路径，表现出规划方法在不同环境下的适应性。2、本发明将无人机集群的协同路径规划有效划分为两部分：环境探索任务与飞行路径生成，通过深度确定性策略梯度方法分别训练环境探索任务模型与飞行路径生成模型两部分的策略，解决了无人机集群执行环境探索任务的协同规划问题，具有良好的实用性。3、本发明利用改进的势场力函数作为飞行路径生成模型的奖励函数的一部分，一定程度上提高了模型在初始训练时的随机性，加速了训练的收敛过程，提供了无人机集群与环境进行信息交互过程中的路径指引。4、本发明能够较好地为无人机集群在模拟地图上规划出集群在执行任务时的协同探索路径，具有一定的现实意义。附图说明图1为本发明的流程图；图2为本发明中进行无人机集群协同规划的网格环境模型示意图；图3为本发明采用的飞行路径生成模型的神经网络结构图；图4为本发明采用的环境探索任务模型的神经网络结构图。具体实施方式以下实施例仅用于更加清楚地说明本发明的技术方案，而不能以此来限制本发明的保护范围。如图1所示，一种面向无人机集群的智能路径规划方法，面向智慧化工业园区监测，具体包括如下步骤：在二维坐标系中对环境信息包括障碍物的位置信息，无人机位置等信息进行建模，将无人机等效为粒子，地面建模为XOY平面，初始化无人机飞行地图环境；步骤1，获取地图信息、障碍物的位置信息、起点位置和终点位置；步骤2，若无人机集群位于起点位置，将起点位置作为无人机集群的当前位置坐标；步骤3，将障碍物的位置信息、无人机集群的当前位置坐标和终点位置输入预先训练获得的环境探索任务模型，预测输出临时目标点；步骤4，将无人机集群的当前位置坐标和临时目标点输入预先训练获得的飞行路径生成模型，预测输出无人机集群的角速度和无人机集群的线速度；步骤5，按照预测输出的无人机集群的角速度和无人机集群的线速度，无人机集群移动到预测输出的临时目标点处；步骤6，判断环境探索任务模型中输出的临时目标点是否为终点位置，若是则结束运行，临时目标点按照时间顺序排列组合获得路径轨迹，否则进入步骤3。进一步地，本实施例中预先训练获得飞行路径生成模型，通过以下步骤实现：利用深度确定性策略梯度算法中的策略训练网络actor1和动作评估网络critic1，构建初始的飞行路径生成模型；利用预先获取的训练数据迭代更新初始的飞行路径生成模型，获得训练完成的飞行路径生成模型。进一步地，本实施例中利用预先获取的飞行路径训练数据迭代更新初始的飞行路径生成模型，获得训练完成的飞行路径生成模型，通过以下步骤实现：步骤11，获取包括历史起点位置、历史终点位置、历史无人机集群距离障碍物的距离信息、历史临时目标点、历史无人机集群线速度和历史无人机集群角速度的训练数据；将历史起点位置信息作为无人机集群的0时刻的历史临时目标点，t≥1；定义马尔可夫决策过程为一个马尔可夫决策序列集合E1：，，式中，表示0时刻飞行路径生成模型的状态空间，表示1时刻飞行路径生成模型的状态空间，表示2时刻飞行路径生成模型的状态空间，表示t-1时刻飞行路径生成模型的状态空间，表示t时刻飞行路径生成模型的状态空间，表示0时刻飞行路径生成模型的动作空间，表示1时刻飞行路径生成模型的动作空间，表示t-1时刻飞行路径生成模型的动作空间，为0时刻的奖励函数，为1时刻的奖励函数，为t-1时刻的奖励函数，为t+1时刻的奖励函数，为t+2时刻的奖励函数，为t+3时刻的奖励函数，为t+τ+1时刻的奖励函数，γ为学习折扣因子，τ为学习率；状态空间、状态空间、动作空间和动作空间分别为：，，，，式中，Dt表示t时刻可探索范围内的历史无人机集群距与障碍物的距离，表示t-1时刻环境探索任务模型的动作空间，ρt表示t-1时刻的历史临时目标点与t时刻的历史临时目标点的方位，dt表示t-1时刻的历史临时目标点与t时刻的历史临时目标点的距离，表示t-1时刻飞行路径生成模型的动作空间，St为t时刻无人机集群的已探索区域值，Gt为t时刻其他无人机集群的已探索区域，Xt-1表示t-1时刻的历史临时目标点，vt-1为t-1时刻的历史无人机集群线速度，ωt-1为t-1时刻的历史无人机集群角速度；集群线速度，ωt-1为t-1时刻的历史无人机集群角速度；步骤12，构建策略训练网络actor1中的在线网络μ、动作评估网络critic1中的在线网络Q、在线网络μ对应的目标网络μ'、在线网络Q对应的目标网络Q'；初始化当前状态空间、学习率τ、在线网络μ对应的网络参数θ、在线网络Q对应的网络参数φ、目标网络对应的网络参数和目标网络对应的网络参数；构建经验回放池并初始化；步骤13，获取t时刻飞行路径生成模型的状态空间和飞行路径生成模型的动作空间，θμ为t时刻的网络参数θ，π为在线网络μ对应的策略，用于判断t时刻无人机集群是否到达t-1时刻的历史临时目标点；无人机集群执行飞行路径生成模型的动作空间，获得奖励rt；基于已知的t+1时刻可探索范围内的历史无人机集群距与障碍物的距离Dt+1、已知的环境探索任务模型的动作空间、已知的t时刻的历史临时目标点与t+1时刻的历史临时目标点的方位ρt+1和已知的t时刻的历史临时目标点与t+1时刻的历史临时目标点的距离dt+1，得到t+1时刻状态空间；步骤14，将放入经验回放池中，为t时刻的奖励函数；步骤15，从经验回放池中随机选取个N样本输入策略训练网络actor1中，i∈；根据折扣因子γ计算在线网络Q在样本i+1下的评估值，并通过梯度下降方法迭代更新训练网络参数θ和网络参数φ：，φ的值更新为，θ的值更新为，式中，为在线网络Q在样本i下的评估值，为样本i的奖励函数，为目标网络在样本i下的评估值，为在样本i下的评估值在网络参数φ下的梯度值，为在样本i下的评估值在动作空间下的梯度值，为动作空间在网络参数θ下的梯度值；步骤16，的值更新为，的值更新为；步骤17，若无人机集群到达历史终点位置，则结束运行，获得训练完成的飞行路径生成模型，否则t的数值增加1，进入步骤13。进一步地，本实施例中的表达式为：,，，，，式中，Xt-1为t-1时刻的历史临时目标点，即无人机集群当前位置坐标，Xt表示t时刻的历史临时目标点，若t时刻的临时目标点Xt处于障碍物上则赋值为负增益-15，若无人机集群所在位置为历史临时目标点则赋值为正增益15，在有效路径生成时给予正增益，为可变常数权重，时分配给设定的较大值，驱动无人机集群做出更合理的下一步动作空间，时分配给设定的较小值，避免局部最优的极值问题；ΔU为t+1时刻的历史临时目标点与t时刻的历史临时目标点之间的势场力差，为常数固定权重，为无人机集群的方向变化差，和ε是两个微小常数权重，为势场引力，为引力因子，表示为t-1时刻的历史临时目标点与t时刻的历史临时目标点的距离，为斥力因子，表示为无人机当前位置坐标与障碍物位置坐标的距离，为障碍物的影响范围，为势场引力与势场斥力之和。进一步地，本实施例中预先训练获得环境探索任务模型，通过以下步骤实现：利用深度确定性策略梯度算法中的策略训练网络actor2和动作评估网络critic2，构建初始的环境探索任务模型；利用预先获取的训练数据迭代更新初始的环境探索任务模型，获得训练完成的环境探索任务模型。进一步地，本实施例中利用预先获取的训练数据迭代更新初始的环境探索任务模型，获得训练完成的环境探索任务模型，通过以下步骤实现：步骤21，获取包括历史起点位置、历史终点位置、历史无人机集群距离障碍物的距离信息、历史临时目标点、历史无人机集群线速度和历史无人机集群角速度的训练数据；将历史起点位置信息作为无人机集群的0时刻的历史临时目标点，t≥1；定义所述的马尔可夫决策过程为一个马尔可夫决策序列集合E2：,,式中，表示0时刻环境探索任务模型的状态空间，表示1时刻环境探索任务模型的状态空间，表示2时刻环境探索任务模型的状态空间，表示t-1时刻环境探索任务模型的状态空间，表示t时刻环境探索任务模型的状态空间，表示0时刻环境探索任务模型的动作空间，表示1时刻环境探索任务模型的动作空间，表示t-1时刻环境探索任务模型的动作空间，为0时刻的奖励函数，为1时刻的奖励函数，为t-1时刻的奖励函数，为t+1时刻的奖励函数，为t+2时刻的奖励函数，为t+3时刻的奖励函数，为t+τ2+1时刻的奖励函数，γ2为环境探索任务模型的学习折扣因子，τ2为环境探索任务模型的学习率；步骤22，构建策略训练网络actor2中的在线网络μ2、动作评估网络critic2中的在线网络Q2、在线网络μ2目对应的目标网络、在线网络Q2对应的目标网络；初始化当前状态空间、学习率τ2、在线网络μ2对应的网络参数θ2、在线网络Q2对应的网络参数φ2、目标网络对应的网络参数和目标网络对应的网络参数；构建第二经验回放池并初始化；步骤23，获取t时刻环境探索任务模型的状态空间和环境探索任务模型的动作空间，为t时刻的网络参数θ2，π为在线网络μ2对应的策略，用于判断t时刻无人机集群是否到达t-1时刻的临时目标点；无人机集群执行飞行路径生成模型的动作空间，获得t时刻的奖励函数；步骤24，基于已知的t+1时刻可探索范围内的历史无人机集群距与障碍物的距离Dt+1、已知的t时刻飞行路径生成模型的动作空间、已知的t+1时刻无人机集群的已探索区域值St+1和已知的t+1时刻其他无人机集群的已探索区域Gt+1，获得t+1时刻状态空间；将放入第二经验回放池中；步骤25，从第二经验回放池中随机选取个M个样本输入策略训练网络actor2中，j∈；根据折扣因子γ2计算在线网络Q2在j+1时刻的评估值，并通过梯度下降方法训练迭代更新网络参数θ2和网络参数φ2：，φ2的值更新为，θ2的值更新为，式中，为在线网络Q2在样本j下的评估值，rj为样本j的奖励函数，为目标网络在样本j下的评估值，为在样本j下的评估值在网络参数φ2下的梯度值，为在样本j下的评估值在动作空间下的梯度值，为动作空间在网络参数θ2下的梯度值；步骤26，的值更新为，的值更新为；步骤27，若无人机集群到达历史终点位置，则结束运行，获得训练完成的环境探索任务模型，否则t的数值增加1，进入步骤23。进一步地，本实施例中t时刻的奖励函数的表达式为：，式中，若t时刻的临时目标点Xt处于障碍物上则赋值为负增益-5，若无人机集群所在位置在其他无人机已探索区域中则赋值为正增益，α为确定性增益权重，Δregion为有效探索面积增加值。一种电子设备，包括存储器、处理器及存储在存储器上并可在处理器上运行的计算机程序，所述处理器执行所述程序时实现上述任一项所述方法的步骤。一种计算机可读存储介质，其上存储有计算机程序，该计算机程序被处理器执行时实现上述任一项所述方法的步骤。 如图3所示，构建的初始的飞行路径生成模型的网络架构包括第一输入层、第一卷积层、第一池化层、第一密集层和第一输出层，第一输入层、第一卷积层、第一池化层、第一密集层和第一输出层依次连接；如图4所示，构建的初始的环境探索任务模型的网络架构包括第二输入层、第二卷积层、第二池化层、第二密集层和第二输出层，第二输入层、第二卷积层、第二池化层、第二密集层和第二输出层依次连接；获取包括历史起点位置、历史终点位置、历史无人机集群距离障碍物的距离信息、历史临时目标点、历史无人机集群线速度和历史无人机集群角速度的训练数据，将历史起点位置作为0时刻的历史临时目标点；在飞行路径生成模型中，构建深度确定性策略梯度算法中的策略训练网络actor1和动作评估网络critic1，策略训练网络actor1和动作评估网络critic1的网络结构一致，如图3所示。在环境探索任务模型中，构建深度确定性策略梯度算法中的策略训练网络actor2和动作评估网络critic2，策略训练网络actor2和动作评估网络critic2网络结构一致，如图4所示。两个模型训练完成之后，对于每架无人机而言，首先将通过已经训练完毕的环境探索任务模型，输入无人机当前状态，c表示第c架无人机，输出动作，将作为训练完毕的飞行路径生成模型输入状态的一部分，输出动作，生成当前阶段的无人机集群路径轨迹，并将作为的一部分再次输入环境探索任务模型进行迭代循环，c=1,2,3,4。无人机集群上述部件在现有技术中可采用的型号很多，本领域技术人员可根据实际需求选用合适的型号，本实施例不再一一举例。本申请是参照根据本申请实施例的方法、设备、和计算机程序产品的流程图和／或方框图来描述的。应理解可由计算机程序指令实现流程图和／或方框图中的每一流程和／或方框、以及流程图和／或方框图中的流程和／或方框的结合。可提供这些计算机程序指令到通用计算机、专用计算机、嵌入式处理机或其他可编程数据处理设备的处理器以产生一个机器，使得通过计算机或其他可编程数据处理设备的处理器执行的指令产生用于实现在流程图一个流程或多个流程和／或方框图一个方框或多个方框中指定的功能的装置。这些计算机程序指令也可存储在能引导计算机或其他可编程数据处理设备以特定方式工作的计算机可读存储器中，使得存储在该计算机可读存储器中的指令产生包括指令装置的制造品，该指令装置实现在流程图一个流程或多个流程和／或方框图一个方框或多个方框中指定的功能。这些计算机程序指令也可装载到计算机或其他可编程数据处理设备上，使得在计算机或其他可编程设备上执行一系列操作步骤以产生计算机实现的处理，从而在计算机或其他可编程设备上执行的指令提供用于实现在流程图一个流程或多个流程和／或方框图一个方框或多个方框中指定的功能的步骤。以上所述仅是本发明的优选实施方式，应当指出，对于本技术领域的普通技术人员来说，在不脱离本发明技术原理的前提下，还可以做出若干改进和变形，这些改进和变形也应视为本发明的保护范围。
