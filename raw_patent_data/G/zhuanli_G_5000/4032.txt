标题title
一种特定情感音乐生成方法及系统
摘要abst
本发明公开了一种特定情感音乐生成方法及系统，包括：获取生成音乐的情感类别，构建MIDI音乐数据集；将原始MIDI数据集转化为特定的结构化数据集，对得到的结构化数据集进行预处理；提取每一首MIDI音乐的不同属性特征，构建并训练基于Transformer网络和高斯混合变分自编码器的深度潜在变量模型，生成特定情感的音乐。能够学习不同音乐属性特征对于情感的影响，生成具有长期依赖结构的音乐，并且在生成过程中能够实现音乐情感的转换。
权利要求书clms
1.一种特定情感音乐生成方法，其特征在于，包括以下步骤：获取生成音乐的情感类别，构建MIDI音乐数据集；将原始MIDI数据集转化为特定的结构化数据集，对得到的结构化数据集进行预处理；提取每一首MIDI音乐的不同属性特征，构建并训练基于Transformer网络和高斯混合变分自编码器的深度潜在变量模型，生成特定情感的音乐。2.如权利要求1所述的特定情感音乐生成方法，其特征在于：所述将原始MIDI数据集转化为特定的结构化数据集，对MIDI音乐进行解析，将其划分为多个小节，并读取其中的事件信息，将音乐表示为一组事件序列，用作深度潜在变量模型的输入。3.如权利要求2所述的特定情感音乐生成方法，其特征在于：所述提取每一首MIDI音乐的不同属性特征，采用音乐的复调特征和调式特征作为音乐与情感相关的特征，通过深度潜在变量模型解耦学习两种特征在情感空间内的潜在分布。4.如权利要求3所述的特定情感音乐生成方法，其特征在于：所述对得到的结构化数据集进行预处理，包括，将整个数据集划分为训练集、验证集和测试集，从每个MIDI事件序列中随机提取固定数量的连续小节，将这些小节中的事件信息标记化为Token序列，并统一序列长度，将得到的标准化输入序列加载到神经网络模型中。5.如权利要求4所述的特定情感音乐生成方法，其特征在于：所述构建并训练基于Transformer网络和高斯混合变分自编码器的深度潜在变量模型，利用LVM的编码器模块对输入序列进行解耦表征学习，得到关于每一组音乐属性特征的潜在变量表示，利用半监督聚类方法在有标签数据集和无标签数据集上学习每组潜在变量上的情感概率分布，并通过优化目标损失函数使得相同情感的音乐具有相同的潜在空间，将多组潜在变量聚合后馈送到LVM的解码器模块。6.如权利要求5所述的特定情感音乐生成方法，其特征在于：所述构建并训练深度潜在变量模型用于情感音乐的生成模型，包括以下步骤：对音乐输入序列 进行Token嵌入和位置编码，得到输入向量/＞；采用Transformer网络编码组件解耦学习音乐在复调特征和调式特征的潜在分布，主要由编码组件输出的隐藏状态/＞来学习/＞的均值向量/＞和标准差向量/＞；对编码组件学习到的潜在分布进行重采样得到相应特征的潜在变量/＞；采用高斯混合模型学习潜在变量相应的情感概率分布/＞来近似得到特征约束下数据在情感空间上的分布信息/＞；采用Transformer网络解码组件聚合特定情感下复调特征和调式特征的潜在变量，并经过多层解码器学习后，将输出的隐藏状态输入到线性层和SoftMax层得到解码后的Token序列；计算损失函数优化GMVAE模型训练目标，通过最大化GMVAE的证据下界得到。7.如权利要求6所述的特定情感音乐生成方法，其特征在于：所述输入向量计算过程表示为：；其中，为Token嵌入，/＞为位置编码。8.如权利要求7所述的特定情感音乐生成方法，其特征在于：所述计算损失函数优化GMVAE模型训练目标，通过最大化GMVAE的证据下界得到，证据下界表示为：;其中，第一项为基于输入向量的重构损失，第二项为模型对复调特征和调式特征的重构损失，第三项为重构两种特征时增加的对抗损失，三者均采用交叉熵损失函数实现，第四项分为两种情况：在无监督方式下，最小化编码器学习的潜在分布/＞与相应情感下的高斯分布/＞之间的KL散度以及用于类别推断的后验分布/＞与类别均匀分布/＞之间的KL散度，KL散度用于衡量两个分布之间的相似度；在有监督方式下，只需最小化前者，超参数/＞用于对该损失项的权重进行调节。9.如权利要求8所述的特定情感音乐生成方法，其特征在于：所述深度潜在变量模型，利用已学习到的不同音乐属性特征的潜在变量实现音乐情感的变换，将源情感下的潜在变量变换为目标情感下的/＞，变换的计算方式表示为：。10.一种特定情感音乐生成系统，其特征在于，包括以下模块：输入模块：用于对音乐输入序列进行Token嵌入和位置编码，得到模型输入向量；编码器模块：采用Transformer网络编码组件实现，用于接收输入向量，解耦学习音乐在复调特征和调式特征下的潜在分布，并根据重采样得到相应特征的潜在变量；高斯混合模块：采用高斯混合模型实现，用于以半监督方法在有标签数据集和无标签数据集上学习每组潜在变量上的情感概率分布，完成情感类别的推断；解码器模块：采用Transformer网络解码组件实现，用于聚合特定情感下复调特征和调式特征的潜在变量，并基于该潜在变量生成相应情感的音乐；损失函数模块：用于计算整个GMVAE模型训练目标，通过最大化GMVAE的证据下界得到；情感变换模块：用于实现音乐从某种情感到另一种情感的变换。
说明书desc
技术领域本发明涉及人工智能技术领域，尤其涉及一种特定情感音乐生成方法及系统。背景技术音乐作为艺术领域里一种重要的表达方式，体现了一系列人类所特有的思维模式，是规则性和创造性的统一。创作一首包含丰富多样元素的乐曲需要许多艺术专家共同参与，同时合作、授权也需要大量时间和资金成本。为了能够解决上述问题，可以借助计算机来自动生成音乐。早期，研究者们提出了基于统计分析的方法进行音乐创作，根据概率模型计算每一个时间步长内音符出现的概率，并通过音高、节奏和和弦等相关的基本乐理规则来调试这些音符，从而随机生成不同的音乐，典型的方法如马尔可夫链模型。近年来，深度神经网络在计算机视觉和自然语言处理领域发挥了重要作用，许多研究人员提出了基于深度神经网络的音乐生成模型，以从中学习不同的音乐创作结构和风格。例如，循环神经网络模型可用于捕获嵌入在音符时间序列中的时间信息。变分自编码器可通过潜在空间正则化技术学习一组代表音乐内在语义特征的隐变量，并根据该变量进行重构生成符合该特征的音乐。生成对抗网络提供了一种对抗训练的思想，可以任何合适的网络为生成器和判别器，并引入与乐理相关的对抗机制，达到一种纳什均衡的理想状态，使得生成的数据尽可能符合真实数据。虽然上述的音乐生成方法已经逐渐成熟，能够很好从音乐中学习内在特征，输出丰富的音乐样本，但很少有基于情感条件来生成音乐的方法。音乐与自然语言一样，在传达内容和抽象概念的同时，还可以作为情感表达的载体，这些情感与音乐的内在结构密切相关，随时间传播的音乐元素诱发。因此，音乐的自动生成不仅需要基于一定的乐理规则，如旋律、节奏、调式、和弦等，更需要揉入情感元素激发创造性。基于情感的音乐生成应用也十分广泛，不仅可以帮助电影、游戏制作者为不同的场景提供特定情感的背景音乐，也可以应用于音乐治疗中，促进患者情绪健康。目前大多基于情感的音乐生成方法主要使用情感标签进行完全监督的训练，而没有考虑情感与音乐元素之间的联系，无法对生成的音乐解释其内在的情感表征。此外，该领域缺乏大量带有情感标签的标准数据集，情感属于音乐的高级特征，具有一定的抽象性和主观性，在进行人工标注时较为混乱，不够统一。因此，使用有监督学习在此类数据集上训练模型会可能会导致生成效果降低。另一方面，现有的情感音乐生成模型缺乏结构感知性和可控性。结构感知性意味着需要生成具有长期依赖性的自然连贯的音乐，包括不同层次重复和变化。可控性意味着在生成过程中能够对情感进行控制，改变其内在的情感语义，而大多的音乐生成模型类似于一个黑匣子，无法探索在生成过程中对于情感的把控。发明内容本部分的目的在于概述本发明的实施例的一些方面以及简要介绍一些较佳实施例。在本部分以及本申请的说明书摘要和发明名称中可能会做些简化或省略以避免使本部分、说明书摘要和发明名称的目的模糊，而这种简化或省略不能用于限制本发明的范围。鉴于上述现有存在的问题，提出了本发明。因此，本发明提供了一种特定情感音乐生成方法及系统解决对音乐生成缺乏结构感知性和可控性的问题。为解决上述技术问题，本发明提供如下技术方案：第一方面，本发明提供了一种特定情感音乐生成方法，包括以下步骤：获取生成音乐的情感类别，构建MIDI音乐数据集；将原始MIDI数据集转化为特定的结构化数据集，对得到的结构化数据集进行预处理；提取每一首MIDI音乐的不同属性特征，构建并训练基于Transformer网络和高斯混合变分自编码器的深度潜在变量模型，生成特定情感的音乐。作为本发明所述的特定情感音乐生成方法的一种优选方案，其中：所述将原始MIDI数据集转化为特定的结构化数据集，对MIDI音乐进行解析，将其划分为多个小节，并读取其中的事件信息，将音乐表示为一组事件序列，用作深度潜在变量模型的输入。作为本发明所述的特定情感音乐生成方法的一种优选方案，其中：所述提取每一首MIDI音乐的不同属性特征，采用音乐的复调特征和调式特征作为音乐与情感相关的特征，通过深度潜在变量模型解耦学习两种特征在情感空间内的潜在分布。作为本发明所述的特定情感音乐生成方法的一种优选方案，其中：所述对得到的结构化数据集进行预处理，包括，将整个数据集划分为训练集、验证集和测试集，从每个MIDI事件序列中随机提取固定数量的连续小节，将这些小节中的事件信息标记化为Token序列，并统一序列长度，将得到的标准化输入序列加载到神经网络模型中。作为本发明所述的特定情感音乐生成方法的一种优选方案，其中：所述构建并训练基于Transformer网络和高斯混合变分自编码器的深度潜在变量模型，利用LVM的编码器模块对输入序列进行解耦表征学习，得到关于每一组音乐属性特征的潜在变量表示，利用半监督聚类方法在有标签数据集和无标签数据集上学习每组潜在变量上的情感概率分布，并通过优化目标损失函数使得相同情感的音乐具有相同的潜在空间，将多组潜在变量聚合后馈送到LVM的解码器模块。作为本发明所述的特定情感音乐生成方法的一种优选方案，其中：所述构建并训练深度潜在变量模型用于情感音乐的生成模型，包括以下步骤：对音乐输入序列进行Token嵌入和位置编码，得到输入向量/＞；采用Transformer网络编码组件解耦学习音乐在复调特征和调式特征的潜在分布，主要由编码组件输出的隐藏状态/＞来学习/＞的均值向量/＞和标准差向量/＞；对编码组件学习到的潜在分布进行重采样得到相应特征的潜在变量/＞；采用高斯混合模型学习潜在变量相应的情感概率分布/＞来近似得到特征约束下数据在情感空间上的分布信息/＞；/＞采用Transformer网络解码组件聚合特定情感下复调特征和调式特征的潜在变量，并经过多层解码器学习后，将输出的隐藏状态输入到线性层和SoftMax层得到解码后的Token序列；计算损失函数优化GMVAE模型训练目标，通过最大化GMVAE的证据下界得到。作为本发明所述的特定情感音乐生成方法的一种优选方案，其中：所述输入向量计算过程表示为：；其中，为Token嵌入，/＞为位置编码。作为本发明所述的特定情感音乐生成方法的一种优选方案，其中：所述计算损失函数优化GMVAE模型训练目标，通过最大化GMVAE的证据下界得到，证据下界表示为：。其中，第一项为基于输入向量的重构损失，第二项为模型对复调特征和调式特征的重构损失，第三项为重构两种特征时增加的对抗损失，三者均采用交叉熵损失函数实现，第四项分为两种情况：在无监督方式下，最小化编码器学习的潜在分布/＞与相应情感下的高斯分布/＞之间的KL散度以及用于类别推断的后验分布/＞与类别均匀分布/＞之间的KL散度，KL散度用于衡量两个分布之间的相似度；在有监督方式下，只需最小化前者。此外，超参数/＞用于对该损失项的权重进行调节。作为本发明所述的特定情感音乐生成方法的一种优选方案，其中：所述深度潜在变量模型，利用已学习到的不同音乐属性特征的潜在变量实现音乐情感的变换，只需将源情感下的潜在变量变换为目标情感下的/＞即可，变换的计算方式表示为：。第二方面，本发明提供了一种特定情感音乐生成系统，包括以下模块：输入模块：用于对音乐输入序列进行Token嵌入和位置编码，得到模型输入向量；编码器模块：采用Transformer网络编码组件实现，用于接收输入向量，解耦学习音乐在复调特征和调式特征下的潜在分布，并根据重采样得到相应特征的潜在变量；高斯混合模块：采用高斯混合模型实现，用于以半监督方法在有标签数据集和无标签数据集上学习每组潜在变量上的情感概率分布，完成情感类别的推断；解码器模块：采用Transformer网络解码组件实现，用于聚合特定情感下复调特征和调式特征的潜在变量，并基于该潜在变量生成相应情感的音乐；损失函数模块：用于计算整个GMVAE模型训练目标，通过最大化GMVAE的证据下界得到；情感变换模块：用于实现音乐从某种情感到另一种情感的变换。本发明的有益效果：本发明采用采用半监督聚类方法能够更好提升GMVAE模型泛化性能，利用GMVAE模型的解耦表征机制学习多组不同的音乐特征在情感空间内的潜在分布，以评估这些特征对于情感的影响，达到情感可解释性的目的，并通过改变音乐在某种情感下的潜在变量来实现音乐的情感转换，能够在保证音乐原有内容的基础上向另外一种情感过渡。附图说明为了更清楚地说明本发明实施例的技术方案，下面将对实施例描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本发明的一些实施例，对于本领域普通技术人员来讲，在不付出创造性劳动性的前提下，还可以根据这些附图获得其它的附图。其中：图1为本发明一个实施例提供的一种特定情感音乐生成方法的基本流程示意图；图2为本发明一个实施例提供的一种特定情感音乐生成方法的Arousal-Valence情感模型的示意图；图3 为本发明一个实施例提供的一种特定情感音乐生成方法的高斯混合变分自编码器的流程图；图4为本发明一个实施例提供的一种特定情感音乐生成方法的Transformer模型示意图。具体实施方式为使本发明的上述目的、特征和优点能够更加明显易懂，下面结合说明书附图对本发明的具体实施方式做详细的说明，显然所描述的实施例是本发明的一部分实施例，而不是全部实施例。基于本发明中的实施例，本领域普通人员在没有做出创造性劳动前提下所获得的所有其他实施例，都应当属于本发明的保护的范围。在下面的描述中阐述了很多具体细节以便于充分理解本发明，但是本发明还可以采用其他不同于在此描述的其它方式来实施，本领域技术人员可以在不违背本发明内涵的情况下做类似推广，因此本发明不受下面公开的具体实施例的限制。其次，此处所称的“一个实施例”或“实施例”是指可包含于本发明至少一个实现方式中的特定特征、结构或特性。在本说明书中不同地方出现的“在一个实施例中”并非均指同一个实施例，也不是单独的或选择性的与其他实施例互相排斥的实施例。本发明结合示意图进行详细描述，在详述本发明实施例时，为便于说明，表示器件结构的剖面图会不依一般比例作局部放大，而且所述示意图只是示例，其在此不应限制本发明保护的范围。此外，在实际制作中应包含长度、宽度及深度的三维空间尺寸。同时在本发明的描述中，需要说明的是，术语中的“上、下、内和外”等指示的方位或位置关系为基于附图所示的方位或位置关系，仅是为了便于描述本发明和简化描述，而不是指示或暗示所指的装置或元件必须具有特定的方位、以特定的方位构造和操作，因此不能理解为对本发明的限制。此外，术语“第一、第二或第三”仅用于描述目的，而不能理解为指示或暗示相对重要性。本发明中除非另有明确的规定和限定，术语“安装、相连、连接”应做广义理解，例如：可以是固定连接、可拆卸连接或一体式连接；同样可以是机械连接、电连接或直接连接，也可以通过中间媒介间接相连，也可以是两个元件内部的连通。对于本领域的普通技术人员而言，可以具体情况理解上述术语在本发明中的具体含义。实施例1参照图1-4，为本发明的一个实施例，提供了一种特定情感音乐生成方法，如图1所示，包括以下步骤：S1：获取生成音乐的情感类别，构建MIDI音乐数据集；更进一步的，采用如图2所示的Arousal-Valence情感空间，它从两个维度对情感进行衡量，Arousal维度表示情感的活跃程度，Valence维度表示情感的愉悦程度。因此，本发明使用两种特征分别学习两个情感维度在高维和低维上的表征，并根据这个二维情感空间的四个象限确定为四种情感，分别对应快乐、激动、悲伤、平静。更进一步的，本发明使用了公开的符号化音乐数据集Lakh Pianoroll Dataset，包含174154首多音轨MIDI音乐的钢琴卷帘矩阵，选取了该数据集的一个子集，去除了具有重复内容、不包含钢琴音轨、总时长小于一分钟的音乐，得到91969条数据，其中有23967条能够与Million Song Dataset音频数据集中的音乐相匹配，通过MSD提供的元数据信息以及Spotify音乐服务平台提供的API可获得匹配数据在Russell情感空间中的Arousal值和Valence值，然后将其量化到两个情感维度的高维或低维上，使用标签A0、A1、V0、V1进行标记，其中0代表低维，1代表高维。通过将这四种标签两两组合最终可得到相对应的情感类别。数据集具体统计信息如表1所示：。S2：将原始MIDI数据集转化为特定的结构化数据集，对得到的结构化数据集进行预处理；更进一步的，利用Python开源工具包Pretty_midi提取每一首MIDI音乐中的事件信息，将音乐表示为一组事件序列。由于不同音乐的拍号和速率不同，因此音乐小节的时长和节拍数也不同，无法统一的对每个小节内的事件进行离散化处理。为此，本发明重新定义音乐小节，将其划分为16个离散区间，每个离散区间代表一个子节拍，时长占120 tick，tick为MIDI中计算时间长短的最小单位，因此一首音乐的时间区间集合为{0,120,240,360,...}。接下来，则按以下步骤获取音乐事件并量化到对应的小节与子节拍中：利用Pretty_midi读取MIDI的速率变换矩阵以及每一条音轨中的音符序列，其中速率变换矩阵记录了某个时刻开始音乐的节奏速率，音符序列记录了音符的开始时间、结束时间、音高、力度、持续时长。同时，对所有音符对象按照开始时间与音高进行排序，保证所有音符在时间上有序，并且上述时间以tick为单位。将音符与速率根据开始时间量化到相应的子节拍中，将音符时长统一量化为占n个子节拍的时长，最长为一个小节。量化计算公式为：。其中value为音符、速率的开始时间或音符的原始时长，TICK_RESOL为一个子节拍的时长，设为120 tick，为四舍五入函数。经过计算可得到某一时刻的音符和速率所在节拍的起始时间以及音符所占节拍时长。由于每一种音乐事件有许多不同的取值，为了减少最终的Token数量，对音乐的速率、音高、力度的取值按步长进行了一定的删减，并将原始值偏移到最近的规定取值上，具体如表2所示。根据上述处理好的音符序列与速率变换矩阵创建事件序列，将Bar事件作为每个音乐小节的开始，Beat事件作为每个子节拍的开始，在每个Beat事件中添加相应的速率事件Tempo与音符事件Pitch、Velocity、Duration。此外，使用EOS标记添加到每条序列的末尾来表示序列的结束，使用PAD标记来用于序列的填充。由于LPD为多音轨音乐数据集，Pitch、Velocity、Duration是每条音轨独有的事件，因此使用Pitch-、Velocity-、Duration-表示在特定音轨Track上播放的音符事件。对于LPD数据集，每首音乐最多包含5条音轨，因此最终产生Token数量为923。。更进一步的，对所有音乐事件数据进行预处理。对于训练集、验证集和测试集的划分按照8:1:1的比例。在模型加载数据前，从每个事件序列中随机选取20 个连续的音乐小节作为最终的输入，并设置最大序列长度/＞为1280，不足的使用PAD标记进行填充。此外，为了使数据更具多样性，可对序列中音符的音高进行增强处理，范围设为不等。最终，将处理好的事件序列转换为长度为/＞的Token序列/＞，定义，其中/＞，表示某个Token的量化值。S3：提取每一首MIDI音乐的不同属性特征，构建并训练基于Transformer网络和高斯混合变分自编码器的深度潜在变量模型，生成特定情感的音乐。更进一步的，根据上述步骤得到的事件序列和原始MIDI提取每一首音乐复调特征和调式特征。复调特征为每一个子节拍中同时演奏的音符数量，是当前节拍中开始演奏的音符数量与之前节拍持续下来的音符数量的总和，可通过Duration事件计算得到。调式特征是一组独热向量，代表该MIDI音乐属于24种大小调式其中之一，可利用Pretty_midi从原始MIDI中计算提取。最终，可得到两组属性特征向量用于解耦表征学习。更进一步的，如图3，4所示，模型训练过程可描述为：首先由编码器模块学习输入数据关于不同特征的潜在分布/＞，并根据/＞采样得到潜在变量/＞，其中，分别对应在复调特征和调式特征下的潜在分布；然后由高斯混合模块学习特征约束下数据在Arousal和Valence情感维度上的分布/＞，得到不同情感的数据的潜在空间；最后由解码器模块基于代表特定情感的潜在变量组合/＞，学习样本的真实分布/＞，对其采样得到重构后的新样本。上述生成过程依赖的联合概率分布表示为：/＞。其中，，为/＞上的均匀分布，/＞为类别总数。，为某个情感类簇下的潜在分布，具有可学习的均值/＞和方差/＞。/＞为输出的条件分布，由神经网络进行参数化。下面基于上述过程对各个模块进行详细设计：更进一步的，输入模块包含两部分，首先，对事件序列进行Token嵌入，嵌入维度设为512，即每一个Token使用512维向量表示；然后，为/＞添加位置编码学习事件的时序关系，在Transformer中使用正余弦函数来生成位置编码信息，每个位置索引也用512维向量表示；最后，将Token嵌入的值与位置编码信息相加得到输入向量/＞并输入到编码器模块中，其中/＞，/＞为嵌入维度，也是整个模型的维度。整个计算过程可表示为：/＞。其中，为Token嵌入，/＞为位置编码。更进一步的，编码器模块利用Transformer编码组件实现，可通过解耦机制将复调特征和调式特征从输入向量中剥离出来，单独学习音乐关于这两种特征的潜在变量/＞，其结构设定如下：编码器层数/＞为12、每层编码器的多头自注意力机制头数/＞为8、前馈神经网络层数为1、隐藏层的维度/＞为512 、FFN层上升特征维度/＞为2048。编码器的核心是MHA层，每组注意力单独维护三个不同的矩阵分别为/＞、/＞、/＞，由输入向量/＞与三个可学习权重矩阵相乘得到，即，，/＞，其中/＞，/＞；然后根据公式使用、/＞、/＞计算得到每组注意力的分数矩阵/＞，其中；接着，根据公式将所有的/＞拼接在一起与权重矩阵/＞相乘得到矩阵/＞，其中/＞，矩阵/＞则融合了所有注意力头的信息；最后，将矩阵/＞输入到残差连接层、归一化层和FFN层进行下一步的计算。在经过多层编码器学习后，最终可输出隐藏状态矩阵/＞，其中/＞，为/＞时刻的隐藏状态，/＞为序列长度。计算单组注意力分数与多头注意力分数分别可表示为：；。更进一步的，为了得到每种特征相对应的潜在变量，可单独利用上述的编码组件进行建模。对于每一种特征，学习一个潜在后验分布q，通过变分推断来近似相应情感下的高斯分布p。因此，可根据公式利用编码组件输出的某一时刻的隐藏状态ht学习两个权重矩阵Wμ、Wσ来预测q的均值向量μ和标准差向量σ，其中dz为潜在变量维度，设为256。值得注意的是，模型并不直接拟合σ，而是拟合logσ2，因为σ是非负的，需要激活函数处理，而拟合logσ2不需要激活函数。接下来，根据μ和σ建立高斯分布/＞即潜在分布q，并在训练过程中逐渐逼近相应情感下的高斯分布。最后，对q进行采样得到潜在变量zi，/＞由于采样的过程在反向传播中是不可导的，因此利用VAE模型的重参数化技巧，从标准高斯分布中采样高斯噪声ε，将其按照公式变换到/＞中，如此可在变换过程中对μ和σ求偏导，完成反向传播。计算均值向量μ、标准差向量σ以及重采样的潜在变量z分别可表示为：μ＝htWμ logσ2＝htWσ更进一步的，高斯混合模块的作用是以半监督方式完成对输入数据在Arousal和Valence情感维度上的类别推断，再生成相应情感的音乐。因此，通过后验分布学习每个情感维度上的数据分布信息，但这并不引入一个新的神经网络，而是利用潜在变量近似得到，近似方式可表示为：。其中，表示潜在变量/＞在相应情感维度上的概率分布，即通过潜在变量完成类别推断。在实现过程中，首先初始化每一个情感高斯分量的均值向量/＞和方差向量/＞，然后通过贝叶斯公式计算得到。/＞的计算方式可表示为：。其中，为/＞的概率密度函数，计算方式可表示为：。更进一步的，在训练过程中，首先使用少量有标签数据集有监督的学习数据在每个情感维度上的真实分布，然后将这种约束扩展到无标签数据集上。对于无标签数据集，同样使用变分推断来近似均匀分布/＞。根据上述过程，最终可推断出潜在变量的类别，并将/＞送入解码器模块中生成相应情感的音乐。更进一步的，解码器模块用于对潜在变量解码输出，采用Transformer解码组件实现，包含多个解码器层，其结构设置与编码器模块类似，不同的是解码器模块在训练时采用并行化处理方式，因此目标数据输入为/＞，即目标Token序列的前/＞项，而在预测时采用串行化处理方式，初始输入为起始Token标记，后续输入则为上一次预测得到的Token序列。此外，每层解码器中包含两层多头注意力机制，其中第一层MHA使用了掩码机制，能够防止模型在训练过程中生成第/＞个Token时，提前知道第/＞个之后的Token信息，保证了模型的可学习性。第二层MHA的/＞、/＞、/＞输入来自不同的地方，其中根据第一层MHA的输出计算得到，/＞、/＞则通常根据编码器模块的输出的潜在变量组合计算得到，即/＞。经过多层解码器学习后，将输出的隐藏状态输入到线性层和SoftMax层得到解码后的Token序列。最后，利用Pretty_midi工具包将Token序列还原为MIDI格式的音乐。更进一步的，为了能够对复调特征和调式特征的潜在变量解耦，确保每一个潜在变量只包含与其相对应特征的信息，同样利用解码器模块基于某个潜在变量/＞重构相应特征，并增加对抗性训练去除/＞中无关特征信息。具体地，对于/＞的解码器/＞，在进行重构特征时，分别以/＞作为 /＞的输入，目标是最小化重构损失，以保留相关特征信息；在进行对抗训练时，以/＞分别作为/＞的输入，目标是最大化对抗损失，以去除无关特征信息。更进一步的，损失函数模块用于计算整个高斯混合变分自编码器模型训练目标，通过最大化GMVAE的证据下界得到。ELBO可表示为：。其中，第一项为基于输入向量的重构损失，第二项为模型对复调特征和调式特征的重构损失，第三项为重构两种特征时增加的对抗损失，三者均采用交叉熵损失函数实现，第四项分为两种情况：在无监督方式下，需要最小化编码器学习的潜在分布/＞与相应情感下的高斯分布/＞之间的KL散度以及用于类别推断的后验分布/＞与类别均匀分布/＞之间的KL散度，KL散度用于衡量两个分布之间的相似度；在有监督方式下，只需最小化前者。此外，超参数/＞用于对该损失项的权重进行调节。更进一步的，基于训练好的情感音乐生成模型实现音乐的情感变换。对于一首MIDI音乐，输入到模型中可得到关于复调特征和调式特征分别在Arousal和Valence情感维度上的潜在变量。当需要变换情感时，可按式计算每一个维度上目标情感相应的高斯分布均值/＞与当前情感相应的潜在分布均值/＞之间的差值，然后将其变换到当前情感的潜在变量/＞上，即可得到目标情感对应的潜在变量/＞，最终将/＞组合后送入模型解码器模块解码输出。情感变换计算方式可表示为：/＞。本实施例还提供一种特定情感音乐生成系统，包括以下模块：输入模块：用于对音乐输入序列进行Token嵌入和位置编码，得到模型输入向量；编码器模块：采用Transformer网络编码组件实现，用于接收输入向量，解耦学习音乐在复调特征和调式特征下的潜在分布，并根据重采样得到相应特征的潜在变量；高斯混合模块：采用高斯混合模型实现，用于以半监督方法在有标签数据集和无标签数据集上学习每组潜在变量上的情感概率分布，完成情感类别的推断；解码器模块：采用Transformer网络解码组件实现，用于聚合特定情感下复调特征和调式特征的潜在变量，并基于该潜在变量生成相应情感的音乐；损失函数模块：用于计算整个GMVAE模型训练目标，通过最大化GMVAE的证据下界得到；情感变换模块：用于实现音乐从某种情感到另一种情感的变换。应说明的是，GMVAE向变分自编码器中引入了高斯混合模型，GMM使用了多个高斯分布的组合来刻画数据分布，能够实现音乐情感的半监督聚类，而原有的VAE模型中只有一个单峰高斯分布，不利于学习音乐在多种情感类别下的分布信息。此外，GMVAE的解耦表征机制能够学习多组不同的音乐特征在情感空间内的潜在分布，从而评估这些特征对于情感的影响，因此GMVAE为决定性指标。实施例2参照表3-6，为本发明的一个实施例，提供了一种特定情感音乐生成方法，为了验证其有益效果，将基于Transformer网络的GMVAE模型与现有的比较先进的深度潜在变量生成模型从多个不同的角度进行对比以评估本发明模型的性能。首先对模型的半监督方法以及整体性能进行了评估，选取的半监督模型有Semi-VAE、Classifier-VAE，监督模型有CVAE、ClaviNet、GMVAE。为了更公平的进行对比，这些模型的主体结构也都重新采用Transformer网络，然后从监督方法和半监督方法两个方面评估生成音乐在Arousal和Valence情感维度上的精确度，并且半监督方法在使用不同数量的标签数据下显示了不同的效果。对于精确度的衡量，本发明采用了较先进的MIDI音乐情感分类模型 EMPOIA-Cls，具体性能如表3所示。实验证明，当采用半监督方法时，只需少量有标签数据与其他无标签数据结合，模型即可达到与监督方法相似甚至更好的性能。由于其他模型都直接基于情感标签生成音乐，没有考虑音乐属性特征对于情感的影响，而GMVAE采用了特征解耦机制，建立了特征与情感之间的联系，进而生成相应情感的音乐，因此性能整体上高于其他模型。此外，从表3中还可以发现，所有模型在Arousal维度上的生成效果总是高于Valence维度，即节奏比调式更能决定快乐和悲伤，复调特征即反映了节奏的快慢，与Arousal相关，而调式则与Valence相关。。然后对模型生成的音乐是否具有结构感知性进行评估，选取的模型主体结构主要包括基于RNN网络和基于Transformer网络、GMVAE-Trans)两种。结构感知性意味着生成音乐需要考虑上下文结构，在生成第个小节时，需要考虑前个小节的影响。因此，本发明随机选取了两首长度相等但内容差异性较大的音乐样本X1、X2，将编码器输出的两个潜在变量表示的前32维度互相交换，以评估对于后续生成的影响，具体评估效果如表4所示。对于评估指标，分别采用音高范围、复调性、节奏密度，音高范围定义为最大音高与最小音高的差距，复调性定义为每个音乐节拍中同时播放的音符的平均数量，节奏密度定义为包含开始状态音符的音乐节拍占所有节拍的比例，通常一个音符按状态可分为开始、保持、休止。实验证明，在交换X1和X2的部分潜在变量内容后，基于Transformer网络的模型对于X1的重构在三种指标上更接近X2，对于X2的重构更接近于X1，而基于RNN网络的模型在三种指标上则变化较小，这说明了将深度潜在变量模型与Transformer网络结合能够更好的学习音乐序列的长期依赖关系。。接着对音乐情感转换效果进行评估，首先在测试集中选取了1000个音乐样本，并使用GMVAE模型在快乐、激动、悲伤、平静四种情感下进行相互转换，对最终的结果使用了MIDI音乐情感分类模型EMPOIA-Cls进行了精确度的评估，得到混淆矩阵，具体结果如表5所示。结果显示，在Arousal维度上进行变换精确度最高，在Valence维度上变换精确度较低，而在两个维度上都进行变换精确度最低，这也与中音乐在Valence维度上的生成效果较低一致。。最后对模型特征解耦能力进行评估，主要方法为保持一种特征不变的情况下改变另一种特征，进而对比生成前后特征的差异。因此，本发明按上述方式选择将快乐情感转换为平静情感以评估复调特征的变化，将平静情感转换悲伤情感以评估调式特征的变化。对于特征变化的评估，则通过计算复调特征的潜在变量和调式特征的潜在变量/＞在生成前后的余弦相似度得到，具体结果如表6所示。例如，在快乐到平静的转换过程中，由于改变了复调特征的潜在变量，因此生成前后的/＞相似度较小，而调式特征未改变，因此相似度较高。由此可见，GMVAE模型能够成功的将复调特征和调式特征从输入序列中剥离出来，从而利于音乐情感的把控。。应说明的是，以上实施例仅用以说明本发明的技术方案而非限制，尽管参照较佳实施例对本发明进行了详细说明，本领域的普通技术人员应当理解，可以对本发明的技术方案进行修改或者等同替换，而不脱离本发明技术方案的精神和范围，其均应涵盖在本发明的权利要求范围当中。
