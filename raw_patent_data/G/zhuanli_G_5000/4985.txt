标题title
基于深度强化学习的自动驾驶车辆交叉口无冲突合作方法
摘要abst
本发明属于自动驾驶技术领域，具体为基于深度强化学习的自动驾驶车辆交叉口无冲突合作方法，包括步骤1：交叉口问题马尔可夫建模，综合考虑安全约束马尔可夫决策与马尔可夫博弈理论，将道路交叉口环境转化为为符合强化学习算法要求的模型；步骤2：单策略网络与双价值网络更新过程设计，设计单策略‑双评论家网络架构；步骤3：马尔可夫形式数据搜集；步骤4：强化学习训练，训练单策略‑双评论家神经网络；步骤5：强化学习测试，在仿真器Carla中实际测试该强化学习算法的性能，能够针对时变的交通网络拓扑结构输出符合预期的车辆通行策略，在安全、舒适与效率方面均能够获得良好性能。
权利要求书clms
1.基于深度强化学习的自动驾驶车辆交叉口无冲突合作方法，其特征在于：包括如下步骤：步骤1：交叉口问题马尔可夫建模，综合考虑安全约束马尔可夫决策与马尔可夫博弈理论，将道路交叉口环境转化为为符合强化学习算法要求的模型；步骤2：单策略网络与双价值网络更新过程设计，设计单策略-双评论家网络架构，以满足高随机动态道路交叉口环境中对安全性的需求：步骤3：马尔可夫形式数据搜集，实时搜集当前环境中所有车辆离开道路交叉口的距离、预期转向、车速以及当前环境中存在的车辆作为强化学习算法的状态空间；步骤4：强化学习训练，训练单策略-双评论家神经网络，通过CMDP环境信息来不断更新神经网络，最终使策略收敛，在高动态的环境中也能实时输出安全、高效、舒适的策略；步骤5：强化学习测试，在仿真器Carla中实际测试该强化学习算法的性能，并与传统MPC方法对比，重点测试公开的策略在计算实效性、安全性、舒适性与通行效率等方面相对于传统方法的优势。2.根据权利要求1所述的基于深度强化学习的自动驾驶车辆交叉口无冲突合作方法，其特征在于：所述步骤2中，策略网络用于输出控制行为，价值网络critic1用于评估全局环境的安全、高效与舒适性，critic2用于专门评估全局环境的安全性。3.根据权利要求1所述的基于深度强化学习的自动驾驶车辆交叉口无冲突合作方法，其特征在于：所述步骤3中，在车辆与环境不断交互的过程中搜集全局奖励函数与安全函数，将搜集到的信息组合形成轨迹以作为神经网络的输入。
说明书desc
技术领域本发明涉及自动驾驶技术领域，具体为基于深度强化学习的自动驾驶车辆交叉口无冲突合作方法。背景技术随着智能化与网联化的发展，多辆自动驾驶汽车通过车联网协作通行的方式将改变未来交通管理和组织的模式，尤其是在交叉路口。自动驾驶车辆在基于信号灯的道路交叉口中往往有着较低的通行效率，由于部分车辆可能会由于红灯而产生许多无意义的等待。相比之下，在无信号灯道路交叉口环境中的自动车辆协作通行近几年得到了许多研究。该方式赋予自动驾驶车辆充分的自主权，能够最大化提升交通场景整体通行效率。然而，由于缺乏交通信号灯的管控，在复杂高动态的道路交叉口中保证多车通行的安全性面临较强的挑战性。目前在无信号道路交叉口自动驾驶车辆协作通行方面，现有公开的技术包括模型预测控制算法、前馈-反馈控制、预测维仿真算法等。该类方法通过将道路交叉口下多台车辆安全、舒适、高效通行问题建模为带约束的优化问题，并通过求解该带约束的优化问题来为交通场景中的每台车辆提供实时通行方式。然而当交通场景中车辆数目较多时求解NP-hard问题需要巨大的计算，求解往往需要较长时间，在实际交通场景中难以部署。作为当下在各个领域均获得广泛关注的技术，强化学习方法在自动驾驶领域得到了越来越多的关注。自动驾驶的本质是在没有驾驶员参与的情况下，仅根据车辆对环境的感知来输出节气门开度、方向盘转角、刹车片开合力度等车辆控制量。在道路交通场景越来越复杂与传统基于解约束问题的计算方法的矛盾越来越显著的情况下，利用神经网络来进行决策正成为重要的解决方案，通过在高维度的环境中不断探索最优策略，经过训练的强化学习模型能够以极少的计算成本输出实时、高效、安全的道路交叉口通行策略。基于上述问题，我们提出一种基于深度强化学习的自动驾驶车辆交叉口无冲突合作方法。发明内容本部分的目的在于概述本发明的实施方式的一些方面以及简要介绍一些较佳实施方式。在本部分以及本申请的说明书摘要和发明名称中可能会做些简化或省略以避免使本部分、说明书摘要和发明名称的目的模糊，而这种简化或省略不能用于限制本发明的范围。鉴于现有技术中存在的问题，提出了本发明。因此，本发明的目的是提供基于深度强化学习的自动驾驶车辆交叉口无冲突合作方法，能够针对时变的交通网络拓扑结构输出符合预期的车辆通行策略，在安全、舒适与效率方面均能够获得良好性能。为解决上述技术问题，根据本发明的一个方面，本发明提供了如下技术方案：基于深度强化学习的自动驾驶车辆交叉口无冲突合作方法，其包括如下步骤：步骤1：交叉口问题马尔可夫建模，综合考虑安全约束马尔可夫决策与马尔可夫博弈理论，将道路交叉口环境转化为为符合强化学习算法要求的模型；步骤2：单策略网络与双价值网络更新过程设计，设计单策略-双评论家 网络架构，以满足高随机动态道路交叉口环境中对安全性的需求：步骤3：马尔可夫形式数据搜集，实时搜集当前环境中所有车辆离开道路交叉口的距离、预期转向、车速以及当前环境中存在的车辆作为强化学习算法的状态空间；步骤4：强化学习训练，训练单策略-双评论家 神经网络，通过CMDP环境信息来不断更新神经网络，最终使策略收敛，在高动态的环境中也能实时输出安全、高效、舒适的策略；步骤5：强化学习测试，在仿真器Carla中实际测试该强化学习算法的性能，并与传统MPC方法对比，重点测试公开的策略在计算实效性、安全性、舒适性与通行效率等方面相对于传统方法的优势。作为本发明所述的基于深度强化学习的自动驾驶车辆交叉口无冲突合作方法的一种优选方案，其中：所述步骤2中，策略网络用于输出控制行为，价值网络critic1用于评估全局环境的安全、高效与舒适性，critic2用于专门评估全局环境的安全性。作为本发明所述的基于深度强化学习的自动驾驶车辆交叉口无冲突合作方法的一种优选方案，其中：所述步骤3中，在车辆与环境不断交互的过程中搜集全局奖励函数与安全函数，将搜集到的信息组合形成轨迹以作为神经网络的输入。与现有技术相比，本发明的有益效果是：考虑到传统基于实时计算的传统方法计算效率第的问题，本发明采用基于强化学习的多智能体约束策略优化算法，通过具备映射功能的神经网络为交通场景中的所有智能联网车辆提供实时、安全、高效、舒适的策略；本发明通过奖励函数来引导交通场景中的所有智能体在整体上表现出安全、舒适、高效的通行模式，与传统强化学习方法不同，本发明在奖励函数的基础上额外引入安全函数，通过安全函数来引导交通场景中的所有智能联网车辆避免执行存在潜在碰撞风险的行为，进而提高了道路交叉口处车辆通行的安全性；为了让算法应用于更贴近现实的交通场景，本发明面向高动态变化的道路交叉口，在传统强化学习方法考虑的距离与车速的基础上，通过在状态空间额外增加转向、交叉口处车辆数目等多维度信息，实现场景动态性，仿真结果表明本发明能够很好地保证高动态变化道路交叉口环境的安全性、高效性与舒适性。附图说明为了更清楚地说明本发明实施方式的技术方案，下面将结合附图和详细实施方式对本发明进行详细说明，显而易见地，下面描述中的附图仅仅是本发明的一些实施方式，对于本领域普通技术人员来讲，在不付出创造性劳动性的前提下，还可以根据这些附图获得其它的附图。其中：图1为本发明基于深度强化学期的自动驾驶车辆道路交叉口无冲突合作策略架构图；图2为本发明基于单价值-双评论家网络的强化学习算法伪代码图；图3为本发明道路交叉口环境图；图4为本发明道路交叉口车辆Ni建模图；图5为本发明基于单价值-双评论家网络的强化学习算法在无信号道路交叉口控制的结果；图6为本发明MPC算法在无信号道路交叉口控制的结果。具体实施方式为使本发明的上述目的、特征和优点能够更加明显易懂，下面结合附图对本发明的具体实施方式做详细的说明。在下面的描述中阐述了很多具体细节以便于充分理解本发明，但是本发明还可以采用其他不同于在此描述的其它方式来实施，本领域技术人员可以在不违背本发明内涵的情况下做类似推广，因此本发明不受下面公开的具体实施方式的限制。其次，本发明结合示意图进行详细描述，在详述本发明实施方式时，为便于说明，表示器件结构的剖面图会不依一般比例作局部放大，而且所述示意图只是示例，其在此不应限制本发明保护的范围。此外，在实际制作中应包含长度、宽度及深度的三维空间尺寸。为使本发明的目的、技术方案和优点更加清楚，下面将结合附图对本发明的实施方式作进一步地详细描述。本发明提供如下技术方案：基于深度强化学习的自动驾驶车辆交叉口无冲突合作方法，能够针对时变的交通网络拓扑结构输出符合预期的车辆通行策略，在安全、舒适与效率方面均能够获得良好性能实施例1步骤一交叉口问题马尔可夫建模该步骤综合考虑安全约束马尔可夫决策与马尔可夫博弈理论，将车辆轨迹与数目均随机的高动态道路交叉口环境转化为符合强化学习算法的模型。在高随机性动态变化的道路交叉口环境中，来自各个路口的车辆均有不同可能性的目标空间。如图3所示，假设道路交叉口环境最大的车辆容量为Nmax，交叉口中第i辆智能网联汽车在时刻t的状态空间包括车速信息转向信息与是否处于强化学习管辖范围信息dc为强化学习管控区长度。在本发明提出的理论中，道路交叉口中N个智能网联汽车为道路交叉口全局的安全、高效与舒适而协同合作，定义道路交叉口环境状态空间其中ci表征道路交叉口中车辆i在某回合中是否存在，若车辆i不存在，则与其相关的离开路口路程di，vi表示车速，在通过道路交叉口时的预期转向diri均取0值；定义定义道路交叉口环境动作空间奖励函数与开销函数作为引导道路交叉口环境中的所有车辆安全、高效、舒适通行的关键，本发明在综合考虑上述三种指标后，为了在V2I框架下综合协同控制交通场景中的所有车辆，本发明在使用奖励函数来表示交通场景中综合安全、高效舒适的同时，使用开销函数来表示交通场景中的危险行为。当交通环境中的任意有碰撞可能性的两台车的距离小于预先设定的阈值时开销函数值加一较小的值；若交通场景中发生碰撞，回合结束，开销函数加一较大的值。总开销函数定义为：c＝cd+cs 使用交通场景中车辆的加速度绝对值来表示驾驶员的驾驶舒适性；使用车辆速度来表示交通场景中的效率；若车辆全部通过交叉口，给予较大的奖励函数值。总奖励函数定义为：r＝ra+rv+rs 步骤二单策略网络与双价值网络更新过程设计本发明提出了全新的Actor-Critic1-Critic2架构，其中Actor是当前策略的执行者，当前策略πk将MDP形式的状态映射为动作，在智能体执行动作的过程中搜集一系列的轨迹用于策略神经网络的更新，搜集的轨迹还可以通过价值函数来评估当前策略，critic1基于环境中设定的全局奖励函数，这保证了策略能够稳定提升，鼓励当前策略向安全、高效与舒适的方向演进，而critic2 则基于环境中设定的全局安全函数，用于惩戒当前策略，使道路交叉口中的所有车辆尽可能避免潜在的危险策略。策略网络与价值网络均需要经过更新来优化当前算法。对于当前策略网络πk，利用在第k次策略迭代时收集到的轨迹分别计算当前策略基于奖励函数与损失函数的梯度g与b，基于KL散度限制δ与黑塞矩阵H 来更新策略。具体策略更新步骤为：计算与策略安全相关的策略风险度f与策略更新度G：若b＜10-8或G＜0&amp;f＜0，策略符合安全约束，使用下式更新：若G＞0&amp;f＜0或G＞0&amp;f＞0，策略部分符合安全约束，使用下式更新：λ*与v*为策略迭代时与安全有关的中间变量。若G＜0&amp;f＞0，当前策略更新不满足安全约束，利用下式更新：基于奖励函数的价值函数ρR与基于开销函数的价值函数ρC的更新可以引导策略网络的更新，其更新方式为：本发明提出安全势能的概念。当车辆的间距小于某个值或者违背TTC就会积累安全势能，当势能积聚到某个临界值交通场景中就会发生碰撞。该网络架构能够让安全势能低于交通事故发生的临界值。通过对交通环境中的安全势能的有效控制，该网络架构极大程度上提升交通场景中的安全性。步骤三马尔可夫形式数据搜集本发明研究高复杂度的道路交叉口环境，该环境中车辆的最大容量为 Nmax＝8，且每台车辆均随机出现在交叉口中，并各自有随机的可能转向。在马尔可夫博弈理论的指导下，本发明搜集当前时刻下交通环境中的每一台车辆的局部观测空间。如图4所示，对于在时间t时的任意车辆Ni，其有左转、直行、右转三种可能的转向。本发明假设车辆在进入道路交叉口前已经预知未来的转向，并提前通过V2I通信将转向信息发送给路侧单元。在提前知晓车辆Ni转向的情况下，计算车辆实时离开道路交叉口的距离并在车辆Ni与道路和交通环境中中其它车辆交互的过程中搜集实时奖励函数Ri与开销函数Ci，以此形成车辆Ni的局部观测空间在获得交通场景中所有车辆的局部观测空间后，使用特定智能体状态修剪方法获得全局状态空间步骤四强化学习训练为了增大道路交叉口的动态性，本发明设置环境中出现的每一台车辆均有随机初始位置与车速，在强化学习训练的每一回合，每个路口所容纳的车辆数目均随机，但规定道路交叉口对车辆的最大容量为Nmax，且如图4所示，每一台车辆在进入交叉口时均有随机的预期转向，随机的预期转向与随机的初始位置与车速增大了车辆离开交叉口距离的随机性，多智能体局部状态空间的组合让本发明中全局状态空间的随机性最大化，能够检验基于安全约束的多智能体强化学习算法在实际道路环境中的效能。强化学习算法集成于路侧单元中，通过V2I通信获取交叉口中全部车辆状态信息并集中控制车辆。在强化学习训练的过程中，所有车辆将实时将自车局部状态空间Oi上传到路侧单元，路侧单元通过FP方法修剪整合获得的局部状态空间获得全局状态空间S作为强化学习算法的输入，算法进而输出当前策略下车辆的动作空间，由路侧单元向交叉口中的所有车辆发送控制信息。步骤五强化学习测试本发明基于仿真器来模拟随机道路交叉口场景，借此来测试强化学习算法的性能。交通场景中的所有车辆是否生成均由仿真器内置的随机数决定，对于确定生成的车辆i，在交叉口道路中使用正态分布选取坐标作为车辆的初始位置：使用正态分布为车辆i选取随机车速：对于交通场景中所有车辆的控制问题，在获得车辆i的预期车速v′i与预期转向diri后使用PID控制策略将其转化为车辆i的方向盘转角δi与油门开合度θi，进而控制对应车辆：为了凸显基于单策略于双价值神经网络的强化学习算法优越性，本发明设计与传统MPC算法的对比实验。在本发明关注的道路交叉口安全性、通行高效性、驾驶舒适性与计算的实效性方面做了检验。安全性方面，其为交通场景中需要关注的首要问题，关系到驾驶安全，本发明通过交通场景中的平均碰撞率来表征；通行高效性方面，其关系到交通场景车流运输的效率，较高的通行效率能够保证在相同时间内，道路交叉口可以让更多的车辆通行，通过车辆由进入强化学习管控区到离开交叉口的平均时间来表征；舒适性方面，其关系到驾驶人员的驾驶体验，车辆突然性的加速或减速均会使驾驶人员感到不适，通过车辆的平均加速度来表征；计算实效性方面，其衡量算法是否能够应用于高动态的环境中，良好的算法应该能够给出即使且有效的解空间，计算实效性通过算法给出的相邻两次解的平均时间间隔来表征。仿真验证仿真平台为Carla结合PyTorch框架，具体试验参数设置如表1所示。表 2为本发明公开基于深度强化学习的策略与传统MPC算法在安全性、高效性、舒适性及计算实效性等方面的对比结果。结果显示，提出的策略在上述四个方面均领先传统MPC算法，稳定收敛后强化学习算法实现了0碰撞率，通行效率为MPC算法的4.84倍，驾驶舒适性为MPC算法的1.55倍，计算效率为 MPC算法的70.76倍。图5与图6为稳定收敛后强化学习算法与MPC算法在某一随机回合的通行效果图，本发明衡量车辆行驶中的离开交叉口的距离、速度、加速度与违背安全约束的次数。综合对比观察可以发现MPC算法倾向于保守的交叉口通行策略，在前车完全离开交叉口前，后方有潜在碰撞可能性的车辆为静止状态，这极大降低了交通场景通行效率。同时，由于MPC算法的计算效率较低，算法往往不能动态把握交通场景的动态变化，这就导致了车辆出现急加速或急减速的情况，在一定程度上降低了驾驶体验。MPC算法计算效率低同时还会带来交通场景中安全性能降低的问题，因为MPC算法不能提供针对当前环境的实时解空间。表1.试验参数设置表2.本发明对比MPC的性能优势虽然在上文中已经参考实施方式对本发明进行了描述，然而在不脱离本发明的范围的情况下，可以对其进行各种改进并且可以用等效物替换其中的部件。尤其是，只要不存在结构冲突，本发明所披露的实施方式中的各项特征均可通过任意方式相互结合起来使用，在本说明书中未对这些组合的情况进行穷举性的描述仅仅是出于省略篇幅和节约资源的考虑。因此，本发明并不局限于文中公开的特定实施方式，而是包括落入权利要求的范围内的所有技术方案。
