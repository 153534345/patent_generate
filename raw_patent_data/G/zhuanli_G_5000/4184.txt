标题title
一种基于深度强化学习和逆强化学习的交通灯控制方法
摘要abst
本发明公开了一种基于深度强化学习和逆强化学习的交通灯控制方法，首先建立交通灯控制系统的马尔科夫决策模型，并依据现有深度网络模型，搭建基于深度强化学习的交通灯控制框架。本发明的创新点在于引入了相对熵逆强化学习算法以优化奖励函数设计。根据专家决策生成的系统状态转移轨迹，通过逆强化学习算法提取专家内含的决策逻辑，即隐藏奖励函数，实现了对专家经验的有效利用，算法对专家轨迹中的噪声具有较好的鲁棒性。本发明能够在单个交叉路口的均衡车流和非均衡车流场景下，取得优于传统控制方案的效果，并进一步提升深度强化学习算法的控制性能。
权利要求书clms
1.一种基于深度强化学习和逆强化学习的交通灯控制方法，其特征在于：该方法包括以下步骤：步骤1：设置超参数，搭建双决斗深度Q神经网络，初始化主网络和目标网络；步骤2：初始化交通灯控制系统，初始化相位长度；步骤3：交通灯控制系统按本周期预设各相位长度进行相位切换，完成一周期运行；获取本周期状态，利用主网络评估各动作价值，并依据ε-贪心方式选择动作，即：以概率ε随机选择动作，以概率1-ε选择价值最大的动作执行；步骤4：智能体获得奖励，保存上一周期状态、上一周期动作、上一周期奖励、本周期状态，作为样本放入重放缓存区；步骤5：若重放缓存区满，则依据样本优先级，从重放缓存区中抽取小批量样本，进行神经网络训练和参数更新；步骤6：根据步骤3的动作选择结果，设置交通系统下一周期各相位长度；步骤7：重复步骤3-6，当本回合结束后，记录本回合折扣回报；步骤8：当训练回合数达到预设值后停止。2.根据权利要求1所述基于深度强化学习和逆强化学习的交通灯控制方法，其特征在于：步骤1中使用的主网络对状态s下动作a的价值Q表示为：其中θ，θv，θa是主网络参数，V表示当前状态下的未来期望回报，A表示状态s下采取动作a的期望回报，表示动作空间，表示动作空间维数；目标网络的动作价值近似为其网络参数初值与主网络参数θ，θv，θa相同。3.根据权利要求1所述基于深度强化学习和逆强化学习的交通灯控制方法，其特征在于：步骤4中使用的奖励函数由F个特征线性加权组合而成：其中，s是本周期状态量，fi是关于状态s的特征函数f的第i维分量，此处取f＝s，wi为权重，wi的计算过程使用相对熵逆强化学习实现，具体包含以下步骤：步骤4.1：配置交通灯控制环境，执行专家策略，生成长度为L的专家轨迹τE共NE条；执行均匀策略，生成长度为L的采样轨迹τS共NS条，在均匀策略中，状态st条件下动作at的执行概率为#)表示状态st下可选动作总数；步骤4.2：提取专家轨迹折扣特征之和、采样轨迹折扣特征之和，其中，专家轨迹折扣特征之和定义为其中为专家轨迹t时刻所处状态，γ为折扣因子，取γ＝0.9；采样轨迹折扣特征之和定义为其中为采样轨迹t时刻所处状态；步骤4.3：计算NE条专家轨迹的特征期望折扣和：其中表示第j个专家在t时刻的状态；步骤4.4：初始化奖励函数权重wi；步骤4.5：搜索的最大最小值，计算∈i：其中Δ表示的置信概率上界，其中D是专家轨迹集合，是对奖励函数权重向量为w时专家轨迹τE的发生概率的估计，实际取Δ＝0.1；步骤4.6：计算权重更新次梯度：其中是采样联合策略，用来产生NS条轨迹τS，采样过程如步骤4.1所述；αi由wi的符号决定，当wi≥0时，αi＝1；当wi＜0时，αi＝0；步骤4.7：更新奖励函数权重wi，其中学习率lr＝0.01；步骤4.8：重复步骤4.6-4.7，直到奖励函数权重wi收敛。4.根据权利要求1所述基于深度强化学习和逆强化学习的交通灯控制方法，其特征在于：步骤5中样本优先级定义如下：定义TD目标Q值为：其中a表示状态s′下的可选动作集合；TD误差表示为：δ＝Qtarget-Q其中si，ai分别表示第i个样本的状态值和动作值；对于重放缓存区内的第k个样本，根据其TD误差的绝对值大小进行降序排列，定义其优先级pk为排序序号的倒数；若重放缓存区满，则依据样本优先级，从重放缓存区中抽取小批量样本，并进行神经网络训练和参数更新，具体步骤为：步骤5.1：计算第k个样本被采样的概率为：其中μ是超参数指数，取为1；R是重放缓存区大小，取为20000；步骤5.2：在重放缓存区中，依据每个样本被采样的概率，抽取小批量样本，批大小B＝64；步骤5.3：主网络参数以小批量样本上的均方TD误差J＝-Q)2为损失函数进行反向传播更新；步骤5.4：目标网络参数θ-，以主网络参数为更新目标，按以下方式更新：θ-＝aθ-+θ其中α是超参数学习率，取α＝0.001；步骤5.5：更新ε-贪心方式的探索率ε：ε＝max其中εmax和εmin为预设的ε可取的最大值与最小值，nt为当前训练步数，εstep为下降步长；取εmax＝1，εmin＝0.01，εstep＝10000。
说明书desc
技术领域本发明属于智能交通控制领域，特别涉及了一种基于深度强化学习和逆强化 学习的交通灯控制方法。背景技术传统的交通灯多采用固定相位顺序与相位时长的控制模式，面对动态环境的 灵活性较差，难以有效解决交通拥堵问题。为缓解拥堵和提高通行效率，深度强 化学习已被应用于交通灯调控问题中，并取得了不少优于传统控制方案的成果， 这是本发明的技术基础。选择合适的奖励函数是深度强化学习算法实现的一大关键，它影响着算法收 敛性、训练效率和控制性能，而现有交通灯控制方案中，奖励函数的设计大多依 赖经验和反复测试，缺少高效率、低人工的解决方案。逆强化学习方法利用专家 控制产生的示例轨迹，提取出隐含的奖励函数信息，并用于深度强化学习的训练 过程中，是一种自适应的奖励函数设计方法。通过将专家经验融入决策过程，逆 强化学习能够进一步优化深度强化学习的算法表现。在逆强化学习算法中，基于熵的逆强化学习算法能够找到专家示例轨迹对应 的唯一最优策略，其应用价值较大。由于交通灯控制的环境模型是未知的，而相 对熵逆强化学习算法适用于解决无模型问题，因此，本发明利用该算法来实现奖 励函数的构造。发明内容为了解决上述背景技术提出的技术问题，本发明研究了基于深度强化学习和 逆强化学习的交通灯控制方法，旨在缓解道路拥堵，提高通行效率。为了实现上 述技术目的，本发明的技术方案如下：一种基于深度强化学习和逆强化学习的交通灯控制方法，其特征在于：该方 法包括以下步骤步骤1：设置超参数，搭建双决斗深度Q神经网络，初始化主网络和目标网 络；步骤2：初始化交通灯控制系统，初始化相位长度；步骤3：交通灯控制系统按本周期预设各相位长度进行相位切换，完成一周 期运行；获取本周期状态，利用主网络评估各动作价值，并依据ε-贪心方式选择 动作，即：以概率ε随机选择动作，以概率1-ε选择价值最大的动作执行；步骤4：智能体获得奖励，保存上一周期状态、上一周期动作、上一周期奖 励、本周期状态，作为样本放入重放缓存区；步骤5：若重放缓存区满，则依据样本优先级，从重放缓存区中抽取小批量 样本，进行神经网络训练和参数更新；步骤6：根据步骤3的动作选择结果，设置交通系统下一周期各相位长度。步骤7：重复步骤3-6，当本回合结束后，记录本回合折扣回报；步骤8：当训练回合数达到预设值后停止。具体地，步骤1中使用的主网络对状态s下动作a的价值Q表示 为：其中θ,θv,θa是主网络参数，V表示当前状态下的未来期望回报， A表示状态s下采取动作a的期望回报，表示动作空间维数；目标网 络的动作价值近似为其网络参数θ-,初值与主网络参数 θ,θv,θa相同。步骤4中使用的奖励函数由F个线性特征加权组合而成：其中，s是本周期状态量，fi是关于状态s的特征函数f的第i维分量，此处取f＝s，wi为权重，wi的计算过程使用相对熵逆强化学习实现，具体包含以 下步骤：步骤4.1：配置交通灯控制环境，执行专家策略，生成长度为L的专家轨迹τE共NE条；执行均匀策略，生成长度为L的采样轨迹τS共NS条。在均匀策略中，状 态st条件下动作at的执行概率为#)表示状态st下可选动 作总数；步骤4.2：提取专家轨迹折扣特征之和、采样轨迹折扣特征之和。其中，专 家轨迹折扣特征之和定义为其中为专家轨迹t时刻所处状态，γ为折扣因子，取γ＝0.9。采样轨迹折扣特征之和定义为其中为采样轨迹t时刻所处状态；步骤4.3：计算NE条专家轨迹的特征期望折扣和：其中表示第j个专家在t时刻的状态；步骤4.4：初始化奖励函数权重wi；步骤4.5：搜索的最大最小值，计算∈i：其中Δ表示的置信概率上界，其中D是专家轨迹集合，是对奖励函数权重向量为w时专家轨迹τE的发生概率的估计，实 际取Δ＝0.1；步骤4.6：计算权重更新次梯度：其中是采样联合策略，用来产生NS条轨迹τS，采样过程如 步骤4.1所述；αi由wi的符号决定，当wi≥0时，αi＝1；当wi＜0时，αi＝0；步骤4.7：更新奖励函数权重wi，取学习率lr＝0.01：步骤4.8：重复步骤4.6-4.7，直到奖励函数权重wi收敛。步骤5中样本优先级定义如下，定义TD目标Q值为：其中a表示状态s′下的可选动作集合；TD误差表示为：δ＝Qtarget-Q其中si,ai分别表示第i个样本的状态值和动作值；对于重放缓存区内的第k个样本， 根据其TD误差的绝对值大小进行降序排列，定义其优先级pk为排序序号的倒数。 若重放缓存区满，则依据样本优先级，从重放缓存区中抽取小批量样本，并进行 神经网络训练和参数更新，具体步骤为：步骤5.1：计算第k个样本被采样的概率为：其中μ是超参数指数，取为1；R是重放缓存区大小，取为20000；步骤5.2：在重放缓存区中，依据每个样本被采样的概率，抽取小批量样本， 批大小B＝64；步骤5.3：主网络参数以小批量样本上的均方TD误差J＝-Q)2为损失函数进行反向传播更新；步骤5.4：目标网络参数θ-,以主网络参数为更新目标，按以下方式更 新：θ-＝αθ-+θ其中α是超参数学习率，取α＝0.001；步骤5.5：更新ε-贪心方式的探索率ε：ε＝max其中εmax和εmin为预设的ε可取的最大值与最小值，nt为当前训练步数，εstep为 下降步长；取εmax＝1，εmin＝0.01，εstep＝10000。采用上述技术方案带来的有益效果：本发明在单独使用基于深度强化学习的交通灯控制方法时，能够在均 衡车流场景下实现最优控制性能，在非均衡车流场景下取得优于固定均匀相位方 案的控制性能。本发明在综合使用基于深度强化学习和逆强化学习的交通灯控制方法 时，能够有效提取专家奖励函数，实现了对专家经验的有效利用，算法对专家轨 迹中存在的控制或观测噪声有较强的鲁棒性。用逆强化学习提取的奖励函数指导 深度强化学习算法，在相同的训练条件下，在均衡车流和非均衡车流场景下的实 验表明，逆强化学习算法的引入均能提升原深度强化学习算法的控制性能，取得 不低于专家方案性能的控制效果。附图说明图1是本发明的算法框图；图2是本发明的算法流程图；图3是实验测试环境CityFlow。具体实施方式下面将结合附图，进一步阐明本发明。下述具体实施方式仅用于说明本发明 而不用于限制本发明的范围。实施例1：本发明提供了一种基于深度强化学习和逆强化学习的交通灯控制 方法，总体算法结构见图1和图2。假设交通灯每个周期的相位顺序是固定的，相位时长是可变的，交通灯控制 通过调整各相位时长实现。受控道路交叉口的马尔科夫决策模型建立如下：状态量。第i个控制周期的状态量表示为si＝，其中si,1表征道 路拥堵程度，si,2表征当前各相位长度大小。具体地，si,1＝， 其中ti表示第i个控制周期的开始时刻，lj表示车道 j在t时刻的静止排队车辆数，表示车道j可容纳的最大车辆数,Nl是道路交叉口 车道总数。si,2＝，其中bi,m表示第i周期内相 位m的时长，表示相位m允许的最大时长，取在四相位控制方案下， m的最大值为4。动作量。动作量指定下一周期各相位时长相对本周期的变化量，可选 的变化方式有：以5s的幅度调整单个相位的长度，或者保持当前各相位长度不 变。对于M相位控制的路口，可选动作共有2M+1种，包括对M种相位单独增、 减5s，或者保持当前各相位长度。奖励函数。在单独使用基于深度强化学习的交通灯控制方法时，第i周 期获得的奖励为：该奖励函数惩罚排队长度较大的状态。由于深度强化学习的训练目标是最大化累积奖励，该奖励函数能够有效降低车辆排队长度。在使用基于深度强化学习和逆 强化学习的交通灯控制方法时，奖励函数由逆强化学习过程得到。本发明使用深度神经网络近似动作价值函数，进行深度强化学习训练，并为 相对熵逆强化学习的应用搭建基本框架，具体包括以下步骤：步骤1：设置超参数，搭建双决斗深度Q神经网络，初始化主网络和目标网 络；步骤2：初始化交通灯控制系统，初始化相位长度；步骤3：交通灯控制系统按本周期预设各相位长度进行相位切换，完成一周 期运行；获取本周期状态，利用主网络评估各动作价值，并依据ε-贪心方式选择 动作，即：以概率ε随机选择动作，以概率1-ε选择价值最大的动作执行；步骤4：智能体获得奖励，保存上一周期状态、上一周期动作、上一周期奖 励、本周期状态，作为样本放入重放缓存区；步骤5：若重放缓存区满，则依据样本优先级，从重放缓存区中抽取小批量 样本，进行神经网络训练和参数更新；步骤6：根据步骤3的动作选择结果，设置交通系统下一周期各相位长度。步骤7：重复步骤3-6，当本回合结束后，记录本回合折扣回报；步骤8：当训练回合数达到预设值后停止。其中，步骤1中使用的主网络对状态s下动作a的价值Q表示为：其中θ,θv,θa是主网络参数，V表示当前状态下的未来期望回报，A表示状态s下采取动作a的期望回报，表示动作空间维数；目标网 络的动作价值近似为其网络参数θ-,初值与主网络参数 θ,θv,θa相同。在此基础上，使用逆强化学习算法设计奖励函数。假设奖励函数由F个线性 特征加权组合而成：其中，s是本周期状态量，fi是关于状态s的特征函数f的第i维分量， 此处取f＝s，wi为权重。wi的计算过程包含以下步骤：步骤4.1：配置交通灯控制环境，执行专家策略，生成长度为L的专家轨迹τE共NE条；执行均匀策略，生成长度为L的采样轨迹τS共NS条。在均匀策略中，状 态st条件下动作at的执行概率为#)表示状态st下可选动 作总数；步骤4.2：提取专家轨迹折扣特征之和、采样轨迹折扣特征之和。其中，专 家轨迹折扣特征之和定义为其中为专家轨迹t时刻所处状态，γ为折扣因子，取γ＝0.9。采样轨迹折扣特征之和定义为其中为采样轨迹t时刻所处状态；步骤4.3：计算NE条专家轨迹的特征期望折扣和：其中表示第j个专家在t时刻的状态；步骤4.4：初始化奖励函数权重wi；步骤4.5：搜索的最大最小值，计算∈i：其中Δ表示的置信概率上界，其中D是专家轨迹集合，是对奖励函数权重向量为w时专家轨迹τE的发生概率的估计，实 际取Δ＝0.1；步骤4.6：计算权重更新次梯度：其中是采样联合策略，用来产生NS条轨迹τS，采样过程如 步骤4.1所述；αi由wi的符号决定，当wi≥0时，αi＝1；当wi＜0时，αi＝0；步骤4.7：更新奖励函数权重wi，取学习率lr＝0.01：步骤4.8：重复步骤4.6-4.7，直到奖励函数权重wi收敛。步骤5中样本优先级定义如下。定义TD目标Q值为：其中a表示状态s′下的可选动作集合；TD误差表示为：δ＝Qtarget-Q其中si,ai分别表示第i个样本的状态值和动作值；对于重放缓存区内的第k个样本， 根据其TD误差的绝对值大小进行降序排列，定义其优先级pk为排序序号的倒数。 若重放缓存区满，则依据样本优先级，从重放缓存区中抽取小批量样本，并进行 神经网络训练和参数更新，具体步骤为：步骤5.1：计算第k个样本被采样的概率为：其中μ是超参数指数，取为1；R是重放缓存区大小，取为20000；步骤5.2：在重放缓存区中，依据每个样本被采样的概率，抽取小批量样本， 批大小B＝64；步骤5.3：主网络参数以小批量样本上的均方TD误差J＝-Q)2为损失函数进行反向传播更新；步骤5.4：目标网络参数θ-,以主网络参数为更新目标，按以下方式更 新：θ-＝αθ-+θ其中α是超参数学习率，取α＝0.001；步骤5.5：更新ε-贪心方式的探索率ε：ε＝max其中εmax和εmin为预设的ε可取的最大值与最小值，nt为当前训练步数，εstep为 下降步长；取εmax＝1，εmin＝0.01，εstep＝10000。下文将通过实例说明本发明的先进性，测试比较传统交通灯控制算法、单独 使用深度强化学习的控制算法和综合使用逆强化学习与深度强化学习的改进算 法的效果。基本设置测试环境选择CityFlow平台，如图3，该平台支持智能体获取各车道车辆 数、行驶速度、所处位置等多种信息，并通过指定交叉口切换到某种信号灯组合 来实现控制。在变周期控制方案下，规定各相位绿灯时长的合法值为10～60s， 取值间隔为5s。每相位结束后，将设置5s的全场红灯，以此起到清场作用。交 叉口由四条垂直道路构成，考虑以下场景：A.单车道均衡车流场景：交叉口的每条道路仅直行车道通行，车辆平均到 达间隔为20s。相位顺序为：东西向直行，南北向直行；B.单车道非均衡车流场景：交叉口的每条道路仅直行车道通行，东西向车 辆平均到达间隔为5s，南北向车辆平均到达间隔为15s。相位顺序为： 东西向直行，南北向直行。专家数据在均衡车流下，固定均匀相位控制方案是最优方案。对于场景A而言，专家 方案为东西直行10s，南北直行10s。而非均衡车流场景不存在固定最优均匀相 位控制方案，场景B中取实际测试后相对较好的方案“东西直行15s，南北直行 15s”作为假定专家。为了测试逆强化学习的鲁棒性，在场景A的专家方案中另 增10％的随机性，作为次优专家方案。设置专家轨迹长度为100，在场景A和B 下分别使用上述专家方案进行控制，得到专家示例轨迹集合并保存。评价指标本发明中，交通灯控制的目标是缓解交叉口拥堵状况、提高通行效率。选择 两个常用指标评估交通灯控制性能，分别为4000s内平均排队长度和所有车辆的 平均通行时间。在奖励函数学习方面，选择皮尔逊相似系数衡量逆强化学习结果与专家实际 奖励函数的相似性。测试结果与分析在场景A中，专家特征权重为wgt＝。生成专家示例轨 迹共900条，重要性采样轨迹共900条。经逆强化学习过程，奖励函数的特征权 重w收敛至对次优专家轨 迹进行学习，特征权重w收敛至 另取服从-0.5～0.5均匀分 布的随机权重wrand＝作为对照。计算以 上三者与wgt的皮尔逊相似系数如表1，表1是专家实际奖励权重与逆强化学习 所得奖励权重、随机权重的皮尔曼相似系数对比表格；表1：逆强化学习结果能够反映专家真实奖励函数，并且当专家数据含噪声时也能 取得很好的学习效果，具有较好的鲁棒性。记固定均匀相位控制方案为Fixed time，单独使用深度强化学习的方案为 DRL，综合使用逆强化学习与深度强化学习的改进方案为IRL-DRL。对三种算法 分别进行4000s仿真实验，表2给出了其控制效果。表2是单车道均衡车流场景下 使用固定均匀相位方案、深度强化学习方案、综合逆强化学习和深度强化学习方 案测试下，车辆平均通行时间和交叉口平均排队长度的数据对比表格；表2：相比固定均匀相位方案，DRL和IRL-DRL控制都能将平均排队长度降低50％， 且IRL-DRL优于DRL。在场景B中，由于车流不均衡，专家实际奖励函数未知，因此只能通过交通 灯实际控制效果来说明逆强化学习过程的有效性。生成专家示例轨迹共600条，重要性采样轨迹共900条，经逆强化学习得到收敛后的奖励函数特征权重为了研究专家轨迹对逆强化学习 算法的影响，在场景B这种特殊的固定不均衡车流中，引入比固定均匀相位有明 显优势的不均匀相位专家轨迹2“东西直行25s，南北直行10s”进行学习，得到 利用和进行深度强化学习训练，其结果分别记为和对两种专家方案、DRL和IRL-DRL方案分别进行4000s仿真实验，表3：是单车道非均衡车流场景下使用固定均匀相位方案、深度强化学习方 案、综合逆强化学习和深度强化学习方案测试下，车辆平均通行时间和交叉口平 均排队长度的数据对比表格。表3：表3所示结果表明：使用固定均匀的专家方案时，IRL-DRL的效果能够优于该 专家方案；使用固定不均匀专家方案时，IRL-DRL能够取得与专家方案同等的控 制效果，进一步提升深度强化学习的性能；显著不同的专家轨迹也会影响逆强化 学习的结果。实例仅为说明本发明的技术思想，不能以此限定本发明的保护范围，凡是按 照本发明提出的技术思想，在技术方案基础上所做的任何改动，均落入本发明保 护范围之内。
