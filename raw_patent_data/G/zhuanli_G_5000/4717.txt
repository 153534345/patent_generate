标题title
一种基于深度逆向强化学习的车辆路径链重构方法
摘要abst
本发明提供一种基于深度逆向强化学习的车辆路径链重构方法，涉及智慧交通领域。首先对车牌识别数据进行预处理，并提取出路段行程时间，利用基于辅助信息的张量补全算法对路段行程时间补全，从而分离出完整路径链和缺失路径链。然后利用深度逆向强化学习，对路网中完整出行路径链进行挖掘，以非线性回报函数的形式拟合出潜藏的路径选择特性，指导智能体自主重构缺失路径链。本发明克服了现有算法决策依据主观性大的缺陷，在少量的示例数据下就能达到稳定性强、准确度高的重构效果，为交通需求结构分析和交通拥堵疏导等重点交通问题提供数据支撑。
权利要求书clms
1.一种基于深度逆向强化学习的车辆路径链重构方法，其特征在于，具体如下：步骤1：对采集到的车牌识别原始数据进行预处理，以清洗异常数据，得到车牌识别数据；步骤2：对所述车牌识别数据按车辆经行时间排列为经行卡口链，计算相邻经行卡口间的时间差；划分时间窗，对各路段各时间窗内的时间差进行统计并作为样本量；若样本量满足阈值，则剔除异常后将样本均值作为该时间窗内的路段行程时间，反之，则视为路段行程时间缺失；步骤3：对于步骤2中缺失的路段行程时间，采用基于辅助信息的张量补全算法进行路段行程时间补全；步骤4：对于步骤2中原本完整的路段行程时间或经步骤3补全后的路段行程时间，对经行卡口链进行拓扑检查和行程时间阈值检查，分离出缺失路径链集合与完整路径链集合；步骤5：将城市路网上的路径链重构问题建模为回报函数未知的马尔可夫决策过程，基于最大熵逆向强化学习，对所述完整路径链集合进行挖掘，以求解最佳回报函数；步骤6：基于所述最佳回报函数，采用Q学习算法求解路径重构的最优策略，指导智能体进行所述缺失路径链集合重构，得到最终的路径重构方案。2.根据权利要求1所述的一种基于深度逆向强化学习的车辆路径链重构方法，其特征在于，所述步骤1中，对于车牌识别原始数据中，由于车牌不明或设备漏检导致的无效数据，以及由于外界干扰或设备故障造成的错误数据，均采用直接剔除的方式进行车牌识别数据预处理；对于车牌识别原始数据中，由于设备故障造成多条过车记录所有字段完全一致的重复数据，采用直接保留最后一条记录的方式进行车牌识别数据预处理；对于车牌识别原始数据中，由于多检错拍或广角错拍造成车牌或卡口名称一致、记录时间稍有差别的重复数据，将重复记录按时序排列，计算连续两条记录之间的过车时间差；若过车时间差小于重复检测时间阈值，则被视为重复记录，采用保留后一条记录的方式进行车牌识别数据预处理，反之，两条记录均予以保留。3.根据权利要求1所述的一种基于深度逆向强化学习的车辆路径链重构方法，其特征在于，所述步骤2具体如下：根据所述车牌识别数据，将车辆一天的记录按经行时间排列为经行卡口链，当所述经行卡口链中相邻节点拓扑相连，则计算其节点间的时间差；以5min为一个时间窗，统计各个路段各时间窗内的路段行程时间；若时间窗内样本量≤n，则样本不具代表性，视为路段行程时间缺失；若样本数量＞n，则利用箱型图法进行异常值分析，异常值剔除后，将时间窗内的样本均值作为该时间窗内的路段行程时间。4.根据权利要求1所述的一种基于深度逆向强化学习的车辆路径链重构方法，其特征在于，所述步骤3具体如下：构建三个维度的相似矩阵作为辅助信息并加入张量补全模型，通过交替更新因子矩阵法对该优化问题进行求解，获得路段行程时间的最优补全张量；所述三个维度的相似矩阵包括：路段相似矩阵M1：基于路段属性矩阵，利用余弦相似度计算路段相似矩阵；所述路段属性包括道路拓扑属性和路段特征属性，道路拓扑属性包括路段入度、出度、邻接度以及路段特征属性，路段特征属性包括路段长度、路段车道数、路段等级和路段兴趣点数；时间窗相似矩阵M2：基于车速属性，利用余弦相似度计算时间窗相似矩阵；天数相似矩阵M3：基于车速和天气属性，利用余弦相似度计算天数相似矩阵；所述余弦相似度的计算公式如下：式中，cos_pn,n+1为对象n与n+1的余弦相似度，bn,j为对象n的第j个属性值，bn+1,j为对象n+1的第j个属性值；加入辅助信息的所述张量补全模型公式如下：式中，T为原始张量；为补全张量，基于tucker分解，其中G为tucker分解后的核心张量，U，V，W为分解后的因子矩阵；×n为张量与矩阵的模态积；||·||2F为F-范数；L为正则化项；M1，M2，M3为相似矩阵。5.根据权利要求1所述的一种基于深度逆向强化学习的车辆路径链重构方法，其特征在于，所述步骤4具体如下：根据路段行程时间ti，o，k，构建路段行程时间的阈值其中，ti，o，k代表i路段第k天第o时间窗的行程时间，δ为调整系数；对于任意一条经行卡口链，按时间顺序对其每一组相邻节点进行遍历；若每组节点均拓扑相连，且节点时间差满足，则将经行卡口链放入完整路径链集合中；若在某组中时间差不满足，则从处分离，j节点前的经行卡口链放入完整路径链集合，从j+1节点处检验下一组相邻节点；若相邻节点拓扑不相连，以路段长度得出两个节点间的最短路径，计算节点间最短路径的时间总阈值，通过累加最短路径中每个路段的即可；若节点时间差满足，则将放入缺失路径链集合，继续检验j+1节点后的卡口链；若节点时间差不满足，则在处分离，j节点前的卡口链放入完整路径链集合，从j+1节点继续向后检验。6.根据权利要求1所述的一种基于深度逆向强化学习的车辆路径链重构方法，其特征在于，所述步骤5中马尔可夫决策过程的构建方法具体如下：a)令环境E为路网；b)令状态空间S为智能体可以到达的所有路段，每一个路段状态通过多个状态特征表示；c)令动作空间A包含左转、右转、直行和掉头；d)令状态转移T为智能体在执行动作a后从当前路段转向下一路段；e)令专家示例D′为从完整路径链集合中抽取部分作为深度逆向强化学习的专家示例；f)令策略π为状态到动作的映射学习，即智能体如何选择动作的决策过程；g)回报函数R用于评价智能体策略的优劣，深度逆向强化学习的目标是使拟合出的回报函数与专家示例的回报函数相近。7.根据权利要求6所述的一种基于深度逆向强化学习的车辆路径链重构方法，其特征在于，所述步骤b)中，路段状态特征包括路段长度、路段等级、路段中心经纬度、行程时间、行程时间标准差以及端点是否为信号交叉口。8.根据权利要求6所述的一种基于深度逆向强化学习的车辆路径链重构方法，其特征在于，所述回报函数的求解具体如下：利用全连接网络DNN来拟合回报函数，回报值r表示为路段状态特征的非线性函数，其公式如下所示：r＝θT·fet式中，fet代表路段s的状态特征；θ是回报函数的参数，也是DNN网络中所有神经元所关联的权值和偏置值；从完整路径链集合中抽取部分路径链d′i，构造一组专家示例D′＝{d′1，d′2，…，d′n-1，d′n}；利用最大熵理论将回报函数求解转化为最小化问题，利用梯度下降法对最小化问题求解，获得最佳回报函数，即挖掘得到路径效用函数；利用最大熵理论将回报函数求解转化为最小化问题，公式如下：式中，在深度逆向强化学习过程中会产生临时回报函数rθ，di是智能体在rθ下执行出的样本路径链，f表示当前样本路径链对应所有状态的特征期望之和，表示专家示例{d′1，d′2，…，d′n-1，d′n}的特征期望。9.根据权利要求1所述的一种基于深度逆向强化学习的车辆路径链重构方法，其特征在于，所述步骤6具体如下：设定所述缺失路径链集合的起终点，在最佳回报函数已知时，在Q学习算法中设定Q为状态-动作值函数，以计算出任意路段状态下的最佳重构动作，即生成重构策略π，以指导强化学习智能体做出决策，得到路径链重构的最优方案。10.根据权利要求9所述的一种基于深度逆向强化学习的车辆路径链重构方法，其特征在于，采用Q学习算法求解路径重构的最优策略具体如下：初始化所有状态动作对的Q；在当前状态s下基于当前Q，选择动作a；执行动作a，获得下一个状态s′和回报r；更新Q，当多次迭代后，Q收敛，则认为获得了最佳的Q；其中，基于Bellman方程的Q更新公式如下：Qnew＝Q+α式中，Qnew为更新后的Q值；Q为当前Q值；α是学习率；γ是折扣因子，用于权衡近期回报和远期回报；maxQ′是下一步状态Q′的最大值。在持续的迭代后，Q最终会收敛于最优动作-状态函数Q*，此时获得最优策略π*＝argmaxQ*。
说明书desc
技术领域本发明涉及智慧交通领域，特别是涉及一种基于深度逆向强化学习的车辆路径链重构方法。背景技术目前，交通信息采集技术主要包括线圈、微波、浮动车、GPS和车牌识别等方法，线圈和微波能记录下某一点的交通信息，但无法与车辆联系起来；浮动车、GPS数据虽然分布范围广，但其仅依靠装有GPS定位的出租、公交等车辆，车辆数据来源较为单一，而车牌识别数据拥有能够识别身份信息、高覆盖率、大样本量等特点，能更好的代表城市路网交通模式，具有广泛的应用前景。随着城市交通监测系统的完善，大量车牌识别数据被存储下来，为研究城市交通提供了新机遇。但在信息采集过程中受制于成本和技术，采集的车牌数据在时空上存在着不连续性，难以还原城市路网的交通运行状态，无法满足目前智慧交通建设的需要。目前基于车牌识别数据的路径链重构方法很少考虑出行者的路径选择偏好，以至于难以应对复杂的路径缺失情况，无法为交通需求结构分析和交通拥堵疏导等重点交通问题提供数据支撑。发明内容本发明为克服现有技术的不足，提供一种基于深度逆向强化学习的车辆路径链重构方法。本发明所采用的具体技术方案如下：本发明提供了一种基于深度逆向强化学习的车辆路径链重构方法，具体如下：步骤1：对采集到的车牌识别原始数据进行预处理，以清洗异常数据，得到车牌识别数据；步骤2：对所述车牌识别数据按车辆经行时间排列为经行卡口链，计算相邻经行卡口间的时间差；划分时间窗，对各路段各时间窗内的时间差进行统计并作为样本量；若样本量满足阈值，则剔除异常后将样本均值作为该时间窗内的路段行程时间，反之，则视为路段行程时间缺失；步骤3：对于步骤2中缺失的路段行程时间，采用基于辅助信息的张量补全算法进行路段行程时间补全；步骤4：对于步骤2中原本完整的路段行程时间或经步骤3补全后的路段行程时间，对经行卡口链进行拓扑检查和行程时间阈值检查，分离出缺失路径链集合与完整路径链集合；步骤5：将城市路网上的路径链重构问题建模为回报函数未知的马尔可夫决策过程，基于最大熵逆向强化学习，对所述完整路径链集合进行挖掘，以求解最佳回报函数；步骤6：基于所述最佳回报函数，采用Q学习算法求解路径重构的最优策略，指导智能体进行所述缺失路径链集合重构，得到最终的路径重构方案。作为优选，所述步骤1中，对于车牌识别原始数据中，由于车牌不明或设备漏检导致的无效数据，以及由于外界干扰或设备故障造成的错误数据，均采用直接剔除的方式进行车牌识别数据预处理；对于车牌识别原始数据中，由于设备故障造成多条过车记录所有字段完全一致的重复数据，采用直接保留最后一条记录的方式进行车牌识别数据预处理；对于车牌识别原始数据中，由于多检错拍或广角错拍造成车牌或卡口名称一致、记录时间稍有差别的重复数据，将重复记录按时序排列，计算连续两条记录之间的过车时间差；若过车时间差小于重复检测时间阈值，则被视为重复记录，采用保留后一条记录的方式进行车牌识别数据预处理，反之，两条记录均予以保留。作为优选，所述步骤2具体如下：根据所述车牌识别数据，将车辆一天的记录按经行时间排列为经行卡口链，当所述经行卡口链中相邻节点拓扑相连，则计算其节点间的时间差；以5min为一个时间窗，统计各个路段各时间窗内的路段行程时间；若时间窗内样本量≤n，则样本不具代表性，视为路段行程时间缺失；若样本数量＞n，则利用箱型图法进行异常值分析，异常值剔除后，将时间窗内的样本均值作为该时间窗内的路段行程时间。作为优选，所述步骤3具体如下：构建三个维度的相似矩阵作为辅助信息并加入张量补全模型，通过交替更新因子矩阵法对该优化问题进行求解，获得路段行程时间的最优补全张量；所述三个维度的相似矩阵包括：路段相似矩阵M1：基于路段属性矩阵，利用余弦相似度计算路段相似矩阵；所述路段属性包括道路拓扑属性和路段特征属性，道路拓扑属性包括路段入度、出度、邻接度以及路段特征属性，路段特征属性包括路段长度、路段车道数、路段等级和路段兴趣点数；时间窗相似矩阵M2：基于车速属性，利用余弦相似度计算时间窗相似矩阵；天数相似矩阵M3：基于车速和天气属性，利用余弦相似度计算天数相似矩阵；所述余弦相似度的计算公式如下：式中，cos_pn，n+1为对象n与n+1的余弦相似度，bn，j为对象n的第j个属性值，bn+1，j为对象n+1的第j个属性值；加入辅助信息的所述张量补全模型公式如下：式中，T为原始张量；为补全张量，基于tucker分解，其中G为tucker分解后的核心张量，U，V，W为分解后的因子矩阵；×n为张量与矩阵的模态积；||·||2F为F-范数；L为正则化项；M1，M2，M3为相似矩阵。作为优选，所述步骤4具体如下：根据路段行程时间ti，o，k，构建路段行程时间的阈值其中，ti，o，k代表i路段第k天第o时间窗的行程时间，δ为调整系数；对于任意一条经行卡口链，按时间顺序对其每一组相邻节点进行遍历；若每组节点均拓扑相连，且节点时间差满足，则将经行卡口链放入完整路径链集合中；若在某组中时间差不满足，则从处分离，j节点前的经行卡口链放入完整路径链集合，从j+1节点处检验下一组相邻节点；若相邻节点拓扑不相连，以路段长度得出两个节点间的最短路径，计算节点间最短路径的时间总阈值，通过累加最短路径中每个路段的即可；若节点时间差满足，则将放入缺失路径链集合，继续检验j+1节点后的卡口链；若节点时间差不满足，则在处分离，j节点前的卡口链放入完整路径链集合，从j+1节点继续向后检验。作为优选，所述步骤5中马尔可夫决策过程的构建方法具体如下：a)令环境E为路网；b)令状态空间S为智能体可以到达的所有路段，每一个路段状态通过多个状态特征表示；c)令动作空间A包含左转、右转、直行和掉头；d)令状态转移T为智能体在执行动作a后从当前路段转向下一路段；e)令专家示例D′为从完整路径链集合中抽取部分作为深度逆向强化学习的专家示例；f)令策略π为状态到动作的映射学习，即智能体如何选择动作的决策过程；g)回报函数R用于评价智能体策略的优劣，深度逆向强化学习的目标是使拟合出的回报函数与专家示例的回报函数相近。进一步的，所述步骤b)中，路段状态特征包括路段长度、路段等级、路段中心经纬度、行程时间、行程时间标准差以及端点是否为信号交叉口。进一步的，所述回报函数的求解具体如下：利用全连接网络DNN来拟合回报函数，回报值r表示为路段状态特征的非线性函数，其公式如下所示：r＝θT·fet式中，fet代表路段s的状态特征；θ是回报函数的参数，也是DNN网络中所有神经元所关联的权值和偏置值；从完整路径链集合中抽取部分路径链d′i，构造一组专家示例D′＝{d′1，d′2，…，d′n-1，d′n}；利用最大熵理论将回报函数求解转化为最小化问题，利用梯度下降法对最小化问题求解，获得最佳回报函数，即挖掘得到路径效用函数；利用最大熵理论将回报函数求解转化为最小化问题，公式如下：式中，在深度逆向强化学习过程中会产生临时回报函数rθ，di是智能体在rθ下执行出的样本路径链，f表示当前样本路径链对应所有状态的特征期望之和，表示专家示例{d′1，d′2，…，d′n-1，d′n}的特征期望。作为优选，所述步骤6具体如下：设定所述缺失路径链集合的起终点，在最佳回报函数已知时，在Q学习算法中设定Q为状态-动作值函数，以计算出任意路段状态下的最佳重构动作，即生成重构策略π，以指导强化学习智能体做出决策，得到路径链重构的最优方案。进一步的，采用Q学习算法求解路径重构的最优策略具体如下：初始化所有状态动作对的Q；在当前状态s下基于当前Q，选择动作a；执行动作a，获得下一个状态s′和回报r；更新Q，当多次迭代后，Q收敛，则认为获得了最佳的Q；其中，基于Bellman方程的Q更新公式如下：Qnew＝Q+α式中，Qnew为更新后的Q值；Q为当前Q值；α是学习率；γ是折扣因子，用于权衡近期回报和远期回报；maxQ′是下一步状态Q′的最大值。在持续的迭代后，Q最终会收敛于最优动作-状态函数Q*，此时获得最优策略π*＝argmaxQ*。本发明相对于现有技术而言，具有以下有益效果：本发明首先对车牌识别数据进行预处理，并提取出路段行程时间，利用基于辅助信息的张量补全算法对路段行程时间补全，从而分离出完整路径链和缺失路径链。然后利用深度逆向强化学习，对路网中完整出行路径链进行挖掘，以非线性回报函数的形式拟合出潜藏的路径选择特性，指导智能体自主重构缺失路径链。本方法克服了现有算法决策依据主观性大的缺陷，在少量的示例数据下就能达到稳定性强、准确度高的重构效果，为交通需求结构分析和交通拥堵疏导等重点交通问题提供数据支撑。附图说明图1为本发明提出的路径重构方法的主要步骤流程图；图2为本发明中回报函数的全连接神经网络结构示意图；图3为本发明实施例1采用的路网示意图；图4为本发明实施例1采用的路网缺失情况模拟示意图；图5为本发明实施例1中回报函数求解的路网回报值分布图；图6为本发明实施例1中不同专家示例数量下的重构准确度示意图；附图仅用于示例性说明，不能理解为对本专利的限制；为了更好说明本实施例，附图某些元素会有省略、放大或缩小，不能理解为对本专利的限制。具体实施方式下面结合附图和具体实施方式对本发明做进一步阐述和说明。本发明中各个实施方式的技术特征在没有相互冲突的前提下，均可进行相应组合。由于未配备识别设备或设备故障会导致出行信息的丢失，对路网中信息丢失的交叉口定义为虚拟采集卡口，正常工作的交叉口定义为实际采集卡口。将路径链重构问题定义为：当一条路径链存在两个路段拓扑不相连，那么该条路径链存在缺失，将这两条路段作为缺失起终点，找出其间经过的路段，形成完整的出行路径链。如图1所示，对于缺失的路径链，本发明提供了如下重构方法：步骤1：车牌识别数据预处理。对于采集到的车牌识别原始数据，进行车牌识别数据预处理，清洗存在的异常数据，进而得到车牌识别数据。在实际应用时，对于车牌识别原始数据，存在以下几种异常类型，如下表1所示：表1由于车牌不明、设备漏检导致的无效数据，以及由于外界干扰、设备故障造成的错误数据，均采用直接剔除的方式；由于设备故障造成多条过车记录所有字段完全一致的重复数据，采用直接保留最后一条记录的方式；由于多检错拍、广角错拍造成车牌、卡口名称等一致，记录时间稍有差别的重复数据，将重复记录按时序排列，计算连续两条记录之间的过车时间差；若过车时间差小于重复检测时间阈值，则被视为重复记录，保留后一条记录；反之，两条记录均予以保留。步骤2：路段行程时间提取。对车牌识别数据按车辆经行时间排列为经行卡口链，计算相邻经行卡口间的时间差。划分时间窗，统计各路段各时间窗内的时间差，对时间窗内样本量满足阈值的，剔除异常后将样本均值作为该时间窗内的路段行程时间，反之，视为缺失。由于在路径链提取中，路段行程时间不仅是划分单次路径链的重要指标，也是后续重构算法的重要特征。在实际应用时，将车辆一天的记录按时间排列为卡口链，当卡口链中相邻节点拓扑相连，则计算其节点间的时间差。以5min为一个时间窗，统计各个路段各时间窗内的路段行程时间。若时间窗内样本量小于等于n，一般取3，则样本不具代表性，视为缺失；若样本数量大于n，利用箱型图法进行异常值分析，异常值剔除后，将时间窗内的样本均值作为该时间窗内的路段行程时间。步骤3：路段行程时间补全。针对缺失的路段行程时间，采用基于辅助信息的张量补全算法进行路段行程时间补全。在实际应用时，将路段行程时间构建为一个具有I×O×K个元素的三阶张量T，I、O、K分别代表路段、时间窗、天数三个维度，ti，o，k代表i路段第k天第o时间窗的行程时间。构建三个维度的相似矩阵作为辅助信息加入张量补全模型，通过交替更新因子矩阵法对该优化问题进行求解，获得路段行程时间的最优补全张量。其中，三个维度的相似矩阵包括：路段相似矩阵M1：基于路段属性矩阵，包含道路拓扑属性以及路段特征属性)，利用余弦相似度计算路段相似矩阵；时间窗相似矩阵M2，基于车速属性利用余弦相似度计算时间窗相似矩阵；天数相似矩阵M3：基于车速-天气属性利用余弦相似度计算天数相似矩阵；余弦相似度计算公式如下：式中，cos_pn，n+1为对象n与n+1的余弦相似度，bn，j为对象n的第j个属性值，bn+1，j为对象n+1的第j个属性值。加入辅助信息的张量补全模型公式如下：式中，T为原始张量；为补全张量，基于tucker分解，其中G为tucker分解后的核心张量，U，V，W为分解后的因子矩阵；×n为张量与矩阵的模态积；||·||2F为F-范数；L为正则化项；M1，M2，M3为相似矩阵。步骤4：路径链分离。基于补全后的路段行程时间，对卡口链进行拓扑检查和行程时间阈值检查，分离出缺失路径链集合与完整路径链集合。在实际应用时，根据路段行程时间ti，o，k，构建路段行程时间的阈值δ为调整系数，一般为1.15，以应对行程时间的区间波动。对于任意一条卡口链，按时间顺序对其每一组相邻节点进行遍历。若每组节点均拓扑相连，且节点时间差满足，将卡口链放入完整路径链集合中；若在某组中时间差不满足，则从处分离，j节点前的卡口链放入完整路径链集合，从j+1节点处检验下一组相邻节点；若相邻节点拓扑不相连，以路段长度得出两个节点间的最短路径，计算节点间最短路径的时间总阈值，通过累加最短路径中每个路段的即可，若节点时间差满足，则将放入缺失路径链集合，继续检验j+1节点后的卡口链；若节点时间差不满足，则在处分离，j节点前的卡口链放入完整路径链集合，从j+1节点继续向后检验。步骤5：回报函数求解。将城市路网上的路径链重构问题建模为回报函数未知的马尔可夫决策过程，基于最大熵逆向强化学习，对完整路径链进行挖掘，求解最佳回报函数，即每个路段的潜在效用函数。在实际应用时，路径链重构问题建模包括：将城市路网上的路径链重构建模为一个马尔可夫决策过程，其设定如下：a)环境E：路网；b)状态空间S：智能体可以到达的所有路段，每一个路段状态可以通过多个状态特征表示；c)动作空间A：包含左转、右转、直行和掉头；d)状态转移T：智能体在执行动作a后从当前路段转向下一路段；e)专家示例D′：从完整路径链集合中抽取部分作为深度逆向强化学习的专家示例；f)策略π：状态到动作的映射学习，即智能体如何选择动作的决策过程；g)回报函数R：用于评价智能体策略的优劣，深度逆向强化学习的目标是使拟合出的回报函数与专家示例的“回报函数”相近；其中，路段状态特征包括路段长度、路段等级、路段中心经纬度、行程时间、行程时间标准差、端点是否为信号交叉口，如下表2所示：表2具体的，如图2所示，回报函数求解包括：利用全连接网络DNN来拟合回报函数，回报值r表示为路段状态特征的非线性函数，其公式如下所示：r＝θT·fet式中，fet代表路段s的状态特征，θ是回报函数的参数，也是DNN网络中所有神经元所关联的权值和偏置值；从完整路径链集合中抽取部分路径链d′i，构造一组专家示例D′＝{d′1，d′2，…，d′n-1，d′n}，从这组专家示例中学会未知的回报函数。利用最大熵理论将回报函数求解转化为最小化问题，即挖掘出了路径效用函数。利用最大熵理论将回报函数求解转化为最小化问题，式中，在深度逆向强化学习过程中会产生临时回报函数rθ，di是智能体在rθ下执行出的样本路径链，f表示当前样本路径链对应所有状态的特征期望之和，表示示例路径链{d′1，d′2，…，d′n-1，d′n}的特征期望。步骤6：路径链重构。基于最佳回报函数，采用Q学习算法求解路径重构的最优策略，指导智能体进行路径链重构，得到最终的路径重构方案。在实际应用时，路径链重构包括：设定缺失路径链的起终点，在回报函数已知时，利用Q学习算法求解路径重构的最优策略。Q学习算法中设定Q为状态-动作值函数，记录着每种状态下采取每种动作的最大累积期望回报，用来评价在任意路段状态下采取某个重构动作的优劣。通过Q可以知道特定状态下的最佳动作，由此生成策略π，指导强化学习智能体做出决策；利用Q学习算法求解路径重构问题的具体步骤如下：初始化所有状态动作对的Q；在当前状态s下基于当前Q，选择动作a；执行动作a，获得下一个状态s′和回报r；更新Q，当多次迭代后，Q收敛，则认为获得了最佳的Q；其中，基于Bellman方程的Q更新公式如下：Qnew＝Q+α式中，Qnew为更新后的Q值；Q为当前Q值；α是学习率；γ是折扣因子，用于权衡近期回报和远期回报；maxQ′是下一步状态Q′的最大值。在持续的迭代后，Q最终会收敛于最优动作-状态函数Q*，此时获得最优策略π*＝argmaxQ*。在最优策略π*指导下，智能体可以快速重构出最优路径链。实施例1本实施例将本发明提出的车辆路径链重构方法用于图3所示的局部路网。该路网共包含43个交叉口，设有采集设备的实际采集卡口33个，剩余10个未设置采集设备，定义为虚拟采集卡口。实际采集卡口提供了车牌识别数据，记录了车辆牌照、过车时间、卡口编号、过车方向等字段。以局部路网的车牌识别数据为基础，执行发明提出的基于深度逆向强化学习的车辆路径链重构方法，步骤如下：车牌识别数据预处理。对于采集到的车牌识别原始数据，进行车牌识别数据预处理，清洗存在的异常数据；路段行程时间提取。将车辆一天的记录按时间排列为卡口链，计算相邻卡口间的时间差。以5min划分时间窗，统计各路段各时间窗内的时间差，对时间窗内样本量满足大于3的，剔除异常后将样本均值作为该时间窗内的路段行程时间，反之，视为缺失。路段行程时间补全。构建了三阶路段行程时间张量模型，并根据余弦相似度计算出三个维度的相似矩阵作为辅助信息，采用基于辅助信息的张量补全模型进行路段行程时间补全，通过交替更新因子矩阵法求解出最优路段行程时间张量。路径链分离。基于补全后的路段行程时间，对卡口链进行拓扑检查和行程时间阈值检查，分离出缺失路径链集合与完整路径链集合。缺失情况模拟。从完整路径链中随机取出若干条路段数大于3的路径链进行演示。演示中以从路网上删除某些实际采集卡口的方式来模拟缺失情况，将完整路径链中经过与该卡口相连的路段删除，作为缺失路径链；以完整路径链中的其他路径链作为专家示例。抽取了路网中六个节点实施缺失情况模拟，如图4所示。回报函数求解。将城市路网上的路径链重构问题建模为回报函数未知的马尔可夫决策过程，基于最大熵逆向强化学习，对完整路径链进行挖掘，求解最佳回报函数，获得每个路段的潜在效用函数；图5显示了缺失六个节点情况下，求解出的回报值分布图，图中路段的粗细代表回报值的大小。路径链重构。基于求解出的最佳回报函数，采用Q学习算法求解路径重构的最优策略，指导智能体进行路径链重构，得到最终的路径重构方案。图6显示了缺失六个节点情况下，经本发明提出的路径链重构方法在200条、400条、600条、800条、1000条专家示例数量下的算法准确度，在专家示例数量小于1000条时，算法准确度能始终保持在90％以上，证明本方法在较小样本量下就能实现对路网出行路径决策的深入挖掘，实现出行路径链的准确重构。其中，算法准确度公式如下：式中，pub为第i条重构路径链di与实际路径链di，true的最长连续公共子串数，即相同的连续路段数；sum代表实际路径链的路段数；m为重构路径链总数。以上所述的实施例只是本发明的一种较佳的方案，然其并非用以限制本发明。有关技术领域的普通技术人员，在不脱离本发明的精神和范围的情况下，还可以做出各种变化和变型。因此凡采取等同替换或等效变换的方式所获得的技术方案，均落在本发明的保护范围内。
