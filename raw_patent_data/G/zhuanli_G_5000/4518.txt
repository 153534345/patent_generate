标题title
基于多智能体强化学习的去混淆游戏策略模型生成方法
摘要abst
本发明公开了一种基于多智能体强化学习的去混淆游戏策略模型生成方法。本发明将深度学、因果推理、图网络结合在面向游戏智能体控制的多智能体强化学习中。相比于一般的多智能体强化学习算法，本发明利用因果推理中的后门准则和图网络来去除多智能体强化学习中由环境带来的混淆，有效地提升了游戏策略模型的整体性能。本发明首次在基于多智能体强化学习中的游戏策略模型生成中应用因果推理技术去除混淆，与其他主流的方法相比，本发明的性能更加优越。
权利要求书clms
1.一种基于多智能体强化学习的去混淆游戏策略模型生成方法，其特征在于，包括如下步骤：S1、查找游戏场景中每一个需要和环境进行交互且能够被游戏玩家控制的独立角色，将每个独立角色视为一个游戏智能体；S2、对每个游戏智能体进行单独建模得到自身游戏策略模型，每个自身游戏策略模型的输入为对应角色自身在游戏环境中的观测，输出为对应角色的局部动作价值；S3、构建一个中心评判器，其输入为游戏场景中所有游戏智能体的局部动作价值、特征和全局状态，输出为联合动作价值；S4、每个游戏智能体与游戏环境进行交互从而获取当前的观测，再依据自身游戏策略模型估计出游戏智能体的特征以及每个可行动作的价值，并根据价值确定下一时间步需要采取的行动；S5、将所有游戏智能体的特征作为图网络的节点构建全局轨迹图，利用全局轨迹图的特征和全局状态的特征完成对每个游戏智能体的信用分配，并由中心评判器依据每个游戏智能体的局部动作价值以及分配到的信用估计出所有游戏智能体的联合动作价值；S6、所有游戏智能体按照S4中确定采取的行动同时各自执行行动，并与游戏环境进行交互，游戏环境进行即时反馈，提供游戏整体状态和实时奖励；S7、依据游戏环境提供的实时奖励，使用反向传播算法，训练整个多智能体系统，同时更新每个游戏智能体的自身游戏策略模型以及中心评判器的参数；S8、不断重复步骤S4～S7，对游戏策略模型进行迭代训练，直至达到设定的终止条件为止，游戏场景中每一个独立角色均得到已训练的游戏策略模型。2.如权利要求1所述的基于多智能体强化学习的去混淆游戏策略模型生成方法，其特征在于，所述S4的具体包括以下子步骤：S401、对于游戏场景中的每个游戏智能体i，通过与游戏环境进行交互，得到该游戏智能体的观测变量oi；S402、对于每个游戏智能体i，建立门控循环单元GRU，由门控循环单元GRU根据该游戏智能体的历史信息hi以及观测变量oi提取出该游戏智能体的特征τi；S403、对于每个游戏智能体i，建立多层感知机MLP，由多层感知机MLP根据特征τi估计出该游戏智能体在下一时间步的每个可行动作的价值；S404、对于每个游戏智能体i，基于预先选定的探索策略，根据所有可行动作的价值确定下一时间步将要执行的动作ai，并记录动作ai的价值Qi。3.如权利要求1所述的基于多智能体强化学习的去混淆游戏策略模型生成方法，其特征在于，所述S5的具体包括以下子步骤：S501、存储游戏场景中的所有游戏智能体的特征{τ1,τ2,…,τN}，N为游戏智能体的数量；S502、构建全局轨迹图G＝＜V,E＞，其中图的节点V＝{τ1,τ2,…,τN}，将任意两个节点进行连接，构成全局轨迹图的边E；S503、通过带有多头注意力机制的图神经网络GNN提取全局轨迹图G的特征fG，其中H为多头注意力机制中的注意力头数量，fG1,fG2…,fGH分别为利用多头注意力机制计算得到的H个特征；S504、通过可学习的特征矩阵M，提取游戏场景的全局状态s的特征fs＝s×M；S505、利用全局轨迹图G的特征fG以及全局状态s的特征fs进行信用分配，从而估计每个游戏智能体的信用值k1,k2,…kN，其中该信用分配过程通过矩阵乘法实现：{k1,k2,…kN}＝fs×fG；S506、基于所有游戏智能体的局部动作价值函数Q1,Q2,…QN以及其相应的信用值k1,k2,…kN，计算得到联合动作价值函数4.如权利要求1所述的基于多智能体强化学习的去混淆游戏策略模型生成方法，其特征在于，所述S7的具体包括以下子步骤：S701、获取游戏环境提供的实时奖励r，利用贝尔曼最优方程计算TD损失其中γ为折扣，为强化学习中目标网络输出的联合动作价值函数估计；S702、利用TD损失L对整个多智能体系统中所有游戏智能体的自身游戏策略模型以及中心评判器进行反向传播，更新所有网络模型的可学习参数。5.如权利要求1所述的基于多智能体强化学习的去混淆游戏策略模型生成方法，其特征在于，所述目标网络的参数每隔一段预定时间间隔进行一次更新。6.如权利要求1所述的基于多智能体强化学习的去混淆游戏策略模型生成方法，其特征在于，所述预先选定的探索策略采用epsilon贪心算法。7.一种游戏场景中的角色控制方法，其特征在于，采用权利要求1～6中任一项所述的去混淆游戏策略模型生成方法得到每个独立角色对应的游戏策略模型，并用于控制对应的游戏智能体；所述方法包括：获取目标游戏智能体在游戏场景中的实时状态数据，将目标游戏智能体的实时状态数据输入目标游戏智能体对应的已训练的游戏策略模型中，得到所述游戏策略模型输出的根据所述游戏智能体的状态数据生成的控制策略；采用得到的控制策略，控制目标游戏智能体进行动作。8.一种游戏场景中的角色控制装置，其特征在于，包括：状态获取单元，用于获取目标游戏智能体在游戏场景中的实时状态数据；控制策略输出单元，用于将目标游戏智能体的实时状态数据输入目标游戏智能体对应的已训练的游戏策略模型中，得到所述游戏策略模型输出的根据所述游戏智能体的状态数据生成的控制策略；所述游戏策略模型是采用权利要求1～6中任一项所述的去混淆游戏策略模型生成方法训练得到的；动作控制单元，用于采用得到的控制策略，控制目标游戏智能体进行动作。9.一种计算机可读存储介质，其特征在于，所述存储介质上存储有计算机程序，当所述计算机程序被处理器执行时，实现如权利要求1～7任一所述方法。10.一种电子设备，其特征在于，包括存储器和处理器；所述存储器，用于存储计算机程序；所述处理器，用于当执行所述计算机程序时，实现如权利要求1～7任一所述方法。
说明书desc
技术领域本发明设计多智能体强化学习、深度学习、因果推理等领域，尤其涉及一种基于因果推理的去混淆游戏策略模型生成方法。背景技术多智能体系统在现实生活中有着广泛的应用，比如交通控制，网络路由，机器人技术，游戏角色控制等。通过将庞大的系统分解为小的多个子系统，可以将复杂的问题分解为多个易于处理的问题。在多智能体系统中，各个智能体相互通信，相互合作，以达成共同的目标。对于多智能体系统来说，鲁棒性，可靠性，高效性是其发挥自身功能的关键因素。多智能体强化学习是实现多智能体系统的关键技术，其优势在于，各个智能体可以在不知道环境的情况下，仅仅通过与环境的交互便可以学习到合理的行为模式。深度学习的发展使得各个智能体学习更加庞而复杂的模型，可以学习处理更加复杂的子任务。基于以上优势，使用深度学习来实现多智能体强化学习已经成为近些年来的研究热点。在游戏场景中，通过将每个单元视作单独的智能体，各个智能体相互协，可以有效的学习出游戏场景中的控制策略。“中心化训练-分布式执行”是多智能体强化学习中的标准范式，其中值分解是最主流的方法之一。值分解方法需要对每个智能体进行信用分配。现有的方法大多建立一层神经网络并利用环境全局状态信息去估计出各个智能体的信用，再用信用值去估计联合动作价值函数。然而，这种方式忽略了环境是混杂因子这一事实。由于环境这一混杂因子的存在，削弱了信用分配对于联合动作价值函数的直接因果效应，从而影响了整个多智能体系统的训练，使得游戏控制策略的不能得到有效的学习。去除环境带来的混淆是改良上述问题，学习良好的游戏控制策略的关键。发明内容本发明的目的是克服多智能体强化学习中的不足，提出一种基于多智能体强化学习的去混淆值分解方法，它能够去除多智能体系统中由环境带来的混淆，使得信用分配可以对联合动作价值函数有直接的因果效应，从而提升整个游戏策略模型的性能。本发明具体采用的技术方案如下：第一方面，本发明提供了一种基于多智能体强化学习的去混淆游戏策略模型生成方法，其包括如下步骤：S1、查找游戏场景中每一个需要和环境进行交互且能够被游戏玩家控制的独立角色，将每个独立角色视为一个游戏智能体；S2、对每个游戏智能体进行单独建模得到自身游戏策略模型，每个自身游戏策略模型的输入为对应角色自身在游戏环境中的观测，输出为对应角色的局部动作价值；S3、构建一个中心评判器，其输入为游戏场景中所有游戏智能体的局部动作价值、特征和全局状态，输出为联合动作价值；S4、每个游戏智能体与游戏环境进行交互从而获取当前的观测，再依据自身游戏策略模型估计出游戏智能体的特征以及每个可行动作的价值，并根据价值确定下一时间步需要采取的行动；S5、将所有游戏智能体的特征作为图网络的节点构建全局轨迹图，利用全局轨迹图的特征和全局状态的特征完成对每个游戏智能体的信用分配，并由中心评判器依据每个游戏智能体的局部动作价值以及分配到的信用估计出所有游戏智能体的联合动作价值；S6、所有游戏智能体按照S4中确定采取的行动同时各自执行行动，并与游戏环境进行交互，游戏环境进行即时反馈，提供游戏整体状态和实时奖励；S7、依据游戏环境提供的实时奖励，使用反向传播算法，训练整个多智能体系统，同时更新每个游戏智能体的自身游戏策略模型以及中心评判器的参数；S8、不断重复步骤S4～S7，对游戏策略模型进行迭代训练，直至达到设定的终止条件为止，游戏场景中每一个独立角色均得到已训练的游戏策略模型。在上述方案基础上，各步骤可以采用如下优选的具体方式实现。作为上述第一方面的优选，所述S4的具体包括以下子步骤：S401、对于游戏场景中的每个游戏智能体i，通过与游戏环境进行交互，得到该游戏智能体的观测变量oi；S402、对于每个游戏智能体i，建立门控循环单元GRU，由门控循环单元GRU根据该游戏智能体的历史信息hi以及观测变量oi提取出该游戏智能体的特征τi；S403、对于每个游戏智能体i，建立多层感知机MLP，由多层感知机MLP根据特征τi估计出该游戏智能体在下一时间步的每个可行动作的价值；S404、对于每个游戏智能体i，基于预先选定的探索策略，根据所有可行动作的价值确定下一时间步将要执行的动作ai，并记录动作ai的价值Qi。作为上述第一方面的优选，所述S5的具体包括以下子步骤：S501、存储游戏场景中的所有游戏智能体的特征{τ1,τ2,…,τN}，N为游戏智能体的数量；S502、构建全局轨迹图G＝＜V,E＞，其中图的节点V＝{τ1,τ2,…,τN}，将任意两个节点进行连接，构成全局轨迹图的边E；S503、通过带有多头注意力机制的图神经网络GNN提取全局轨迹图G的特征fG，其中H为多头注意力机制中的注意力头数量，fG1,fG2…,fGH分别为利用多头注意力机制计算得到的H个特征；S504、通过可学习的特征矩阵M，提取游戏场景的全局状态s的特征fs＝s×M；S505、利用全局轨迹图G的特征fG以及全局状态s的特征fs进行信用分配，从而估计每个游戏智能体的信用值k1,k2,…kN，其中该信用分配过程通过矩阵乘法实现：{k1,k2,…kN}＝fs×fG；S506、基于所有游戏智能体的局部动作价值函数Q1,Q2,…QN以及其相应的信用值k1,k2,…kN，计算得到联合动作价值函数作为上述第一方面的优选，所述S7的具体包括以下子步骤：S701、获取游戏环境提供的实时奖励r，利用贝尔曼最优方程计算TD损失其中γ为折扣，为强化学习中目标网络输出的联合动作价值函数估计；S702、利用TD损失L对整个多智能体系统中所有游戏智能体的自身游戏策略模型以及中心评判器进行反向传播，更新所有网络模型的可学习参数。作为上述第一方面的优选，所述目标网络的参数每隔一段预定时间间隔进行一次更新。作为上述第一方面的优选，所述预先选定的探索策略采用epsilon贪心算法。第二方面，本发明提供了一种游戏场景中的角色控制方法，其采用第一方面中任一项方案所述的去混淆游戏策略模型生成方法得到每个独立角色对应的游戏策略模型，并用于控制对应的游戏智能体；所述方法包括：获取目标游戏智能体在游戏场景中的实时状态数据，将目标游戏智能体的实时状态数据输入目标游戏智能体对应的已训练的游戏策略模型中，得到所述游戏策略模型输出的根据所述游戏智能体的状态数据生成的控制策略；采用得到的控制策略，控制目标游戏智能体进行动作。第三方面，本发明提供了一种游戏场景中的角色控制装置，其特征在于，包括：状态获取单元，用于获取目标游戏智能体在游戏场景中的实时状态数据；控制策略输出单元，用于将目标游戏智能体的实时状态数据输入目标游戏智能体对应的已训练的游戏策略模型中，得到所述游戏策略模型输出的根据所述游戏智能体的状态数据生成的控制策略；所述游戏策略模型是采用第一方面中任一项方案所述的去混淆游戏策略模型生成方法训练得到的；动作控制单元，用于采用得到的控制策略，控制目标游戏智能体进行动作。第四方面，本发明提供了一种计算机可读存储介质，所述存储介质上存储有计算机程序，当所述计算机程序被处理器执行时，实现如第一方面任一项方案所述的去混淆游戏策略模型生成方法或第二方面中任一项方案所述的游戏场景中的角色控制方法。第五方面，本发明提供了一种电子设备，其包括存储器和处理器；所述存储器，用于存储计算机程序；所述处理器，用于当执行所述计算机程序时，实现如第一方面任一项方案所述的去混淆游戏策略模型生成方法或第二方面中任一项方案所述的游戏场景中的角色控制方法。本发明将深度学、因果推理、图网络结合在面向游戏智能体控制的多智能体强化学习中。相比于一般的多智能体强化学习算法，本发明利用因果推理中的后门准则和图网络来去除多智能体强化学习中由环境带来的混淆，有效地提升了游戏策略模型的整体性能。本发明首次在基于多智能体强化学习中的游戏策略模型生成中应用因果推理技术去除混淆，与其他主流的方法相比，本发明在算法上有自己的独创性和独特性，且性能更加优越。附图说明图1是基于多智能体强化学习的去混淆游戏策略模型生成方法的步骤流程图。图2是基于多智能体强化学习的去混淆游戏策略模型生成方法的训练框架示意图。图3是值分解因果图。图4是实施例中的测试结果图。具体实施方式为使本发明的上述目的、特征和优点能够更加明显易懂，下面结合附图对本发明的具体实施方式做详细的说明。在下面的描述中阐述了很多具体细节以便于充分理解本发明。但是本发明能够以很多不同于在此描述的其它方式来实施，本领域技术人员可以在不违背本发明内涵的情况下做类似改进，因此本发明不受下面公开的具体实施例的限制。本发明各个实施例中的技术特征在没有相互冲突的前提下，均可进行相应组合。以下先对本发明中的部分用语进行解释说明，以便于本领域技术人员理解。游戏场景：是应用程序在终端上运行时显示的虚拟环境。该游戏环境可以是对真实世界的仿真环境，也可以是半仿真半虚构的三维环境，还可以是纯虚构的三维环境。游戏环境可以是二维游戏环境、2.5维游戏环境和三维游戏环境中的任意一种。可选地，该游戏环境还用于至少两个游戏角色之间的游戏环境对战，在该游戏环境中具有可供至少两个游戏角色使用的游戏资源。游戏独立角色：也可以称为虚拟对象，一些游戏中称为英雄，是指在游戏环境中的可被游戏玩家控制的活动对象。该可活动对象可以是虚拟人物、虚拟动物、虚拟武器类设备、动漫人物中的至少一种，游戏角色可以是二维、2.5维和三维的。可选的，游戏角色可以是对战游戏中的人物角色或者非人物角色，例如英雄人物、士兵或中立生物。智能体：是指游戏中能够与游戏环境进行交互的游戏角色。例如，智能体可以在某个特定的游戏环境下，根据自身对游戏环境的感知，按照已有的指示或者通过自主学习，与其他智能体进行沟通协作或者对战，在其所处的游戏环境中自主地完成设定的目标。下面结合附图及具体实施例对本申请的技术方案作进一步详细的说明。如图1所示，在本发明的一个较佳实施例中，提供了一种基于多智能体强化学习的去混淆游戏策略模型生成方法，其包括如下步骤：S1、查找游戏场景中每一个需要和环境进行交互且能够被游戏玩家控制的独立角色，将每个独立角色视为一个游戏智能体。S2、对每个游戏智能体进行单独建模得到自身游戏策略模型，每个自身游戏策略模型的输入为对应角色自身在游戏环境中的观测，输出为对应角色的局部动作价值。S3、构建一个中心评判器，其输入为游戏场景中所有游戏智能体的局部动作价值、特征和全局状态，输出为联合动作价值。S4、每个游戏智能体与游戏环境进行交互从而获取当前的观测，再依据自身游戏策略模型估计出游戏智能体的特征以及每个可行动作的价值，并根据价值确定下一时间步需要采取的行动。在本实施例中，S4的具体包括以下子步骤：S401、对于游戏场景中的每个游戏智能体i，通过与游戏环境进行交互，得到该游戏智能体的观测变量oi；S402、对于每个游戏智能体i，建立门控循环单元GRU，由门控循环单元GRU根据该游戏智能体的历史信息hi以及观测变量oi提取出该游戏智能体的特征τi；S403、对于每个游戏智能体i，建立多层感知机MLP，由多层感知机MLP根据特征τi估计出该游戏智能体在下一时间步的每个可行动作的价值；S404、对于每个游戏智能体i，基于预先选定的探索策略，根据所有可行动作的价值确定下一时间步将要执行的动作ai，并记录动作ai的价值Qi。S5、将所有游戏智能体的特征作为图网络的节点构建全局轨迹图，利用全局轨迹图的特征和全局状态的特征完成对每个游戏智能体的信用分配，并由中心评判器依据每个游戏智能体的局部动作价值以及分配到的信用估计出所有游戏智能体的联合动作价值。在本实施例中，S5的具体包括以下子步骤：S501、存储游戏场景中的所有游戏智能体的特征{τ1,τ2,…,τN}，N为游戏智能体的数量。S502、构建全局轨迹图G＝＜V,E＞，其中图的节点V＝{τ1,τ2,…,τN}，将任意两个节点都进行连接，构成全局轨迹图的边E。S503、通过带有多头注意力机制的图神经网络GNN提取全局轨迹图G的特征fG。为了去除环境带来的混淆，利多头注意力机制计算得到的H个特征fG1,fG2…,fGH，而H为多头注意力机制中的注意力头数量。在本实施例中，带有多头注意力机制的图神经网络GNN可直接采用图注意力网络实现，其中的H数量可以根据实际进行优化调整。S504、通过可学习的特征矩阵M，提取游戏场景的全局状态s的特征fs＝s×M。S505、利用全局轨迹图G的特征fG以及全局状态s的特征fs进行信用分配，从而估计每个游戏智能体的信用值k1,k2,…kN，其中该信用分配过程通过矩阵乘法实现：{k1,k2,…kN}＝fs×fG。S506、基于所有游戏智能体的局部动作价值函数Q1,Q2,…QN以及其相应的信用值k1,k2,…kN，计算得到联合动作价值函数其中步骤S503～S506等价于使用蒙特卡洛采样来实现因果推理的后门调整：P)＝∑GPP和P)≈∑GPP，其实现了去除环境中的混淆。上述训练框架如图2所示，其值分解因果图如图3所示。S6、所有游戏智能体按照S4中确定采取的行动同时各自执行行动，并与游戏环境进行交互，游戏环境进行即时反馈，提供游戏整体状态和实时奖励。S7、依据游戏环境提供的实时奖励，使用反向传播算法，训练整个多智能体系统，同时更新每个游戏智能体的自身游戏策略模型以及中心评判器的参数。在本实施例中，S7的具体包括以下子步骤：S701、获取游戏环境提供的实时奖励r，利用贝尔曼最优方程计算TD损失其中γ为折扣，为强化学习中目标网络输出的联合动作价值函数估计；S702、利用TD损失L对整个多智能体系统中所有游戏智能体的自身游戏策略模型以及中心评判器进行反向传播，更新所有网络模型的可学习参数；S703、每隔一段预定时间间隔，对目标网络的参数进行一次更新。S8、不断重复步骤S4～S7，对游戏策略模型进行迭代训练，直至达到设定的终止条件为止，游戏场景中每一个独立角色均得到已训练的游戏策略模型。需要说明的是，上述预先选定的探索策略可以根据实际情况进行调整，通过改变该探索策略可以调整基于所有可行动作的价值选定下一时间步将要执行的动作ai的策略。在本实施例中，该探索策略可采用epsilon贪心算法。本发明的基于多智能体强化学习的去混淆游戏策略模型生成方法利用因果推理中的知识，重点关注在环境带来的混杂因素，通过后门准则来实现多智能体强化学习中的去混淆。相比主流的算法，本发明可以使得多智能体强化学习中的信用分配可以对联合动作价值函数有着直接的因果效应，本发明可以应用到主流的多智能体强化学习值分解算法当中，使得模型结构更加合理，大幅度提升多智能体系统的性能。在本发明中，上述基于多智能体强化学习的去混淆游戏策略模型生成方法，可应用在多人在线竞技游戏中，通过用多智能体系统来实现敌方AI。另外除了在游戏领域之外还可以有着广泛实际应用，例如可以用于交通控制，网络路由等领域。例如，在交通控制中，可以使用多智能体模拟交通信号灯，实现交通信号灯的智能自主控制。下面利用前述实施例中的基于多智能体强化学习的去混淆游戏策略模型生成方法，通过一个具体的应用实例来展示本发明方法的具体效果。具体的方法步骤如前所述，不再赘述，下面仅展示其具体效果。实施例本实施例将上述S1～S8所示的基于多智能体强化学习的去混淆游戏策略模型生成方法，在星际争霸II微操作挑战上进行测试。星际争霸II是流行的多人在线竞技类游戏，在微操作挑战中，每个盟军单位都由一个智能体控制，并根据当局部的观察结果采取行动。对手的军队由手工设计的内置AI控制。该挑战的目标是在每个战斗场景中杀死所有敌人。每个游戏场景中，环境都会产生奖励，其奖励的数量由造成的生命值伤害和杀死的敌方单位所决定。此外，当战斗获胜时，环境还会给予另一个奖励。在每个时间步内，每个智能体只获得其视野内的环境信息。此外，所有智能体只能攻击其射击范围内的敌人，该范围设置为6。全局状态信息不受视线范围限制，将用于预测每个智能体的信用值。星际争霸II微操作挑战包含了复杂而丰富的场景，按照智能体的类型可以分为同构场景和异构场景，按照敌我双方兵力，可以分为对称场景和非对称场景。所有的场景可以分为三个难度：简单难度，困难难度，超级困难难度。为了展示出本发明的高效性，在困难难度和超级困难难度的两种场景下都进行了测试。具体地，本实施例在六个场景中进行测试3s_vs_5z，5m_vs_6m，MMM2，3s5z_vs_3s6z，corridor，6h_vs_8z。其中3s_vs_5z，5m_vs_6m，MMM2为困难难度场景，3s5z_vs_3s6z，corridor，6h_vs_8z为超级困难难度场景。为了展示本发明方法的测试结果如图4所示。结果表明，本发明的去混淆游戏策略模型生成方法可以大幅提升各个多智能体强化学习算法的性能。另外，基于与上述实施例的去混淆游戏策略模型生成方法相同的发明构思，本申请的另一实施例还提供了一种游戏场景中的角色控制方法，其采用上述去混淆游戏策略模型生成方法得到每个独立角色对应的游戏策略模型，并用于控制对应的游戏智能体；所述角色控制方法包括：获取目标游戏智能体在游戏场景中的实时状态数据，将目标游戏智能体的实时状态数据输入目标游戏智能体对应的已训练的游戏策略模型中，得到所述游戏策略模型输出的根据所述游戏智能体的状态数据生成的控制策略；采用得到的控制策略，控制目标游戏智能体进行动作。基于与上述实施例的游戏场景中的角色控制方法相同的发明构思，本申请的另一实施例还提供了一种游戏场景中的角色控制装置，其包括：状态获取单元，用于获取目标游戏智能体在游戏场景中的实时状态数据；控制策略输出单元，用于将目标游戏智能体的实时状态数据输入目标游戏智能体对应的已训练的游戏策略模型中，得到所述游戏策略模型输出的根据所述游戏智能体的状态数据生成的控制策略；所述游戏策略模型是采用上述去混淆游戏策略模型生成方法训练得到的；动作控制单元，用于采用得到的控制策略，控制目标游戏智能体进行动作。需要说明的是，上述装置中的各单元，可以通过软件程序模块来实现。基于与上述实施例的游戏场景中的角色控制方法相同的发明构思，本申请的另一实施例还提供了一种电子设备，其包括存储器和处理器；所述存储器，用于存储计算机程序；所述处理器，用于当执行所述计算机程序时，实现前述的去混淆游戏策略模型生成方法或前述的游戏场景中的角色控制方法。此外，上述的存储器中的逻辑指令可以通过软件功能单元的形式实现并作为独立的产品销售或使用时，可以存储在一个计算机可读取存储介质中。基于这样的理解，本发明的技术方案本质上或者说对现有技术做出贡献的部分或者该技术方案的部分可以以软件产品的形式体现出来，该计算机软件产品存储在一个存储介质中，包括若干指令用以使得一台计算机设备执行本发明各个实施例所述方法的全部或部分步骤。基于与上述实施例的游戏场景中的角色控制方法相同的发明构思，本申请的另一实施例还提供了一种计算机可读存储介质，该存储介质上存储有计算机程序，当所述计算机程序被处理器执行时，实现前述的去混淆游戏策略模型生成方法或前述的游戏场景中的角色控制方法。可以理解的是，上述存储介质、存储器可以采用随机存取存储器，也可以采用非易失性存储器，例如至少一个磁盘存储器。同时存储介质还可以是U盘、移动硬盘、磁碟或者光盘等各种可以存储程序代码的介质。可以理解的是，上述的处理器可以是通用处理器，包括中央处理器、网络处理器等；还可以是数字信号处理器、专用集成电路、现场可编程门阵列或者其他可编程逻辑器件、分立门或者晶体管逻辑器件、分立硬件组件。另外需要说明的是，所属领域的技术人员可以清楚地了解到，为描述的方便和简洁，上述描述的装置的具体工作过程，可以参考前述方法实施例中的对应过程，在此不再赘述。在本申请所提供的各实施例中，所述的装置和方法中对于步骤或者模块的划分，仅仅为一种逻辑功能划分，实际实现时可以有另外的划分方式，例如多个模块或步骤可以结合或者可以集成到一起，一个模块或者步骤亦可进行拆分。以上所述的实施例只是本发明的一种较佳的方案，然其并非用以限制本发明。有关技术领域的普通技术人员，在不脱离本发明的精神和范围的情况下，还可以做出各种变化和变型。因此凡采取等同替换或等效变换的方式所获得的技术方案，均落在本发明的保护范围内。
