标题title
一种姿势引导对齐的轻量行人重识别方法
摘要abst
本发明公开了一种姿势引导对齐的轻量行人重识别方法，所述方法包括以下步骤：使用主干网络和显著区域提取聚焦行人图像关键区域进行特征增强并得到全局特征；使用姿势估计算法进行行人图像的关键点检测并获取关键点局部特征；通过特征聚合模块对关键点局部特征进行聚合和对齐，并使用全局特征进行特征弥补；通过交叉图匹配模块，对输入行人图像的特征进行匹配，得到图像相互匹配的区域，并将得到的特征计算相似度；构建损失函数，对网络参数进行迭代优化；进行相似性度量，输出匹配列表。本发明高效地提取了行人图像的鲁棒的特征，有效的解决了特征非对齐问题，在保持模型参数量的同时达到了优异的行人重识别性能。
权利要求书clms
1.一种姿势引导对齐的轻量行人重识别方法，其特征在于，所述方法包括以下步骤：使用轻量的主干网络提取输入行人图像的特征，通过显著区域提取聚焦行人图像关键区域进行特征增强并得到全局特征；利用轻量的姿势估计算法进行行人图像的关键点检测，并与增强之后的特征图进行外积操作得到行人图像的关键点局部特征；通过特征聚合模块对关键点局部特征进行聚合和对齐，再将全局特征与超参数结合弥补关键点局部特征，得到最终的局部特征表示；通过交叉图匹配模块，使用图匹配算法对输入行人图像的特征进行匹配，得到图像相互匹配的区域进行特征增强，将得到的特征进行相似性度量；构建损失函数，对网络参数进行迭代优化；将最终提取到的行人图像特征与图像库中的各个图像进行相似性度量，输出匹配列表。2.根据权利要求1所述的姿势引导对齐的轻量行人重识别方法，其特征在于，整体框架包括，包含了主干网络、姿势估计网络和显著区域提取的语义提取模块；包含了特征聚合模块的聚合对齐模块和包含了交叉图匹配模块的特征匹配模块。3.根据权利要求1所述的姿势引导对齐的轻量行人重识别方法，其特征在于，所述的主干网络为OSNet，轻量姿势估计网络为Lite-HRNet。4. 根据权利要求1所述的姿势引导对齐的轻量行人重识别方法，其特征在于，所述的显著区域提取为空间注意力，包括分割、池化、卷积块、BN、ReLU、 Sigmoid和残差结构。5.根据权利要求4所述的姿势引导对齐的轻量行人重识别方法，其特征在于，所述卷积块由深度可分离卷积构成，具体执行顺序为1x1卷积，3x3卷积，3x3卷积和1x1卷积。6.根据权利要求1所述的姿势引导对齐的轻量行人重识别方法，其特征在于，所述特征聚合模块由分区、更新和聚合三部分组成。7.根据权利要求6所述的姿势引导对齐的轻量行人重识别方法，其特征在于，所述分区、更新和聚合包括以下内容：所述分区操作是将行人图像的关键点局部特征根据人体结构和对称性划分成不同的六部分；所述更新操作是使用图卷积网络，将各个区域内的节点进行消息传递；所述聚合操作是将各个节点信息在区域内和区域间进行特征相加操作。8.根据权利要求1所述的姿势引导对齐的轻量行人重识别方法，其特征在于，所述交叉图匹配模块包括特征嵌入、交叉匹配、特征变换和特征聚合模块。9.根据权利要求1所述的姿势引导对齐的轻量行人重识别方法，其特征在于，所述相似性度量包括特征相减、全连接和Sigmoid操作。10.根据权利要求1所述的姿势引导对齐的轻量行人重识别方法，其特征在于，所述得到最终的全局特征和局部特征使用交叉熵损失、中心损失和难样本三元组损失进行训练；相似性度量得到的特征使用验证损失进行训练。
说明书desc
技术领域本发明涉及一种姿势引导对齐的轻量行人重识别方法，属于计算机视觉技术领域。背景技术随着智能监控网络的快速发展以及人们对公共安全需求的不断增加，行人重识别已成为计算机视觉领域的研究热点之一。我国视频监控系统规模庞大，视频监控系统每天不间断的运作会产生海量的数据，如果安排人力对视频数据进行监察和分析会造成低效率并且难以实现有效处理监控数据的情况，因此，如何在海量视频监控数据中检索出有效行人的信息是当前值得研究的问题。行人重识别是一种利用计算机视觉在不同摄像机下检索同一个行人图像的技术。随着科技的发展和对公共安全需求的增加，行人ReID被广泛用于社区寻人和犯罪调查等智能安全领域。传统的ReID提取底层视觉特征的方法不适用于复杂场景和各种干扰存在的情况。随着深度学习的广泛应用和深度卷积神经网络的快速发展，行人ReID研究取得了重大的进展。在实际的摄像监控系统中，行人检测是行人ReID之前的重要步骤，行人检测的质量直接影响行人ReID的性能。当检测到的行人未填充于整个包围框中或不完全存在于边界框中时，ReID的准确性将大大降低。为了解决这个问题，AlignedReID计算两个被分割的行人图像对之间的距离，并动态规划从上到下的最短路径，以实现相应区域的匹配。行人检测和行人ReID相结合的方法是根据行人ReID的效果来指导行人检测中边界框的生成，因此可以更准确地获得行人图像。然而，上述基于对齐的方法忽略了图像背景，并且在计算相应区域的相似度时会引入背景干扰信息。当背景发生显著变化时，会导致某些区域提取的特征发生偏差，从而影响模型的匹配性能。一些方法使用注意机制来解决特征未对准的问题。VPM添加了区域的可见性作为提取特征的补充，忽略了不可见部分，并使用两个行人图像的公共部分来计算相似度。AANet设计了一个注意力对齐网络，该网络使模型聚焦于行人图像的前景信息，并提取特征以实现特征对齐。然而，这些方法依赖于行人图像的空间特征。当行人姿势改变时，提取的特征在行人匹配过程中仍然会受到影响。随着姿势估计网络的发展，许多方法将其引入行人ReID中以学习人类拓扑关系，以增强特征的表示能力。HOReID使用交叉图嵌入对齐层来实现行人图像的非遮挡区域的特征对齐。PAII学习人类拓扑关系以获得特征，并引入超参数以实现特征融合以实现特征对齐。但是，上述基于姿势估计网络的方法具有一定的参数开销，这忽略了模型的复杂性，导致了高模型复杂性。因此，虽然现有基于特征对齐的行人重识别研究在提高行人重识别模型的准确率方面发展较为成熟，但是忽略了模型的复杂度和实时性，难以在终端进行部署，存在实时性差和模型冗余复杂等问题，并且已有轻量化模型的精度效果比较差，无法做好模型精确度和复杂度的平衡。因此，亟待一种有效基于特征对齐的行人重识别方法以解决上述问题。发明内容针对现有方法中存在的问题，本发明的目的在于提供一种姿势引导对齐的轻量行人重识别方法，包括以下步骤：使用轻量的主干网络提取输入行人图像的特征，通过显著区域提取聚焦行人图像关键区域进行特征增强并得到全局特征；利用轻量的姿势估计算法进行行人图像的关键点检测，并与增强之后的特征图进行外积操作得到行人图像的关键点局部特征；通过特征聚合模块对关键点局部特征进行聚合和对齐，再将全局特征与超参数结合弥补关键点局部特征，得到最终的局部特征表示；通过交叉图匹配模块，使用图匹配算法对输入行人图像的特征进行匹配，得到图像相互匹配的区域进行特征增强，将得到的特征进行相似性度量；构建损失函数，对网络参数进行迭代优化；将最终提取到的行人图像特征与图像库中的各个图像进行相似性度量，输出匹配列表。进一步的，整体框架包括，包含了主干网络、姿势估计网络和显著区域提取的语义提取模块；包含了特征聚合模块的聚合对齐模块和包含了交叉图匹配模块的特征匹配模块。进一步的，所述的主干网络为OSNet，轻量姿势估计网络为Lite-HRNet。进一步的，所述的显著区域提取为空间注意力，包括分割、池化、卷积块、BN、ReLU和Sigmoid和残差结构。进一步的，所述卷积块由深度可分离卷积构成，具体执行顺序为1x1卷积，3x3卷积， 3x3卷积和1x1卷积。进一步的，特征聚合模块由分区、更新和聚合三部分组成。进一步的，所述分区、更新和聚合包括以下内容：所述分区操作是将行人图像的关键点局部特征根据人体结构和对称性划分成不同的六部分；所述更新操作是使用图卷积网络，将各个区域内的节点进行消息传递；所述聚合操作是将各个节点信息在区域内和区域间进行特征相加操作。进一步的，所述交叉图匹配模块包括特征嵌入、交叉匹配、特征变换和特征聚合模块。进一步的，所述相似性度量包括特征相减、全连接和Sigmoid操作。进一步的，所述得到最终的全局特征和局部特征使用交叉熵损失、中心损失和难样本三元组损失进行训练；相似性度量得到的特征使用验证损失进行训练。与现有技术相比，本发明具有如下有益效果：1、本发明提出了一种新颖的姿势引导对齐的轻量行人重识别方法，有效提取鲁棒性特征，所有模块的设计都考虑了模型的大小，可以充分挖掘行人图像特征进行特征对齐从而有效匹配行人。2、针对本发明创新性地将深度可分离卷积应用于注意力机制中，以一种轻量的方式，在有效切分特征图的前提下有效的挖掘了行人图像的显著区域特征，克服了特征挖掘不充分，关注背景特征的问题。3、针对本发明创新性地考虑人体结构及对称性，将行人特征进行聚合以进行匹配，并利用图匹配算法，高效实现特征对齐，解决行人匹配时的特征不对应问题，保证了提取特征的有效性。4、本发明在Market1501和DukeMTMC-reID两个广泛使用的数据集上进行了充分的实验，采用了全面的评价指标对模型精确度和复杂度进行评价：包括mAP、Rank-1和模型的参数量。在两种指标下的实验结果充分证明了本发明方法的有效性。附图说明通过阅读参照以下附图对非限制性实施例所作的详细描述，本发明的其它特征、目的和优点将会变得更明显：图1为本发明提供的一种姿势引导对齐的轻量行人重识别方法的网络结构图；图2为本发明提供的一种姿势引导对齐的轻量行人重识别方法的步骤流程图；图3为本发明提供的一种姿势引导对齐的轻量行人重识别方法的整体框架图；图4为本发明提供的一个优选实施例的显著区域提取的结构示意图；图5为本发明提供的一个优选实施例的特征聚合模块的结构示意图；图6为本发明提供的一个优选实施例的交叉图匹配模块的结构示意图。具体实施方式为了使本领域的技术人员可以更清楚地对本发明进行了解，下面结合具体实施例进行说明。此处所描述的具体实施例仅用于解释本发明，并不用于限定本发明。如图2所示，为本发明提供的一个实施例的姿势引导对齐的轻量行人重识别方法的步骤流程图，包括：S1,使用轻量的主干网络提取输入行人图像的特征，通过显著区域提取聚焦行人图像关键区域进行特征增强并得到全局特征；S2,利用轻量的姿势估计算法进行行人图像的关键点检测，并与增强之后的特征图进行外积操作得到行人图像的关键点局部特征；S3,通过特征聚合模块对关键点局部特征进行聚合和对齐，再将全局特征与超参数结合弥补关键点局部特征，得到最终的局部特征表示；S4,通过交叉图匹配模块，使用图匹配算法对输入行人图像的特征进行匹配，得到图像相互匹配的区域进行特征增强，将得到的特征进行相似性度量；S5,构建损失函数，对网络参数进行迭代优化；S6,将最终提取到的行人图像特征与图像库中的各个图像进行相似性度量，输出匹配列表。如图3所示，为本发明提供的一个实施例的姿势引导对齐的轻量行人重识别方法的整体框架图。其主要包括三个模块，包含了主干网络、姿势估计网络和显著区域提取的语义提取模块；包含了特征聚合模块的聚合对齐模块和包含了交叉图匹配模块的特征匹配模块。本发明提供一个优选实施例执行S1。本实施例的目的在于利用轻量网络，充分提取行人图像的整体结构以及拓扑信息。给定一组行人图像，其中，为输入行人图像的数量。行人图像特征通过OSNet主干网络获得，其中、和分别表示图像的高度、宽度和通道数。然后，我们使用轻量的显著区域提取对获取的行人图像特征进行特征增强，如图4所示，为显著区域提取的结构示意图。具体实现步骤如下：S11，获取行人图像特征及特征图分割。将从主干网络获得的特征图分为两部分，这两部分可以更好地聚焦于行人图像的关键区域，以获得特征和。其公式如下：其中，表示OSNet骨干网络，表示分割操作。S12，池化操作提取特征。沿着通道维度对特征图的每个部分执行全局最大池化和全局平均池化，并且沿着通道维度拼接所获得的特征图。然后，在每个部分中执行卷积块以提取特征和。其公式如下：其中，表示1x1卷积，表示3x3卷积。S13，特征图大小恢复。我们沿着高度维度拼接特征图以恢复原始特征图大小。S14，得到增强的特征。在BN、ReLU和Sigmoid操作之后，我们将特征图与原始特征图执行元素乘法以获得加权特征。最终的输出特征定义为：其中，表示拼接函数，表示Sigmoid函数。S15，获取全局特征。我们使用增强的特征来获得全局特征 。公式如下：其中，表示全局最大池，表示全局平均池。本发明提供了一个优选实施例执行S2。我们使用Lite-HRNet获得了17个人体关键点的热力图，它们属于0～16。其中，是鼻子、左眼、右眼、左耳和右耳；为左肩和右肩；为左肘和右肘；为左手腕和右手腕；为左髋和右髋；为左膝和右膝；是左脚踝和右脚踝。为了降低网络复杂度，我们将的热力图聚合为，随后的关键点热力图按顺序依次排序。随后，使用从显著区域提取获得的特征图和从Lite-HRNet获得的关键点热力图进行外积运算，以获得13个关键点局部特征。其公式如下：其中，表示外积操作。本发明提供了一个优选实施例执行S3。本实施例的目的在于利用得到的关键点局部特征进行特征聚合和对齐，充分提取特征并进行处理，设计轻量的特征聚合模块保持网络轻量。如图5所示，为特征聚合模块的结构示意图。该模块包括分区、更新和聚合三部分内容。具体实现步骤如下：S31，分区操作。分区是通过人体结构和对称性来划分局部特征，以形成与头部、身体、左臂、右臂、左腿和右腿相对应的不同区域。划分的不同区域的局部特征表示如下：S32，更新操作。我们使用图卷积网络将六个区域中的局部特征进行信息交互操作，并将局部特征视为GCN操作中的节点。在每个区域中相邻节点信息交互之后，我们得到每个节点的更新特征。GCN的正向传播定义为：其中，是权重矩阵，是对应区域中的邻接矩阵，是对应区域的度矩阵，是节点特征，表示GCN中的节点层。S33，聚合操作。首先，聚合每个区域相对应的局部特征，其表示如下：其中，表示对应的区域中的局部特征，表示对应的区域中局部特征的数量。然后进一步融合不同身体区域的特征，以获得对应于不同身体结构的不同局部特征。由于关键点的预测可能不准确，并且所获得的图像局部特征仅是一些关键点特征，这些关键点特征不充分，不能鲁棒地表示行人的局部特征，因此使用全局特征与超参数相结合来补充局部特征，以生成最终的局部特征表示。最终的局部特征公式如下：其中，表示相加函数。本发明提供了一个优选实施例执行S4。本实施例的目的在于进一步增强行人图像对应区域位置的特征并增强特征的鲁棒性。如图6所示，为交叉图匹配模块的结构示意图。输入特征列表和是两个输入图像的子图特征，其包含了输入图像的全局特征和关键点局部特征。具体实现步骤如下：S41，特征嵌入。嵌入输入特征和，以获得特征和。其中，表示关键点局部特征的数量。S42，获得相似性矩阵。将嵌入的特征用于图匹配以获得相似矩阵，表示输入图像之间的对应关系。其公式如下：其中，是图匹配运算。S43，交叉匹配。使用交叉运算来增强特征，并与原始特征进行拼接操作，以获得特征和。其公式如下：S44，特征变换。将特征转换回原始的非嵌入状态，并通过使用特征聚合模块进行有效的特征处理来简化所获得的特征，以获得对齐的局部特征和。在训练期间，相应图像的全局特征也被附加到具有局部特征的特征列表中，以同时处理它们。最终特征列表和表示如下：其中，和为相应输入图像的全局特征。S45，相似性度量。我们使用嵌入特征和计算输入图像之间的相似度，相似度计算公式如下：其中，FC为全连接层。本发明提供一个实施例执行S5。构建训练所需损失函数。在本实施例中，训练损失函数包含交叉熵损失、难样本三元组损失、中心损失和验证损失四部分内容，具体步骤如下：S51，构建交叉熵损失。我们采用带有平滑标签的交叉熵损失来计算真实值与预测值之间的概率。交叉熵损失公式如下所示：其中，表示批大小，是真值身份标签，是类的身份预测分数。S52，构建难样本三元组损失。为了优化嵌入空间，引入了难样本三元组损失，使得类间距离大于类内距离。传统的三元组损失随机从训练数据中抽取三张图片，虽然操作简单，但是抽样出来的绝大多数都易于区分的样本对，在现实场景下，难以区分的行人样本比较多，所以使用Hard Triplet Loss对模型训练，考虑锚样本与相似度最低的正样本和锚样本与相似度最高的负样本之间的距离，使网络学习到更好的表征，提高网络的分类能力。难样本三元组损失公式如下所示：其中，是随机选择的不同身份的行人，是每个身份随机选择的图像数量，表示批量大小。，是边界，初始化为0.3。S53，构建中心损失。由于难样本三元组损失学习的是样本之间的相对距离，没有考虑类内的紧凑性。因此通过使用中心损失学习每个类的类中心，使得类内的距离变得更紧凑。中心损失公式如下所示：其中，是第个特征的特征向量；是特征的类中心。S54，构建验证损失。在特征匹配模块中，我们获得输入图像之间的相似度，我们使用相似度特征来计算验证损失。其公式如下：其中，表示真实值，其中表示输入图像属于同一个人，否则。对于全局特征，我们使用难样本三元组损失计算获得，中心损失计算获得，交叉熵损失获得。对于局部特征，我们首先沿着通道维度拼接局部特征，并且使用与全局特征相同的三个损失函数来获得，和。总损失表达式如下所示：基于上述实施例，本发明提供一个优选实施例S6，通过计算余弦距离进行行人图像特征之间的相似性度量，得到最终的匹配结果，以相似度从大到小生成匹配列表。余弦相似度计算如下所示：其中，和为特征向量。在本实施例中，我们使用在ImageNet上预训练的OSNet作为主干网络，并删除模型最后的全局平均池层层和全连接层。在COCO数据集上预训练的Lite-HRNet被用作姿势估计网络，以获得关键点。输入图像的大小调整为256x128 ，然后通过随机裁剪、随机翻转、随机擦除和归一化进行增强。批量大小设置为64，每次使用4个不同行人的16张图像进行训练，并使用Adam优化器进行优化。为了进一步提高模型的性能，我们使用Warmup余弦退火学习率策略。初始学习率设置为，权重迭代设置为0.0005，学习率在20代内从线性增长到，然后进行余弦衰减。为了验证以上实施例的有效性，我们在两个广泛使用的数据集Market1501和DukeMTMC-reID上进行验证。Market1501数据集包含1501个身份和32217张行人图像。训练集包含751个身份的12936张图像，测试集包含750个身份的19732张图像。DukeMTMC-reID数据集包含2514个身份和总共36441个行人图像。训练集包含702个身份的16522个图像，测试集包含1812个身份的19889个图像。以累积匹配特征中Rank-n和平均精度均值对模型精确度效果进行评估；以模型参数量对模型复杂度进行评估。其中，CMC中的Rank-n表示行人匹配结果列表中前n个图像与查询图像匹配成功的概率；mAP表示多分类任务中将平均精度AP相加求和之后的平均值。表1展示了在Market1501和DukeMTMC-reID数据集上所提方法和其他基于对齐的SOTA方法的Rank-1和mAP精度。可以看出，所提方法在DukeMTMC-reID数据集上实现了优异的精度，这验证了我们提出的方法具有解决特征未对齐问题的优异能力。表1 所提方法与Market1501和DukeMTMC-reID数据集上的几种基于对齐的SOTA方法进行比较表2显示了Market1501和DukeMTMC-reID数据集上所提方法和其他SOTA方法的Rank-1和mAP精度。可以获得以下观察结果：1、在Market1501上，所提方法的Rank-1和mAP的准确率分别为95.5%和88.1%，在DukeMTMC-reID上的准确率为89.2%和78.8%。2、SPMP是一种强大的基于特征对齐的行人ReID方法。它是Market1501和DukeMTMC-reID数据集上最接近的竞争对手。在DukeMTMC-reID数据集上，所提方法的mAP超过了SPMP的1.2%，Rank-1超过SPMP的1.7%。在Market1501数据集上所提方法的mAP超过SPMP的0.8%，并具有相近的Rank-1精度。SPMP的Rank-1精度仅比我们的方法高0.2%。然而，SPMP使用了复杂度主干网络和姿势估计网络，并具有较大模型复杂度，但其Rank-1精度只略高于我们的方法。与其他SOTA相比，我们的方法仍然实现了优异的性能，并保持了模型的轻量级。3、上述结果表明，与公共数据集上的SOTA相比，所提方法获得了优异的性能。这是因为所提出的方法可以有效地提取鲁棒特征，并解决行人图像的特征非对齐问题。表2 所提方法与SOTA方法在与Market1501和DukeMTMC-reID数据集上的比较所提方法将轻量级OSNet作为主干网络，将轻量级姿态估计网络用于获取人体关键点，并将深度可分离卷积应用于显著区域提取。因此，语义提取模块的Param复杂度相对较低。在聚合对齐模块中，我们不单独处理每个关键点特征，而是使用特征聚合模块来获得鲁棒的简化之后的特征。我们只需要处理行人聚合之后三个部分的特征，并且GCN可以在所有节点上并行计算，这不需要特征分解或其他内存消耗较大的矩阵运算。在特征匹配模块中，我们使用聚合后的简化特征进行特征匹配，以避免对每个关键点进行操作，因此模型复杂度较低。如表3所示，所提方法只有17.13M的参数量。与其他先进的基于姿势对齐的行人ReID方法相比，例如HOReID、PAII和SPMP，它们使用高度复杂的主干网络和姿势估计网络来获得行人语义特征，他们的Param比我们提出的方法高得多。与其他先进方法相比，本文提出的方法具有更高的识别精度，并保持较少的模型复杂性。表3 在Market1501数据集上不同方法的参数复杂性和准确性的比较以上对本发明的具体实施例进行了描述。需要理解的是，本发明并不局限于上述特定实施方式，本领域技术人员可以在权利要求的范围内做出各种变形或修改，这并不影响本发明的实质内容。上述各优选特征在互不冲突的情况下，可以任意组合使用。
