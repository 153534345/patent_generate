标题title
一种视觉驱动的虚拟角色处理系统
摘要abst
本发明提供了一种视觉驱动的虚拟角色处理系统，该系统包括非虚拟角色图像组列表A={A1，A2，……，Ai，……，Am}、虚拟角色图像组B、处理器和存储有计算机程序的存储器，i=1，2，……，m，m为非虚拟角色图像组数量，Ai为第i帧非虚拟角色图像组，还包括根据A获得的非虚拟姿态图像组C={C1，C2，……，Ci，……，Cm}和非虚拟面部图像组D={D1，D2，……，Di，……，Dm}，以及根据虚拟角色图像组B获得的虚拟角色的姿态图像Bb和虚拟角色的表情图像Bm，当所述计算机程序被处理器执行时，将Ci与Bb对齐，将Di与Bm对齐，提高了虚拟角色与非虚拟角色的吻合度。
权利要求书clms
1.一种视觉驱动的虚拟角色处理系统，其特征在于，所述系统包括：非虚拟角色图像组列表A={A1，A2，……，Ai，……，Am}、虚拟角色图像组B、处理器和存储有计算机程序的存储器，i=1，2，……，m，m为非虚拟角色图像组数量，Ai为第i帧非虚拟角色图像组，还包括根据A获得的非虚拟姿态图像组C={C1，C2，……，Ci，……Cm}和非虚拟面部图像组D={D1，D2，……，Di，……，Dm}，Ci为Ai中的姿态图像，Di为Ai中的面部图像，以及根据虚拟角色图像组B获得的虚拟角色的姿态图像Bb和虚拟角色的表情图像Bm，当所述计算机程序被处理器执行时，实现以下步骤：S1：将Ci与Bb对齐，具体步骤如下：S11：获取姿态相似度阈值St，令i=1；S12：根据Ci获得非虚拟角色的姿态位姿向量V1i，根据Bb获得虚拟角色的姿态位姿向量Vb；S13：根据V1i和Vb获得位姿向量相似度Sc=/，其中||V1i||为对V1i取模，V1i•Vb为V1i和Vb进行点乘；S14：若Sc＞St，则记录g=i，g为与Bb对齐的非虚拟角色的姿态图像在C中的次序；否则令i=i+1并执行步骤S12；S2：将Di与Bm对齐，具体步骤如下：S21：获取面部相似度阈值Dt，令i=1；S22：根据Di获得非虚拟角色面部的关键点坐标集Li={，，……，，……，}，其中xLij为Di的第j个关键点的横坐标，yLij为Di的第j个关键点的纵坐标，根据Bm获得虚拟角色的面部关键点坐标集Q={,,……，，……，}，面部的关键点为面部的关键区域的位置，其中xQj为Bm的第j个关键点的横坐标，yQj为Bm的第j个关键点的纵坐标，j=1，2，……，n，n为面部关键点的总个数；S23：根据Li和Q获得两者之间的面部相似度L=∑nj=12+2）1/2；S24：若L＜Dt，则记录z=i，z为与Bm对齐的非虚拟角色的面部图像在D中的次序；S3：若g=z则执行S31到S32，否则执行S33：S31：从A中删除A1，A2，……，Ag-1，获取到A0={Ag，Ag+1，……，Am}；从C中删除C1，C2，……，Cg-1，获取到C0={Cg，Cg+1，……，Cm}；从D中删除D1，D2，……，Dg-1，获取到D0={Dg，Dg+1，……，Dm}；S32：将A0初始化为A0={A1，A2，……，Ar，……，As}，其中A1=Ag，A2=Ag+1，……，As=Am；C0初始化为C0={C1，C2，……，Cr，……，Cs}，其中C1=Cg，C2=Cg+1，……，Cs=Cm；D0初始化为D0={D1，D2，……，Dr，……，Ds}，其中D1=Dg，D2=Dg+1，……，Ds=Dm，r=1，2……s；S33：若g＜z，则计算Cz对应的Sc，若Cz对应的Sc＞St，则令g=z并执行S31，否则令i=z+1并重新执行S12到S14以及S22到S24；若g＞z，则计算Dg对应的L，若Dg对应的L＜Dt，则令z=g并执行S31，否则令i=g+1并重新执行S12到S14以及S22到S24。2.根据权利要求1所述的系统，其特征在于，所述根据A获得的非虚拟姿态图像组C，具体包括：获取Ai中非虚拟角色的边界框的坐标集Aci=，，，，为边界框的四个顶点的坐标；以/2，/2）为中点，将边界框等比例扩大为原边界框面积的1.1到1.2倍，即1.10.5×|xi1-xi2|≤|xi3-xi4|≤1.20.5×|xi1-xi2|，1.10.5×|yi1-yi2|≤|yi3-yi4|≤1.20.5×|yi1-yi2|；令Ac=，，，，为扩大之后的边界框的四个顶点的坐标；将Ai中位于边界框内的图像作为Ci；对A中的每一帧Ai进行上述步骤。3.根据权利要求1所述的系统，其特征在于，所述计算机程序被处理器执行时，在S3后还实现以下步骤：S4：根据C0和Bb生成虚拟角色的姿态图像组Vb={Vb1，Vb2，……，Vbr，……，Vbs}，Vbr为根据Cr生成的虚拟角色的姿态图像；S5：根据D0和Bm生成虚拟角色的面部图像组Vm={Vm1，Vm2，……，Vmr，……，Vms}，Vmr为根据Dr生成的虚拟角色的面部图像。4.根据权利要求3所述的系统，所述步骤S5具体包括：S51：对Bm进行深度估计，获得Bm的深度图像Db；S52：对D0的每一帧Dr进行深度估计，获得D0的深度图像组Dp={Dp1，Dp2，……，Dpr，……，Dps}，Dpr为Dr的深度图像；S53：根据Bm，Db以及Dp获得Vm。5.根据权利要求4所述的系统，其特征在于，所述计算机程序被处理器执行时，还包括以下步骤：S6：根据Vb和Vm生成虚拟角色图像组V={V1，V2，……，Vr，Vs}，其中Vr为第r帧虚拟角色图像。6.根据权利要求2所述的系统，其特征在于：令1.10.5×|xi1-xi2|=|xi3-xi4|，1.10.5×|yi1-yi2|=|yi3-yi4|。7.根据权利要求3所述的系统，其特征在于，所述根据C0和Bb生成Vb所使用的算法模型为TPSM模型。
说明书desc
技术领域本发明涉及计算机视觉领域，特别是涉及一种视觉驱动的虚拟角色处理系统。背景技术近年来随着人工智能技术的不断突破和元宇宙概念的蓬勃发展，数字虚拟角色已经进入了人们生活的各个领域。数字虚拟角色是指具有数字化外形的虚拟人物，其具备形象能力、表达能力、感知能力、互动能力。数字虚拟角色可分为真人驱动虚拟角色和AI驱动虚拟角色两大类别，目前真人驱动虚拟角色广泛应用于直播领域，虚拟主播在直播带货等节目中的互动环节能增加直播的个性化和趣味性。目前大部分真人驱动虚拟角色的方法都需要用到较为昂贵的动捕设备，若不使用动捕设备的驱动方法，则无法将真人人体的关键点与虚拟角色的关键点一一对应，导致虚拟角色与非虚拟角色的吻合度较差。发明内容针对上述技术问题，本发明采用的技术方案为：一种视觉驱动的虚拟角色处理系统，系统包括：非虚拟角色图像组列表A={A1，A2，……，Ai，……，Am}、虚拟角色图像组B、处理器和存储有计算机程序的存储器，i=1，2，……，m，m为非虚拟角色图像组数量，Ai为第i帧非虚拟角色图像组，还包括根据A获得的非虚拟姿态图像组C={C1，C2，……，Ci，……Cm}和非虚拟面部图像组D={D1，D2，……，Di，……，Dm}，Ci为Ai中的姿态图像，Di为Ai中的面部图像，以及根据虚拟角色图像组B获得的虚拟角色的姿态图像Bb和虚拟角色的表情图像Bm，当所述计算机程序被处理器执行时，实现以下步骤：S1：将Ci与Bb对齐，具体步骤如下：S11：获取姿态相似度阈值St，令i=1；S12：根据Ci获得非虚拟角色的姿态位姿向量V1i，根据Bb获得虚拟角色的姿态位姿向量Vb；S13：根据V1i和Vb获得位姿向量相似度Sc=/，其中||V1i||为对V1i取模，V1i•Vb为V1i和Vb进行点乘；S14：若Sc＞St，则记录g=i，g为与Bb对齐的非虚拟角色的姿态图像在C中的次序；否则令i=i+1并执行步骤S12；S2：将Di与Bm对齐，具体步骤如下：S21：获取面部相似度阈值Dt，令i=1；S22：根据Di获得非虚拟角色面部的关键点坐标集Li={，，……，，……，}，其中xLij为Di的第j个关键点的横坐标，yLij为Di的第j个关键点的纵坐标，根据Bm获得虚拟角色的面部关键点坐标集Q={,,……，，……，}，面部的关键点为面部的关键区域的位置，其中xQj为Bm的第j个关键点的横坐标，yQj为Bm的第j个关键点的纵坐标，j=1，2，……，n，n为面部关键点的总个数；S23：根据Li和Q获得两者之间的面部相似度L=∑nj=12+2）1/2；S24：若L＜Dt，则记录z=i，z为与Bm对齐的非虚拟角色的面部图像在D中的次序；S3：若g=z则执行S31到S32，否则执行S33：S31：从A中删除A1，A2，……，Ag-1，获取到A0={Ag，Ag+1，……，Am}；从C中删除C1，C2，……，Cg-1，获取到C0={Cg，Cg+1，……，Cm}；从D中删除D1，D2，……，Dg-1，获取到D0={Dg，Dg+1，……，Dm}；S32：将A0初始化为A0={A1，A2，……，Ar，……，As}，其中A1=Ag，A2=Ag+1，……，As=Am；C0初始化为C0={C1，C2，……，Cr，……，Cs}，其中C1=Cg，C2=Cg+1，……，Cs=Cm；D0初始化为D0={D1，D2，……，Dr，……，Ds}，其中D1=Dg，D2=Dg+1，……，Ds=Dm，r=1，2……s；S33：若g＜z，则计算Cz对应的Sc，若Cz对应的Sc＞St，则令g=z并执行S31，否则令i=z+1并重新执行S12到S14以及S22到S24；若g＞z，则计算Dg对应的L，若Dg对应的L＜Dt，则令z=g并执行S31，否则令i=g+1并重新执行S12到S14以及S22到S24。本发明至少具有以下有益效果：当系统执行程序时，第一步先将Ci与Bb进行对齐，首先获取C1的位姿向量V11和Bb的位姿向量Vb，计算二者的位姿向量相似度Sc并与预先设定的阈值St进行比较，若Sc＞St，则C1与Bb的姿态已经对齐，否则该帧的姿态未与Bb对齐，此时对C2进行相同操作，直到取到Ci与Bb相对齐。第二步将Di与Bm进行对齐，首先获取D1的关键点的坐标集Li以及Bm的关键点的坐标集Q，之后计算Li和Q之间的面部相似度L，并将L与预先设定的阈值Dt进行比较，若L＜Dt，则D1与Bm的面部已经对齐。否则该帧的面部未与Bm对齐，此时对D2进行相同操作，直到Di与Bm对齐。因为在确定虚拟角色时需要面部与姿态同时对齐才能够获得更好的效果，因此之后判断与Bb对齐的Ci和与Bm对齐的Di是否属于同一个Ai，如果并不属于同一个Ai，则需要重新进行对齐，直到同一个Ai所对应的Ci和Di均分别与Bb和Bm对齐。此时非虚拟角色的面部和姿态均已与虚拟角色的面部和姿态相对齐，非虚拟角色和虚拟角色获得了较高的拟合程度，此时将A、C、D进行初始化，将对齐的Ai、Ci、Di作为确定虚拟角色前的首帧图像，之后对虚拟角色进行确定，从而提高了虚拟角色与非虚拟角色之间的吻合度。附图说明为了更清楚地说明本发明实施例中的技术方案，下面将对实施例描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本发明的一些实施例，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。图1为本发明实施例提供的一种视觉驱动的虚拟角色处理系统流程图。具体实施方式下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例仅仅是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域技术人员在没有作出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。一种视觉驱动的虚拟角色处理系统，包括：非虚拟角色图像组列表A={A1，A2，……，Ai，……，Am}、虚拟角色图像组B、处理器和存储有计算机程序的存储器，i=1，2，……，m，m为非虚拟角色图像组数量，Ai为第i帧非虚拟角色图像组，还包括根据A获得的非虚拟姿态图像组C={C1，C2，……，Ci，……，Cm}和非虚拟面部图像组D={D1，D2，……，Di，……，Dm}，Ci为Ai中的姿态图像，Di为Ai中的面部图像，以及根据虚拟角色图像组B获得的虚拟角色的姿态图像Bb和虚拟角色的表情图像Bm。其中，Ai为通过摄像头等视觉采集设备获取的图像，使用摄像头采集到的真人直播视频流对虚拟人的动作进行驱动，无需使用动捕设备、捕捉点等穿戴设备，降低了制作成本。其中，根据A获得C的步骤包括：获取Ai中非虚拟角色的边界框的坐标集Aci=，，，，为边界框的四个顶点的坐标。此实施例中采用YOLOv5行人检测模型获取真人全身在图像上的像素范围，获取行人边界框坐标的公式为：Aci=human-Detect，其中human-Detect为YOLOv5行人检测模型，本领域技术人员知晓的任意一种行人检测的方法均在此发明的保护范围内。以/2，/2）为中点，将边界框等比例扩大为原边界框面积的1.1到1.2倍；即1.10.5×|xi1-xi2|≤|xi3-xi4|≤1.20.5×|xi1-xi2|，1.10.5×|yi1-yi2|≤|yi3-yi4|≤1.20.5×|yi1-yi2|；令Ac=，，，，为扩大之后的边界框的四个顶点的坐标。选取人体区域并适当扩大区域范围用于真人动作驱动，当行人边界框只选用人体区域的部分时，人体在姿态发生改变的过程中肢体容易超出到行人边界框的范围之外，而当行人边界框扩大过多时，框内的人体占比较低时，会对最终生成效果产生影响。将Ai中位于边界框内的部分作为Ci；对A中的每一帧Ai进行上述步骤。优选的，令1.10.5×|xi1-xi2|=|xi3-xi4|，1.10.5×|yi1-yi2|=|yi3-yi4|。其中，根据A获得D的步骤包括：获取虚拟姿态图像Bb的步骤与获取真人姿态序列C的步骤一致，通过YOLOv5模型对虚拟素材图像B中的人物部分进行截取，并适当扩大行人边界框，将行人边界框内的图像作为虚拟姿态图像Bb。获取D时，采用YOLOv5人脸检测模型获取Ai中人脸边界框区域的坐标，并以人脸边界框的几何中心为基点将面积扩大1.1至1.2倍，将更新后的人脸边界框内的图片作为Di，并对A中的每一帧图像进行上述操作，从而生成D。获取Bm的步骤与获取D的方法一致，此处不再赘述。当程序被执行时，进行以下步骤：S1：将Ci与Bb对齐，具体步骤如下：S11：获取姿态相似度阈值St，令i=1；S12：根据Ci获得非虚拟角色的姿态位姿向量V1i，根据Bb获得虚拟角色的姿态位姿向量Vb；S13：根据V1i和Vb获得位姿向量相似度Sc=/，其中||V1i||为对V1i取模，V1i•Vb为V1i和Vb做点乘；S14：若Sc＞St，则记录g=i，g为与Bb对齐的非虚拟角色的姿态图像在C中的次序；否则令i=i+1并执行步骤S12。本实施例通过PoseNet算法获取V1i，将Ci以及Bb分别输入到PoseNet算法中，输出V1i和Vb并进行标准化。S2：将Di与Bm对齐，具体步骤如下：S21：获取面部相似度阈值Dt，令i=1；S22：根据Di获得非虚拟角色面部的关键点坐标集Li={，…………}，其中xLij为Di的第j个关键点的横坐标，yLij为Di的第j个关键点的纵坐标，根据Bm获得虚拟角色的面部关键点坐标集Q={,,…………}，面部的关键点为面部的关键区域的位置，如五官、脸部轮廓等，其中xQj为Bm的第j个关键点的横坐标，yQj为Bm的第j个关键点的纵坐标，n为面部关键点的总个数；S23：根据Li和Q获得两者之间的面部相似度L=∑nj=12+2）1/2；S24：若L＜Dt，则记录z=i，z为与Bm对齐的非虚拟角色的面部图像在D中的次序。本实施例通过人脸对齐算法进行人脸Landmarks检测，获取人脸的关键点坐标并归一化，公式表示为Ls=FaceAlignment，其中FaceAlignment为人脸对齐算法。S3：若g=z则执行步骤S31到S32，否则执行步骤S33：S31：从A中删除A1，A2，……，Ag-1，获取到A0={Ag，Ag+1，……，Am}；从C中删除C1，C2，……，Cg-1，获取到C0={Cg，Cg+1，……，Cm}；从D中删除D1，D2，……，Dg-1，获取到D0={Dg，Dg+1，……，Dm}；S32：将A0初始化为A0={A1，A2，……，Ar，……，As}，其中A1=Ag，A2=Ag+1，……，As=Am；C0初始化为C0={C1，C2，……，Cr，……，Cs}，其中C1=Cg，C2=Cg+1，……，Cs=Cm；D0初始化为D0={D1，D2，……，Dr，……，Ds}，其中D1=Dg，D2=Dg+1，……，Ds=Dm，r=1，2……s；S33：若g＜z，则计算Cz对应的Sc，若Cz对应的Sc＞St，则令g=z并执行S31，否则令i=z+1并重新执行S12到S14以及S22到S24；若g＞z，则计算Dg对应的L，若Dg对应的LL＜Dt，则令z=g并执行S31，否则令i=g+1并重新执行S12到S14以及S22到S24。当系统执行程序时，第一步先将Ci与Bb进行对齐，首先获取C1的位姿向量V11和Bb的位姿向量Vb，计算二者的位姿向量相似度Sc并与预先设定的阈值St进行比较，若Sc＞St，则C1与Bb的姿态已经对齐，否则该帧的姿态未与Bb对齐，此时对C2进行相同操作，直到取到Ci与Bb相对齐。第二步将Di与Bm进行对齐，首先获取D1的关键点的坐标集Li以及Bm的关键点的坐标集Q，之后计算Li和Q之间的面部相似度L，并将L与预先设定的阈值Dt进行比较，若L＜Dt，则D1与Bm的面部已经对齐。否则该帧的面部未与Bm对齐，此时对D2进行相同操作，直到Di与Bm对齐。因为在确定虚拟角色时需要面部与姿态同时对齐才能够获得更好的效果，因此之后判断与Bb对齐的Ci和与Bm对齐的Di是否属于同一个Ai，如果并不属于同一个Ai，则需要重新进行对齐，直到同一个Ai所对应的Ci和Di均分别与Bb和Bm对齐。此时非虚拟角色的面部和姿态均已与虚拟角色的面部和姿态相对齐，非虚拟角色和虚拟角色获得了较高的拟合程度，此时将A、C、D进行初始化，将对齐的Ai、Ci、Di作为确定虚拟角色前的首帧图像，之后对虚拟角色进行确定，从而减少了虚拟角色的确定过程中姿态和面部错位的情况出现。S4：根据C0和Bb生成虚拟角色的姿态图像组Vb={Vb1，Vb2，……，Vbr，……，Vbs}，Vbr为根据Cr生成的虚拟角色的姿态图像；本技术通过TPSM算法模型实现，当Ai和B对齐后，算法初始化并建立两者初始特征参数之间的对应关系。在真人直播姿态序列的后续帧，算法通过背景运动估计模型预测图像的仿射变化，使用关键点检测器估计人体的多组关键点，然后通过密集运动网络进行估计光流和多分辨率遮挡掩模。最后将源图像输入修复网络，使用光流对编码器提取的特征图进行扭曲，并用相应的分辨率对其进行掩码遮罩，生成虚拟人的驱动图像，虚拟人驱动图像的动作姿态随着真人主播实时更新，公式表示为Vbr=TPSM。S5：根据D0和Bm生成虚拟角色的面部图像组Vm={Vm1，Vm2，……，Vmr，……，Vms}，Vmr为根据Dr生成的虚拟角色的面部图像。其中S5还包括：S51：对Bm进行深度估计，获得Bm的深度图像Db；S52：对D0的每一帧Dr进行深度估计，获得D0的深度图像组Dp={Dp1，Dp2，……，Dpr，……，Dps}，Dpr为Dr的深度图像；S53：根据Bm，Db以及Dp获得Vm。通过C生成的虚拟姿态序列Vb，侧重于对虚拟人整体动作的驱动，对面部表情等细节的还原度较差，因此本阶段采用人脸驱动算法对虚拟人的面部表情进行高清化驱动。通过DaGAN人脸驱动算法实现，利用真人表情序列驱动虚拟表情序列，生成高拟真虚拟表情图像的人脸表情、姿态、口型等真人图像一致，清晰度高。采用深度预测编码器和解码器对Bm进行深度估计，从而生成深度图像Db：Db=Depth Estimate。之后结合Bm和Db通过人脸关键点检测模型预测人脸的15个关键点得到虚拟表情关键点集Kb：Kb=Kp Detector。之后对D0的每一帧真人表情图像均重复上述操作，即：Dpr=Depth Estimate，得到Dp。基于深度表情序列Dp和真人表情序列D通过人脸关键点检测模型得到真人表情关键点序列KD={KD1，KD2，……，KDr，……，KDs}，其中KDr为第r帧真人表情图像Dr对应的真人表情关键点集。之后，根据Kb、D1的KD1以及KD通过标准化参数模块得到关键点标准化参数集Kn={Kn1，Kn2，……，Knr，……，Kns}，其中Knr为第r帧真人表情图像Dr对应的关键点标准化参数，Knr=Normalize。最后通过人脸合成网络的特征性便模块和跨注意力模块扭曲人脸特征并修正，得到Vm，其中Vmr=Generator。S6：根据Vb和Vm生成虚拟角色图像组V={V1，V2，……，Vr，……，Vs}，其中Vr为第r帧虚拟角色图像。此处需要通过换脸算法将Vm中的Vmr换到Vb中的Vbr的图像上，以细化Vb的脸部区域，从而生成V。常规拼接算法无法精确到面部图像和身体图像的像素级衔接，存在色差、错位或过渡不平滑等明显拼接痕迹。本部分采用了像素级的人脸分割算法，避免全身图像合成时头部区域和其他区域的拼接感；优化了的换脸算法逻辑，细化并扩大了换脸区域的范围，同时也提升了下巴的随动性。其中，步骤S6还包括：S61：根据Vb生成虚拟人脸序列Cf={Cf1，Cf2，……，Cfr，……，Cfs}，其中Cfr是第r帧Vbr中的虚拟人脸图像；获取虚拟人脸序列Cf的步骤与上述获取真人表情序列D的方法一致，通过YOLOv5人脸检测模型进行获取，此处不再赘述。S62：根据虚拟人脸序列Cf生成人脸分割序列Cm={Cm1，Cm2，……，Cmr，……，Cms}，Cmr是第r帧Cfr对应的人脸分割图像；通过人脸细粒度分割算法Face Parsing对虚拟人脸图像Cfr虚拟人的面部器官及脖子、头发等进行语义分割，共计19个类别标签，然合并面部区域和脖子区域的像素范围得到人脸分割图像Cmr，即Cmr=Face Parsing。S63：根据虚拟表情序列Vm生成表情分割序列Dm={Dm1，Dm2，……，Dmr，……，Dms}，Dmr是第r帧虚拟Vmr对应的表情分割图像；根据Vm生成Dm的方法与上述相同，此处不再赘述。S64：根据D0生成转换矩阵序列M={M1，M2，……，Mr……，Ms}，其中Mr是第r帧真人表情图像Dr所对应的人脸转换矩阵；将Dr通过人脸对齐算法转换为标准脸，将其转换矩阵记为Mr。S65：根据Vb、Cf、Cm、Vm、Dm和转换矩阵序列M生成V。通过FaceSwap换脸技术将Vmr对应的Dmr融合到Vbr上，替换其面部表情，最终得到真人实时驱动虚拟人动作和表情的虚拟图像Vr，及逆行直播推流，即：Vr=FaceSwap。由此完成了基于纯视觉的真人驱动虚拟人的操作，且输出的V表情更为细致，且拼接过渡区域的衔接自然平滑，进而使得虚拟人的动作姿态和表情与真人实时同步。虽然已经通过示例对本发明的一些特定实施例进行了详细说明，但是本领域的技术人员应该理解，以上示例仅是为了进行说明，而不是为了限制本发明的范围。本领域的技术人员还应理解，可以对实施例进行多种修改而不脱离本发明的范围和精神。本发明开的范围由所附权利要求来限定。
