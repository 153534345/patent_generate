标题title
基于马尔科夫的智能决策方法、装置、设备及存储介质
摘要abst
本申请涉及一种基于马尔科夫的智能决策方法、装置、设备及存储介质。所述方法包括：获取无人机执行任务的马尔科夫策略。马尔科夫策略中的智能体为执行任务的无人机，智能体的动作为无人机执行任务时控制动作，智能体的状态为无人机所处任务环境中的状态。获取马尔科夫策略中智能体当前时刻的状态空间，以及包含先验信息的动作空间。根据状态空间计算动作空间中每一个控制动作对应的状态转移概率，根据状态转移概率得到控制动作与状态之间对应关系的期望奖励值。将期望奖励值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。采用本方法能够在无人机低采样率条件下，有效缩短智能体探索时间，加快无人机执行任务的速度。
权利要求书clms
1.基于马尔科夫的智能决策方法，其特征在于，所述方法包括：获取无人机执行任务的马尔科夫策略；所述马尔科夫策略中的智能体为执行任务的无人机，所述智能体的动作为所述无人机执行任务时控制动作，所述智能体的状态为无人机所处任务环境中的状态；获取所述马尔科夫策略中智能体当前时刻的状态空间，以及包含先验信息的动作空间；所述动作空间是由历史时刻每一时间步对应的所述控制动作组成；所述状态空间由所述状态组成；根据所述状态空间计算所述动作空间中每一个所述控制动作对应的状态转移概率，根据所述状态转移概率得到所述控制动作与所述状态之间对应关系的期望奖励值；分别采用所述控制动作对应的动作价值函数和所述状态对应的状态价值函数表示所述期望奖励值；通过强化学习算法优化所述动作价值函数和所述状态价值函数，以此将所述期望奖励值对应的所述控制动作作为所述无人机下一时刻任务执行的智能决策结果。2.根据权利要求1所述的方法，其特征在于，所述状态包括：所述无人机的位置坐标信息、所述无人机的姿态、所述无人机的飞行速度、所述无人机的飞行轨迹以及所述无人机在当前时刻所处的三维地理环境。3.根据权利要求2所述的方法，其特征在于，根据所述状态空间计算所述动作空间中每一个所述控制动作对应的状态转移概率，包括：根据智能体自身策略与当前时刻的先验知识信息输出当前时刻的动作信息集合，通过当前时刻的所述动作信息集合确定当前时刻的动作，通过执行所述当前时刻的动作得到当前时刻的期望奖励为：；其中为当前时刻的状态与所述状态执行的控制动作转换至下一时刻状态的状态转移概率，/＞为下一时刻的状态空间，/＞为当前时刻的期望奖励值，/＞为当前时刻所述状态空间的转移概率。4.根据权利要求3所述的方法，其特征在于，根据所述状态空间计算所述动作空间中每一个所述控制动作对应的状态转移概率，根据所述状态转移概率得到所述控制动作与所述状态之间对应关系的期望奖励值，包括：根据执行任务的无人机自身状态与当前时刻的先验知识信息输出当前时刻的动作空间，通过当前时刻的所述动作空间确定当前时刻的控制动作，根据所述当前时刻的控制动作对应的所述状态转移概率执行所述当前时刻的控制动作，得到当前时刻的期望奖励值为：；其中，为当前时刻的期望奖励值，/＞为当前时刻的期望奖励值的集合，/＞为当前时刻状态空间，/＞为当前时刻的状态，/＞为当前时刻的状态对应的先验知识信息的集合，/＞为当前时刻的动作空间，/＞为当前时刻的动作。5.根据权利要求4所述的方法，其特征在于，分别采用所述控制动作对应的动作价值函数和所述状态对应的状态价值函数表示所述期望奖励值，包括：所述当前时刻的期望奖励值通过所述状态转移概率进行转化，得到当前时刻状态的期望奖励值的集合：；其中，为当前时刻的期望奖励值，/＞为当前时刻状态空间的所述状态转移概率；根据当前时刻状态的期望奖励值的集合与下一时刻状态通过贝尔曼方程生成下一时刻状态的状态价值。6.根据权利要求5所述的方法，其特征在于，通过强化学习算法优化所述动作价值函数和所述状态价值函数，以此将所述期望奖励值对应的所述控制动作作为所述无人机下一时刻任务执行的智能决策结果，还包括：生成决策训练模型，将执行任务的所述无人机的状态与所述无人机的预设策略作为训练样本输入至决策训练模型，所述决策训练模型通过强化学习优化所述动作价值函数和所述状态价值函数，得到状态价值的最大值及所述状态价值的最大值对应的控制动作，以所述状态价值的最大值对应的控制动作作为所述无人机下一时刻任务执行的智能决策结果。7.一种基于马尔科夫的智能决策装置，其特征在于，所述装置包括：策略获取模块，用于获取无人机执行任务的马尔科夫策略；所述马尔科夫策略中的智能体为执行任务的无人机，所述智能体的动作为所述无人机执行任务时控制动作，所述智能体的状态为无人机所处任务环境中的状态；结合空间获取模块，用于获取所述马尔科夫策略中智能体当前时刻的状态空间，以及包含先验信息的动作空间；所述动作空间是由历史时刻每一时间步对应的所述控制动作组成；所述状态空间由所述状态组成；期望奖励值生成模块，用于根据所述状态空间计算所述动作空间中每一个所述控制动作对应的状态转移概率，根据所述状态转移概率得到所述控制动作与所述状态之间对应关系的期望奖励值；期望奖励值转化模块，用于分别采用所述控制动作对应的动作价值函数和所述状态对应的状态价值函数表示所述期望奖励值；决策结果生成模块，用于通过强化学习算法优化所述动作价值函数和所述状态价值函数，以此将所述期望奖励值对应的所述控制动作作为所述无人机下一时刻任务执行的智能决策结果。8.一种计算机设备，包括存储器和处理器，所述存储器存储有计算机程序，其特征在于，所述处理器执行所述计算机程序时实现权利要求1至6中任一项所述方法的步骤。9.一种计算机可读存储介质，其上存储有计算机程序，其特征在于，所述计算机程序被处理器执行时实现权利要求1至6中任一项所述的方法的步骤。
说明书desc
技术领域本申请涉及人工智能技术领域，特别是涉及一种基于马尔科夫的智能决策方法、装置、设备及存储介质。背景技术随着人工智能在制造业、工业、航空航天领域等高芯领域的快速发展，出现了策略学习的人工智能技术，其中一类为优化理论与经验知识的无人机平台的任务搜索策略，一类为基于试错机制的无人机策略学习方法，前者需要借助过往实际经验形成完善的规则库，通过规则触发无人机决策，虽然可以借助经验知识在决策前期加快决策进度，但规则库的容量和设计的局限性导致策略固定化，第二类策略学习方法主要通过强化学习在信息不完全、规则不完备的情况下采用试错机制与无人机所处环境不断交互，最大化累积奖励来学习优化策略，虽然可以在智能体与环境的交互过程中加入随机因素，使其可以广泛应用于解决动态与随机性问题，常用的学习方法为深度强化学习结合深度学习的感知能力与强化学习的决策能力为复杂环境状态下的决策问题提供解决思路，通过利用神经网络强大的表征能力直接拟合状态-动作值或策略，然而无人机捕捉到的环境状态样本的复杂性，导致其需要更多的采样和智能体需要更久的探索时间对策略进行梯度更新，使得无人机执行任务时无法及时做出下一步控制动作，执行任务时间拉长，效率降低。发明内容基于此，有必要针对上述技术问题，提供一种能够提高无人机任务执行效率的基于马尔科夫的智能决策方法、装置、设备及存储介质。基于马尔科夫的智能决策方法，所述方法包括：获取无人机执行任务的马尔科夫策略。马尔科夫策略中的智能体为执行任务的无人机，智能体的动作为无人机执行任务时控制动作，智能体的状态为无人机所处任务环境中的状态。获取马尔科夫策略中智能体当前时刻的状态空间，以及包含先验信息的动作空间。动作空间是由历史时刻每一时间步对应的控制动作组成。状态空间由状态组成。根据状态空间计算动作空间中每一个控制动作对应的状态转移概率，根据状态转移概率得到控制动作与状态之间对应关系的期望奖励值。分别采用控制动作对应的动作价值函数和状态对应的状态价值函数表示期望奖励值。通过强化学习算法优化动作价值函数和状态价值函数，以此将期望奖励值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。在其中一个实施例中，状态包括：无人机的位置坐标信息、无人机的姿态、无人机的飞行速度、无人机的飞行轨迹以及无人机在当前时刻的三维地理环境。在其中一个实施例中，包括：；其中，为当前时刻的状态与状态执行的控制动作转换至下一时刻状态的转移概率，/＞为下一时刻的状态空间，/＞为当前时刻的期望奖励值，/＞为当前时刻状态空间的转移概率。在其中一个实施例中，还包括：根据执行任务的无人机自身策略与当前时刻的先验知识信息输出当前时刻的动作空间，通过当前时刻的动作空间确定当前时刻的控制动作，根据当前时刻的控制动作对应的状态转移概率执行当前时刻的控制动作，得到当前时刻的期望奖励值为：；其中，为当前时刻的期望奖励值，/＞为当前时刻的期望奖励值的集合，/＞为当前时刻状态空间，/＞为当前时刻的状态，，/＞为当前时刻的状态对应的先验知识信息的集合，/＞为当前时刻的动作空间，/＞为当前时刻的动作。在其中一个实施例中，还包括：当前时刻的期望奖励值通过状态转移概率进行转化，得到当前时刻状态的期望奖励值的集合：；其中，为当前时刻的期望奖励值，/＞为当前时刻状态空间的转移概率。根据当前时刻状态的期望奖励值的集合与下一时刻状态通过贝尔曼方程生成下一时刻状态的状态价值。在其中一个实施例中，还包括：生成决策训练模型，将执行任务的无人机的状态与无人机的预设策略作为训练样本输入至决策训练模型，决策训练模型通过强化学习优化动作价值函数和所述状态价值函数，得到状态价值的最大值及状态价值的最大值对应的控制动作，以状态价值的最大值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。一种基于马尔科夫的智能决策装置，所述装置包括：策略获取模块，用于获取无人机执行任务的马尔科夫策略。马尔科夫策略中的智能体为执行任务的无人机，智能体的动作为无人机执行任务时控制动作，智能体的状态为无人机所处任务环境中的状态。结合空间获取模块，用于获取马尔科夫策略中智能体当前时刻的状态空间，以及包含先验信息的动作空间。动作空间是由历史时刻每一时间步对应的控制动作组成。状态空间由状态组成。期望奖励值生成模块，用于根据状态空间计算动作空间中每一个控制动作对应的状态转移概率，根据状态转移概率得到控制动作与状态之间对应关系的期望奖励值。期望奖励值转化模块，用于分别采用控制动作对应的动作价值函数和状态对应的状态价值函数表示期望奖励值。决策结果生成模块，用于通过强化学习算法优化动作价值函数和状态价值函数，以此将期望奖励值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。一种计算机设备，包括存储器和处理器，所述存储器存储有计算机程序，所述处理器执行所述计算机程序时实现以下步骤：获取无人机执行任务的马尔科夫策略。马尔科夫策略中的智能体为执行任务的无人机，智能体的动作为无人机执行任务时控制动作，智能体的状态为无人机所处任务环境中的状态。获取马尔科夫策略中智能体当前时刻的状态空间，以及包含先验信息的动作空间。动作空间是由历史时刻每一时间步对应的控制动作组成。状态空间由状态组成。根据状态空间计算动作空间中每一个控制动作对应的状态转移概率，根据状态转移概率得到控制动作与状态之间对应关系的期望奖励值。分别采用控制动作对应的动作价值函数和状态对应的状态价值函数表示期望奖励值。通过强化学习算法优化动作价值函数和状态价值函数，以此将期望奖励值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。一种计算机可读存储介质，其上存储有计算机程序，所述计算机程序被处理器执行时实现以下步骤：获取无人机执行任务的马尔科夫策略。马尔科夫策略中的智能体为执行任务的无人机，智能体的动作为无人机执行任务时控制动作，智能体的状态为无人机所处任务环境中的状态。获取马尔科夫策略中智能体当前时刻的状态空间，以及包含先验信息的动作空间。动作空间是由历史时刻每一时间步对应的控制动作组成。状态空间由状态组成。根据状态空间计算动作空间中每一个控制动作对应的状态转移概率，根据状态转移概率得到控制动作与状态之间对应关系的期望奖励值。分别采用控制动作对应的动作价值函数和状态对应的状态价值函数表示期望奖励值。通过强化学习算法优化动作价值函数和状态价值函数，以此将期望奖励值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。上述基于马尔科夫的智能决策方法、装置、设备及存储介质，通过获取无人机执行任务的状态和包含先验信息的动作空间，其中动作空间是由历史时刻每一时间步对应的无人机执行任务时的控制动作组成，使得无人机获取的状态为结合状态，允许智能体据此结合状态进行马尔科夫决策分析，使得无人机飞行移动的过程中，可以利用捕获到的状态组成状态空间，使得无人机可以利用结合状态作为先验信息，以此形成初始策略生成动作空间，在复杂环境或者通信信号受到屏蔽的情况下，利用无人机有限的采样信息组成结合状态的状态空间，根据状态空间计算每一个控制动作对应的状态转移概率，获得控制动作与状态之间具备一定规律的对应关系，针对该对应关系进行期望奖励值的强化学习，最终取期望奖励值的最大值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。由此可以有效缩短无人机对状态的探索时间，进而，保障决策结果的精度的同时，加快无人机执行任务的速度。附图说明图1为一个实施例中基于马尔科夫的智能决策方法的流程示意图；图2为一个实施例中基于马尔科夫的智能决策方法中结合状态下执行动作的示意图；图3为一个实施例中基于马尔科夫的智能决策装置的结构框图；图4为一个实施例中状态扩展的马尔科夫决策装置的结构框图；图5为一个实施例中计算机设备的内部结构图。具体实施方式为了使本申请的目的、技术方案及优点更加清楚明白，以下结合附图及实施例，对本申请进行进一步详细说明。应当理解，此处描述的具体实施例仅仅用以解释本申请，并不用于限定本申请。在一个实施例中，本申请提供的基于马尔科夫的智能决策方法，如图1所示，具体包括以下步骤：步骤102，获取无人机执行任务的马尔科夫策略。马尔科夫策略中的智能体为执行任务的无人机，智能体的动作为无人机执行任务时控制动作，智能体的状态为无人机所处任务环境中的状态。另外，状态的信息包括无人机的位置坐标信息、无人机的姿态、无人机的飞行速度、无人机的飞行轨迹以及无人机在当前时刻的三维地理环境等；控制动作可以是追踪或者识别。无人机中内嵌CPU处理器处理数据计算，无人机中还内嵌GPU、DSP等处理不同功能数据的芯片组件或者芯片集合，以此获取自身状态。加之，无人机外接红外传感器、摄像头、雷达探测仪等硬件设备，无人机与其所在的周围环境，通过硬件设备进行地理三维数据采样，以此获取执行任务中的状态。具体的，获取无人机执行任务的马尔科夫策略，采用无人机获取的当前时刻的状态空间和无人机执行任务时控制动作组成的动作空间作为先验信息组成二元组，其中，/＞为包括无人机与环境互动的所有状态的集合和每一个状态对应的动作空间作为先验信息，先验信息为智能体执行状态后生成的动作空间，/＞，/＞为先验信息策略的参数，/＞为根据先验知识信息在当前状态的策略，由此获取结合状态的状态空间。步骤104，获取马尔科夫策略中智能体当前时刻的状态空间，以及包含先验信息的动作空间。动作空间是由历史时刻每一时间步对应的控制动作组成。状态空间由状态组成。步骤106，根据状态空间计算动作空间中每一个控制动作对应的状态转移概率，根据状态转移概率得到控制动作与状态之间对应关系的期望奖励值。具体的，根据状态空间计算动作空间中每一个控制动作对应的状态转移概率，基于马尔科夫决策理论，状态转移概率可以转化为：；其中，为当前时刻的状态与状态执行的控制动作转换至下一时刻的状态的转移概率，/＞为下一时刻的状态空间，/＞为当前时刻状态-动作的期望奖励值，为当前时刻状态空间的转移概率。具体的，根据执行任务的无人机自身策略采用选项理论，根据选项理论，将时刻执行一个二元组即结合状态/＞，结合状态选择/＞时刻的控制动作，其中/＞，根据智能体自身策略与当前/＞时刻的先验信息输出当前/＞时刻的动作空间/＞，通过当前/＞时刻的动作空间确定当前/＞时刻的动作/＞，通过执行当前/＞时刻的动作得到/＞当前时刻的期望奖励值为：；其中，为/＞时刻对应的状态，/＞为/＞时刻选择的动作空间，/＞为当前时刻选择的控制动作，/＞为/＞时刻的状态-动作的期望奖励值，/＞为/＞时刻期望奖励值的集合。步骤108，分别采用控制动作对应的动作价值函数和状态对应的状态价值函数表示期望奖励值。步骤110，通过强化学习算法优化动作价值函数和状态价值函数，以此将期望奖励值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。具体的，当前时刻的期望奖励值通过状态转移概率进行转化，得到当前时刻状态空间的期望奖励值的集合可以转化为：；其中，为当前时刻的期望奖励值，/＞为当前时刻状态空间的转移概率。进一步的，根据当前时刻状态空间的期望奖励值的集合与下一时刻状态通过贝尔曼方程生成下一时刻状态的状态价值，无人机通过芯片组或者芯片集合生成决策训练模型，采用强化学习将状态与无人机的预设策略作为训练样本输入至决策训练模型，决策训练模型通过强化学习，得到状态价值的最大值及其最大值对应的控制动作，进而以最大状态价值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。上述基于马尔科夫的智能决策方法、装置、设备及存储介质，通过获取无人机执行任务的状态和包含先验信息的动作空间，其中动作空间是由历史时刻每一时间步对应的无人机执行任务时的控制动作组成，使得无人机获取的状态为结合状态，允许智能体据此结合状态进行马尔科夫决策分析，使得无人机飞行移动的过程中，可以利用捕获到的状态组成状态空间，使得无人机可以利用结合状态作为先验信息，以此形成初始策略生成动作空间，在复杂环境或者通信信号受到屏蔽的情况下，利用无人机有限的采样信息组成结合状态的状态空间，根据状态空间计算每一个控制动作对应的状态转移概率，获得控制动作与状态之间具备一定规律的对应关系，针对该对应关系进行期望奖励值的强化学习，最终取期望奖励值的最大值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。由此可以有效缩短无人机对状态的探索时间，进而，保障决策结果的精度的同时，加快无人机执行任务的速度。在其中一个实施例中，状态包括：无人机的位置坐标信息、无人机的姿态、无人机的飞行速度、无人机的飞行轨迹以及无人机在当前时刻的三维地理环境。值得说明的是，无人机获取状态和控制动作的过程中，智能体只能利用马尔科夫决策理论，根据当前时刻获取的状态进行动作决策，因此，通过引入先验知识信息，将知识编码成控制动作与状态相关联，形成结合状态的状态空间，采用状态和与状态相关的先验信息策略的动作空间，用二元组形式表示：/＞，其中，/＞为先验信息策略的参数，/＞为根据先验信息在当前状态的策略，/＞为包括无人机获取的状态空间和每一个状态对应的先验信息，即下一时刻的状态空间。在其中一个实施例中，；其中，为当前时刻的状态与状态执行的控制动作转换至下一时刻状态的转移概率，/＞为下一时刻的状态空间，/＞为当前时刻的期望奖励值，/＞为当前时刻状态空间的转移概率。值得说明的是，如图2所示，先将任务目标的形式化为一个单一的MDP，在时刻进入状态/＞时，首先通过先验信息输出控制动作/＞，其中/＞，对应图2中时间轴下侧方框区域，/＞为先验信息策略的参数，然后允许智能体根据结合状态/＞选择控制动作/＞，其中/＞，对应图中时间轴上侧区域，/＞为无人机自身策略的参数。通过执行/＞获得奖励，根据状态转移概率在/＞时刻进入状态空间/＞，如果/＞不是终止状态则继续执行/＞的决策过程。因为结合状态并不会对原始状态产生影响，其中的动作空间只是用于帮助智能体选择控制动作/＞，状态转移概率对于所有可能的下一时刻状态/＞求和为1。由此可以在无人机低采样率的情况下，提高智能体决策精度。在其中一个实施例中，根据执行任务的无人机自身策略与当前时刻的先验知识信息输出当前时刻的动作空间，通过当前时刻的动作空间确定当前时刻的控制动作，根据当前时刻的控制动作对应的状态转移概率执行当前时刻的控制动作，得到当前时刻的期望奖励值为：；其中，为当前时刻的期望奖励值，/＞为当前时刻的期望奖励值的集合，为当前时刻状态空间，/＞为当前时刻的状态，/＞为当前时刻的状态对应的先验知识信息的集合，/＞为当前时刻的动作空间，/＞为当前时刻的动作。值得说明的是，生成下一时刻状态空间的状态价值表示为：；其中，为无人机自身策略参数，/＞为无人机自身策略参数对应的策略集合，/＞为当前时刻状态生成的下一时刻状态的状态价值，/＞为当前时刻状态空间的终止函数的折扣因子，/＞为当前时刻状态的终止状态。在其中一个实施例中，生成决策训练模型，将执行任务的无人机的状态与无人机的预设策略作为训练样本输入至决策训练模型，决策训练模型通过强化学习优化动作价值函数和所述状态价值函数，得到状态价值的最大值及状态价值的最大值对应的控制动作，以状态价值的最大值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。值得说明的是，通过对结合状态用于同一控制动作，提高了智能体在实时决策的探索时间，以及加快无人机执行任务的速度。应该理解的是，虽然图1-图2的流程图中的各个步骤按照箭头的指示依次显示，但是这些步骤并不是必然按照箭头指示的顺序依次执行。除非本文中有明确的说明，这些步骤的执行并没有严格的顺序限制，这些步骤可以以其它的顺序执行。而且，图1-图2中的至少一部分步骤可以包括多个子步骤或者多个阶段，这些子步骤或者阶段并不必然是在同一时刻执行完成，而是可以在不同的时刻执行，这些子步骤或者阶段的执行顺序也不必然是依次进行，而是可以与其它步骤或者其它步骤的子步骤或者阶段的至少一部分轮流或者交替地执行。在一个实施例中，如图3所示，提供了一种基于马尔科夫的智能决策装置，包括：策略获取模块302、结合空间获取模块304、期望奖励值生成模块306、期望奖励值转化模块308和决策结果生成模块310，其中：策略获取模块302用于获取无人机执行任务的马尔科夫策略。马尔科夫策略中的智能体为执行任务的无人机，智能体的动作为无人机执行任务时控制动作，智能体的状态为无人机所处任务环境中的状态。结合空间获取模块304，用于获取马尔科夫策略中智能体当前时刻的状态空间，以及包含先验信息的动作空间。动作空间是由历史时刻每一时间步对应的控制动作组成。状态空间由状态组成。期望奖励值生成模块306，用于根据状态空间计算动作空间中每一个控制动作对应的状态转移概率，根据状态转移概率得到控制动作与状态之间对应关系的期望奖励值。期望奖励值转化模块308，用于分别采用控制动作对应的动作价值函数和状态对应的状态价值函数表示期望奖励值。决策结果生成模块310，用于通过强化学习算法优化动作价值函数和状态价值函数，以此将期望奖励值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。关于基于马尔科夫的智能决策装置的具体限定可以参见上文中对于基于马尔科夫的智能决策方法的限定，在此不再赘述。上述基于马尔科夫的智能决策装置中的各个模块可全部或部分通过软件、硬件及其组合来实现。上述各模块可以硬件形式内嵌于或独立于计算机设备中的处理器中，也可以以软件形式存储于计算机设备中的存储器中，以便于处理器调用执行以上各个模块对应的操作。在一个实施例中，如图4所示，提供了一种状态扩展的马尔科夫决策装置，包括：强化学习模块和先验知识模块，其中：先验知识模块，用于生成控制动作的信息，连同环境状态/＞生成结合状态/＞。强化学习模块，用于以结合状态为输入数据，输出控制动作，执行动作，按照图2所示流程执行控制动作，由此实现了先验知识与RL的有机融合。在一个实施例中，提供了一种计算机设备，该计算机设备可以是终端，其内部结构图可以如图5所示。该计算机设备包括通过系统总线连接的处理器、存储器、网络接口、显示屏和输入装置。其中，该计算机设备的处理器用于提供计算和控制能力。该计算机设备的存储器包括非易失性存储介质、内存储器。该非易失性存储介质存储有操作系统和计算机程序。该内存储器为非易失性存储介质中的操作系统和计算机程序的运行提供环境。该计算机设备的网络接口用于与外部的终端通过网络连接通信。该计算机程序被处理器执行时以实现一种基于马尔科夫的智能决策方法。该计算机设备的显示屏可以是液晶显示屏或者电子墨水显示屏，该计算机设备的输入装置可以是显示屏上覆盖的触摸层，也可以是计算机设备外壳上设置的按键、轨迹球或触控板，还可以是外接的键盘、触控板或鼠标等。本领域技术人员可以理解，图3-图5中示出的结构，仅仅是与本申请方案相关的部分结构的框图，并不构成对本申请方案所应用于其上的计算机设备的限定，具体的计算机设备可以包括比图中所示更多或更少的部件，或者组合某些部件，或者具有不同的部件布置。在一个实施例中，提供了一种计算机设备，包括存储器和处理器，该存储器存储有计算机程序，该处理器执行计算机程序时实现以下步骤：获取无人机执行任务的马尔科夫策略。马尔科夫策略中的智能体为执行任务的无人机，智能体的动作为无人机执行任务时控制动作，智能体的状态为无人机所处任务环境中的状态。获取马尔科夫策略中智能体当前时刻的状态空间，以及包含先验信息的动作空间。动作空间是由历史时刻每一时间步对应的控制动作组成。状态空间由状态组成。根据状态空间计算动作空间中每一个控制动作对应的状态转移概率，根据状态转移概率得到控制动作与状态之间对应关系的期望奖励值。分别采用控制动作对应的动作价值函数和状态对应的状态价值函数表示期望奖励值。通过强化学习算法优化动作价值函数和状态价值函数，以此将期望奖励值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。在一个实施例中，状态包括：无人机的位置坐标信息、无人机的姿态、无人机的飞行速度、无人机的飞行轨迹以及无人机在当前时刻的三维地理环境。在一个实施例中，处理器执行计算机程序时还实现以下步骤：；其中，为当前时刻的状态与状态执行的控制动作转换至下一时刻状态的转移概率，/＞为下一时刻的状态空间，/＞为当前时刻的期望奖励值，/＞为当前时刻状态空间的转移概率。在一个实施例中，处理器执行计算机程序时还实现以下步骤：根据执行任务的无人机自身策略与当前时刻的先验知识信息输出当前时刻的动作空间，通过当前时刻的动作空间确定当前时刻的控制动作，根据当前时刻的控制动作对应的状态转移概率执行当前时刻的控制动作，得到当前时刻的期望奖励值为：；其中，为当前时刻的期望奖励值，/＞为当前时刻的期望奖励值的集合，/＞为当前时刻状态空间，/＞为当前时刻的状态，/＞为当前时刻的状态对应的先验知识信息的集合，/＞为当前时刻的动作空间，/＞为当前时刻的动作。在一个实施例中，处理器执行计算机程序时还实现以下步骤：当前时刻的期望奖励值通过状态转移概率进行转化，得到当前时刻状态的期望奖励值的集合：；其中，为当前时刻的期望奖励值，/＞为当前时刻状态空间的转移概率。根据当前时刻状态的期望奖励值的集合与下一时刻状态通过贝尔曼方程生成下一时刻状态的状态价值。在一个实施例中，处理器执行计算机程序时还实现以下步骤：生成决策训练模型，将执行任务的无人机的状态与无人机的预设策略作为训练样本输入至决策训练模型，决策训练模型通过强化学习优化动作价值函数和所述状态价值函数，得到状态价值的最大值及状态价值的最大值对应的控制动作，以状态价值的最大值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。在一个实施例中，提供了一种计算机可读存储介质，其上存储有计算机程序，计算机程序被处理器执行时实现以下步骤：获取无人机执行任务的马尔科夫策略。马尔科夫策略中的智能体为执行任务的无人机，智能体的动作为无人机执行任务时控制动作，智能体的状态为无人机所处任务环境中的状态。获取马尔科夫策略中智能体当前时刻的状态空间，以及包含先验信息的动作空间。动作空间是由历史时刻每一时间步对应的控制动作组成。状态空间由状态组成。根据状态空间计算动作空间中每一个控制动作对应的状态转移概率，根据状态转移概率得到控制动作与状态之间对应关系的期望奖励值。分别采用控制动作对应的动作价值函数和状态对应的状态价值函数表示期望奖励值。通过强化学习算法优化动作价值函数和状态价值函数，以此将期望奖励值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。在一个实施例中，状态包括：无人机的位置坐标信息、无人机的姿态、无人机的飞行速度、无人机的飞行轨迹以及无人机在当前时刻的三维地理环境。在一个实施例中，计算机程序被处理器执行时还实现以下步骤：根据执行任务的无人机自身策略与当前时刻的先验知识信息输出当前时刻的动作空间，通过当前时刻的动作空间确定当前时刻的控制动作，根据当前时刻的控制动作对应的状态转移概率执行当前时刻的控制动作，得到当前时刻的期望奖励值为：；其中，为当前时刻的期望奖励值，/＞为当前时刻的期望奖励值的集合，为当前时刻状态空间，/＞为当前时刻的状态，/＞为当前时刻的状态对应的先验知识信息的集合，/＞为当前时刻的动作空间，/＞为当前时刻的动作。在一个实施例中，计算机程序被处理器执行时还实现以下步骤：当前时刻的期望奖励值通过状态转移概率进行转化，得到当前时刻状态的期望奖励值的集合：；其中，为当前时刻的期望奖励值，/＞为当前时刻状态空间的转移概率。根据当前时刻状态的期望奖励值的集合与下一时刻状态通过贝尔曼方程生成下一时刻状态的状态价值。在一个实施例中，计算机程序被处理器执行时还实现以下步骤：生成决策训练模型，将执行任务的无人机的状态与无人机的预设策略作为训练样本输入至决策训练模型，决策训练模型通过强化学习优化动作价值函数和所述状态价值函数，得到状态价值的最大值及状态价值的最大值对应的控制动作，以状态价值的最大值对应的控制动作作为无人机下一时刻任务执行的智能决策结果。本领域普通技术人员可以理解实现上述实施例方法中的全部或部分流程，是可以通过计算机程序来指令相关的硬件来完成，所述的计算机程序可存储于一非易失性计算机可读取存储介质中，该计算机程序在执行时，可包括如上述各方法的实施例的流程。其中，本申请所提供的各实施例中所使用的对存储器、存储、数据库或其它介质的任何引用，均可包括非易失性和/或易失性存储器。非易失性存储器可包括只读存储器、可编程ROM、电可编程ROM、电可擦除可编程ROM或闪存。易失性存储器可包括随机存取存储器或者外部高速缓冲存储器。作为说明而非局限，RAM以多种形式可得，诸如静态RAM、动态RAM、同步DRAM、双数据率SDRAM、增强型SDRAM、同步链路 DRAM、存储器总线直接RAM、直接存储器总线动态RAM、以及存储器总线动态RAM等。以上实施例的各技术特征可以进行任意的组合，为使描述简洁，未对上述实施例中的各个技术特征所有可能的组合都进行描述，然而，只要这些技术特征的组合不存在矛盾，都应当认为是本说明书记载的范围。以上所述实施例仅表达了本申请的几种实施方式，其描述较为具体和详细，但并不能因此而理解为对发明范围的限制。应当指出的是，对于本领域的普通技术人员来说，在不脱离本申请构思的前提下，还可以做出若干变形和改进，这些都属于本申请的保护范围。因此，本申请的保护范围应以所附权利要求为准。
