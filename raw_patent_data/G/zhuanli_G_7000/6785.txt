标题title
基于自监督学习及多通道超图神经网络的漏洞检测方法与系统
摘要abst
本发明公开了一种基于自监督学习及多通道超图神经网络的漏洞检测方法，包括：获取函数级代码漏洞数据集并进行预处理，将代码文本通过代码分析工具转化为代码序列图。为不同通道构建表征高阶信息的模体，根据模体对代码序列图进行采样，得到多通道的代码序列超图，对预处理的代码文本数据利用word2vec训练为词向量表示。最后将代码序列超图和标签作为训练数据，训练超图神经网络，通过学习得到节点表征与超图表征，然后将超图表征进行拼接，经过单层感知机进行图分类。本发明同时引入了自监督学习，弥补了多通道之间的信息损失，通过自监督学习融合了多通道之间的互信息，具有更好的可解释性与漏洞检测效果。本发明还提供了实现上述方法的系统。
权利要求书clms
1.一种基于自监督学习及多通道超图神经网络的漏洞检测方法，其特征在于，所述方法包括如下步骤：步骤一，选取代码数据集，并对所述代码数据集中的函数级代码进行预处理；步骤二，使用代码分析工具将代码转化为代码序列图；所述代码序列图中包含语法结构，代码控制流信息，数据流向信息，代码先后序信息，语义信息上述五种高阶关系；步骤三，利用Word2vec将代码序列图节点上的代码Token转化成代码向量特征表示；步骤四，根据代码序列图上存在的高阶关系，将其分为多个通道，为每个通道构建表征高阶关系的模体；步骤五，根据步骤四中构建获得的模体将所述代码序列图转化为多通道的代码序列超图；步骤六，对每一个通道的超图，使用超图神经网络聚合对应所述通道的代码序列超图的节点表征，对节点表征进行平均池化，得到对应通道的超图向量；步骤七，使用自监督学习方法，将节点级，子超图级，超图级的互信息最大化，弥补信息聚合的损失；步骤八，将多通道超图向量进行聚合，通过一个单层感知机，得到最终的图分类结果，判断是否存在漏洞；其中，所述代码数据是指软件未编译的高级语言源代码。2.根据权利要求1所述的漏洞检测方法，其特征在于，步骤一中，所述代码数据集的收集方式包括通过检查项目安全相关的代码提交，将修复代码的提交标记为安全的代码，将修复之前的代码标记为有漏洞的代码。3.根据权利要求1所述的漏洞检测方法，其特征在于，步骤一中，所述代码预处理的方法为去除代码字符串中没有信息量的特殊符号和链接，然后对代码进行代码标准化；其中，所述特殊符号包括基本逗号、句号、回车符、换行符和数学符号以及表情符号；所述链接为描述对象的网站链接，在本发明数据预处理过程中进行去除；所述代码标准化即为将变量名，类名，函数名进行标准化；变量名、类名、函数名按照代码编译序列，分别使用VAR、CLASS、FUNC代替。4.根据权利要求1所述的漏洞检测方法，其特征在于，步骤二中，所述代码分析工具包括：Joern、ANTLR、Soot；所述代码序列图为一种通过代码分析工具由代码转化成的表示代码语法结构，代码控制流信息，数据流向信息，代码先后序信息，语义信息组成的多关系复合图；所述代码序列图的生成步骤如下：首先通过代码分析工具生成抽象语法树AST与控制流图CFG，其中抽象语法树中包含着代码语法结构信息，控制流图CFG中包含着控制流信息；接着遍历AST中叶子节点的token序列，得到叶子节点，即代码token之间的数据传递关系，生成数据流图DFG；最后遍历AST中叶子节点中的token序列，与代码文本进行匹配，生成表示代码token前后序列关系的序列关系图SRG；对于语义信息，使用文档主题模型中的线性判别法LDA，确定代码主题词，将每个主题确定为一个节点，选择与主题相关的TOPK个词，分别与主题节点连接，构建代码主题图CTG；将多种关系聚合在同一张图上，生成代码序列图CSG。5.根据权利要求1所述的漏洞检测方法，其特征在于，步骤三中，所述将代码Token转化为向量特征表示的步骤如下：对步骤一预处理后的代码文本数据，利用词向量模型word2vec进行训练，学习出整个数据集中每个词的词向量表示；记词向量的维度为d，并将对应词向量映射到代码序列图上；所述词向量是指根据word2vec预训练模型得到的向量，每个词对应一个向量，以唯一的表示词向量的信息；所述图节点是指代码序列图中的节点，其中除了控制流节点外的每个节点都对应着一个代码Token。6.根据权利要求1所述的漏洞检测方法，其特征在于，步骤四中，所述模体为在数据的复杂网络中频繁出现的一种网络模式，表示着复杂网络上是否存在着表征某一类型特征的超边；将表征为同一超边的模体划归一个通道，针对不同通道的超图分别进行超图卷积；在所述复杂网络中存在着两种表征复杂网络特征的元结构，分别是模体与元路径；所述模体相对于元路径，拥有更复杂的结构，表征更复杂的结构信息。7.根据权利要求1所述的漏洞检测方法，其特征在于，步骤四中，所述根据代码序列图上存在的高阶关系，将其分为多个通道，为每个通道构建表征高阶关系的模体的步骤如下：根据代码序列图上存在的语法结构，控制流信息，数据流信息，先后序信息，语义信息上述五种高阶关系，将其分为5个通道，分别使用符号g，c，d，o，s来表示；对于语法结构通道，控制流信息通道，数据流信息通道，先后序信息通道，语义信息通道，分别设计相对应模体来表示通道对应的含有高阶关系的信息。8.根据权利要求1所述的漏洞检测方法，其特征在于，步骤五中，所述根据模体将代码序列图转化为多通道的代码序列超图的步骤如下：对代码序列图进行模体的采样，对于采样模体样本过多的通道与模体进行裁剪，样本的阈值设为模体最大采样数的百分之八十，将剩余每一个模体作为超边，模体中的每一个点作为超边的节点；将同一通道的超边进行拼接，获得对应通道的超图邻接矩阵，分别用符号Hg,Hc,Hd,Ho,Hs来表示。9.根据权利要求1所述的漏洞检测方法，其特征在于，步骤六中，所述生成各通道的超图向量的步骤如下：步骤、为每一个通道设置一个可训练的转移矩阵，将Word2vec的每一个节点向量通过转移矩阵，得到该通道的初始超图节点向量，转移矩阵分别用符号Wg,Wc,Wd,Wo,Ws来表示；其中X∈Rn×d为Word2vec得到的节点向量，表示i通道对应的初始超图节点向量，n为节点数量，d为节点向量维度；步骤、对每一个通道中的节点向量进行超图卷积，超图神经网络中每一层的超图卷积分为两步，第一步将节点的向量进行卷积，得到本层的超边向量，第二步将超边向量进行卷积，得到节点向量；在两个过程中，都引入了注意力机制；对于通道i，节点级注意力机制公式如下：其中l是图神经网络的层数，H是某一通道超图的邻接矩阵，矩阵每一列代表一个超边，矩阵每一行代表一个节点，每一个矩阵元素代表此节点是否存在此超边中，0为节点不在超边中，1为节点在超边中，k,p为节点标号，j为超边标号，υk,υp代表k,p节点，ej代表超边j，vp∈ej代表属于超边j的所有节点，是超边j在l层的表征，a1是可训练的注意力向量，角标T为转置矩阵，uk是针对每个节点的可训练的注意力向量，αjk为超边j节点k的注意力权重，exp为指数函数，σ与LeakyReLU为非线性的激活函数；对于通道i，超边级的注意力机制公式如下：其中l是图神经网络的层数，H是某一通道超图的邻接矩阵，矩阵每一列代表一个超边，矩阵每一行代表一个节点，每一个矩阵元素代表此节点是否存在此超边中，0为节点不在超边中，1为节点在超边中，j为节点标号，k,p为超边标号，vk,vp代表k,p超边，εj代表节点j，ep∈εj代表连接节点j的所有节点，是超边j在l层的表征，a2是可训练的注意力向量，角标T为转置矩阵，vk是针对每个超边的可训练的注意力向量，βjk为节点j超边k的注意力权重，exp为指数函数，σ与LeakyReLU为非线性的激活函数；步骤、将节点表征经过平均池化，得到该通道的超图向量：其中AVGPOOL为平均池化操作，其中为i通道第l层的节点向量，ci为i通道的超图表征向量。10.根据权利要求1所述的漏洞检测方法，其特征在于，步骤七中，所述使用自监督学习方法，将节点级，子超图级，超图级的互信息最大化步骤如下：通过对节点连接的超边进行平均池化，得到子超图表征，公式如下：其中k为节点标号，Xi为超图在i通道的节点表征矩阵，其中每一行代表一个节点的节点向量，为k节点在i通道的超图邻接向量，0代表着节点不存在于超边中，1代表节点存在于超边中，即代表着节点连接着的超边，即为节点连接的超边数量，为i通道关于k节点的子超图表征向量；将超图节点转移矩阵随机打乱，得到新的转移矩阵同样对节点连接的超边进行平均池化，得到消极样例超图表征，公式如下：其中k为节点标号，Xi为超图在i通道的节点表征矩阵，其中每一行代表一个节点的节点向量，为打乱后的k节点在i通道的关于新的邻接矩阵超图邻接向量，0代表着节点不存在于超边中，1代表节点存在于超边中，即代表着节点连接着的超边，即为打乱后的节点连接的超边数量，为i通道关于打乱后的k节点的子超图表征向量；计算节点级，超图级与子超图级的互信息，并以损失函数的形式使其最大化，所述损失函数公式如下：其中fD:Rd×Rd→R是一个向量相似性的判别器，在实现中采用点积函数作为此判别器，Ls为自监督损失函数，σ为非线性的激活函数。11.根据权利要求1所述的漏洞检测方法，其特征在于，步骤八中将多通道超图向量进行聚合，通过一个单层感知机，得到最终的图分类结果，判断是否存在漏洞，具体公式如下：c＝cg||cc||cd||co||cs，其中||为向量拼接操作，cg,cc,cd,co,cs为每个通道对应向量，c为最终用于图分类的超图向量；其中Wfull为全连接层权重，b为全连接层偏移向量，softmax与tanh为非线性的激活函数；上述公式计算的为一个0-1的值，代表着代码漏洞检测的结果，越接近1代表函数级代码中存在漏洞的概率越大，越接近0代表函数级代码中出现漏洞的概率越小；在使用中算法将大于0.5的时候看作函数级代码存在漏洞，将小于等于0.5看作函数级代码不存在漏洞；根据标签值与真实值计算得到超图神经网络的损失函数：其中λ1||Φ||2是神经网络所有参数的l2正则项，yi为标签值，为预测值，将所有代码样本的预测值与标签值的交叉熵相加即为损失函数的第一项。损失函数的第二项为l2正则项，其中λ1为正则项权重，Φ代表了超图神经网络的所有参数；结合权利要求10的自监督任务损失函数，神经网络最终的损失函数为：其中λ2为自监督任务的损失函数权重，由损失函数反向传播即可进行模型的训练。12.一种实现如1-11之任一项所述漏洞检测方法的系统，所述系统包括：代码处理及分析模块、超图生成及转化模块、代码检测模块；所述代码处理及分析模块用于对代码进行预处理，并通过代码分析工具将代码转化为代码序列图，将代码序列图节点上的代码Token转化成代码向量特征表示；所述超图生成及转化模块用于构建不同通道上的模体，并通过模体将代码序列图转化为多通道的代码序列超图，得到代码序列超图上的节点表征；所述代码检测模块用于聚合不同通道上的节点表征与超边表征，通过MEANPOOL将超图聚合后的节点表征表示转化为多通道的超图表征，最后将多通道超图向量进行拼接，通过感知机获得漏洞分类结果；所述代码检测模块还引入了自监督学习，通过节点级，子超图级，超图级的表征互信息最大化，弥补不同通道之间的信息损失。
说明书desc
技术领域本发明属于计算机信息安全技术领域，涉及一种基于自监督学习及多通道超图神经网络的漏洞检测方法与系统，具体为一种将函数级代码构建为超图，利用自监督学习与超图神经网络判断代码是否存在漏洞的方法。背景技术近年来，随着计算机软件技术的快速发展，海量的软件被开发出来，软件中存在着隐藏的漏洞。开发人员不正确的编程习惯与测试人员不充分的软件测试导致代码中有大量隐藏的漏洞尚未被发现。黑客可以利用隐藏的漏洞，破坏系统，盗取数据，给企业和国家造成较大的危害。因此，漏洞检测技术成为了寻找未知漏洞，避免损失的关键方法。漏洞检测技术是指对未知漏洞的探索，综合应用各种技术和工具，尽可能地找出软件中的隐藏的漏洞，并对已发现漏洞的细节进行深入分析的方法。传统的漏洞检测方法往往是使用静态分析、符号执行等方法。近年来随着深度学习的发展，使用深度学习的方法进行漏洞检测已经成为了一种趋势。但是大部分方法无法充分考虑代码的高阶结构关系，仅仅将代码建模为文本或者简单图，导致在真实的代码漏洞数据集上漏洞检测效果较差。如VulDeePecker使用一种基于双向LSTM的方法来自动化的对代码文本进行分析，该方法充分的使用了代码的文本信息，但是基于文本的建模方法仅仅能考虑到代码文本的先后序关系，忽略了代码的控制流，数据流，语法信息以及其他高阶关系。又如申请号为CN202010040159.1的中国专利公开了一种基于图神经网络的细粒度源代码漏洞检测方法，该方法将代码构建为代码属性图，使用了代码的控制流信息与数据流信息，但是代码属性图不能充分合理的表示代码的高阶关系，导致在真实的场景中漏洞检测效果较差，且缺乏可解释性。发明内容为了解决目前存在的漏洞检测方法不能充分的使用代码的高阶关系，检测漏洞效果较差的问题，本发明的目的是提供一种基于自监督学习及多通道超图神经网络的方法。本发明将代码建模为代码序列超图，通过多通道的超图神经网络聚合代码的节点信息，得到各个通道的超图表征，将多通道超图表征拼接，通过一个感知机获得最后的漏洞检测结果。为了弥补多通道超图之间的信息损失，本发明还引入了自监督任务，通过最大化节点级，子超图级，超图级的互信息，进行多通道超图之间的信息交互。超图是一种广义上的图，其边可以和任意数量的节点进行连接，因此可以易于表征代码结构的高阶关系，本发明将代码构建为语法结构，控制流信息，数据流信息，先后序信息，语义信息五个通道，五个通道分别对应了一个高阶关系。本发明通过模体构建超边，将处在一个模体中的节点构建为一个超边，处于一个超边中的节点，拥有模体对应的通道的高阶关系。自监督方法是一种具有监督形式的非监督学习方法，本发明基于自监督学习的方法，最大化不同层级表征的互信息，弥补不同通道之间的信息损失。本发明使用自监督学习方法以及超图神经网络进行训练，正确地使用了代码的高阶关系，漏洞检测效果较好。在超图神经网络中需要设置标签，所述标签为0或1，用来表示函数级代码是否存在漏洞。0代表不存在漏洞，1代表存在漏洞。标签是深度学习中重要的数据，通过对预测值与标签的差值求损失，进行梯度下降，来训练模型。本发明提出了一种基于自监督学习及多通道超图神经网络的漏洞检测方法，其特征在于，所述方法包括如下步骤：步骤一，选取代码数据集，并对所述代码数据集中的函数级代码进行预处理；步骤二，使用代码分析工具将代码转化为代码序列图；所述代码序列图中包含语法结构，代码控制流信息，数据流向信息，代码先后序信息，语义信息上述五种高阶关系；步骤三，利用Word2vec将代码序列图节点上的代码Token转化成代码向量特征表示；步骤四，根据代码序列图上存在的高阶关系，将其分为多个通道，为每个通道构建可以表征高阶关系的模体；步骤五，根据步骤四中构建获得的模体将所述代码序列图转化为多通道的代码序列超图；步骤六，对每一个通道的超图，使用超图神经网络聚合对应所述通道的代码序列超图的节点表征，对节点表征进行平均池化，得到对应通道的超图向量；步骤七，使用自监督学习方法，将节点级，子超图级，超图级的互信息最大化，弥补信息聚合的损失。步骤八，将多通道超图向量进行聚合，通过一个单层感知机，得到最终的图分类结果，判断是否存在漏洞。其中，所述代码数据是指软件未编译的高级语言源代码。步骤一中，所述代码数据集的收集方式包括通过检查项目安全相关的代码提交，将修复代码的提交标记为安全的代码，将修复之前的代码标记为有漏洞的代码。步骤一中，所述代码预处理的方法为去除代码字符串中没有信息量的特殊符号和链接，然后对代码进行代码标准化；其中，所述特殊符号包括基本逗号、句号、回车符、换行符和数学符号以及表情符号；所述链接为描述对象的网站链接，在本发明数据预处理过程中进行去除；所述代码标准化即为将变量名，类名，函数名进行标准化；变量名、类名、函数名按照代码编译序列，分别使用VAR、CLASS、FUNC代替。步骤二中，所述代码分析工具包括：Joern、ANTLR、Soot；所述代码序列图为一种通过代码分析工具由代码转化成的表示代码语法结构，代码控制流信息，数据流向信息，代码先后序信息，语义信息组成的多关系复合图；所述代码序列图的生成步骤如下：首先通过代码分析工具生成抽象语法树AST与控制流图CFG，其中抽象语法树中包含着代码语法结构信息，控制流图CFG中包含着控制流信息；接着遍历AST中叶子节点的token序列，得到叶子节点，即代码token之间的数据传递关系，生成数据流图DFG；最后遍历AST中叶子节点中的token序列，与代码文本进行匹配，生成表示代码token前后序列关系的序列关系图SRG；对于语义信息，使用文档主题模型中的线性判别法LDA，确定代码主题词，将每个主题确定为一个节点，选择与主题相关的TOP K个词，分别与主题节点连接，构建代码主题图CTG；将多种关系聚合在同一张图上，生成代码序列图CSG。步骤三中，所述将代码Token转化为向量特征表示的步骤如下：对步骤一预处理后的代码文本数据，利用词向量模型word2vec进行训练，学习出整个数据集中每个词的词向量表示；记词向量的维度为d，并将对应词向量映射到代码序列图上；所述词向量是指根据word2vec预训练模型得到的向量，每个词对应一个向量，以唯一的表示词向量的信息；所述图节点是指代码序列图中的节点，其中除了控制流节点外的每个节点都对应着一个代码Token。步骤四中，所述模体为在数据的复杂网络中频繁出现的一种网络模式，表示着复杂网络上是否存在着可以表征某一类型特征的超边。将可以表征为同一超边的模体划归一个通道，针对不同通道的超图分别进行超图卷积。在所述复杂网络中存在着两种可以表征复杂网络特征的元结构，分别是模体与元路径。模体相对与元路径，拥有更复杂的结构，能够表征更复杂的结构信息。步骤四中，所述高阶关系是指更高层次的关系，相对于简单图而言，超图的边同时连接多个节点，可以表示出更高阶的关系。如，对于引文网络包含了作者，论文，期刊，以及作者论文之间的关系，论文期刊之间的关系，而超图将多个异构节点之间的关系建模为模体，通过模体构建超边，一个超边直接可以表达关于一篇文章的共同作者关系等多节点，即其中的高阶关系。于本发明而言，高阶关系主要针对这几个通道：语法结构，控制流信息，数据流信息，先后序信息，语义信息高阶关系，如针对数据流，简单图只能表征从一个变量到另一个变量的数据流关系，而不能表示出此变量是多少变量如何计算得出。而超图可以完全地表示这个变量的表征是由多少变量流入，以及这些变量之间是如何进行计算的。步骤四中，所述根据代码序列图上存在的高阶关系，将其分为多个通道，为每个通道构建可以表征高阶关系的模体的步骤如下：根据代码序列图上存在的语法结构，控制流信息，数据流信息，先后序信息，语义信息上述五种高阶关系，将其分为5个通道。分别使用符号g，c，d，o，s来表示这五个通道。对于语法结构通道，控制流信息通道，数据流信息通道，先后序信息通道，语义信息通道，分别设计相对应模体来表示通道对应的含有高阶关系的信息。在本发明的实施中，可以将语义信息的这个高阶关系先建模到简单图中，然后使用和其他通道相同的方法构建模体与超图，表示高阶关系。步骤五中，所述根据模体将代码序列图转化为多通道的代码序列超图的步骤如下：根据文献Motif-based convolutional neural network on graphs的方法对代码序列图进行模体的采样，对于采样模体样本过多的通道与模体进行裁剪，样本的阈值设为模体最大采样数的百分之八十，将剩余每一个模体作为超边，模体中的每一个点作为超边的节点。将同一通道的超边进行拼接，获得对应通道的超图邻接矩阵，分别用符号Hg,Hc,Hd,Ho,Hs来表示。步骤六中，所述生成各通道的超图向量的步骤如下：步骤、为每一个通道设置一个可训练的转移矩阵，将Word2vec的每一个节点向量通过转移矩阵，得到该通道的初始超图节点向量。转移矩阵分别用符号Wg,Wc,Wd,Wo,Ws来表示。其中X∈Rn×d为Word2vec得到的节点向量，表示i通道对应的初始超图节点向量。n为节点数量，d为节点向量维度。步骤、对每一个通道中的节点向量进行超图卷积，超图神经网络中每一层的超图卷积分为两步，第一步将节点的向量进行卷积，得到本层的超边向量，第二步将超边向量进行卷积，得到节点向量。在两个过程中，都引入了注意力机制。对于通道i，节点级注意力机制公式如下：其中l是图神经网络的层数，H是某一通道超图的邻接矩阵，矩阵每一列代表一个超边，矩阵每一行代表一个节点，每一个矩阵元素代表此节点是否存在此超边中，0为节点不在超边中，1为节点在超边中，k,p为节点标号，j为超边标号，υkυp代表k,p节点，ej代表超边j，vp∈ej代表属于超边j的所有节点，是超边j在l的表征，a1是可训练的注意力向量，角标T为转置矩阵，uk是针对每个节点的可训练的注意力向量，αjk为超边j节点k的注意力权重，exp为指数函数，σ与LeakyReLU为非线性的激活函数。对于通道i，超边级的注意力机制公式如下：其中l是图神经网络的层数，H是某一通道超图的邻接矩阵，矩阵每一列代表一个超边，矩阵每一行代表一个节点，每一个矩阵元素代表此节点是否存在此超边中，0为节点不在超边中，1为节点在超边中，j为节点标号，k,p为超边标号，vk,vp代表k,p超边，εj代表节点j，ep∈εj代表连接节点j的所有节点，是超边j在l的表征，a2是可训练的注意力向量，角标T为转置矩阵，vk是针对每个超边的可训练的注意力向量，βjk为节点j超边k的注意力权重，exp为指数函数，σ与LeakyReLU为非线性的激活函数。步骤、将节点表征经过平均池化，得到该通道的超图向量。其中AVGPOOL为平均池化操作，其中为i通道第l层的节点向量，ci为i通道的超图表征向量。步骤七中，所述使用自监督学习方法，将节点级，子超图级，超图级的互信息最大化步骤如下：通过对节点连接的超边进行平均池化，得到子超图表征。公式如下：其中k为节点标号，Xi为超图在i通道的节点表征矩阵，其中每一行代表一个节点的节点向量，为k节点在i通道的超图邻接向量，0代表着节点不存在于超边中，1代表节点存在于超边中，即代表着节点连接着的超边，即为节点连接的超边数量，为i通道关于k节点的子超图表征向量。将超图节点转移矩阵随机打乱，得到新的转移矩阵同样对节点连接的超边进行平均池化，得到消极样例超图表征。公式如下：其中k为节点标号，Xi为超图在i通道的节点表征矩阵，其中每一行代表一个节点的节点向量，为打乱后的k节点在i通道的关于新的邻接矩阵超图邻接向量，0代表着节点不存在于超边中，1代表节点存在于超边中，即代表着节点连接着的超边，即为打乱后的节点连接的超边数量，为i通道关于打乱后的k节点的子超图表征向量。计算节点级，超图级与子超图级的互信息，并以自监督学习损失函数的形式使其最大化。所述自监督学习的损失函数公式如下：其中fD:Rd×Rd→R是一个向量相似性的判别器，在实现中通常采用点积函数作为此判别器，Ls为自监督损失函数，σ为非线性的激活函数。步骤八中将多通道超图向量进行聚合，通过一个单层感知机，得到最终的图分类结果，判断是否存在漏洞，具体公式如下：x＝xg||xc||xd||xo||xs其中||为向量拼接操作，cg,cc,cd,co,cs为每个通道对应向量，c为最终用于图分类的超图向量；其中Wfull为全连接层权重，b为全连接层偏移向量，softmax与tanh为非线性的激活函数。上述公式计算的为一个0-1的值，代表着代码漏洞检测的结果，越接近1代表函数级代码中存在漏洞的概率越大，越接近0代码函数级代码中出现漏洞的概率越小；在使用中算法将大于0.5的时候看作函数级代码存在漏洞，将小于等于0.5看作函数级代码不存在漏洞；根据标签值与真实值计算得到超图神经网络的损失函数：其中λ||Φ||2是神经网络所有参数的l2正则项，yi为标签值，为预测值，将所有代码样本的预测值与标签值的交叉熵相加即为损失函数的第一项。损失函数的第二项为l2正则项，其中λ1为正则项权重，Φ代表了超图神经网络的所有参数。结合上述自监督任务损失函数，神经网络最终的损失函数为：其中λ2为自监督任务的损失函数权重，由损失函数反向传播即可进行模型的训练。本发明还提供了一种实现上述所述漏洞检测方法的系统，所述系统包括：代码处理及分析模块、超图生成及转化模块、代码检测模块；所述代码处理及分析模块用于对代码进行预处理，并通过代码分析工具将代码转化为代码序列图，将代码序列图节点上的代码Token转化成代码向量特征表示；所述超图生成及转化模块用于构建不同通道上的模体，并通过模体将代码序列图转化为多通道的代码序列超图，得到代码序列超图上的节点表征；所述代码检测模块用于聚合不同通道上的节点表征与超边表征，通过MEANPOOL将超图聚合后的节点表征表示转化为多通道的超图表征，最后将多通道超图向量进行拼接，通过感知机获得漏洞分类结果。本模块还引入了自监督学习，通过节点级，子超图级，超图级的表征互信息最大化，弥补不同通道之间的信息损失。本发明的有益效果包括：提出了一种新的代码图结构—代码序列图，能够同时考虑代码的数据流信息，控制流信息，以及代码序列信息，将数据流信息下放到叶子节点，让图中只有一种数据流边，精简了代码图结构，使数据流信息更加干净。将代码编译为超图，从代码语法结构，数据流，控制流，代码前后序列，代码语义信息，五个方面学习代码内部的高阶关系，让本发明拥有了更低的误报率并且能够检测到更多的漏洞。对于代码语法结构，数据流，控制流，代码前后序列，本发明使用模体创建超图，降低了生成超图的时间开销。附图说明图1为本发明的流程图。图2为本发明实施例中代码处理流程图。图3为本发明实施例中自监督学习及多通道超神经网络的模型架构图。图4为本发明系统模块图。具体实施方式结合以下具体实施例和附图，对本发明作进一步的详细说明。实施本发明的过程、条件、实验方法等，除以下专门提及的内容之外，均为本领域的普遍知识和公知常识，本发明没有特别限制内容。本发明提供的代码漏洞检测方法，如图1所示，该方法包括以下步骤：步骤一，选取代码数据集，并对所述代码数据集中的函数级代码进行预处理；步骤二，使用代码分析工具将代码转化为代码序列图；所述代码序列图中包含语法结构，代码控制信息，数据流向信息，代码先后序信息，语义信息上述五种高阶关系；步骤三，利用Word2vec将代码序列图节点上的代码Token转化成代码向量特征表示；步骤四，根据代码序列图上存在的高阶关系，将其分为多个通道，为每个通道构建可以表征高阶关系的模体。步骤五，根据步骤四中构建获得的模体将所述代码序列图转化为多通道的代码序列超图。步骤六，对每一个通道的超图，使用超图神经网络聚合对应所述通道的代码序列超图的节点表征，对节点表征进行平均池化，得到此通道的超图向量。步骤七，使用自监督学习方法，将节点级，子超图级，超图级的互信息最大化，弥补信息聚合的损失。步骤八，将多通道超图向量进行聚合，通过一个单层感知机，得到最终的图分类结果，判断是否存在漏洞。其中，所述代码数据是指软件未编译的高级语言源代码。本发明还提供了一种实现上述所述漏洞检测方法的系统，所述系统包括：代码处理及分析模块、超图生成及转化模块、代码检测模块；所述代码处理及分析模块用于对代码进行预处理，并通过代码分析工具将代码转化为代码序列图，将代码序列图节点上的代码Token转化成代码向量特征表示；所述超图生成及转化模块用于构建不同通道上的模体，并通过模体将代码序列图转化为多通道的代码序列超图，得到代码序列超图上的节点表征；所述代码检测模块用于聚合不同通道上的节点表征与超边表征，通过MEANPOOL将超图聚合后的节点表征表示转化为多通道的超图表征，最后将多通道超图向量进行拼接，通过感知机获得漏洞分类结果。本模块还引入了自监督学习，通过节点级，子超图级，超图级的表征互信息最大化，弥补不同通道之间的信息损失。实施例本实施例的具体流程如下：首先，选取代码数据集QEMU与FFmpeg：对于选取的代码数据，下面描述了对代码文本的转换方式，如图二所示：对代码进行预处理，去除代码中的链接，特殊字符等。对代码变量名进行标准化。使用编译工具Joern生成函数代码的抽象语法树与控制流图。接着遍历AST中叶子节点的token序列，得到代码token之间的数据传递关系，生成数据流图。接着遍历AST中叶子节点中的token序列，与代码文本进行匹配，生成表示代码token前后序列关系的序列关系图。接着根据文档主题模型中的线性判别法确定多个主题词，针对每个主题词，确定最相关的Top k个代码token节点，在本实施例中，k取3。将多个主题词视为节点分别与最相关的代码token节点构建边，生成了代码主题图。将多张图聚合在同一张图上，生成代码序列图。对预处理后的代码文本数据，利用词向量模型word2vec进行训练，学习数据集中每个词的词向量表示。记词向量的维度为d。根据模体将代码序列图转化为多通道的代码序列超图。将多通道的代码序列超图放入pytorch实现的超图神经网络中，得到多张超图的表征，将多个超图的表征进行拼接通过一个分类器，获得最后的代码漏洞判断结果。其中训练集，验证集，测试集比例为7:2:1。本发明方法还可以适用于其他各类代码数据集，具体过程不再详细说明。本发明上述实施例中的参数是根据验证集实验结果确定的，即在验证集上测试不同的参数组合，选取准确率较优的一组参数。具体来说对于可训练的参数，如权重W，根据神经网络的梯度下降算法进行训练。对于学习率，超图神经网络层数，正则化参数等超参数往往通过经验或者网格搜索的方法确定。在以往的经验中，学习率为0.00001，超图神经网络层数为6，正则化参数为0.000005。在实际以上的测试中，根据需求对上述参数进行适当调整也可实现本发明的目的。本发明的保护内容不局限于以上实施例。在不背离本发明构思的精神和范围下，本领域技术人员能够想到的变化和优点都被包括在本发明中，并且以所附的权利要求书为保护范围。
