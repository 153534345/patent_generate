标题title
面向智能计算的流水并行训练自适应调整系统、方法
摘要abst
本发明提供一种面向智能计算的流水并行训练自适应调整系统、方法，系统包括监控模块和调整模块，调整模块在计算节点的计算任务划分不均衡时，根据计算节点的不均衡类型，确定调整策略，并根据调整策略，调整子模型在计算集群中的分配；调整包括以下至少一种：将计算任务划分不均衡的计算节点的至少部分子模型的层由该计算节点迁移至其他计算节点；控制计算任务划分不均衡的计算节点执行CPU‑GPU内存交换或重计算，或者控制计算任务划分不均衡的计算节点取消当前执行的CPU‑GPU内存交换或重计算；对计算集群的网络拓扑结构进行调整。本发明能动态调整子模型在计算集群中的分配。
权利要求书clms
1.一种面向智能计算的流水并行训练自适应调整系统，其特征在于，计算集群包括多个计算节点，多个计算节点之间能够相互通信，各计算节点包括至少一CPU和至少一个GPU，待训练模型包括多层子模型，所述待训练模型的训练过程包括前向计算阶段及反向计算阶段，其中，在所述前向计算阶段，参数由多层所述子模型的第一层子模型依次向最后一层子模型传递，在所述反向计算阶段，参数由所述最后一层子模型依次向所述第一层子模型传递，各计算节点用于训练至少一个子模型；所述系统包括：监控模块，用于负责监控和收集所述计算集群内各计算节点的资源运行情况，并根据各计算节点的资源运行情况，确定该计算节点的计算任务划分是否均衡，以及当所述计算节点的计算任务划分不均衡时，确定所述计算节点的不均衡类型；调整模块，当所述计算节点的计算任务划分不均衡时，用于根据所述计算节点的不均衡类型，确定调整策略，并根据所述调整策略，调整子模型在计算集群中的分配；其中，所述调整包括以下至少一种：将计算任务划分不均衡的计算节点的至少部分子模型的层由该计算节点迁移至其他计算节点；控制计算任务划分不均衡的计算节点执行CPU-GPU内存交换或重计算，或者控制计算任务划分不均衡的计算节点取消当前执行的CPU-GPU内存交换或重计算；对所述计算集群的网络拓扑结构进行调整。2.根据权利要求1所述的面向智能计算的流水并行训练自适应调整系统，其特征在于，所述资源运行情况包括计算延迟、GPU利用率、网络传输延迟及内存使用率；所述监控模块在根据各计算节点的资源运行情况，确定该计算节点的计算任务划分是否均衡时，具体用于：当根据当前计算节点的资源运行情况，确定当前计算节点存在以下中的至少一种时，确定该计算节点的计算任务划分不均衡：所述当前计算节点的计算延迟大于或等于预设延迟阈值；所述当前计算节点的内存使用率大于或等于预设内存使用率阈值且所述当前计算节点的GPU利用率小于所述计算集群中所有计算节点的GPU利用率的平均值；当前计算节点的网络延迟超过所述计算集群中其他计算节点的最大网络延迟的预设倍数，其中所述预设倍数大于或等于1。3.根据权利要求2所述的面向智能计算的流水并行训练自适应调整系统，其特征在于，所述监控模块在所述计算节点的计算任务划分不均衡时，确定所述计算节点的不均衡类型时，具体用于：当当前计算节点的计算延迟大于或等于预设延迟阈值、和/或所述当前计算节点的内存使用率大于或等于预设内存使用率阈值且所述当前计算节点的GPU利用率小于所述计算集群中所有计算节点的GPU利用率的平均值时，所述当前计算节点的不均衡类型包括：当前计算阶段分配的层过多；当当前计算节点的网络延迟超过所述计算集群中其他计算节点的最大网络延迟的预设倍数时，所述当前计算节点的不均衡类型包括：网络异常。4.根据权利要求3所述的面向智能计算的流水并行训练自适应调整系统，其特征在于，当当前计算节点的计算延迟大于或等于预设延迟阈值时，所述调整策略包括计算调整策略；所述计算调整策略包括：当当前计算节点采用CPU-GPU内存交换或重计算时，取消所述当前计算节点采用的CPU-GPU内存交换或重计算；在取消所述当前计算节点采用的CPU-GPU内存交换或重计算后，若所述当前计算节点执行所述当前计算节点上的子模型所需要的内存需求超出所述当前计算节点的最大内存，则根据所述当前计算节点前一个计算节点的GPU利用率及所述当前计算节点的后一个计算节点的GPU利用率，将所述当前计算节点的至少部分子模型的至少部分层迁移至其他计算节点执行。5.根据权利要求4所述的面向智能计算的流水并行训练自适应调整系统，其特征在于，所述根据所述当前计算节点前一个计算节点的GPU利用率及所述当前计算节点的后一个计算节点的GPU利用率，将所述当前计算节点的至少部分子模型的至少部分层迁移至其他计算节点执行，包括：当所述当前计算节点前一个计算节点的GPU利用率小于所述当前计算节点的后一个计算节点的GPU利用率时，将所述当前计算节点前一个计算节点为初始目标计算节点；当所述当前计算节点前一个计算节点为初始目标计算节点时，比较所述初始目标计算节点的GPU利用率与所述初始目标计算节点的前一个计算节点的GPU利用率，若所述初始目标计算节点的GPU利用率小于所述初始目标计算节点的前一个计算节点的GPU利用率，则将所述初始目标计算节点作为最终的目标计算节点；若所述初始目标计算节点的GPU利用率大于所述初始目标计算节点的前一个计算节点的GPU利用率，则将所述初始目标计算节点的前一个计算节点作为新的初始目标计算节点，继续前迁移比较，依次进行，直至最前面的目标计算节点；将所述当前计算节点的至少部分子模型的至少部分层迁移至最终的目标计算节点执行。6.根据权利要求4所述的面向智能计算的流水并行训练自适应调整系统，其特征在于，所述根据所述当前计算节点前一个计算节点的GPU利用率及所述当前计算节点的后一个计算节点的GPU利用率，将所述当前计算节点的至少部分子模型的至少部分层迁移至其他计算节点执行，包括：当所述当前计算节点前一个计算节点的GPU利用率大于所述当前计算节点的后一个计算节点的GPU利用率时，将所述当前计算节点的后一个计算节点作为初始目标计算节点；当所述当前计算节点后一个计算节点为初始目标计算节点时，比较所述初始目标计算节点的GPU利用率与所述初始目标计算节点的后一个计算节点的GPU利用率，若所述初始目标计算节点的GPU利用率小于所述初始目标计算节点的后一个计算节点的GPU利用率，则将所述初始目标计算节点作为最终的目标计算节点；若所述初始目标计算节点的GPU利用率大于所述初始目标计算节点的后一个计算节点的GPU利用率，则将所述初始目标计算节点的后一个计算节点作为新的初始目标计算节点，继续前迁移比较，依次进行，直至最前面的目标计算节点；将所述当前计算节点的至少部分子模型的至少部分子单元迁移至最终的目标计算节点执行。7.根据权利要求5或6所述的面向智能计算的流水并行训练自适应调整系统，其特征在于，所述计算调整策略还包括：在将所述当前计算节点的至少部分子模型的至少部分层迁移至其他计算节点执行后，所述当前计算节点重新生成模型参数，并更新所述当前计算节点的模型版本信息。8.根据权利要求3或4所述的面向智能计算的流水并行训练自适应调整系统，其特征在于，当当前计算节点的内存使用率大于或等于预设内存使用率阈值且所述当前计算节点的GPU利用率小于所述计算集群中所有计算节点的GPU利用率的平均值时，所述调整策略包括内存调整策略；所述内存调整策略包括：当所述当前计算节点进行重计算的GPU开销大于所述当前计算节点进行CPU-GPU内存交换的GPU开销时，所述当前计算节点采用CPU-GPU内存交换以降低所述当前计算节点的内存使用率；当所述当前计算节点进行重计算的GPU开销小于所述当前计算节点进行CPU-GPU内存交换的GPU开销时，所述当前计算节点采用重计算以降低所述当前计算节点的内存使用率。9.根据权利要求8所述的面向智能计算的流水并行训练自适应调整系统，其特征在于，所述内存调整策略还包括：根据所述当前计算节点的原任务训练时长和所述当前计算节点执行所述重计算或所述CPU-GPU内存交换所需的时长，确定所述当前计算节点的计算时长；当所述当前计算节点的计算时长大于或等于所述计算集群中所有计算节点的平均任务训练时长时，将所述当前计算节点的至少部分子模型的至少部分子单元迁移至所述当前计算节点的相邻计算节点执行；当所述当前计算节点的计算时长小于所述计算集群中所有计算节点的平均任务训练时长时，将所述当前计算节点作为其他计算节点进行层迁移的计算节点的子单元迁入的目标计算节点。10.根据权利要求3或4所述的面向智能计算的流水并行训练自适应调整系统，其特征在于，当当前计算节点的网络延迟超过所述计算集群中其他计算节点的最大网络延迟的预设倍数时，所述调整策略包括拓扑调整策略；所述拓扑调整策略包括：选择与该当前计算节点的网络延迟最小的三个连续计算节点，并确定当前网络未延迟的计算节点的最大网络延迟一倍以上；将所述当前计算节点与所述三个连续计算节点的中间计算节点进行任务互换；分别判断任务相互换的两个计算节点的前后计算节点的网络延迟是否超过所述最大网络延迟，若超过，则继续选择延迟次小的三个连续计算节点，重复任务互换过程，直至遍历所有计算节点，若仍不存在不超过所述最大网络延迟的计算节点，则结束所述计算集群的网络拓扑调整；若未超过，则互换所述任务相互换的两个计算节点之间的模型参数和中间变量。11.根据权利要求10所述的面向智能计算的流水并行训练自适应调整系统，其特征在于，所述拓扑调整策略还包括：若所述任务相互换的两个计算节点中任一计算节点的内存使用率大于或等于预设内存使用率阈值，则采用内存调整策略继续调整子模型在计算集群中的分配；在采用所述内存调整策略继续调整子模型在计算集群中的分配后，若任务相互换的两个计算节点中任一计算节点的内存使用率仍大于或等于预设内存使用率阈值，则采用计算调整策略来迁移所述内存使用率仍大于或等于预设内存使用率阈值的计算节点的至少部分子模型的至少部分子单元。12.一种面向智能计算的流水并行训练自适应调整方法，其特征在于，计算集群包括多个计算节点，多个计算节点之间能够相互通信，各计算节点包括至少一CPU和至少一个GPU，待训练模型包括多层子模型，所述待训练模型的训练过程包括前向计算阶段及反向计算阶段，其中，在所述前向计算阶段，参数由多层所述子模型的第一层子模型依次向最后一层子模型传递，在所述反向计算阶段，参数由所述最后一层子模型依次向所述第一层子模型传递，各计算节点用于训练至少一个子模型；所述方法包括：监控模块负责监控和收集所述计算集群内各计算节点的资源运行情况，并根据各计算节点的资源运行情况，确定该计算节点的计算任务划分是否均衡，以及当所述计算节点的计算任务划分不均衡时，确定所述计算节点的不均衡类型；调整模块在所述计算节点的计算任务划分不均衡时，根据所述计算节点的不均衡类型，确定调整策略，并根据所述调整策略，调整子模型在计算集群中的分配；其中，所述调整包括以下至少一种：将计算任务划分不均衡的计算节点的至少部分子模型的至少部分层由该计算节点迁移至其他计算节点；控制计算任务划分不均衡的计算节点执行CPU-GPU内存交换或重计算，或者控制计算任务划分不均衡的计算节点取消当前执行的CPU-GPU内存交换或重计算；对所述计算集群的网络拓扑结构进行调整。13.一种面向智能计算的流水并行训练自适应调整装置，其特征在于，包括存储器和一个或多个处理器，所述存储器中存储有可执行代码，所述一个或多个处理器执行所述可执行代码时，用于实现权利要求12所述的面向智能计算的流水并行训练自适应调整方法。14.一种计算机可读存储介质，其特征在于，其上存储有程序，该程序被处理器执行时，实现权利要求12所述的面向智能计算的流水并行训练自适应调整方法。
说明书desc
技术领域本发明涉及一种智能计算领域，尤其涉及一种面向智能计算的流水并行训练自适应调整系统、方法。背景技术深度学习的出现给自然语言处理、音视频处理、融媒体等领域带来了巨大的更新。但是随着深度学习模型越来越大，有些大模型的参数量甚至超过了几百亿，如此大规模的模型往往通过构建分布式机器学习系统来完成模型训练。分布式训练可以突破单张GPU的算力限制，在模型训练的时候通过在多台机器和多张GPU卡上构建分布式训练方法来加快模型训练，已经成为一种非常普遍的方法。其中，流水并行是一种常见的分布式训练方法，流水并行方法将模型按层划分成多个阶段stage，每个阶段部署到GPU上，多个阶段顺序执行前向计算，并在最后一个阶段计算损失函数，再从最后一个阶段到第一个阶段依次进行反向计算。整个过程中不同阶段的前向计算和反向计算之间的空闲等待时间并不相同。然后通过在每个阶段同时执行多个mini-batch，实现多个阶段流水执行，减少GPU空闲等待时间，提高效率。但是深度学习模型的不同层的计算需求、内存需求、通信需求都存在差异，如何在各阶段均衡分配计算、内存和网络资源，对于提高流水并行的计算效率至关重要。当前方法普遍采用静态方法进行层的划分，比如pipedream采用动态规划的方法进行划分。但是，当前AI框架支持动态计算图，如Pytorch，因此不同训练时期模型可能发生变化，静态的划分结果在模型变更后会面临整体效率降低的问题。发明内容本发明的目的在于提供一种面向智能计算的流水并行训练自适应调整系统、方法，解决了现有技术中采用静态方法进行层的划分在模型变更后会面临整体效率降低的的问题。本发明采用的技术方案如下：本发明实施例提供一种面向智能计算的流水并行训练自适应调整系统，计算集群包括多个计算节点，多个计算节点之间能够相互通信，各计算节点包括至少一CPU和至少一个GPU，待训练模型包括多层子模型，所述待训练模型的训练过程包括前向计算阶段及反向计算阶段，其中，在所述前向计算阶段，参数由多层所述子模型的第一层子模型依次向最后一层子模型传递，在所述反向计算阶段，参数由所述最后一层子模型依次向所述第一层子模型传递，各计算节点用于训练至少一个子模型；所述系统包括：监控模块，用于负责监控和收集所述计算集群内各计算节点的资源运行情况，并根据各计算节点的资源运行情况，确定该计算节点的计算任务划分是否均衡，以及当所述计算节点的计算任务划分不均衡时，确定所述计算节点的不均衡类型；调整模块，当所述计算节点的计算任务划分不均衡时，用于根据所述计算节点的不均衡类型，确定调整策略，并根据所述调整策略，调整子模型在计算集群中的分配；其中，所述调整包括以下至少一种：将计算任务划分不均衡的计算节点的至少部分子模型的层由该计算节点迁移至其他计算节点；控制计算任务划分不均衡的计算节点执行CPU-GPU内存交换或重计算，或者控制计算任务划分不均衡的计算节点取消当前执行的CPU-GPU内存交换或重计算；对所述计算集群的网络拓扑结构进行调整。本发明还提供一种面向智能计算的流水并行训练自适应调整方法，计算集群包括多个计算节点，多个计算节点之间能够相互通信，各计算节点包括至少一CPU和至少一个GPU，待训练模型包括多层子模型，所述待训练模型的训练过程包括前向计算阶段及反向计算阶段，其中，在所述前向计算阶段，参数由多层所述子模型的第一层子模型依次向最后一层子模型传递，在所述反向计算阶段，参数由所述最后一层子模型依次向所述第一层子模型传递，各计算节点用于训练至少一个子模型；所述方法包括：监控模块负责监控和收集所述计算集群内各计算节点的资源运行情况，并根据各计算节点的资源运行情况，确定该计算节点的计算任务划分是否均衡，以及当所述计算节点的计算任务划分不均衡时，确定所述计算节点的不均衡类型；调整模块在所述计算节点的计算任务划分不均衡时，根据所述计算节点的不均衡类型，确定调整策略，并根据所述调整策略，调整子模型在计算集群中的分配；其中，所述调整包括以下至少一种：将计算任务划分不均衡的计算节点的至少部分子模型的至少部分层由该计算节点迁移至其他计算节点；控制计算任务划分不均衡的计算节点执行CPU-GPU内存交换或重计算，或者控制计算任务划分不均衡的计算节点取消当前执行的CPU-GPU内存交换或重计算；对所述计算集群的网络拓扑结构进行调整。本发明还提供一种面向智能计算的流水并行训练自适应调整装置，包括存储器和一个或多个处理器，所述存储器中存储有可执行代码，所述一个或多个处理器执行所述可执行代码时，用于实现上述的面向智能计算的流水并行训练自适应调整方法。本发明还提供一种计算机可读存储介质，其上存储有程序，该程序被处理器执行时，实现上述的面向智能计算的流水并行训练自适应调整方法。本发明的有益效果是：调整模块在述计算节点的计算任务划分不均衡时，动态调整子模型在计算集群中的分配，有效提升了流水并行的动态适应能力，提升了智能计算集群的GPU利用率。附图说明为了更清楚地说明本发明实施例中的技术方案，下面将对实施例描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本发明的一些实施例，对于本领域普通技术人员来讲，在不付出创造性劳动性的前提下，还可以根据这些附图获得其他的附图。图1为本发明一实施例提供的一种计算集群的结构示意图；图2为本发明一实施例提供的一种面向智能计算的流水并行训练自适应调整系统的结构示意图；图3为本发明一实施例提供的一种计算调整策略的流程示意图；图4为本发明一实施例提供的一种内存调整策略的流程示意图；图5为本发明一实施例提供的一种拓扑调整策略的流程示意图；图6为本发明一实施例提供的一种面向智能计算的流水并行训练自适应调整方法的流程示意图；图7为本发明一实施例提供的一种面向智能计算的流水并行训练自适应调整装置的结构示意图。附图标记：10、监控模块；20、调整模块。具体实施方式下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例仅仅是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有做出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。需要说明的是，在不冲突的情况下，下述的实施例及实施方式中的特征可以相互组合。计算集群普遍支持多组户，特别是公有云场景，计算集群中的计算节点的性能也会受共享任务的变化而出现变化。因此，亟需一种针对流水并行层划分的自适应调整方法，以适应动态变化的场景。参见图1，本发明实施例中的计算集群可包括多个计算节点，多个计算节点之间能够相互通信，各计算节点包括至少一CPU和至少一个GPU。如图1所示，计算集群可包括计算节点1、计算节点2、…、计算节点N，其中，N为正整数，且N大于或等于3。本发明实施例的待训练模型可以为神经网络模型，也可以为其他类型的模型，如待训练的数学模型。在本发明实施例中，待训练模型可包括多层子模型，采用流水并行方式训练所述待训练模型，具体地，待训练模型的训练过程包括前向计算阶段及反向计算阶段。其中，在前向计算阶段，参数由多层子模型的第一层子模型依次向最后一层子模型传递，在反向计算阶段，参数由最后一层子模型依次向第一层子模型传递。需要说明的是，一轮训练迭代包括一次前向计算阶段和一次反向计算阶段。示例性地，待训练模型为神经网络模型，该神经网络模型包括第一层网络、第二层网络、第三层网络和第四层网络，第一层网络、第二层网络、第三层网络和第四层网络顺序连接，第一层网络为第一层子模型，第四层网络为最后一层子模型。在前向计算阶段，参数由第一层网络依次向第二层网络、第三层网络和第四层网络传递；在反向计算阶段，参数由第四层网络依次向第三层网络、第二层网络和第一层网络传递。需要说明的是，第一层网络、第二层网络和第三层网络的类型可根据需要设计，例如，第一层网络为输入层、第二层网络为卷积层，第三层网络为池化层，第四层网络为输出层。计算集群中的各计算节点用于训练至少一个子模型，即每个计算节点分配至少一个子模型，从而提升智能计算集群的GPU利用率。参见图2，本发明实施例提供一种面向智能计算的流水并行训练自适应调整系统，该系统可包括监控模块10和调整模块20。在本发明实施例中，监控模块10用于负责监控和收集计算集群内各计算节点的资源运行情况，并根据各计算节点的资源运行情况，确定该计算节点的计算任务划分是否均衡，以及当计算节点的计算任务划分不均衡时，确定计算节点的不均衡类型。监控模块10在确定计算集群中存在计算任务划分不均衡的计算节点时，会向调整模块20通知计算集群中存在计算任务划分不均衡的计算节点以及对应的不均衡类型。沿用上述实施例，监控模块10用于负责监控和收集计算节点1、计算节点2、…、计算节点N的资源运行情况，并根据计算节点1、计算节点2、…、计算节点N的资源运行情况，确定出计算节点2的计算任务划分不均衡，进一步确定计算节点2的不均衡类型。并且，监控模块10会向调整模块20通知计算节点2的计算任务划分不均衡及不均衡的类型。调整模块20用于在计算节点的计算任务划分不均衡时，根据计算节点的不均衡类型，确定调整策略，并根据调整策略，调整子模型在计算集群中的分配。本发明实施例中，调整模块20在接收到监控模块10发送的用于指示计算集群中存在计算任务划分不均衡的计算节点的指示信息时，根据计算任务划分不均衡的计算节点的不均衡类型，确定调整策略，并根据调整策略，调整子模型在计算集群中的分配。其中，指示信息携带有计算任务划分不均衡的计算节点的不均衡类型。在本发明实施例中，调整子模型在计算集群中的分配中的调整可包括以下至少一种方式：、将计算任务划分不均衡的计算节点的至少部分子模型的层由该计算节点迁移至其他计算节点；、控制计算任务划分不均衡的计算节点执行CPU-GPU内存交换或重计算，或者控制计算任务划分不均衡的计算节点取消当前执行的CPU-GPU内存交换或重计算；、对计算集群的网络拓扑结构进行调整。本发明实施例中的面向智能计算的流水并行训练自适应调整系统的调整模块20在述计算节点的计算任务划分不均衡时，动态调整子模型在计算集群中的分配，有效提升了流水并行的动态适应能力，提升了智能计算集群的GPU利用率。在本实施例中，资源运行情况可包括计算延迟、GPU利用率、网络传输延迟及内存使用率等信息，即监控模块10监控和收集计算集群内各计算节点的计算延迟、GPU利用率、网络传输延迟及内存使用率等信息，具体地，监控模块10监控和收集每次训练迭代过程中的前向计算阶段和后向计算阶段的计算延迟、GPU利用率、网络传输延迟、内存使用率等信息，如此，通过监控和收集较全的运行信息，有助于后续调整策略的选择，从而有效提升计算集群的GPU利用率。可以理解的是，在其他实施例中，资源运行情况可包括计算延迟、GPU利用率、网络传输延迟及内存使用率等信息中的一部分。监控模块10收集计算集群内各计算节点在每次训练迭代过程中的资源运行情况的方式可根据需要设置，例如，在一些实施例中，在每一轮迭代训练结束后，计算集群内各计算节点将本节点在该轮迭代训练的计算延迟、GPU利用率、网络传输延迟、内存使用率等信息发送给监控模块10。在另外一些实施例中，监控模块10主动从计算集群内各计算节点读取各计算节点的计算延迟、GPU利用率、网络传输延迟、内存使用率等信息，例如，监控模块10可周期性的从计算集群内各计算节点读取各计算节点的计算延迟、GPU利用率、网络传输延迟、内存使用率等信息。其中，读取周期可根据需要设定，例如，监控模块10每隔10分钟从计算集群内各计算节点读取各计算节点的计算延迟、GPU利用率、网络传输延迟、内存使用率等信息。可选地，监控模块10在根据各计算节点的资源运行情况，确定该计算节点的计算任务划分是否均衡时，具体地，当根据当前计算节点的资源运行情况，确定当前计算节点存在以下中的至少一种情况时，确定该当前计算节点的计算任务划分不均衡：情况1、当前计算节点的计算延迟大于或等于预设延迟阈值。预设延迟阈值的大小可以根据需要设置，例如，当当前计算节点的计算延迟超过其他计算节点的计算延迟大小的一半以上，确定该当前计算节点的计算任务划分不均衡。情况2、当前计算节点的内存使用率大于或等于预设内存使用率阈值且当前计算节点的GPU利用率小于计算集群中所有计算节点的GPU利用率的平均值。预设内存使用率阈值的大小可以根据需要设置，例如，当当前计算节点的内存使用率超过90%，且当前计算节点的GPU利用率小于计算集群中所有计算节点的GPU利用率的平均值时，确定该当前计算节点的计算任务划分不均衡。情况3、当前计算节点的网络延迟超过计算集群中其他计算节点的最大网络延迟的预设倍数，其中预设倍数大于或等于1。预设倍数的大小可以根据需要设置，例如，当当前计算节点的网络传输延迟超过计算集群内其他计算节点的最大访问延迟的一倍以上时，确定该当前计算节点的计算任务划分不均衡。可选地，监控模块10在计算节点的计算任务划分不均衡时，确定计算节点的不均衡类型时，具体地，当当前计算节点的计算延迟大于或等于预设延迟阈值、和/或当前计算节点的内存使用率大于或等于预设内存使用率阈值且当前计算节点的GPU利用率小于计算集群中所有计算节点的GPU利用率的平均值时，当前计算节点的不均衡类型包括：当前计算阶段分配的层过多。当当前计算节点的网络延迟超过计算集群中其他计算节点的最大网络延迟的预设倍数时，当前计算节点的不均衡类型包括：网络异常。当当前计算节点的计算延迟大于或等于预设延迟阈值时，调整所述调整策略包括计算调整策略，计算调整策略负责调整某些计算延迟非常高的计算节点，通过计算调整策略对这个计算节点的层进行重新调整，减少该计算节点的计算任务，从而降低该计算节点的计算延迟。例如，当当前计算节点Nodeadjust的计算延迟超过其他计算节点的计算延迟大小的一半以上，那么认为当前计算节点Nodeadjust在当前计算阶段分配的层过多，调整模块20采用计算调整策略进行重新分配。本发明实施例的计算调整策略可包括：当当前计算节点采用CPU-GPU内存交换或重计算时，取消当前计算节点采用的CPU-GPU内存交换或重计算；在取消当前计算节点采用的CPU-GPU内存交换或重计算后，若当前计算节点执行当前计算节点上的子模型所需要的内存需求超出当前计算节点的最大内存，则根据当前计算节点前一个计算节点的GPU利用率及当前计算节点的后一个计算节点的GPU利用率，将当前计算节点的至少部分子模型的至少部分层迁移至其他计算节点执行。另外，若当前计算节点执行当前计算节点上的子模型所需要的内存需求未超出当前计算节点的最大内存，那么调整模块20结束调整。取消当前计算节点正在进行的CPU-GPU内存交换或重计算，节省当前计算节点能够用于计算的内存大小，减少该计算节点的计算任务，从而降低该当前计算节点的计算延迟。并且，调整子模型在计算集群中的分配时考虑计算所需的内存需求和当前计算节点的最大内存，在计算所需的内存需求超出当前计算节点的最大内存时，进行子模型的重新分配，如此，也能减少该当前计算节点的计算任务，从而降低该当前计算节点的计算延迟。其中，根据当前计算节点前一个计算节点的GPU利用率及当前计算节点的后一个计算节点的GPU利用率，将当前计算节点的至少部分子模型的至少部分层迁移至其他计算节点执行可包括以下至少一种步骤：I、当当前计算节点前一个计算节点的GPU利用率小于当前计算节点的后一个计算节点的GPU利用率时，将当前计算节点前一个计算节点为初始目标计算节点；当当前计算节点前一个计算节点为初始目标计算节点时，比较初始目标计算节点的GPU利用率与初始目标计算节点的前一个计算节点的GPU利用率，若初始目标计算节点的GPU利用率小于初始目标计算节点的前一个计算节点的GPU利用率，则将初始目标计算节点作为最终的目标计算节点；若初始目标计算节点的GPU利用率大于初始目标计算节点的前一个计算节点的GPU利用率，则将初始目标计算节点的前一个计算节点作为新的初始目标计算节点，继续前迁移比较，依次进行，直至最前面的目标计算节点；将当前计算节点的至少部分子模型的至少部分层迁移至最终的目标计算节点执行。调整子模型在计算集群中的分配时考虑计算所需的内存需求和当前计算节点的最大内存，在计算所需的内存需求超出当前计算节点的最大内存时，进行子模型的重新分配，将当前计算节点的至少部分子模型的至少部分层迁移至当前计算阶段中处于该当前计算节点之前的其他计算节点中GPU利用率较低的计算节点上，利于计算集群的计算延迟最小化。II、当当前计算节点前一个计算节点的GPU利用率大于当前计算节点的后一个计算节点的GPU利用率时，将当前计算节点的后一个计算节点作为初始目标计算节点；当当前计算节点后一个计算节点为初始目标计算节点时，比较初始目标计算节点的GPU利用率与初始目标计算节点的后一个计算节点的GPU利用率，若初始目标计算节点的GPU利用率小于初始目标计算节点的后一个计算节点的GPU利用率，则将初始目标计算节点作为最终的目标计算节点；若初始目标计算节点的GPU利用率大于初始目标计算节点的后一个计算节点的GPU利用率，则将初始目标计算节点的后一个计算节点作为新的初始目标计算节点，继续前迁移比较，依次进行，直至最前面的目标计算节点；将当前计算节点的至少部分子模型的至少部分子单元迁移至最终的目标计算节点执行。调整子模型在计算集群中的分配时考虑计算所需的内存需求和当前计算节点的最大内存，在计算所需的内存需求超出当前计算节点的最大内存时，进行子模型的重新分配，将当前计算节点的至少部分子模型的至少部分层迁移至当前计算阶段中处于该当前计算节点之后的其他计算节点中GPU利用率较低的计算节点上，利于计算集群的计算延迟最小化。需要说明的是，步骤I、步骤II中，当前计算阶段为当前计算延迟非常高的计算节点处于训练迭代中的计算阶段。可选地，在一些实施例中，调整子模型在根据当前计算节点前一个计算节点的GPU利用率及当前计算节点的后一个计算节点的GPU利用率，将当前计算节点的至少部分子模型的至少部分层迁移至其他计算节点执行时，具体地，调整子模型先执行步骤I，若当前计算阶段中计算时刻处于该当前计算节点之前的其他计算节点中存在GPU利用率小于该当前计算节点的GPU利用率的计算节点，则在找到最终的目标计算节点，结束调整。若前计算阶段中计算时刻处于该当前计算节点之前的其他计算节点中不存在GPU利用率小于该当前计算节点的GPU利用率的计算节点，则执行步骤II，若当前计算阶段中计算时刻处于该当前计算节点之后的其他计算节点中存在GPU利用率小于该当前计算节点的GPU利用率的计算节点，则再找到最终的目标计算节点，结束调整；若当前计算阶段中计算时刻处于该当前计算节点之后的其他计算节点中不存在GPU利用率小于该当前计算节点的GPU利用率的计算节点，则说明无法进行子模型的重分配。在另外一些实施例中，调整子模型在根据当前计算节点前一个计算节点的GPU利用率及当前计算节点的后一个计算节点的GPU利用率，将当前计算节点的至少部分子模型的至少部分层迁移至其他计算节点执行时，具体地，调整子模型先执行步骤II，若当前计算阶段中计算时刻处于该当前计算节点之后的其他计算节点中存在GPU利用率小于该当前计算节点的GPU利用率的计算节点，则再找到最终的目标计算节点，结束调整。若当前计算阶段中计算时刻处于该当前计算节点之后的其他计算节点中不存在GPU利用率小于该当前计算节点的GPU利用率的计算节点，则执行步骤I，若当前计算阶段中计算时刻处于该当前计算节点之前的其他计算节点中存在GPU利用率小于该当前计算节点的GPU利用率的计算节点，则再找到最终的目标计算节点，结束调整；若前计算阶段中计算时刻处于该当前计算节点之前的其他计算节点中不存在GPU利用率小于该当前计算节点的GPU利用率的计算节点，则说明无法进行子模型的重分配。进一步地，在一些实施例中，计算调整策略还可包括：在将当前计算节点的至少部分子模型的至少部分层迁移至其他计算节点执行后，当前计算节点重新生成模型参数，并更新当前计算节点的模型版本信息。如此，层发生变更的计算阶段stage会重新生成模型参数，并且更新版本号。同一批的数据训练使用同一版本的模型训练，保证训练一致性。因为流水并行方式本身支持多版本参数，旧版本的模型使用流水并行的模型版本管理进行释放。示例性地，在一具体实施例中，参见图3，计算调整策略可包括以下步骤：当当前计算节点Nodeadjust的计算延迟较大时，先判断当前计算节点Nodeadjust是否采用CPU-GPU内存交换或重计算，如果是，则先取消CPU-GPU内存交换或重计算。如果取消CPU-GPU内存交换或重计算后，内存需求没有超过前计算节点Nodeadjust最大内存，那么就结束调整。否则，继续步骤S12。再比较当前计算节点Nodeadjust前后两个计算节点上两个阶段stage的GPU利用率，将当前计算节点Nodeadjust的层迁移到GPU利用率较低的计算节点Nodetarget。如果Nodetarget是前一个计算节点，然后继续比较计算节点Nodetarget的前一个计算节点Nodebefore，如果Nodebefore的GPU利用率小于Nodetarget的GPU利用率，那么继续将Nodetarget的层往前迁移。如果Nodetarget是后一个计算节点，然后比较计算节点Nodetarget的后一个计算节点Nodeafter，如果Nodeafter的GPU利用率小于Nodetarget的GPU利用率，那么继续将Nodetarget的层往后迁移。依次进行，直达最后一个计算节点。最后，层发生变更的阶段stage重新生成模型参数，并且更新版本号。同一批的数据训练使用同一版本的模型训练，保证训练一致性。因为流水并行本身支持多版本参数，旧版本的模型使用流水并行的模型版本管理进行释放。当当前计算节点的内存使用率大于或等于预设内存使用率阈值且当前计算节点的GPU利用率小于计算集群中所有计算节点的GPU利用率的平均值时，调整调整策略包括内存调整策略，内存调整策略负责调整内存使用率非常高但是GPU利用率却很低的计算节点，通过内存调整策略对这个计算节点的层进行重新调整，减少该计算节点的计算任务，从而降低该计算节点的计算延迟。例如，当当前计算节点的内存使用率超过90%，但该当前计算节点的GPU利用率却低于计算集群内所有计算节点的GPU利用率的平均值，那么认为当前计算节点在当前计算阶段分配的层过多，采用内存调整策略进行重新分配。需要说明的是，当前计算阶段为当前内存使用率非常高但是GPU利用率却很低的计算节点处于训练迭代中的计算阶段。本发明实施例的内存调整策略可包括：当当前计算节点进行重计算的GPU开销大于当前计算节点进行CPU-GPU内存交换的GPU开销时，当前计算节点采用CPU-GPU内存交换以降低当前计算节点的内存使用率。当当前计算节点进行重计算的GPU开销小于当前计算节点进行CPU-GPU内存交换的GPU开销时，当前计算节点采用重计算以降低当前计算节点的内存使用率。针对GPU显存较小，或者模型参数和中间变量等模型本身原因导致内存使用率较高而计算效果较低的问题，因为显存受限无法通过迁移相邻阶段的层进一步提高GPU计算效率。因此，本实施例首先通过重计算或者CPU-GPU内存交换来降低该当前计算阶段的内存使用率。如果重计算的GPU开销超过内存交换的GPU开销，那么采用内存交换；反之，如果内存交换的GPU开销超过重计算的GPU开销，那么采用重计算。进一步地，在一些实施例中，内存调整策略还可包括：根据当前计算节点的原任务训练时长和当前计算节点执行重计算或CPU-GPU内存交换所需的时长，确定当前计算节点的计算时长；当当前计算节点的计算时长大于或等于计算集群中所有计算节点的平均任务训练时长时，将当前计算节点的至少部分子模型的至少部分子单元迁移至当前计算节点的相邻计算节点执行；当当前计算节点的计算时长小于计算集群中所有计算节点的平均任务训练时长时，将当前计算节点作为其他计算节点进行层迁移的计算节点的子单元迁入的目标计算节点。因为内存交换或者重计算都会增加额外的计算开销，在当前计算节点的原任务训练时长上加上内存交换或者重计算所需的时长作为该阶段的计算时间Tadjust。然后比较计算时间Tadjust和流水并行中各阶段的平均计算时间Taverage，如果Tadjust ＜ Taverage, 那么将其作为计算效率较低的计算阶段，并采用层迁移策略从相邻的计算阶段迁入层来平衡各计算阶段的计算效率。如果Tadjust ＞= Taverage, 则从该当前计算阶段中拆出一些层，迁移到相邻计算阶段。最终达到计算集群的计算延迟最小化的效果。示例性地，在一具体实施例中，参见图4，内存调整策略可包括以下步骤：决定使用重计算还是内存交换：首先通过重计算或者CPU-GPU内存交换来降低该阶段的内存使用率。如果重计算的GPU开销超过内存交换的GPU开销，那么采用内存交换；反之，如果内存交换的GPU开销超过重计算的GPU开销，那么采用重计算。更新当前计算阶段的计算时间：在当前计算节点的原任务训练时长上加上内存交换或者重计算所需的时长作为该阶段的计算时间Tadjust。与其他计算阶段的平均计算时间比较：比较计算时间Tadjust和流水并行中各阶段的平均计算时间Taverage。使用计算调整策略调整：如果Tadjust ＜ Taverage, 那么将其作为计算效率较低的阶段，并采用层迁移策略从相邻的阶段迁入层来平衡各阶段的计算效率。如果Tadjust ＞=Taverage, 则从该阶段中拆出一些层，迁移到相邻阶段。当当前计算节点的网络延迟超过计算集群中其他计算节点的最大网络延迟的预设倍数时，调整调整策略包括拓扑调整策略，拓扑调整策略负责调整某些网络延迟非常高的计算节点。例如，某个计算节点的网络传输延迟超过计算集群内其他计算节点之间最大访问延迟一倍以上，那么认为该计算节点的网络可能存在异常，采用拓扑调整策略进行重新调整。本发明实施例的拓扑调整策略可包括：选择与该当前计算节点的网络延迟最小的三个连续计算节点，并确定当前网络未延迟的计算节点的最大网络延迟一倍以上；将当前计算节点与三个连续计算节点的中间计算节点进行任务互换；分别判断任务相互换的两个计算节点的前后计算节点的网络延迟是否超过最大网络延迟，若超过，则继续选择延迟次小的三个连续计算节点，重复任务互换过程，直至遍历所有计算节点，若仍不存在不超过最大网络延迟的计算节点，则结束计算集群的网络拓扑调整；若未超过，则互换任务相互换的两个计算节点之间的模型参数和中间变量。进一步地，在一些实施例中，拓扑调整策略还可包括：若任务相互换的两个计算节点中任一计算节点的内存使用率大于或等于预设内存使用率阈值，则采用内存调整策略继续调整子模型在计算集群中的分配；在采用内存调整策略继续调整子模型在计算集群中的分配后，若任务相互换的两个计算节点中任一计算节点的内存使用率仍大于或等于预设内存使用率阈值，则采用计算调整策略来迁移内存使用率仍大于或等于预设内存使用率阈值的计算节点的至少部分子模型的至少部分子单元。示例性地，在一具体实施例中，参见图5，拓扑调整策略可包括以下步骤：访问延迟是否超过其他计算节点最大访问延迟一倍：当前计算节点Nodeslow和前后阶段的其他计算节点通信延迟较慢后，首先测试该当前计算节点和其他节点的网络访问延迟。如果该当前计算节点和相邻计算节点的网络访问延迟超过其他计算节点之间最大访问延迟一倍以上，那么判断网络之间可能出现异常，继续进行拓扑调整；否则，停止该计算节点的拓扑调整。访问延迟最小的3个连续计算节点：选择和该当前计算节点Nodeslow访问延迟最小的3个连续计算节点NodeA, NodeB, NodeC。与中间节点互换：和3个连续计算节点的中间计算节点NodeB和Nodeslow进行互换。判断延迟是否正常：判断测试NodeB和Nodeslow的前后计算节点之间的延迟是否正常，即不超过当前正常计算节点的最大访问延迟。如果延迟不正常，则执行下面判断是否最后一批计算节点的步骤。如果延迟正常，则执行下面判断内存是否足够的步骤。判断是否最后一批计算节点：如果不是最后一批计算节点，则那么Nodeslow继续选择延迟次小的3个连续计算节点，然后和中间计算节点进行交换，直至遍历完所有计算节点。如果遍历完所有计算节点仍未找到满足条件的交换计算节点，那么结束这次网络拓扑调整。判断内存是否足够：互换计算节点Nodeslow和计算节点NodeB之间模型参数和中间变量，判断内存是否足够。使用内存调整策略：如果内存不够，则采用内存调整策略进行调整，如果仍然不够则通过计算调整策略迁移层来降低内存需求。如果内存足够，则结束这次网络拓扑调整。本发明还提供一种面向智能计算的流水并行训练自适应调整方法，参见图6，所述面向智能计算的流水并行训练自适应调整方法可包括：S100、监控模块负责监控和收集计算集群内各计算节点的资源运行情况，并根据各计算节点的资源运行情况，确定该计算节点的计算任务划分是否均衡，以及当计算节点的计算任务划分不均衡时，确定计算节点的不均衡类型；S200、调整模块在计算节点的计算任务划分不均衡时，根据计算节点的不均衡类型，确定调整策略，并根据调整策略，调整子模型在计算集群中的分配；其中，调整包括以下至少一种：将计算任务划分不均衡的计算节点的至少部分子模型的至少部分层由该计算节点迁移至其他计算节点；控制计算任务划分不均衡的计算节点执行CPU-GPU内存交换或重计算，或者控制计算任务划分不均衡的计算节点取消当前执行的CPU-GPU内存交换或重计算；对计算集群的网络拓扑结构进行调整。本发明还提供一种面向智能计算的流水并行训练自适应调整装置，包括存储器和一个或多个处理器，存储器中存储有可执行代码，一个或多个处理器执行可执行代码时，用于实现上述的面向智能计算的流水并行训练自适应调整方法。本发明还提供一种计算机可读存储介质，其上存储有程序，该程序被处理器执行时，实现上述的面向智能计算的流水并行训练自适应调整方法。与前述面向智能计算的流水并行训练自适应调整方法的实施例相对应，本发明还提供了一种面向智能计算的流水并行训练自适应调整装置的实施例。参见图7，本发明实施例提供的一种面向智能计算的流水并行训练自适应调整装置，包括存储器和一个或多个处理器，存储器中存储有可执行代码，一个或多个处理器执行可执行代码时，用于实现上述实施例中的面向智能计算的流水并行训练自适应调整方法。本发明实施例提供的面向智能计算的流水并行训练自适应调整装置的实施例可以应用在任意具备数据处理能力的设备上，该任意具备数据处理能力的设备可以为诸如计算机等设备或装置。装置实施例可以通过软件实现，也可以通过硬件或者软硬件结合的方式实现。以软件实现为例，作为一个逻辑意义上的装置，是通过其所在任意具备数据处理能力的设备的处理器将非易失性存储器中对应的计算机程序指令读取到内存中运行形成的。从硬件层面而言，如图7所示，为本发明实施例提供的面向智能计算的流水并行训练自适应调整装置所在任意具备数据处理能力的设备的一种硬件结构图，除了图7所示的处理器、内存、网络接口、以及非易失性存储器之外，实施例中装置所在的任意具备数据处理能力的设备通常根据该任意具备数据处理能力的设备的实际功能，还可以包括其他硬件，对此不再赘述。上述装置中各个单元的功能和作用的实现过程具体详见上述方法中对应步骤的实现过程，在此不再赘述。对于装置实施例而言，由于其基本对应于方法实施例，所以相关之处参见方法实施例的部分说明即可。以上所描述的装置实施例仅仅是示意性的，其中所述作为分离部件说明的单元可以是或者也可以不是物理上分开的，作为单元显示的部件可以是或者也可以不是物理单元，即可以位于一个地方，或者也可以分布到多个网络单元上。可以根据实际的需要选择其中的部分或者全部模块来实现本发明方案的目的。本领域普通技术人员在不付出创造性劳动的情况下，即可以理解并实施。本发明实施例还提供一种计算机可读存储介质，其上存储有程序，该程序被处理器执行时，实现上述实施例中的面向智能计算的流水并行训练自适应调整方法。所述计算机可读存储介质可以是前述任一实施例所述的任意具备数据处理能力的设备的内部存储单元，例如硬盘或内存。所述计算机可读存储介质也可以是任意具备数据处理能力的设备的外部存储设备，例如所述设备上配备的插接式硬盘、智能存储卡、SD卡、闪存卡等。进一步的，所述计算机可读存储介质还可以既包括任意具备数据处理能力的设备的内部存储单元也包括外部存储设备。所述计算机可读存储介质用于存储所述计算机程序以及所述任意具备数据处理能力的设备所需的其他程序和数据，还可以用于暂时地存储已经输出或者将要输出的数据。以上所述仅为本发明的优选实施例而已，并不用于限制本发明，对于本领域的技术人员来说，本发明可以有各种更改和变化。凡在本发明的精神和原则之内，所作的任何修改、等同替换、改进等，均应包含在本发明的保护范围之内。
