标题title
基于深度强化学习优化Volterra均衡器结构的方法和系统
摘要abst
本发明提供了一种基于深度强化学习优化Volterra均衡器结构的方法和系统，包括：初始化智能体Agent、经验回放池、Volterra均衡器的记忆长度状态；对Agent随机产生动作，Volterra均衡器更新其记忆长度状态直至结束状态，根据Volterra均衡器的复杂度和对信号均衡后的误码率计算奖励值，并将转移过程作为经验，存入经验回放池；从经验回放池中采样经验，对Agent进行训练和软更新；根据收敛值确定Volterra均衡器的各阶记忆长度。本发明实现了在给定计算资源的情况下，不同类型Volterra均衡器最优结构的自动搜索方法，相比传统贪心搜索，不仅可以进一步提升均衡效果，而且大幅降低了均衡器的复杂度。
权利要求书clms
1.一种基于深度强化学习优化Volterra均衡器结构的方法，其特征在于，包括：步骤S1：初始化智能体Agent，初始化经验回放池，初始化Volterra均衡器的记忆长度状态并定义状态转移过程；步骤S2：从Volterra均衡器的初始记忆长度状态开始，对Agent随机产生动作，Volterra均衡器更新其记忆长度状态直至结束状态，根据Volterra均衡器的复杂度和对信号均衡后的误码率计算奖励值，并将转移过程作为经验，存入经验回放池中，再次从初始状态循环，直至产生指定数量的经验；步骤S3：从经验回放池中采样经验，对Agent进行训练，然后每隔预设步数，对Agent进行软更新；步骤S4：对更新后的Agent从Volterra均衡器的初始记忆长度状态开始产生确定性动作，直至状态转移过程结束，计算奖励值并将转移过程存入经验回放池，然后重复步骤S3和步骤S4，直至奖励值和Agent输出的动作收敛，最后根据收敛值确定Volterra均衡器的各阶记忆长度。2.根据权利要求1所述的基于深度强化学习优化Volterra均衡器结构的方法，其特征在于，所述步骤S1包括：步骤S11：定义Agent中的四个神经网络：Actor网络μθ，Critic网络Qw，Target Actor网络和Target Critic网络使用随机参数θ,w初始化Actor网络μθ和Critic网络Qw，使用随机参数初始化Target Actor网络和Target Critic网络其中，设置的初始值等于θ，设置的初始值等于w；步骤S12：初始化经验回放池，其存储经验的格式为，其中，si表示当前Volterra均衡器的记忆长度状态；ai表示Agent根据当前状态si产生的动作，为每一阶记忆长度占最大记忆长度限制的比例；ri表示Agent面对状态si时采取动作ai获得的奖励；si+1表示Agent采取动作ai后，Volterra均衡器更新后的记忆长度状态；done是整个状态转移过程是否结束的标志；步骤S13：根据Volterra均衡器的类型来初始化Volterra均衡器的记忆长度状态并定义状态转移过程。3.根据权利要求2所述的基于深度强化学习优化Volterra均衡器结构的方法，其特征在于，所述步骤S2包括：步骤S21：根据Volterra均衡器类型选定状态转移过程，Agent从初始状态开始，产生服从上均匀分布的随机动作，更新Volterra均衡器的记忆长度状态，Agent继续根据当前状态产生随机动作，直到Volterra均衡器的记忆长度状态更新至结束状态；步骤S22：计算奖励值，根据Volterra均衡器各阶最大记忆长度限制和Agent的动作确定各阶记忆长度，对信号数据做2折交叉验证，用当前均衡器的复杂度和均衡后的平均误码率计算奖励值；步骤S23：将状态转移过程作为经验，存入经验回放池中；步骤S24：重复步骤S21至步骤S23，直至产生预设数量的经验。4.根据权利要求1所述的基于深度强化学习优化Volterra均衡器结构的方法，其特征在于，所述步骤S3包括：采用数据引导池技术，从经验回放池中采样N条经验用于Agent训练，其中包含奖励最大的K条经验；计算目标Q值其中，ε～clip,-c,c)，式中，表示Target Actor网络面对状态为si+1时输出的动作；ε为策略噪声，服从均值为0，方差为σ2的高斯分布，并截断于-c～c之间；N表示均值为0，方差为σ2的高斯分布；clip表示截断；γ为折扣因子；表示Target Critic网络面对状态为si+1和动作为时的输出；通过最小化误差更新Critic网络Qw，Qw表示Critic网络Qw面对状态为si和动作为ai时的输出；每隔d步，通过最小化误差更新Actor网络μθ，μθ表示Actor网络μθ面对状态为si时输出的动作；Qw)表示Critic网络Qw面对状态为si和动作为μθ时的输出；按照下式对Target Actor网络和Target Critic网络进行软更新：τ为远小于1的正数，负责调节软更新程度。5.根据权利要求1所述的基于深度强化学习优化Volterra均衡器结构的方法，其特征在于，所述步骤S4包括：更新后的Agent从Volterra均衡器的初始状态开始产生动作，更新Volterra均衡器的记忆长度状态，Agent继续根据当前状态产生动作，直到Volterra均衡器的记忆长度状态更新至结束状态，Agent每次产生的动作都要添加服从均值为0，方差为σ2的高斯分布的探索噪声e；根据Volterra均衡器各阶最大记忆长度限制和Agent的动作确定各阶记忆长度，对信号数据做2折交叉验证，用当前均衡器的复杂度和均衡后的平均误码率计算奖励值；将状态转移过程作为经验存入经验回放池中；然后执行步骤S3；每次更新结束后，对探索噪声e的方差进行衰减：σ2←σ2ξn，式中ξ为衰减率，n为更新次数；重复上述操作，直至当前奖励值与上一次奖励值之差的绝对值小于χ1，当前Agent输出动作与上一次Agent输出动作之差的绝对值小于χ2，则判断训练结果已经收敛，其中，χ1≥0，χ2≥0为设定的判决阈值，最后根据Agent输出动作的收敛值和各位置的最大记忆长度限制来确定Volterra均衡器各位置的记忆长度，完成对Volterra均衡器最优结构的确定。6.一种基于深度强化学习优化Volterra均衡器结构的系统，其特征在于，包括：模块M1：初始化智能体Agent，初始化经验回放池，初始化Volterra均衡器的记忆长度状态并定义状态转移过程；模块M2：从Volterra均衡器的初始记忆长度状态开始，对Agent随机产生动作，Volterra均衡器更新其记忆长度状态直至结束状态，根据Volterra均衡器的复杂度和对信号均衡后的误码率计算奖励值，并将转移过程作为经验，存入经验回放池中，再次从初始状态循环，直至产生指定数量的经验；模块M3：从经验回放池中采样经验，对Agent进行训练，然后每隔预设步数，对Agent进行软更新；模块M4：对更新后的Agent从Volterra均衡器的初始记忆长度状态开始产生确定性动作，直至状态转移过程结束，计算奖励值并将转移过程存入经验回放池，然后重复模块M3和模块M4，直至奖励值和Agent输出的动作收敛，最后根据收敛值确定Volterra均衡器的各阶记忆长度。7.根据权利要求6所述的基于深度强化学习优化Volterra均衡器结构的系统，其特征在于，所述模块M1包括：模块M11：定义Agent中的四个神经网络：Actor网络μθ，Critic网络Qw，Target Actor网络和Target Critic网络使用随机参数θ,w初始化Actor网络μθ和Critic网络Qw，使用随机参数初始化Target Actor网络和Target Critic网络其中，设置的初始值等于θ，设置的初始值等于w；模块M12：初始化经验回放池，其存储经验的格式为，其中，si表示当前Volterra均衡器的记忆长度状态；ai表示Agent根据当前状态si产生的动作，为每一阶记忆长度占最大记忆长度限制的比例；ri表示Agent面对状态si时采取动作ai获得的奖励；si+1表示Agent采取动作ai后，Volterra均衡器更新后的记忆长度状态；done是整个状态转移过程是否结束的标志；模块M13：根据Volterra均衡器的类型来初始化Volterra均衡器的记忆长度状态并定义状态转移过程。8.根据权利要求7所述的基于深度强化学习优化Volterra均衡器结构的系统，其特征在于，所述模块M2包括：模块M21：根据Volterra均衡器类型选定状态转移过程，Agent从初始状态开始，产生服从上均匀分布的随机动作，更新Volterra均衡器的记忆长度状态，Agent继续根据当前状态产生随机动作，直到Volterra均衡器的记忆长度状态更新至结束状态；模块M22：计算奖励值，根据Volterra均衡器各阶最大记忆长度限制和Agent的动作确定各阶记忆长度，对信号数据做2折交叉验证，用当前均衡器的复杂度和均衡后的平均误码率计算奖励值；模块M23：将状态转移过程作为经验，存入经验回放池中；模块M24：重复模块M21至模块M23，直至产生预设数量的经验。9.根据权利要求6所述的基于深度强化学习优化Volterra均衡器结构的系统，其特征在于，所述模块M3包括：采用数据引导池技术，从经验回放池中采样N条经验用于Agent训练，其中包含奖励最大的K条经验；计算目标Q值其中，ε～clip,-c,c)，式中，表示Target Actor网络面对状态为si+1时输出的动作；ε为策略噪声，服从均值为0，方差为σ2的高斯分布，并截断于-c～c之间；N表示均值为0，方差为σ2的高斯分布；clip表示截断；γ为折扣因子；表示Target Critic网络面对状态为si+1和动作为时的输出；通过最小化误差更新Critic网络Qw，Qw表示Critic网络Qw面对状态为si和动作为ai时的输出；每隔d步，通过最小化误差更新Actor网络μθ，μθ表示Actor网络μθ面对状态为si时输出的动作；Qw)表示Critic网络Qw面对状态为si和动作为μθ时的输出；按照下式对Target Actor网络和Target Critic网络进行软更新：τ为远小于1的正数，负责调节软更新程度。10.根据权利要求6所述的基于深度强化学习优化Volterra均衡器结构的系统，其特征在于，所述模块M4包括：更新后的Agent从Volterra均衡器的初始状态开始产生动作，更新Volterra均衡器的记忆长度状态，Agent继续根据当前状态产生动作，直到Volterra均衡器的记忆长度状态更新至结束状态，Agent每次产生的动作都要添加服从均值为0，方差为σ2的高斯分布的探索噪声e；根据Volterra均衡器各阶最大记忆长度限制和Agent的动作确定各阶记忆长度，对信号数据做2折交叉验证，用当前均衡器的复杂度和均衡后的平均误码率计算奖励值；将状态转移过程作为经验存入经验回放池中；然后调用模块M3；每次更新结束后，对探索噪声e的方差进行衰减：σ2←σ2ξn，式中ξ为衰减率，n为更新次数；重复上述操作，直至当前奖励值与上一次奖励值之差的绝对值小于χ1，当前Agent输出动作与上一次Agent输出动作之差的绝对值小于χ2，则判断训练结果已经收敛，其中，χ1≥0，χ2≥0为设定的判决阈值，最后根据Agent输出动作的收敛值和各位置的最大记忆长度限制来确定Volterra均衡器各位置的记忆长度，完成对Volterra均衡器最优结构的确定。
说明书desc
技术领域本发明涉及光通信技术领域，具体地，涉及一种基于深度强化学习优化Volterra均衡器结构的方法和系统。背景技术Volterra非线性均衡器广泛应用于光纤通信系统中，用来缓解信号在传输过程中受到的线性和非线性损伤。在长距光纤通信系统中，非线性损伤主要来自光纤非线性，而在短距光纤通信系统中，非线性损伤主要来源于收发器件，如调制器的非线性响应、光电探测器的平方律检测等。均衡效果和实现复杂度是评价均衡器的重要指标，为了能够在硬件上实时实现，高性能低复杂度的Volterra非线性均衡器结构成为研究的热点。Volterra非线性均衡器的均衡效果和复杂度取决于其阶数、各阶记忆长度、有无反馈以及反馈记忆长度等，阶数越高、记忆长度越大则复杂度越高。为保证均衡效果，在短距光纤通信系统中，Volterra非线性均衡器的阶数通常取3阶，各阶记忆长度则通常由人工经验或者贪心搜索确定。在降低Volterra非线性均衡器复杂度方面，目前的方法主要是通过两种方式：一种是剪枝，包括通过设定剪枝阈值和L1正则化实现的非结构化剪枝和通过设定Volterra交叉项信号间相关距离实现的结构化剪枝；另一种是基于器件物理特性和信道特性设计简化的Volterra非线性均衡器结构。然而现有的方法仍需通过人工经验或者贪心搜索确定Volterra非线性均衡器的各阶记忆长度、剪枝阈值或者相关距离等，效率低下，既不能很好地发挥Volterra非线性均衡器的最佳性能，也难以实现均衡效果与复杂度的折中。发明内容针对现有技术中的缺陷，本发明的目的是提供一种基于深度强化学习优化Volterra均衡器结构的方法和系统。根据本发明提供的基于深度强化学习优化Volterra均衡器结构的方法，包括：步骤S1：初始化智能体Agent，初始化经验回放池，初始化Volterra均衡器的记忆长度状态并定义状态转移过程；步骤S2：从Volterra均衡器的初始记忆长度状态开始，对Agent随机产生动作，Volterra均衡器更新其记忆长度状态直至结束状态，根据Volterra均衡器的复杂度和对信号均衡后的误码率计算奖励值，并将转移过程作为经验，存入经验回放池中，再次从初始状态循环，直至产生指定数量的经验；步骤S3：从经验回放池中采样经验，对Agent进行训练，然后每隔预设步数，对Agent进行软更新；步骤S4：对更新后的Agent从Volterra均衡器的初始记忆长度状态开始产生确定性动作，直至状态转移过程结束，计算奖励值并将转移过程存入经验回放池，然后重复步骤S3和步骤S4，直至奖励值和Agent输出的动作收敛，最后根据收敛值确定Volterra均衡器的各阶记忆长度。优选的，所述步骤S1包括：步骤S11：定义Agent中的四个神经网络：Actor网络μθ，Critic网络Qw，TargetActor网络和Target Critic网络使用随机参数θ,w初始化Actor网络μθ和Critic网络Qw，使用随机参数初始化Target Actor网络和Target Critic网络其中，设置的初始值等于θ，设置的初始值等于w；步骤S12：初始化经验回放池，其存储经验的格式为，其中，si表示当前Volterra均衡器的记忆长度状态；ai表示Agent根据当前状态si产生的动作，为每一阶记忆长度占最大记忆长度限制的比例；ri表示Agent面对状态si时采取动作ai获得的奖励；si+1表示Agent采取动作ai后，Volterra均衡器更新后的记忆长度状态；done是整个状态转移过程是否结束的标志；步骤S13：根据Volterra均衡器的类型来初始化Volterra均衡器的记忆长度状态并定义状态转移过程。优选的，所述步骤S2包括：步骤S21：根据Volterra均衡器类型选定状态转移过程，Agent从初始状态开始，产生服从上均匀分布的随机动作，更新Volterra均衡器的记忆长度状态，Agent继续根据当前状态产生随机动作，直到Volterra均衡器的记忆长度状态更新至结束状态；步骤S22：计算奖励值，根据Volterra均衡器各阶最大记忆长度限制和Agent的动作确定各阶记忆长度，对信号数据做2折交叉验证，用当前均衡器的复杂度和均衡后的平均误码率计算奖励值；步骤S23：将状态转移过程作为经验，存入经验回放池中；步骤S24：重复步骤S21至步骤S23，直至产生预设数量的经验。优选的，所述步骤S3包括：采用数据引导池技术，从经验回放池中采样N条经验用于Agent训练，其中包含奖励最大的K条经验；计算目标Q值其中，ε～clip,-c,c)，式中，表示TargetActor网络面对状态为si+1时输出的动作；ε为策略噪声，服从均值为0，方差为σ2的高斯分布，并截断于-c～c之间；N表示均值为0，方差为σ2的高斯分布；clip表示截断；γ为折扣因子；表示Target Critic网络面对状态为si+1和动作为时的输出；通过最小化误差更新Critic网络Qw，Qw表示Critic网络Qw面对状态为si和动作为ai时的输出；每隔d步，通过最小化误差更新Actor网络μθ，μθ表示Actor网络μθ面对状态为si时输出的动作；Qw)表示Critic网络Qw面对状态为si和动作为μθ时的输出；按照下式对TargetActor网络和Target Critic网络进行软更新：τ为远小于1的正数，负责调节软更新程度。优选的，所述步骤S4包括：更新后的Agent从Volterra均衡器的初始状态开始产生动作，更新Volterra均衡器的记忆长度状态，Agent继续根据当前状态产生动作，直到Volterra均衡器的记忆长度状态更新至结束状态，Agent每次产生的动作都要添加服从均值为0，方差为σ2的高斯分布的探索噪声e；根据Volterra均衡器各阶最大记忆长度限制和Agent的动作确定各阶记忆长度，对信号数据做2折交叉验证，用当前均衡器的复杂度和均衡后的平均误码率计算奖励值；将状态转移过程作为经验存入经验回放池中；然后执行步骤S3；每次更新结束后，对探索噪声e的方差进行衰减：σ2←σ2ξn，式中ξ为衰减率，n为更新次数；重复上述操作，直至当前奖励值与上一次奖励值之差的绝对值小于χ1，当前Agent输出动作与上一次Agent输出动作之差的绝对值小于χ2，则判断训练结果已经收敛，其中，χ1≥0，χ2≥0为设定的判决阈值，最后根据Agent输出动作的收敛值和各位置的最大记忆长度限制来确定Volterra均衡器各位置的记忆长度，完成对Volterra均衡器最优结构的确定。根据本发明提供的基于深度强化学习优化Volterra均衡器结构的系统，包括：模块M1：初始化智能体Agent，初始化经验回放池，初始化Volterra均衡器的记忆长度状态并定义状态转移过程；模块M2：从Volterra均衡器的初始记忆长度状态开始，对Agent随机产生动作，Volterra均衡器更新其记忆长度状态直至结束状态，根据Volterra均衡器的复杂度和对信号均衡后的误码率计算奖励值，并将转移过程作为经验，存入经验回放池中，再次从初始状态循环，直至产生指定数量的经验；模块M3：从经验回放池中采样经验，对Agent进行训练，然后每隔预设步数，对Agent进行软更新；模块M4：对更新后的Agent从Volterra均衡器的初始记忆长度状态开始产生确定性动作，直至状态转移过程结束，计算奖励值并将转移过程存入经验回放池，然后重复模块M3和模块M4，直至奖励值和Agent输出的动作收敛，最后根据收敛值确定Volterra均衡器的各阶记忆长度。优选的，所述模块M1包括：模块M11：定义Agent中的四个神经网络：Actor网络μθ，Critic网络Qw，TargetActor网络和Target Critic网络使用随机参数θ,w初始化Actor网络μθ和Critic网络Qw，使用随机参数初始化TargetActor网络和Target Critic网络其中，设置的初始值等于θ，设置的初始值等于w；模块M12：初始化经验回放池，其存储经验的格式为，其中，si表示当前Volterra均衡器的记忆长度状态；ai表示Agent根据当前状态si产生的动作，为每一阶记忆长度占最大记忆长度限制的比例；ri表示Agent面对状态si时采取动作ai获得的奖励；si+1表示Agent采取动作ai后，Volterra均衡器更新后的记忆长度状态；done是整个状态转移过程是否结束的标志；模块M13：根据Volterra均衡器的类型来初始化Volterra均衡器的记忆长度状态并定义状态转移过程。优选的，所述模块M2包括：模块M21：根据Volterra均衡器类型选定状态转移过程，Agent从初始状态开始，产生服从上均匀分布的随机动作，更新Volterra均衡器的记忆长度状态，Agent继续根据当前状态产生随机动作，直到Volterra均衡器的记忆长度状态更新至结束状态；模块M22：计算奖励值，根据Volterra均衡器各阶最大记忆长度限制和Agent的动作确定各阶记忆长度，对信号数据做2折交叉验证，用当前均衡器的复杂度和均衡后的平均误码率计算奖励值；模块M23：将状态转移过程作为经验，存入经验回放池中；模块M24：重复模块M21至模块M23，直至产生预设数量的经验。优选的，所述模块M3包括：采用数据引导池技术，从经验回放池中采样N条经验用于Agent训练，其中包含奖励最大的K条经验；计算目标Q值其中，ε～clip,-c,c)，式中，表示TargetActor网络面对状态为si+1时输出的动作；ε为策略噪声，服从均值为0，方差为σ2的高斯分布，并截断于-c～c之间；N表示均值为0，方差为σ2的高斯分布；clip表示截断；γ为折扣因子；表示Target Critic网络面对状态为si+1和动作为时的输出；通过最小化误差更新Critic网络Qw，Qw表示Critic网络Qw面对状态为si和动作为ai时的输出；每隔d步，通过最小化误差更新Actor网络μθ，μθ表示Actor网络μθ面对状态为si时输出的动作；Qw)表示Critic网络Qw面对状态为si和动作为μθ时的输出；按照下式对TargetActor网络和Target Critic网络进行软更新：τ为远小于1的正数，负责调节软更新程度。优选的，所述模块M4包括：更新后的Agent从Volterra均衡器的初始状态开始产生动作，更新Volterra均衡器的记忆长度状态，Agent继续根据当前状态产生动作，直到Volterra均衡器的记忆长度状态更新至结束状态，Agent每次产生的动作都要添加服从均值为0，方差为σ2的高斯分布的探索噪声e；根据Volterra均衡器各阶最大记忆长度限制和Agent的动作确定各阶记忆长度，对信号数据做2折交叉验证，用当前均衡器的复杂度和均衡后的平均误码率计算奖励值；将状态转移过程作为经验存入经验回放池中；然后调用模块M3；每次更新结束后，对探索噪声e的方差进行衰减：σ2←σ2ξn，式中ξ为衰减率，n为更新次数；重复上述操作，直至当前奖励值与上一次奖励值之差的绝对值小于χ1，当前Agent输出动作与上一次Agent输出动作之差的绝对值小于χ2，则判断训练结果已经收敛，其中，χ1≥0，χ2≥0为设定的判决阈值，最后根据Agent输出动作的收敛值和各位置的最大记忆长度限制来确定Volterra均衡器各位置的记忆长度，完成对Volterra均衡器最优结构的确定。与现有技术相比，本发明具有如下的有益效果：本发明实现了在给定计算资源的情况下，不同类型Volterra均衡器最优结构的自动搜索方法，均衡效果优于传统贪心搜索；本发明针对单纯追求高性能的场景和考虑性能与复杂度折中的场景设计了不同类型的奖励值计算方式，应用更为灵活，其中采用考虑性能与复杂度折中的奖励值计算方式，搜索出的前馈Volterra均衡器和反馈Volterra均衡器最优结构，可以保证均衡效果损失很小的情况下，大幅降低均衡器复杂度，对于结构化剪枝的Volterra均衡器，则可以提升其剪枝质量；本发明在DDPG的基础框架上，引入策略噪声和数据引导池技术，提升了训练过程的稳定性以及最终搜索到的结构的泛化性。附图说明通过阅读参照以下附图对非限制性实施例所作的详细描述，本发明的其它特征、目的和优点将会变得更明显：图1为本发明的流程示意图；图2为本发明采用的直调直检实验系统示例图；图3为采用单纯追求高性能的奖励值计算方式时，本发明与基于贪心搜索的Volterra-FFE的误码率对比图；图4为采用单纯追求高性能的奖励值计算方式时，本发明与基于贪心搜索的Volterra-DFE的误码率对比图；图5为采用考虑性能与复杂度折中的奖励值计算方式时，本发明与基于贪心搜索的Volterra-FFE的误码率对比图；图6为采用考虑性能与复杂度折中的奖励值计算方式时，本发明与基于贪心搜索的Volterra-FFE的复杂度对比图；图7为采用考虑性能与复杂度折中的奖励值计算方式时，本发明与基于贪心搜索的Volterra-DFE的误码率对比图；图8为采用考虑性能与复杂度折中的奖励值计算方式时，本发明与基于贪心搜索的Volterra-DFE的复杂度对比图；图9为采用考虑性能与复杂度折中的奖励值计算方式时，本发明与基于贪心搜索的Volterra-Pruning的误码率对比图；图10为采用考虑性能与复杂度折中的奖励值计算方式时，本发明与基于贪心搜索的Volterra-Pruning的复杂度对比图。具体实施方式下面结合具体实施例对本发明进行详细说明。以下实施例将有助于本领域的技术人员进一步理解本发明，但不以任何形式限制本发明。应当指出的是，对本领域的普通技术人员来说，在不脱离本发明构思的前提下，还可以做出若干变化和改进。这些都属于本发明的保护范围。实施例1：本发明公开了一种基于深度强化学习确定Volterra均衡器最优结构的方法。该方法以深度强化学习中的深度确定性策略梯度算法算法为智能体，根据Volterra均衡器的复杂度和对信号均衡后的误码率计算奖励值，优化Agent的决策，为前馈Volterra均衡器、反馈Volterra均衡器以及三阶结构化剪枝Volterra均衡器选取最优结构。如图1，本发明方法的步骤如下：S1：初始化阶段：初始化Agent；初始化经验回放池；初始化Volterra均衡器的记忆长度状态并定义状态转移过程；S2：预热阶段：从Volterra均衡器的初始记忆长度状态开始，Agent随机产生动作，即每一阶记忆长度占最大记忆长度限制的比例，Volterra均衡器更新其记忆长度状态直至结束状态，根据Volterra均衡器的复杂度和对信号均衡后的误码率计算奖励值，并将转移过程作为经验，存入经验回放池中；再次从初始状态循环，直至产生指定数量的经验；S3：Agent训练和更新阶段：从经验回放池中采样一批经验，对Agent进行训练，然后每隔一定步数，对Agent进行软更新；S4：探索和收敛阶段：更新后的Agent从Volterra均衡器的初始记忆长度状态开始产生确定性动作，直至状态转移过程结束，计算奖励值并将转移过程存入经验回放池；然后重复步骤S3，直至奖励值和Agent输出的动作收敛；最后根据收敛值确定Volterra均衡器的各阶记忆长度。进一步地，所述步骤S1的具体步骤如下：S11：定义Agent中的四个神经网络：Actor网络μθ，Critic网络Qw，Target Actor网络和Target Critic网络使用随机参数θ,w初始化Actor网络μθ和Critic网络Qw，使用随机参数初始化TargetActor网络和Target Critic网络其中，设置的初始值等于θ，设置的初始值等于w；S12：初始化经验回放池，其存储经验的格式为，其中si表示当前Volterra均衡器的记忆长度状态，ai表示Agent根据当前状态si产生的动作，即每一阶记忆长度占最大记忆长度限制的比例，ri表示Agent面对状态si时采取动作ai获得的奖励，si+1表示Agent采取动作ai后，Volterra均衡器更新后的记忆长度状态，done是整个状态转移过程是否结束的标志；S13：根据Volterra均衡器的类型来初始化Volterra均衡器的记忆长度状态并定义状态转移过程：对于M阶前馈型Volterra均衡器，其公式表示为：其中x是均衡器的输入信号，y是均衡后的输出信号，hi，i＝1,2,...,M表示Volterra-FFE的第i阶抽头权重，2Li+1，i＝1,2,...,M表示Volterra-FFE的第i阶记忆长度；Volterra-FFE的记忆长度的第i个状态定义为si＝,l1,l2,...,lM)，id表示待确定记忆长度位置的索引，按照从1阶记忆长度、2阶记忆长度直到M阶记忆长度的顺序对位置进行排序，Llimit表示第id个位置的最大记忆长度限制，lid，id＝1,2,...,M表示第id个位置的记忆长度占其最大记忆长度限制的比例；初始化Volterra-FFE均衡器的记忆长度状态为s0＝,0,0,...,0)；定义Volterra-FFE均衡器的状态转移过程为：从初始记忆长度状态s0＝,0,0,...,0)开始，Agent产生动作a0＝l1，即第1个位置的记忆长度占其最大记忆长度限制的比例，然后获得的奖励r0＝0，更新状态至s1＝,l1,0,...,0)，设置状态转移结束标志done＝False，表示整个状态转移过程未结束；从记忆长度状态si＝,l1,l2,...,li,...,0)开始，Agent产生动作ai＝li+1，即第i+1个位置的记忆长度占其最大记忆长度限制的比例，然后获得的奖励ri＝0，更新状态至si+1＝,l1,l2,...,li,li+1,...,0)，设置状态转移结束标志done＝False，表示整个状态转移过程未结束；依次类推，直至更新状态至结束状态sM＝，设置状态转移结束标志done＝True，表示整个状态转移过程结束，当done＝True时，令r0＝...＝ri＝...＝rM-1＝reward，reward表示整个状态转移过程结束后，根据当前均衡器的复杂度以及对信号数据做2折交叉验证后的平均误码率计算出的奖励值；对于M阶反馈型Volterra均衡器，其公式表示为：其中x是均衡器的输入信号，y是均衡后的输出信号，是对y硬判决后的输出信号，hi，i＝1,2,...,M表示Volterra-DFE的第i阶抽头权重，2Li+1，i＝1,2,...,M表示Volterra-FFE的第i阶记忆长度，hfb表示Volterra-DFE反馈抽头权重，Lfb表示Volterra-DFE的反馈记忆长度；Volterra-DFE的记忆长度的第i个状态定义为si＝,l1,l2,...,lM,lM+1)，id表示待确定记忆长度位置的索引，按照1阶记忆长度、2阶记忆长度直到M阶记忆长度和反馈记忆长度的顺序对位置进行排序，Llimit表示第id个位置的最大记忆长度限制，lid，id＝1,2,...,M+1表示第id个位置的记忆长度占其最大记忆长度限制的比例；初始化Volterra-DFE均衡器的记忆长度状态为s0＝,0,0,...,0)；定义Volterra-DFE均衡器的状态转移过程为：从初始记忆长度状态s0＝,0,0,...,0)开始，Agent产生动作a0＝l1，即第1个位置的记忆长度占其最大记忆长度限制的比例，然后获得的奖励r0＝0，更新状态至s1＝,l1,0,...,0)，设置状态转移结束标志done＝False，表示整个状态转移过程未结束；从记忆长度状态si＝,l1,l2,...,li,...,0)开始，Agent产生动作ai＝li+1，即第i+1个位置的记忆长度占其最大记忆长度限制的比例，然后获得的奖励ri＝0，更新状态至si+1＝,l1,l2,...,li,li+1,...,0)，设置状态转移结束标志done＝False，表示整个状态转移过程未结束；依次类推，直至更新状态至结束状态sM+1＝，设置状态转移结束标志done＝True，表示整个状态转移过程结束，当done＝True时，令r0＝...＝ri＝...＝rM＝reward，reward表示整个状态转移过程结束后，根据当前均衡器的复杂度以及对信号数据做2折交叉验证后的平均误码率计算出的奖励值；对于M阶结构化剪枝型Volterra均衡器，其公式表示为：其中x是均衡器的输入信号，y是均衡后的输出信号，hi，i＝1,2,...,M表示Volterra-FFE的第i阶抽头权重，2Li+1，i＝1,2,...,M表示Volterra-FFE的第i阶记忆长度，Lmd，m＝2,...,M表示Volterra-Pruning的第m阶相关距离，用于调控剪枝程度；Volterra-Pruning的记忆长度的第i个状态定义为si＝,l1,l2,...,lM,l2M-1)，id表示待确定记忆长度或相关距离的位置的索引，按照1阶记忆长度、2阶记忆长度、2阶相关距离、3阶记忆长度、3阶相关距离直到M阶记忆长度、M阶相关距离的顺序对位置进行排序，Llimit表示第id个位置的最大记忆长度限制或最大相关距离限制，lid，id＝1,2,...,2M-1表示第id个位置的记忆长度或相关距离占其最大记忆长度限制或最大相关距离限制的比例；初始化Volterra-Pruning均衡器的记忆长度状态为s0＝,0,0,...,0)；定义Volterra-Pruning均衡器的状态转移过程为：从初始记忆长度状态s0＝,0,0,...,0)开始，Agent产生动作a0＝l1，即第1个位置的记忆长度占其最大记忆长度限制的比例，然后获得的奖励r0＝0，更新状态至s1＝,l1,0,...,0)，设置状态转移结束标志done＝False，表示整个状态转移过程未结束；从记忆长度状态si＝,l1,l2,...,li,...,0)开始，Agent产生动作ai＝li+1，即第i+1个位置的记忆长度占其最大记忆长度限制的比例，然后获得的奖励ri＝0，更新状态至si+1＝,l1,l2,...,li,li+1,...,0)，设置状态转移结束标志done＝False，表示整个状态转移过程未结束；依次类推，直至更新状态至结束状态s2M-1＝，设置状态转移结束标志done＝True，表示整个状态转移过程结束，当done＝True时，令r0＝...＝ri＝...＝r2M-2＝reward，reward表示整个状态转移过程结束后，根据当前均衡器的复杂度以及对信号数据做2折交叉验证后的平均误码率计算出的奖励值。进一步地，所述步骤S2的具体步骤如下：S21：首先根据Volterra均衡器类型选定状态转移过程，Agent从初始状态开始，产生服从上均匀分布的随机动作，即每一阶记忆长度占最大记忆长度限制的比例，更新Volterra均衡器的记忆长度状态，Agent继续根据当前状态产生随机动作，直到Volterra均衡器的记忆长度状态更新至结束状态；S22：接着计算奖励值，根据Volterra均衡器各阶最大记忆长度限制和Agent的动作确定各阶记忆长度，对信号数据做2折交叉验证，用均衡后的平均误码率和均衡器的复杂度计算奖励值reward；对于单纯追求高性能，即低误码率的场景，奖励值reward＝100*；对于考虑性能与复杂度折中的场景，奖励值reward＝-100*BERvalid*log，BERvalid为Volterra均衡器对信号做二折交叉验证的平均误码率；Vol_MACs为Volterra均衡器所需的乘法器数目；S23：然后将状态转移过程作为经验，存入经验回放池中；S24：最后重复上述操作，直至产生指定数量的经验。进一步地，所述步骤S3的具体方法如下：采用数据引导池技术，从经验回放池中采样N条经验用于Agent训练，其中包含奖励最大的K条经验；计算目标Q值其中ε～clip,-c,c)，式中，表示Target Actor网络μθ面对状态为si+1时输出的动作，ε为策略噪声，服从均值为0，方差为σ2的高斯分布，并截断于-c～c之间，N表示均值为0，方差为σ2的高斯分布，clip表示截断，γ为折扣因子，表示Target Critic网络Qw面对状态为si+1和动作为时的输出；通过最小化误差更新Critic网络Qw，Qw表示Critic网络Qw面对状态为si和动作为ai时的输出；每隔d步，通过最小化误差更新Actor网络μθ，μθ表示Actor网络μθ面对状态为si时输出的动作，Qw)表示Critic网络Qw面对状态为si和动作为μθ时的输出；按照下式对Target Actor网络和Target Critic网络进行软更新：τ为远小于1的正数，负责调节软更新程度。进一步地，所述步骤S4的具体方法如下：更新后的Agent从Volterra均衡器的初始状态开始产生动作，即每一阶记忆长度占最大记忆长度限制的比例，更新Volterra均衡器的记忆长度状态，Agent继续根据当前状态产生动作，直到Volterra均衡器的记忆长度状态更新至结束状态，Agent每次产生的动作都要添加服从均值为0，方差为σ2的高斯分布的探索噪声e；根据Volterra均衡器各阶最大记忆长度限制和Agent的动作确定各阶记忆长度，对信号数据做2折交叉验证，用均衡后的平均误码率和均衡器复杂度计算奖励值；将状态转移过程作为经验存入经验回放池中；然后执行步骤S3；每次更新结束后，对探索噪声e的方差进行衰减：σ2←σ2ξn，式中ξ为衰减率，n为更新次数；重复上述操作，直至当前奖励值与上一次奖励值之差的绝对值小于χ1，当前Agent输出动作与上一次Agent输出动作之差的绝对值小于χ2，则判断训练结果已经收敛，其中χ1≥0，χ2≥0为设定的判决阈值，最后根据Agent输出动作的收敛值和各位置的最大记忆长度限制来确定Volterra均衡器各位置的记忆长度，完成对Volterra均衡器最优结构的确定。实施例2：实施例2为实施例1的优选例。本发明以三阶的Volterra均衡器的优化为例，实验说明本发明的有效性。考虑C波段直调直检系统，发送端通过任意波形发生器产生速率为50Gbps的PAM4信号，经过电放大器放大后，再由C波段、10GHz级别的马赫曾德尔调制器调制，同时AWG加载一个速率为100Mbps的NRZ信号到直调激光器上以展宽中心载波，抑制受功率影响的受激布里渊散射效应，经掺铒光纤放大器放大，通过20km标准单模光纤传输，再由30GHz级别的雪崩光电探测器接收后，进行离线的数字信号处理，包括同步、重采样、均衡、符号判决和解码等步骤，系统整体的3dB带宽为6GHz左右，具体系统设置如图2所示。固定APD接收功率为-15dBm，从8dBm到20dBm改变入纤功率，在每个入纤功率下，发送3组由不同随机数种子产生的长度为98304的符号。设定均衡器的计算资源限制为1000MACs，MACs表示乘法器个数，均衡器输入信号为x，均衡后的输出信号为y，对y进行符号判决后的输出信号为则不同类型3阶Volterra均衡器各阶最大记忆长度限制的计算方式如下：对于三阶前馈型Volterra均衡器，其公式表示为：其中h1,h2,h3分别表示Volterra-FFE的1阶、2阶和3阶抽头权重，2L1+1,2L2+1,2L3+1分别表示Volterra-FFE的1阶、2阶和3阶记忆长度；则Volterra-FFE的1阶、2阶和3阶抽头数目分别为2L1+1，/2，/6，对应所需的乘法器数目分别为2L1+1，，/2；设当前剩余乘法器数目为rest_MACs，则1阶最大记忆长度限制Llimit的计算方式如下：2Llimit+1≤rest_MACs式中，表示向下取整；2阶最大记忆长度限制Llimit的计算方式如下：+1)+2)≤rest_MACs3阶最大记忆长度限制Llimit的计算方式如下：+1)+2)+3)/2≤rest_MACs化简为：4)3+12)2+11Llimit+3-rest_MACs≤0根据盛金公式求解上式：令a＝4，b＝12，c＝11，d＝3-rest_MACs，A＝b2-3ac，B＝bc-9ad，C＝c2-3bd，则其中，最终化简得到：其中，对于三阶反馈型Volterra均衡器，其公式表示为：其中h1,h2,h3,hfb分别表示Volterra-DFE的1阶、2阶、3阶以及反馈抽头权重，2L1+1,2L2+1,2L3+1,Lfb分别表示Volterra-DFE的1阶、2阶、3阶以及反馈记忆长度；则Volterra-DFE的1阶、2阶、3阶以及反馈抽头数目分别为2L1+1，/2，/6，Lfb，对应所需的乘法器数目分别为2L1+1，，/2，Lfb；参照Volterra-FFE，设当前剩余乘法器数目为rest_MACs，则Volterra-DFE中1阶最大记忆长度限制Llimit的计算方式如下：2阶最大记忆长度限制Llimit的计算方式如下：3阶最大记忆长度限制Llimit的计算方式如下：其中，反馈最大记忆长度限制Llimit的计算方式如下：Llimit＝rest_MACs对于三阶结构化剪枝型Volterra均衡器，其公式表示为：其中h1,h2,h3分别表示Volterra-Pruning的1阶、2阶和3阶抽头权重，2L1+1,2L2+1,2L3+1分别表示Volterra-Pruning的1阶、2阶和3阶记忆长度；L2d,L3d分别表示Volterra-Pruning的2阶和3阶相关距离，用于调控剪枝程度；参照Volterra-FFE，设当前剩余乘法器数目为rest_MACs，则Volterra-Pruning中1阶最大记忆长度限制Llimit的计算方式如下：2阶最大记忆长度限制Llimit的计算方式如下：2阶相关距离的最大长度限制Llimit的计算方式如下：Llimit＝2L2+13阶最大记忆长度限制Llimit的计算方式如下：其中，3阶相关距离最大长度限制Llimit的计算方式如下：Llimit＝2L3+1本发明采用深度强化学习中的深度确定性策略梯度算法作为智能体，逐阶确定Volterra均衡器的各阶记忆长度或者相关距离等。DDPG算法属于Actor-Critic框架，Actor部分包含Actor网络μθ和对应的Target Actor网络Critic部分包含Critic网络Qw和对应的Target Critic网络Actor部分负责在当前环境状态下做出价值尽可能高的决策，Critic部分负责对当前环境状态下采取的动作的价值做出尽可能准确的估计。具体来说，Agent接收当前Volterra均衡器的记忆长度状态为输入，输出动作为当前阶记忆长度占最大记忆长度限制的比例，然后Volterra均衡器更新其记忆长度状态，计算奖励值，如此反复，Agent根据奖励不断优化策略。对于不同类型的Volterra均衡器，其初始记忆长度状态和状态转移过程定义如下：对于三阶前馈型Volterra均衡器：Volterra-FFE的记忆长度状态定义为,l1,l2,l3)，id表示待确定记忆长度位置的索引，Llimit表示第id个位置的最大记忆长度限制；l1,l2,l3分别表示1阶、2阶和3阶记忆长度占最大记忆长度限制的比例；Volterra-FFE均衡器的初始记忆长度状态为s0＝,0,0,0)，从初始状态出发，按照从1阶、2阶、3阶的顺序逐一确定这些位置的记忆长度，整个状态转移过程为：done是整个状态转移过程是否结束的标志，当done＝True时，令r0＝r1＝r2＝reward，reward表示整个状态转移过程结束后，根据当前均衡器的复杂度以及对信号数据做2折交叉验证后的平均误码率计算出的奖励值；式中，s0表示初始状态，s3表示结束状态，s1,s2表示中间状态，a0,a1,a2表示Agent输出的动作，r0,r1,r2表示在状态转移过程中获得的奖励；对于三阶反馈型Volterra均衡器：Volterra-DFE的记忆长度状态定义为,l1,l2,l3,lfb)，id表示待确定记忆长度位置的索引，Llimit表示第id个位置的最大记忆长度限制；l1,l2,l3,lfb表示1阶、2阶、3阶以及反馈记忆长度占最大记忆长度限制的比例；Volterra-DFE均衡器的初始记忆长度状态为s0＝,0,0,0，0)，从初始状态出发，按照从1阶、2阶、3阶、反馈的顺序逐一确定这些位置的记忆长度，整个状态转移过程为：当done＝True时，令r0＝r1＝r2＝r3＝reward，reward表示整个状态转移过程结束后，根据当前均衡器的复杂度以及对信号数据做2折交叉验证后的平均误码率计算出的奖励值；式中，s0表示初始状态，s4表示结束状态，s1,s2,s3表示中间状态，a0,a1,a2,a3表示Agent输出的动作，r0,r1,r2,r3表示在状态转移过程中获得的奖励；对于三阶结构化剪枝型Volterra均衡器：Volterra-Pruning的记忆长度状态定义为,l1,l2,l2d,l3,l3d)，id表示待确定记忆长度位置的索引，Llimit表示第id个位置的最大记忆长度或相关距离的限制；l1,l2,l2d,l3,l3d表示1阶记忆长度、2阶记忆长度、2阶相关距离、3阶记忆长度以及3阶相关距离占最大记忆长度或相关距离限制的比例；Volterra-Pruning均衡器的记忆长度状态为s0＝,0,0,0,0,0)，从初始状态出发，按照从1阶、2阶、2阶相关距离、3阶、3阶相关距离的顺序逐一确定这些位置的记忆长度和相关距离，整个状态转移过程为：当done＝True时，令r0＝r1＝r2＝r3＝r4＝reward，reward表示整个状态转移过程结束后，根据当前均衡器的复杂度以及对信号数据做2折交叉验证后的平均误码率计算出的奖励值；式中，s0表示初始状态，s5表示结束状态，s1,s2,s3,s4表示中间状态，a0,a1,a2,a3,a4表示Agent输出的动作，r0,r1,r2,r3,r4表示在状态转移过程中获得的奖励。当整个状态转移过程结束后，根据Agent的动作计算出各阶记忆长度，对Volterra均衡器进行2折交叉验证，每一折信号数据的长度为16384。训练Volterra-FFE和Volterra-Pruning的学习率设置为0.005，训练20轮；训练Volterra-DFE的学习率设置为0.001，训练20轮。然后通过平均验证误码率BERvalid计算奖励值。本发明设计了两种奖励值的计算方式以适应不同的应用场景，对于单纯追求高性能的场景，奖励值reward＝100*；对于考虑性能与复杂度折中的场景，如Volterra-Pruning，奖励值reward＝-100*BERvalid*log，Vol_MACs为Volterra均衡器所需的乘法器数目。为了提高DDPG算法决策的泛化效果和训练稳定性，本发明在Agent的训练过程中使用了策略噪声和数据引导池技术。策略噪声通过向TargetActor网络的输出动作添加噪声扰动，用于平滑策略期望；数据引导池技术通过定期在训练过程中向Agent回放奖励值最高的经验，引导Agent做出更优的决策。因此，基于DDPG算法确定Volterra均衡器最优结构的步骤如下：Step-1：初始化阶段：定义Agent中的四个神经网络：Actor网络μθ，Critic网络Qw，TargetActor网络和Target Critic网络使用随机参数θ,w初始化Agent中的Actor网络μθ和Critic网络Qw，使用随机参数初始化TargetActor网络和Target Critic网络设置的初始值等于θ，设置的初始值等于w；初始化经验回放池；Step-2：预热阶段：Agent从初始状态开始，产生服从上均匀分布的随机动作，即每一阶记忆长度占最大记忆长度限制的比例，更新Volterra均衡器的记忆长度状态，直至达到结束状态；根据Volterra均衡器各阶最大记忆长度限制和Agent的动作确定各阶记忆长度，对信号数据做2折交叉验证，用均衡器的复杂度和均衡后的平均验证误码率计算奖励值，然后将状态转移过程作为经验，存入经验回放池中；重复上述操作，直至产生指定数量的经验；Step-3：Agent训练与更新：从经验回放池中采样N条经验用于Agent训练，其中包含奖励最大的K条经验；计算目标Q值其中ε～clip,-c,c)，式中，表示TargetActor网络面对状态为si+1时的输出，ε为策略噪声，服从均值为0，方差为σ2的高斯分布，并截断于-c～c之间，γ为折扣因子，为Target Critic网络接收状态si+1和动作为输入时的输出；通过最小化误差L＝N-1∑i)2更新Critic网络Qw；每隔d步，通过策略梯度更新Actor网络μθ；按照下式对Target Actor网络和Target Critic网络进行软更新：τ为远小于1的正数，负责调节软更新程度。Step-4：Agent更新后从Volterra均衡器的初始状态开始产生确定动作，更新Volterra均衡器的记忆长度状态直至结束状态，Agent每次产生的动作都要添加服从均值为0，方差为σ2的高斯分布的探索噪声e；根据Volterra均衡器2折交叉验证的平均验证误码率计算奖励值；将状态转移过程存入经验回放池中，然后执行步骤Step-3；每次更新结束后，对探索噪声e的方差进行衰减：σ2←σ2ξn，式中ξ为衰减率，n为更新次数；重复上述操作，直至当前奖励值与上一次奖励值之差的绝对值小于χ1，当前Agent输出动作与上一次Agent输出动作之差的绝对值小于χ2，则判断训练结果已经收敛，其中χ1≥0，χ2≥0为设定的判决阈值，最后根据Agent输出动作的收敛值和各阶最大记忆长度限制来确定Volterra均衡器的各阶记忆长度，完成对Volterra均衡器最优结构的确定。Volterra均衡器结构确定后，选取3组由不同随机数种子产生的信号数据，用前32768个样本对Volterra均衡器进行训练，剩余65536个样本对Volterra均衡器进行测试，然后取平均测试误码率作为性能评价指标，消耗的乘法器数目作为复杂度评价指标。图3、图4分别展示了采用单纯追求高性能的奖励值计算方式时，本发明与基于贪心搜索的Volterra-FFE、Volterra-DFE的误码率对比图。从图3可以看出，当发射功率大于等于14dBm时，本发明搜索出的Volterra-FFE结构比贪心搜索的结构在误码率表现上有一定提升，尤其是在16dBm和18dBm这些误码率原本就表现较好的发射功率下，提升更为显著。从图4可以看出，本发明搜索出的Volterra-DFE结构相比贪心搜索的结构在误码率表现上有明显提升，特别地，在发射功率为16dBm时，二者曲线显示出巨大的差别，此时贪心搜索的1阶、2阶、3阶以及反馈记忆长度为，消耗的乘法器数目为997，本发明搜索的1阶、2阶、3阶以及反馈记忆长度为，消耗的乘法器数目为608，由于贪心搜索是逐阶确定最优记忆长度的，因此会出现一些极端情况，如1阶、2阶贪婪分走大部分计算资源，3阶和反馈的记忆长度只有很小的选择空间，导致最后搜索到的结构表现很差，而本发明基于DDPG算法，在整个结构空间上采样，根据奖励值不断优化，学习到最优结构。图5、图6分别展示了采用考虑性能与复杂度折中的奖励值计算方式时，本发明与基于贪心搜索的Volterra-FFE的误码率对比图和复杂度对比图。图例中的“本发明Volterra-FFE-TradeOff”是指利用本发明搜索Volterra-FFE的最优结构时，计算奖励值得方式选取为考虑性能与复杂度折中的方式。从图5中可以看出，本发明搜索出的Volterra-FFE-TradeOff结构相比本发明搜索出的Volterra-FFE结构和贪心搜索出的Volterra-FFE结构，在误码率表现上损失很小。而在复杂度方面，从图6可以看出，本发明搜索出的Volterra-FFE-TradeOff结构远小于贪心搜索的结果。因此，通过本发明可以保证在均衡性能损失很小的情况下，大幅降低Volterra-FFE的复杂度，很好地实现了Volterra-FFE的均衡性能与复杂度的折中。图7、图8分别展示了采用考虑性能与复杂度折中的奖励值计算方式时，本发明与基于贪心搜索的Volterra-DFE的误码率对比图和复杂度对比图，图例中的“本发明Volterra-DFE-TradeOff”是指利用本发明搜索Volterra-DFE的最优结构时，计算奖励值得方式选取为性能和复杂度折中的方式。从图7中可以看出，本发明搜索出的Volterra-DFE-TradeOff结构相对于同样用本发明搜索出的Volterra-DFE结构，误码率表现比较接近，并且普遍优于基于贪心搜索的Volterra-DFE结构；在复杂度方面，从图8可以看出，首先，本发明搜索出的Volterra-DFE-TradeOff结构相对于同样用本发明搜索出的Volterra-DFE结构，在误码率表现接近的情况下，复杂度得到显著下降，例如在发射功率为16dBm时，二者误码率表现相同，但是本发明搜索出的Volterra-DFE-TradeOff结构复杂度降低了一半以上；其次，本发明搜索出的Volterra-DFE-TradeOff结构相比贪心搜索的Volterra-DFE结构，其误码率表现普遍优于后者，同时，在部分发射功率下，如8dBm，16dBm，18dBm，其复杂度远远小于后者。因此，通过本发明能够很好地实现Volterra-DFE的均衡性能与复杂度的折中，且具有优秀的结构泛化性和训练稳定性。图9、图10分别展示了采用考虑性能与复杂度折中的奖励值计算方式时，本发明与基于贪心搜索的Volterra-Pruning的误码率对比图和复杂度对比图，图例中的“贪心搜索Volterra-Pruning”是指在本发明搜索出的Volterra-FFE最优结构的基础上，运用贪心搜索确定Volterra-Pruning中的2阶相关距离和3阶相关距离。从图9中可以看出，本发明搜索出的Volterra-FFE-TradeOff结构、贪心搜索出的Volterra-Pruning结构以及本发明搜索出的Volterra-Pruning结构三者的误码率表现十分接近；在复杂度方面，从图10可以看出，本发明搜索出的Volterra-Pruning结构相对于贪心搜索的Volterra-Pruning结构，复杂度进一步降低。因此，通过本发明能够提升Volterra-Pruning的剪枝质量，获得更紧凑的均衡器结构。本领域技术人员知道，除了以纯计算机可读程序代码方式实现本发明提供的系统、装置及其各个模块以外，完全可以通过将方法步骤进行逻辑编程来使得本发明提供的系统、装置及其各个模块以逻辑门、开关、专用集成电路、可编程逻辑控制器以及嵌入式微控制器等的形式来实现相同程序。所以，本发明提供的系统、装置及其各个模块可以被认为是一种硬件部件，而对其内包括的用于实现各种程序的模块也可以视为硬件部件内的结构；也可以将用于实现各种功能的模块视为既可以是实现方法的软件程序又可以是硬件部件内的结构。以上对本发明的具体实施例进行了描述。需要理解的是，本发明并不局限于上述特定实施方式，本领域技术人员可以在权利要求的范围内做出各种变化或修改，这并不影响本发明的实质内容。在不冲突的情况下，本申请的实施例和实施例中的特征可以任意相互组合。
