标题title
一种基于强化学习的盾构掘进纠偏智能决策方法
摘要abst
本发明属于盾构施工技术领域，具体涉及一种基于强化学习的盾构掘进纠偏智能决策方法。设计环境状态集、动作集以及奖励函数，搭建盾构仿真纠偏环境；构建盾构纠偏决策模型；构建模型评估方法，得到盾构纠偏决策模型与盾构纠偏仿真环境交互后奖励分数最高的盾构纠偏决策模型；通过网格搜索方法确定值函数网络结构的参数；根据网格搜索结果，将确定的盾构纠偏决策模型在仿真环境中进行多回合训练；将盾构纠偏决策模型所处的状态数据输入最终模型，模型将直接输出执行动作的取值，作为决策方案。通过本发明提供盾构的纠偏决策方案，避免了盾构司机根据现场情况自行纠偏，以及手动操作造成蛇形纠偏的问题。
权利要求书clms
1.一种基于强化学习的盾构掘进纠偏智能决策方法，其特征在于，包括以下步骤：步骤1：结合盾构项目现场的掘进纠偏策略过程与技术经验，设计环境状态集、动作集以及奖励函数，搭建基于强化学习框架的盾构纠偏仿真环境；步骤2：构建与盾构纠偏仿真环境交互的盾构纠偏决策模型；步骤3：构建模型评估方法，得到盾构纠偏决策模型与盾构纠偏仿真环境交互后奖励分数最高的盾构纠偏决策模型；步骤4：通过网格搜索方法确定盾构纠偏决策模型中值函数网络结构的参数；步骤5：根据网格搜索结果，将确定值函数网络结构的盾构纠偏决策模型在仿真环境中进行多回合训练；步骤6：将盾构纠偏决策模型所处的状态数据输入最终模型，模型将直接输出执行动作的取值，作为决策方案。2.根据权利要求1所述的一种基于强化学习的盾构掘进纠偏智能决策方法，其特征在于，所述环境状态集为盾构测量系统测算出的盾构关键姿态参数。3.根据权利要求2所述的一种基于强化学习的盾构掘进纠偏智能决策方法，其特征在于，所述盾构关键姿态参数包括切口环水平偏差、切口换竖直偏差、盾尾水平偏差、盾尾竖直偏差、滚动角、俯仰角、水平偏航角、竖直偏航角。4.根据权利要求1所述的一种基于强化学习的盾构掘进纠偏智能决策方法，其特征在于，所述动作集根据盾构纠偏原理进行设计。5.根据权利要求1所述的一种基于强化学习的盾构掘进纠偏智能决策方法，其特征在于，所述奖励函数基于盾构的纠偏方向、纠偏速度以及盾构机与设计曲线的偏差来设计奖励函数；式中表示盾构纠偏方向的奖励，表示纠偏速度的奖励，表示盾构机的轴线偏差奖励；所述纠偏方向的奖励如下式所示：式中表示盾构纠偏方向的奖励，为t的前一时刻t-1时刻盾构机与设计轴线的差值；所述纠偏速度的奖励如下式所示：式中表示纠偏速度的奖励，为盾构机轴线偏差的绝对值；所述轴线偏差的奖励如下式所示：式中表示盾构机的轴线偏差奖励，表示盾构机偏离既定线路的距离。6.根据权利要求1所述的一种基于强化学习的盾构掘进纠偏智能决策方法，其特征在于，所述步骤2还包括采用队列的结构构建一个经验池，用于储存盾构纠偏决策模型与盾构纠偏仿真环境交互得到的训练数据；经验池中的训练数据用于值函数网络的训练。7.根据权利要求1所述的一种基于强化学习的盾构掘进纠偏智能决策方法，其特征在于，所述盾构纠偏决策模型包括两个结构一致的卷积神经网络形成双网络机制；一个作为在线网络选择，使价值最大的决策动作与盾构纠偏仿真环境交互得到样本；另一个作为目标网络用于计算在线网络所执行决策的价值，每一步训练均根据以下训练公式对在线网络的参数进行更新，经过迭代数次后再将目标网络的参数替换为在线网络的参数；所述训练公式如下所示：式中：，代表在线网络，为盾构纠偏决策模型所处的环境状态，为盾构纠偏决策模型在所处的环境状态下做出的决策动作，代表执行决策动作后转移到的下一个环境状态，代表执行决策动作后从环境获得的反馈，是一个折扣因子，代表值函数网络，表示在线网络的参数，表示目标网络的参数，表示每次更新参数的程度。8.根据权利要求6所述的一种基于强化学习的盾构掘进纠偏智能决策方法，其特征在于，所述步骤4包括以下步骤：步骤4.1：梳理值函数网络需要确定的结构参数；步骤4.2：确定待定参数的候选取值列表；步骤4.3：将待定参数候选取值进行组合，形成多个候选盾构纠偏决策模型；步骤4.4：将候选盾构纠偏决策模型分别在盾构纠偏仿真环境中训练相同回合数并进行测试，根据步骤3中建立的模型评估方法计算各候选盾构纠偏决策模型的测试平均得分；步骤4.5：将测试平均得分最高的盾构纠偏决策模型对应的参数取值确定为值函数网络结构参数。9.根据权利要求8所述的一种基于强化学习的盾构掘进纠偏智能决策方法，其特征在于，所述步骤5包括以下步骤：步骤5.1：初始化模型参数；包括初始化经验池容量、参数ε、采样批量batch_size、在线值函数网络参数，以及用在线网络参数初始化目标网络的参数；步骤5.2：初始盾构纠偏化仿真环境，得到初始状态并进行预处理；步骤5.3：若ε大于0.5，以ε的概率随机选取动作，反之以1-ε的概率选择使价值最大的动作；步骤5.4：盾构纠偏决策模型执行步骤3中选择的价值最大的动作，得到盾构纠偏仿真环境返回的新状态、奖励以及终止信号，并将前一状态、执行动作、奖励、新状态、终止信号作为一条经验数据存入经验池中；步骤5.5：若新状态为终止状态，或者连续规定次数内得到的均为负向奖励，则终止当前回合的训练，返回执行步骤5.2；步骤5.6：当经验池样本数量大于训练批量样本数时，从经验池中随机采样batch_size个样本，用梯度下降法更新在线值函数网络；步骤5.7：每隔预设的固定迭代次数，用在线值函数网络的参数更新目标网络；步骤5.8：重复步骤5.2到步骤5.7，直至到达设置的训练回合数，保存训练的盾构纠偏决策模型。10.根据权利要求9所述的一种基于强化学习的盾构掘进纠偏智能决策方法，其特征在于，所述步骤5.2包括以下步骤：步骤5.21：状态图像的色彩处理，对图像的色彩空间进行转换，将图像变为灰度图像；步骤5.22：设置盾构纠偏决策模型每经历4帧图像进行一次决策；步骤5.23：使用对图像帧数叠加的方法，将3帧图像叠加作为网络的输入。
说明书desc
技术领域本发明属于盾构施工技术领域，具体涉及一种基于强化学习的盾构掘进纠偏智能决策方法。背景技术隧道建设作为地下空间开发的重要组成部分，采用盾构机来完成隧道的贯通任务已经十分广泛。盾构施工项目中，盾构姿态是盾构操作人员进行推进方案决策的关键因素，当盾构机偏离设计轴线时，需要对相关掘进参数进行及时调整，让盾构机逐渐回到轴线上来。盾构机的姿态与地表沉降、管片拼装等密切相关，直接影响到成型隧道的质量和路线。因此，盾构机的推进姿态控制是盾构施工项目质量管理中的关键问题。目前盾构姿态的纠偏技术可以大致分为以下几类：以三点法为基础，结合全站仪、棱镜、倾斜仪等装置，改进盾构坐标及姿态偏差的计算方法，为盾构姿态的纠偏控制决策提供必需基础支持。通过对施工历史数据的描述性统计和回归分析，分析油缸行程、土舱压力等掘进参数对盾构姿态的影响规律，探索掘进参数与盾构位姿间的相关关系，根据规律反向调整相关掘进参数，进而控制盾构方向，对盾构操作者的纠偏操作决策提供理论基础。通过递归特征消除、随机森林等方法对掘进参数进行特征选择，通过XGBoost方法、构建神经网络等方法，预测盾构的姿态偏差、姿态角度等。由于盾构所处姿态是盾构操作控制决策的主要依据，因此，根据施工参数预测得到的盾构姿态便可以作为下一步参数调整决策的参考，提前调整参数，以控制掘进的方向。采用模糊数学和PID控制系统，基于运动学模型建立盾构的纠偏曲线模型，构建外环规划、内环精确控制的PID控制系统，对盾构姿态实现纠偏控制。盾构姿态的精确测算、相关参数对盾构姿态的影响规律以及姿态的预测研究从决策基础、决策依据上为盾构的操作人员提供了基础的决策支持，但其并不能提供直接的决策方案，盾构的各个操作仍需要人为控制。而单纯依靠人为手动纠偏容易因为纠偏时机和纠偏量控制不好而造成蛇形纠偏，盾构姿态的精确控制非常依赖操作者的项目经验。发明内容本发明公开了一种基于强化学习的盾构掘进纠偏智能决策方法，拟解决背景技术中提到的盾构姿态的精确控制非常依赖操作者的项目经验，单纯依靠人为手动纠偏容易因为纠偏时机和纠偏量控制不好造成蛇形纠偏的问题。为解决上述技术问题，本发明采用的技术方案如下：一种基于强化学习的盾构掘进纠偏智能决策方法，包括以下步骤：步骤1：结合盾构项目现场的掘进纠偏策略过程与技术经验，设计环境状态集、动作集以及奖励函数，搭建基于强化学习框架的盾构纠偏仿真环境；步骤2：构建与盾构纠偏仿真环境交互的盾构纠偏决策模型；步骤3：构建模型评估方法，得到盾构纠偏决策模型与盾构纠偏仿真环境交互后奖励分数最高的盾构纠偏决策模型；步骤4：通过网格搜索方法确定盾构纠偏决策模型中值函数网络结构的参数；步骤5：根据网格搜索结果，将确定值函数网络结构的盾构纠偏决策模型在仿真环境中进行多回合训练；由于值函数网络是盾构纠偏决策模型的组成部分，通过网络搜索结果确定值函数网络结构，也就确定了盾构纠偏决策模型的结构；步骤6：将盾构纠偏决策模型所处的状态数据输入最终模型，模型将直接输出执行动作的取值，作为决策方案。将在步骤5中确定的盾构纠偏决策模型在仿真环境中进行数个回合的训练后，保存训练模型，此处最终模型即为上述保存的训练模型。本发明根据环境状态集得出的关键决策参数的调整策略，可以为盾构司机提供决策方案参考，有助于减轻盾构司机的决策疑虑与决策强度，降低盾构司机的工作强度，有效的保障盾构司机在面对突发状况时能有充沛的精力作出及时、高效的应对方案，进而提高项目安全、保证项目的稳定进行；通过本发明提供盾构的纠偏决策方案，避免了盾构司机根据现场情况自行纠偏，以及手动操作造成蛇形纠偏的问题。优选的，所述环境状态集为盾构测量系统测算出的盾构关键姿态参数。优选的，所述盾构关键姿态参数包括切口环水平偏差、切口换竖直偏差、盾尾水平偏差、盾尾竖直偏差、滚动角、俯仰角、水平偏航角、竖直偏航角。优选的，所述动作集根据盾构纠偏原理进行设计。优选的，所述奖励函数基于盾构的纠偏方向、纠偏速度以及盾构机与设计曲线的偏差来设计奖励函数；R＝r_d+r_v+r_y式中r_d表示盾构纠偏方向的奖励，r_v表示纠偏速度的奖励，r_y表示盾构机的轴线偏差奖励；所述纠偏方向的奖励如下式所示：式中r_d表示盾构纠偏方向的奖励，Δyt-1为t的前一时刻t-1时刻盾构机与设计轴线的差值；所述纠偏速度的奖励如下式所示：式中r_v表示纠偏速度的奖励，|drt|为盾构机轴线偏差的绝对值；所述轴线偏差的奖励如下式所示：式中r_y表示盾构机的轴线偏差奖励，|Δyt|表示盾构机偏离既定线路的距离。进一步的，为了提高样本的利用率并减少样本相关性，所述步骤2还包括采用队列的结构构建一个经验池，用于储存盾构纠偏决策模型与盾构纠偏仿真环境交互得到的交互历史数据；从经验池中采样的样本数据用于值函数网络的训练。所述样本为从经验池中采样得到的数据。进一步的，由于与盾构纠偏仿真环境的交互和更新均来自同一个盾构纠偏决策模型，交互产生的数据会对迭代产生影响，为了减少模型更新的不稳定性，所述盾构纠偏决策模型包括两个结构一致的卷积神经网络形成双网络机制；一个作为在线网络选择，使价值最大的决策动作与盾构纠偏仿真环境交互得到样本；另一个作为目标网络用于计算在线网络所执行决策的价值；每一步训练均根据以下训练公式对在线网络的参数进行更新，经过迭代数次后再将目标网络的参数替换为在线网络的参数；所述训练公式如下所示：Q←Q+α式中：，Q代表在线网络，s为盾构纠偏决策模型所处的环境状态，a为盾构纠偏决策模型在所处的环境状态下做出的决策动作，s′代表执行决策动作后转移到的下一个环境状态，r代表执行决策动作后从环境获得的反馈，γ是一个折扣因子，Q代表值函数网络，θ表示在线网络的参数，θ-表示目标网络的参数，α表示每次更新参数的程度。优选的，所述步骤4包括以下步骤：步骤4.1：梳理值函数网络需要确定的结构参数；步骤4.2：确定待定参数的候选取值列表；步骤4.3：将待定参数候选取值进行组合，形成多个候选盾构纠偏决策模型；步骤4.4：将候选盾构纠偏决策模型分别在盾构纠偏仿真环境中训练相同回合数并进行测试，根据步骤3中建立的模型评估方法计算各候选盾构纠偏决策模型的测试平均得分；步骤4.5：将测试平均得分最高的盾构纠偏决策模型对应的参数取值确定为值函数网络结构参数。优选的，所述步骤5包括以下步骤：步骤5.1：初始化盾构纠偏决策模型参数；包括初始化经验池容量、参数ε、采样批量batch_size、在线值函数网络参数，以及用在线网络参数初始化目标网络的参数；步骤5.2：初始化仿真环境，得到初始状态并进行预处理；步骤5.3：若ε大于0.5，以ε的概率随机选取动作，反之以1-ε的概率选择使价值最大的动作；步骤5.4：盾构纠偏决策模型执行步骤3中选择的价值最大的动作，得到盾构纠偏仿真环境返回的新状态、奖励以及终止信号，并将前一状态、执行动作、奖励、新状态、终止信号作为一条经验数据存入经验池中；步骤5.5：若新状态为终止状态，或者连续规定次数内得到的均为负向奖励，则终止当前回合的训练，返回执行步骤5.2；步骤5.6：当经验池样本数量大于训练批量样本数时，从经验池中随机采样batch_size个样本，用梯度下降法更新在线值函数网络；步骤5.7：每隔预设的固定迭代次数，用在线值函数网络的参数更新目标网络；步骤5.8：重复步骤5.2到步骤5.7，直至到达设置的训练回合数，保存训练的盾构纠偏决策模型。优选的，所述步骤5.2包括以下步骤：步骤5.21：状态图像的色彩处理，对图像的色彩空间进行转换，将图像变为灰度图像；所述状态图像是指盾构纠偏决策模型与可视化仿真环境交互时的屏幕快照，是反映盾构机与设计轴线相对位置的图像。步骤5.22：设置盾构纠偏决策模型每经历4帧图像进行一次决策；步骤5.23：使用对图像帧数叠加的方法，将3帧图像叠加作为网络的输入。综上所述，由于采用了上述技术方案，本发明的有益效果是：1.本发明将数据分析、机器学习和人工智能等技术融入行业施工过程，可以及时发现当前施工过程中存在的问题，提前预判施工过程中可能发生的风险、当前决策方案产生的后果等，进而规避风险、做出更优决策。2.本发明通过构建强化学习模型，实现盾构纠偏的智能化决策，符合包括盾构项目在内的建筑施工行业的智能化决策管理趋势，有助于推动行业的信息化、数据化发展，扩展了强化学习的应用领域。3.根据环境状态得出的关键决策参数的调整策略，可以为盾构司机提供决策方案参考，有助于减轻盾构司机的决策疑虑与决策强度，降低盾构司机的工作强度，有效的保障盾构司机在面对突发状况时能有充沛的精力作出及时、高效的应对方案，进而提高项目安全、保证项目的稳定进行；通过本发明提供盾构的纠偏决策方案，避免了盾构司机根据现场情况自行纠偏，以及手动操作造成蛇形纠偏的问题。4.在盾构项目决策智能化探索中引入深度强化学习，所设计的模型方案也可以应用于相关大型装备的智能化管理研究应用，丰富强化学习在大型工程领域的应用研究。附图说明本发明将通过例子并参照附图的方式说明，其中：图1为本发明的盾构掘进纠偏智能决策方法的流程图。图2为本发明的盾构纠偏决策可视化仿真环境示例。图3为本发明的盾构纠偏决策模型的整体框架。图4为本发明的多模型对比评估方法。图5为本发明的盾构纠偏决策模型决策结果。具体实施方式为使本申请实施例的目的、技术方案和优点更加清楚，下面将结合本申请实施例中附图，对本申请实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例仅是本申请一部分实施例，而不是全部的实施例。通常在此处附图中描述和示出的本申请实施例的组件可以各种不同的配置来布置和设计。因此，以下对在附图中提供的本申请的实施例的详细描述并非旨在限制要求保护的本申请的范围，而是仅仅表示本申请的选定实施例。基于本申请的实施例，本领域技术人员在没有做出创造性劳动的前提下所获得的所有其他实施例，都属于本申请保护的范围。下面结合附图1和附图5对本发明的实施例作详细描述；其中附图1中提到的盾构纠偏智能化决策模型简称为盾构纠偏决策模型。一种基于强化学习的盾构掘进纠偏智能决策方法，包括以下步骤：步骤1：搭建基于强化学习框架的盾构纠偏仿真环境；环境状态集的设计在盾构施工过程中，确定如何掘进、如何调整参数的直接决策者就是盾构司机。根据盾构切口、盾尾中心两点在水平方向和数值方向上与既定轴线的距离以及姿态角、掘进趋势综合推断，盾构司机可以判断出盾构机是否有上仰或“栽头”、是否偏离轴线、偏离轴线的程度是否可以接受、是否需要纠偏等。故环境状态集中的状态数据包括盾构测量系统测算出的关键姿态参数。因此，定义仿真环境的状态空间为S＝S1×S2×S3×S4×S5×S6×S7×S8，Si表示各关键姿态参数，如下表1所示。表1测量系统测算的关键姿态参数S参数单位描述S1切口环水平偏差mm盾构机头部与轴线的水平距离S2切口环竖直偏差mm盾构机头部与轴线的竖向距离S3盾尾水平偏差mm盾构机尾部与轴线的水平距离S4盾尾竖直偏差mm盾构机尾部与轴线的水平距离S5滚动角度盾构机绕其机身轴线转过的角度S6俯仰角度盾构机轴线方向和水平面形成的夹角S7水平偏航角mm/m按原水平方向继续掘进，每掘进1米而产生的偏差S8竖直偏航角mm/m按原竖直方向继续掘进，每掘进1米而产生的偏差在项目现场的实际操作中，盾构掘进的决策人员进行决策主要依据监控屏幕上显示的系统图表，由于状态集中盾构的姿态参数都是有测量系统单独测算得到，在进行仿真环境构建时无法得到测量系统的实时支持，因此使用反映盾构掘进情况的图片作为状态，即采用监控屏幕上实时显示的盾构掘进状态来作为反映盾构掘进的状态。动作集设计对盾构的姿态进行控制，一般通过改变刀盘的前进与转动方向来修正盾构机的轨迹偏差；盾构刀盘后方的千斤顶通常分为A、B、C、D四组，分别代表右、下、左、上四个区域，对水平方向上的偏差，主要通过调整左右推进压力来改变千斤顶的推进行程，通过行程差来调整纠偏力矩，竖直方向的纠偏控制原理与水平相同，主要调整B、D两组千斤顶推进的行程差。因此，以水平方向为例，将动作集定义为A＝A1×A2×A3，其中，A1代表A组推进压力变化，A2代表C组推进压力变化，A3代表刀盘转速。为了避免压力改变过快，将推进压力变化范围设为，负号表示减小推进的压力，取值0则表示压力不变，正号表示增大压力；在执行改变压力的动作时，还将限定调整后的推进压力大小也在允许的阈值范围内，具体可参见下表2。所述阈值范围为推进压力的合理取值范围，该合理取值范围为盾构机技术规格参数中定义的范围。表2盾构水平纠偏基本动作奖励函数的设计本发明从盾构的纠偏方向、纠偏速度以及盾构机与设计轴线的偏差三个方面来设计奖励函数，公式如下：R＝r_d+r_v+r_y式中r_d表示盾构纠偏方向的奖励，r_v表示纠偏速度的奖励，r_y表示盾构机的轴线偏差奖励：纠偏方向令勾时刻t盾构与设计轴线的偏差值，Δyt-1为t的前一时刻t-1盾构与设计轴线的差值，则drt＝Δyt-Δyt-1为纠偏前后偏差的变化值，根据偏差的变化值和前一时刻的偏差符号，可以判断出盾构是否在朝着设计轴线的方向掘进，并给予相应的正反馈或负反馈作为对纠偏方向的奖励r_d：例如：若前一时刻盾构处于轴线的左边即Δyt-1＜0，后一时刻盾构处于原位置的右边，则drt＞0，表明盾构实现了向右纠偏，给予正向奖励1；若前一时刻盾构处于轴线的右边即Δyt-1＞0，后一时刻盾构处于原位置的左边，则drt＜0，表明盾构实现了向左纠偏，因此也给予正向奖励1。将这两种情况合并就是当盾构原位置不处于轴线上，实现了对应方向纠偏时，环境将反馈正向奖励1；反之，需要纠偏而没有朝着纠偏的方向掘进时，环境将反馈负向的奖励-1。若开始时刻盾构就处于轴线上即Δyt-1＝0，则前后时刻的偏差变化drt＝0时表明盾构沿着轴线前进，环境将反馈正向的奖励1；前后时刻的偏差变化drt不等于0时则表明当盾构开始偏离了轴线，环境将给予负向的奖励-1。纠偏速度根据“逐步纠偏，不可猛纠猛调”的调整原则，对于纠偏前后偏差变化值过大的动作，需要给予较大的负向反馈：其中，|drt|为盾构-轴线偏差的绝对值。本发明将纠偏的变化量进行分段，当纠偏量大于0mm且小于1mm时，符合缓慢纠偏的原则，给予正向的奖励1；当纠偏量大于或等于1mm而小于3mm时，认为当前的纠偏量符合条件，不给予奖励；当纠偏量大于或等于3mm而小于5mm时，认为当前的纠偏量偏大；给予负向奖励-1；大于或等于5mm而小于10mm时，纠偏量超过接受范围，可能会造成严重的事故，因此给予一个较大的负向奖励-10；大于10mm时，认定纠偏速度严重超过规定，给予负向奖励-100。偏差大小为了严格控制盾构机的掘进符合设计轴线，除了保证正确的纠偏方向和纠偏速度外，同时也应该控制与设计轴线的偏差不能太大；上式中，|Δyt|表示盾构机偏离既定线路的距离。验收标准通常对成型隧道偏差的要求是在5mm以内，因此在纠偏的过程中除了考虑纠偏方向和纠偏的速度之外，还要控制纠偏的大小；纠偏等于或大于0mm且小于3mm为可接受范围，不进行负向的奖励反馈；等于或大于3mm而小于5mm时，给予负向奖励-1；偏差等于或大于5mm而小于10mm时，给以负向奖励-10；偏差等于或大于10时，给于负向奖励-100。可视化盾构纠偏仿真环境以开源工具包Gym为环境搭建基础，对设计的仿真环境进行可视化。如图2所示。对应于盾构掘进过程，图中黑色矩形代表设置的盾构纠偏决策模型，黑色矩形框两侧为设计轴线周边的岩土环境；黑色矩形所处的线路为设计轴线。整体图像反映了在交互过程中，盾构纠偏决策模型的前进方向以及与设计轴线的偏离情况等。步骤2：构建盾构纠偏决策模型。基于强化学习算法，构建的盾构纠偏决策模型整体框架如图3所示。盾构纠偏决策模型是与盾构纠偏仿真环境交互的智能体；其目标是最大化从环境中获得总体奖励。智能体的输入为所处的环境状态，输出为对应状态下价值最大的动作，也就是决策。在智能体中，采用卷积神经网络作为值函数网络评估决策的价值。智能体在与环境的每一次交互中，都会产生一条经验数据，这条数据包括来自环境的奖励r；在一个回合的交互中，智能体会与环境交互n次，目标就是通过训练，智能体与环境交互一个回合的总体奖励能达到最大。考虑到一般情况下未来时刻执行动作的奖励对当下的影响会更小，因此，可以将折扣因子γ引入总体奖励的计算公式：Rt＝∑k＝0γkrt+k+1其中，t代表当前时刻，k代表t之后的k个时刻，γ为折扣因子，Rt表示到t时刻时的总体奖励值。其中，智能体训练数据来源于智能体与盾构纠偏仿真环境交互得到的经验数据，为了提高经验数据的利用率并减少样本数据的相关性，采用队列的机制构建一个经验池，先将一定的经验数据储存到经验池，再从经验池中批量采样出数据用于值函数网络的训练；此处批量采样出的数据即为样本数据。此外，若智能体与盾构纠偏仿真环境的交互和更新都来自于同一个模型交互数据就会对迭代产生影响，为了减少智能体更新的不稳定性，采用两个结构完全一致的卷积神经网络形成双网络机制，一个作为在线网络选择使价值最大的决策动作与盾构纠偏仿真环境交互得到经验数据。另外一个网络作为目标网络用于计算在线网络所执行决策的价值，每一步训练均根据以下公式对在线网络的参数进行更新，经过特定迭代次数后再将目标网络的参数替换为在线网络的参数。公式如下：Q←Q+α式中：，Q代表在线网络，s为盾构纠偏决策模型所处的环境状态，a为盾构纠偏决策模型在所处的环境状态下做出的决策动作，s′代表执行决策动作后转移到的下一个环境状态，r代表执行决策动作后从环境获得的反馈，γ是一个折扣因子，Q代表值函数网络，θ表示在线网络的参数，θ-表示目标网络的参数，α表示每次更新参数的程度。步骤3：设计智能体的评估方法将训练保存的智能体与盾构纠偏仿真环境交互100个回合，统计智能体在100个回合的测试中得到的平均奖励分数，奖励分数越高，则表明模型的策略学习越好。若是需要进行多个智能体的对比，则将多个智能体分别与盾构纠偏仿真环境交互100个回合，统计各智能体在各回合得到的综合奖励值，平均回合得分最高的智能体策略最好。步骤4：确定值函数网络结构值函数网络主要从盾构当前的状态图中提取信息来进行价值的计算，考虑到图片的尺寸大小与卷积层的计算方法，神经网络的深度不宜过深；本实施例中设计模型包含两个卷积层-最大汇聚层对，实现对输入状态的特征提取，通过Flatten层将提取的特征数据“压平”，再通过两个全连接层的运算得出每一个动作的价值。步骤4.1：梳理值函数网络需要确定的结构参数，基于模型初步设置的值函数网络结构，需要确定的结构参数包括两个卷积层的卷积核大小和数量，以及全连接层的单元数；步骤4.2：确定待定参数的候选取值列表。根据卷积神经网络参数设置经验确定待定参数的候选取值，如下表3所示：表3值函数网络待定参数步骤4.3：将待定参数候选取值进行组合，形成54个候选智能体；步骤4.4：将54个候选智能体分别在盾构纠偏仿真环境中训练600个回合并进行100个回合的测试，根据模型评估方法计算各候选模型的测试平均得分；步骤4.5：基于网格搜索结果，智能体“3-32-4-64-64”在测试中取得的最高得分和平均每个回合执行的动作数都排在前列。因此最终模型的值函数网络结构为：第一卷积层使用32个卷积核，大小为3×3，步长设置为3，采用Relu激活函数；第一个汇聚层采取最大汇聚来降维，使用的滤波器大小为2×2；第二个卷积层使用64个卷积核，大小为4×4，步长设置为1，激活函数仍然使用Relu；第二个汇聚层为最大汇聚层，滤波器大小设置为2×2；展平后的数据将输入到神经元为64个的全连接层，通过Relu激活函数输出。最后一个全连接层即输出层。步骤5：智能体训练。根据网络搜索结果，将确定的盾构纠偏智能决策模型在仿真环境中进行多个回合的训练。步骤5.1：初始化智能体参数及相关训练参数设置，如表4所示：表4初始化参数设置参数值含义episode5000智能体与环境交互的回合数batch_size32采样的批量数据大小save_training_frequency50保存训练模型的间隔回合数update_target_model_frequency5更新目标网络的间隔回合数memory_size50000经验池大小gamma0.95奖励折扣因子epsilon1.0探索率epsilon_decay0.99999探索率每次的衰减率epsilon_min0.1探索率最小值learning_rate0.001CNN网络学习率步骤5.2：初始化仿真环境，得到初始状态并进行处理，包括：a.状态图像的色彩处理，即对图像的色彩空间进行转换，将图像变为灰度图像；b.跳帧，设置智能体每经历4帧图像进行一次决策；c.帧数堆叠，由于多帧图像的叠加可以反应出agent的动态变化，因此，本发明使用对图像帧数叠加的方法，将3帧图像叠加作为网络的输入；步骤5.3：若ε大于0.5，以ε的概率随机选取动作，反之以1-ε的概率选择使价值最大的动作；步骤5.4：智能体执行步骤5.3中选择的动作，得到环境返回的新状态、奖励以及终止信号，并将前一状态、执行动作、奖励、新状态、终止信号作为一条经验数据存入经验池中；步骤5.5：若新状态为终止状态，或者连续10次得到的均为负向奖励，则终止当前回合的训练，并返回执行步骤5.2；步骤5.6：当经验池中的经验数据大于训练批量经验数据时，从经验池中随机采样32个经验数据，用梯度下降法更新在线值函数网络；由于经验池中一开始数据量为0，随着智能体与盾构纠偏仿真环境的不断交互，交互数据会一条条的存入经验池中，智能体的训练数据由经验池中的数据随机采样得到，每次训练需要采样的数据量为一个批量，本实施例的批量为32；当经验池中储存的数据量大于32时，智能体才能批量采样数据进行训练。步骤5.7：每个5个训练回合，用在线值函数网络的参数更新目标网络；步骤5.8：重复执行步骤5.2到步骤5.7，训练550个回合，保存训练的智能体。步骤6：盾构纠偏模型智能决策。将智能体在构建的盾构纠偏仿真环境中进行决策测试，测试过程中以所处的状态图像输入模型，智能体执行模型决策输出的动作取值。智能体模型在仿真环境中的部分控制轨迹如图5所示。根据控制决策结果，所设计的盾构纠偏智能决策模型在缓和弯道、直线弯道部分都基本能沿设计轴线前行，表明模型的决策方案有效，为项目现场的盾构纠偏决策提供智能化决策支持。以上所述实施例仅表达了本申请的具体实施方式，其描述较为具体和详细，但并不能因此而理解为对本申请保护范围的限制。应当指出的是，对于本领域的普通技术人员来说，在不脱离本申请技术方案构思的前提下，还可以做出若干变形和改进，这些都属于本申请的保护范围。
