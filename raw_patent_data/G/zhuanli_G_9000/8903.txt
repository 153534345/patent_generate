标题title
基于深度神经网络逼近MPC的航天器姿态控制方法
摘要abst
本发明公开一种基于深度神经网络逼近MPC的航天器姿态控制方法，步骤包括：步骤S01.配置MPC控制器，控制将多个输入姿态参数输入至MPC控制器中，得到对应的多个控制力矩输出，由各输入输出组合构建形成训练数据集；步骤S02.构建DNN模型，使用训练数据集对DNN模型进行训练，以使得DNN模型逼近MPC控制器，训练完成后得到逼近于MPC控制器的目标DNN模型；步骤S03.使用训练后得到的目标DNN模型对目标航天器进行姿态控制，控制过程中获取航天器的实时姿态参数并输入至目标DNN模型中，将输出的预期的控制力矩提供给航天器。本发明具有实现方法简单、控制性能好，同时控制效率高、计算复杂度低等优点。
权利要求书clms
1.一种基于深度神经网络逼近MPC的航天器姿态控制方法，其特征在于，步骤包括：步骤S01. 训练数据集生成：配置基于模型预测控制方法的MPC控制器，所述MPC控制器输入航天器的姿态参数、输出对航天器的期望控制力矩，所述姿态参数包括姿态误差或根据所述姿态误差转化得到的参数，所述姿态误差为航天器当前姿态参数值与期望姿态参数值之间的误差值，控制将多个姿态参数输入至所述MPC控制器中，得到对应多个期望控制力矩输出，由各输入的姿态参数、输出的期望控制力矩的对应组合构建形成训练数据集；步骤S02. DNN训练：构建基于深度神经网络的DNN模型，所述DNN模型的输入层输入航天器的姿态参数、输出层输出预期的控制力矩，使用步骤S01构建形成的训练数据集对所述DNN模型进行训练，以使得所述DNN模型逼近所述MPC控制器，训练完成后得到逼近于所述MPC控制器的目标DNN模型；步骤S03. 在线姿态控制：使用步骤S02训练后得到的目标DNN模型对目标航天器进行Move-to-rest在线姿态控制，控制过程中获取航天器的实时姿态参数并输入至所述目标DNN模型中，将所述目标DNN模型输出的预期的控制力矩提供给航天器中姿态控制执行器。2.根据权利要求1所述的基于深度神经网络逼近MPC的航天器姿态控制方法，其特征在于，所述步骤S01中控制将多个姿态参数输入至所述MPC控制器中时，具体在指定的预测姿态数据范围内选择多个均匀分布的姿态数据节点形成多组输入数据，对于每组输入数据，基于模型预测控制方法采用预设的预测方程计算出每组输入姿态数据下的期望控制力矩输出，其中所述预测方程的目标函数中只考虑姿态误差、忽略能量部分。3.根据权利要求2所述的基于深度神经网络逼近MPC的航天器姿态控制方法，其特征在于，基于模型预测控制方法采用的所述预测方程具体为：其中，J表示目标函数，Uk表示第k步的控制力矩，U0为初始控制力矩，tk为第k步的时刻，k为执行步数，A、B分别表示动力学系统离散线性化的后转换矩阵和控制矩阵， G表示为等式约束，h表示为不等式约束，T为预测时域长度，N为采样节点数，xk为第k步的姿态参数，所述姿态参数包括欧拉角、角速率。4.根据权利要求1所述的基于深度神经网络逼近MPC的航天器姿态控制方法，其特征在于，所述步骤S2中使用步骤S01构建形成的训练数据集对所述DNN模型进行训练时，具体采用比例共轭梯度下降法，所述比例共轭梯度下降法为通过将所述DNN模型的拟合误差对所需的神经元参数进行求导，按照使得所述拟合误差减小的方向调节所述神经元参数，直至DNN模型收敛到满足预设要求；当采用所述比例共轭梯度下降法使得DNN模型收敛到满足预设要求时，且连续多次训练后DNN模型的性能参数不再减小，则判定训练完成，训练得到所述目标DNN模型。5.根据权利要求1所述的基于深度神经网络逼近MPC的航天器姿态控制方法，其特征在于，所述DNN模型具体采用4层隐含层、每层具有100个神经元的结构，其中单个神经元的输入输出关系满足：其中，yi表示第i个神经元的输出，f为非线性激活函数，xi表示第j个输入，ωij表示第i个神经元的第j个输入的权重值，θi表示第i个神经元的偏置值。6.根据权利要求1~5中任意一项所述的基于深度神经网络逼近MPC的航天器姿态控制方法，其特征在于，还包括预先构建运动学和动力学方程以用于所述MPC控制器、DNN模型中进行运动学和动力学分析，所述运动学和动力学方程的构建时，通过先基于航天器的姿态参数构建初始运动学和动力学方程，然后使用所述姿态误差构建基于姿态误差的运动学和动力学方程，对所述基于姿态误差的运动学和动力学方程简化后形成最终的运动学和动力学方程，所述运动学和动力学方程的构建步骤具体包括：步骤S101. 使用四元数q表示航天器的姿态，所述四元数q为：其中，r为旋转欧拉轴，φ为旋转角度，qv=T为矢量部分，q0为标量部分，q1~q3为矢量部分的分量表示；基于所述四元数q，不考虑执行器模型构建所述初始运动学和动力学方程为：其中，E3和I分别表示为单位矩阵和转动惯量，ω是物体坐标系相对于惯性坐标系的角速度，U是控制力矩矢量，；步骤S102. 定义姿态误差qe以及角速度误差ωe为：其中，qT为期望姿态，ωT为期望角速度，为四元数乘法，为从期望的本体坐标系到当前本体坐标系的旋转矩阵；根据所述初始航天器运动学和动力学方程，基于所述姿态误差qe、角速度误差ωe构建得到基于姿态误差的运动学和动力学方程为：其中，qev为误差四元数的矢量部分，为，qe0为误差四元数的标量部分；步骤S103. 对步骤S102中构建的基于所述姿态误差的运动学和动力学方程进行简化，将其中角速度误差ωT和角加速度误差简化为零，形成最终的运动学和动力学方程为：。7.根据权利要求1~5中任意一项所述的基于深度神经网络逼近MPC的航天器姿态控制方法，其特征在于：所述步骤S03中预期的控制力矩后还包括最优力矩补偿步骤，包括：对控制力矩Uk施加一个补偿力矩δUk，以使得航天器的状态满足约束条件，经过补偿后的力矩为，其中δ表示为预设的调整系数，根据补偿目的构建关于所述补偿力矩的多个线性不等式约束条件，基于所述线性不等式约束条件构建最优问题，并将所述最优问题转换为求解一组多变量线性方程，对所述多变量线性方程求解寻找出所述补偿力矩δUk的最小补偿力矩值，即为最优补偿力矩，按照所述最优补偿力矩执行对当前控制力矩的补偿。8.根据权利要求6所述的基于深度神经网络逼近MPC的航天器姿态控制方法，其特征在于，构建的所述最优问题为：其中，ci为第i个约束条件且ci≤0，为第i个约束条件对时间的导数，xk为第k步的姿态参数，所述姿态参数包括欧拉角、角速率，k为执行步数，i为约束条件数，在z为以状态和控制力矩表示下的约束条件的导数，即，△t为控制间隔，uk1、uk2、uk3分别为Uk沿体坐标系的分量；将所述最优问题具体转换为求解以下一组多变量线性方程：其中，ωik为第k步时i轴上的角速度，为第k步时i轴上对应的角加速度，为待求解的常数，ωmax为约束条件允许的角速度上界。9.根据权利要求8所述的基于深度神经网络逼近MPC的航天器姿态控制方法，其特征在于：求解所述多变量线性方程时，如果满足：对所有i都成立，那么解是和，为待求最优补偿力矩的解，对应场景ci≤0，ci为第k+1步姿态参数所对应的约束条件，判定为不需要补偿；如果对于，对应场景为ci＞0，求解线性方程得到所述补偿力矩和的解，根据是否满足判断解是否可行。10.一种存储有计算机程序的计算机可读存储介质，其特征在于，所述计算机程序执行时实现如权利要求1～9中任意一项所述的方法。
说明书desc
技术领域本发明涉及航天器控制技术领域，尤其涉及一种基于深度神经网络逼近MPC的航天器姿态控制方法。背景技术模型预测控制在工业实践中得到了广泛的应用，其通过反复求解给定预测范围内的优化问题，因而具有可以显性的考虑约束、实现目标函数的最优化等优势。当涉及到约束控制问题时，也就是姿态控制的情况下，MPC都具有非常突出的性能。但是MPC对在线计算能力的要求较高，而受功耗的影响星上芯片的性能非常有限，致使星上实时解算MPC往往会耗时太长，即直接基于MPC实现姿态控制较为困难，特别是对于低功耗的星载计算机，MPC就难以实时满足航天器姿态控制问题的效率需求，因而MPC在航天器姿态控制中应用较少。针对MPC的计算效率问题，现有技术中通常是采用离线形成查找表的方式，即采用离线计算显式MPC，然后形成查找表，在线控制器通过从表中插入参数来获得输出。但是由于状态维度会导致表的大小呈指数增长，上述形成查找表的方式不适用于状态维度太多的系统，在航天器姿态控制中状态维度即较多，因而上述形成查找表的方式也不适用于航天器姿态控制中。为解决上述问题，有从业者提出利用神经网络来代替MPC以实现加速，该类方法中通常是使用神经网络替代MPC中复杂和耗时长的动态预测方程，使用训练了的神经网络直接完全取代原有的MPC。该类方法中，由于无论原始的MPC系统有多复杂，神经网络仅仅只需依赖于它自己的架构，即主要为层数量、神经元数量等，因而可以大大减少控制解算的计算复杂度、减少计算耗时。但是常规的神经网络的结构和规模并不适用于航天器的姿态控制，实际应用时难以确定神经网络的规模，且DNN太大的话，会导致训练通常较为困难且耗时较长，直接使用此神经网络实现航天器姿态控制方式仍然会存在耗时长的问题，仍然不能很好的满足航天器姿态控制中的效率、实时性要求。发明内容本发明要解决的技术问题就在于：针对现有技术存在的技术问题，本发明提供一种现方法简单、控制性能好，同时控制效率高、计算复杂度低的基于深度神经网络逼近MPC的航天器姿态控制方法。为解决上述技术问题，本发明提出的技术方案为：一种基于深度神经网络逼近MPC的航天器姿态控制方法，步骤包括：步骤S01.训练数据集生成：配置基于模型预测控制方法的MPC控制器，所述MPC控制器输入航天器的姿态参数、输出对航天器的期望控制力矩，所述姿态参数包括姿态误差或根据所述姿态误差转化得到的参数，所述姿态误差为航天器当前姿态参数值与期望姿态参数值之间的误差值，控制将多个姿态参数输入至所述MPC控制器中，得到对应多个期望控制力矩输出，由各输入的姿态参数、输出的期望控制力矩的对应组合构建形成训练数据集；步骤S02. DNN训练：构建基于深度神经网络的DNN模型，所述DNN模型的输入层输入航天器的姿态参数、输出层输出预期的控制力矩，使用步骤S01构建形成的训练数据集对所述DNN模型进行训练，以使得所述DNN模型逼近所述MPC控制器，训练完成后得到逼近于所述MPC控制器的目标DNN模型；步骤S03.在线姿态控制：使用步骤S02训练后得到的目标DNN模型对目标航天器进行Move-to-rest在线姿态控制，控制过程中获取航天器的实时姿态参数并输入至所述目标DNN模型中，将所述目标DNN模型输出的预期的控制力矩提供给航天器中姿态控制执行器。进一步的，所述步骤S01中控制将多个姿态参数输入至所述MPC控制器中，时，具体在预测姿态数据范围内选择多个均匀分布的姿态数据节点形成多组输入数据，对于每组输入数据，基于模型预测控制方法采用预测方程计算出每组输入姿态数据下的期望控制力矩输出，其中所述预测方程的目标函数中只考虑姿态误差、忽略能量部分。进一步的，基于模型预测控制方法采用的所述预测方程具体为：其中，J表示目标函数，Uk表示第k步的控制力矩，U0为初始控制力矩，tk为第k步的时刻，k为执行步数，A、B分别表示动力学系统离散线性化的后转换矩阵和控制矩阵，G表示为等式约束，h表示为不等式约束，T为预测时域长度，N为采样节点数，xk为第k步的姿态参数，所述姿态参数包括欧拉角、角速率。进一步的，所述步骤S2中使用步骤S01构建形成的训练数据集对所述DNN模型进行训练时，具体采用比例共轭梯度下降法，所述比例共轭梯度下降法为通过将所述DNN模型的拟合误差对所需的神经元参数进行求导，按照使得拟合误差减小的方向调节所述神经元参数，直至DNN模型收敛到满足预设要求；当采用所述比例共轭梯度下降法使得DNN模型收敛到满足预设要求时，且连续多次训练后DNN模型的性能参数不再减小，则判定训练完成，训练得到所述目标DNN模型。进一步的，所述DNN模型具体采用4层隐含层、每层具有100个神经元的结构，其中单个神经元的输入输出关系满足：其中，yi表示第i个神经元的输出，f为非线性激活函数，xi表示第j个输入，ωij表示第i个神经元的第j个输入的权重值，θi表示第i个神经元的偏置值。进一步的，还包括预先构建运动学和动力学方程以用于所述MPC控制器、DNN模型中进行运动学和动力学分析，所述运动学和动力学方程的构建时，通过先基于航天器的姿态参数构建初始运动学和动力学方程，然后使用所述姿态误差构建基于姿态误差的运动学和动力学方程，对所述基于姿态误差的运动学和动力学方程简化后形成最终的运动学和动力学方程，所述运动学和动力学方程的构建步骤具体包括：步骤S101. 使用四元数q表示航天器的姿态，所述四元数q为：其中，r为旋转欧拉轴，φ为旋转角度，qv=T为矢量部分，q0为标量部分，q1~ q3为矢量部分的分量表示；基于所述四元数q，不考虑执行器模型构建所述初始运动学和动力学方程为：其中，E3和I分别表示为单位矩阵和转动惯量，ω是物体坐标系相对于惯性坐标系的角速度，U是控制力矩矢量，；步骤S102. 定义姿态误差qe以及角速度误差ωe为：其中，qT为期望姿态，ωT为期望角速度，为四元数乘法，为从期望的本体坐标系到当前本体坐标系的旋转矩阵；根据所述初始航天器运动学和动力学方程，基于所述姿态误差qe、角速度误差ωe构建得到基于姿态误差的运动学和动力学方程为：其中，qev为误差四元数的矢量部分，为，qe0为误差四元数的标量部分；步骤S103. 对步骤S102中构建的基于所述姿态误差的运动学和动力学方程进行简化，将其中角速度误差ωT和角加速度误差简化为零，形成最终的运动学和动力学方程为：。进一步的，所述步骤S03中预期的控制力矩后还包括最优力矩补偿步骤，包括：对控制力矩Uk施加一个补偿力矩δUk，以使得航天器的状态满足约束条件，经过补偿后的力矩为，其中δ表示为预设的调整系数，根据补偿目的构建关于所述补偿力矩的多个线性不等式约束条件，基于所述线性不等式约束条件构建最优问题，并将所述最优问题转换为求解一组多变量线性方程，对所述多变量线性方程求解寻找出所述补偿力矩δUk的最小补偿力矩值，即为最优补偿力矩，按照所述最优补偿力矩执行对当前控制力矩的补偿。进一步的，构建的所述最优问题为：其中，ci为第i个约束条件且ci≤0，为第i个约束条件对时间的导数，xk的姿态参数，所述姿态参数包括欧拉角、角速率，k为执行步数，i为约束条件数，在z为以状态和控制力矩表示下的约束条件的导数，即，△t为控制间隔，uk1、uk2、uk3分别为Uk沿体坐标系的分量。将所述最优问题具体转换为求解以下一组多变量线性方程：其中，ωik为第k步时i轴上的角速度，为第k步时i轴上对应的角加速度，为待求解的常数，ωmax为约束条件允许的角速度上界。进一步的，求解所述多变量线性方程时，如果满足：对所有i都成立，那么解是和，为待求最优补偿力矩的解，对应场景ci≤0，ci为第k+1步姿态参数所对应的约束条件，判定为不需要补偿；如果对于，对应场景为ci＞0，求解线性方程得到所述补偿力矩和的解，根据是否满足判断解是否可行。一种存储有计算机程序的计算机可读存储介质，所述计算机程序执行时实现如上述的方法。与现有技术相比，本发明的优点在于：1、本发明通过由MPC控制器计算多个输入姿态参数、输出控制力矩的对应组合，生成训练数据集，对DNN模型利用MPC生成的训练数据来进行训练，使得DNN能够近似MPC的输出，最终由逼近于MPC的DNN模型实现航天器的姿态控制，可以大大提高DNN模型训练所需的时间，降低DNN训练的复杂程度，能够保持和MPC控制器相当的姿态控制性能的同时，还能够有效的减少控制过程中在线计算时间，大大提高航天器姿态的在线控制效率。2、本发明进一步对DNN输出的控制力矩采用最优力矩补偿方法进行补偿，使得即便存在逼近误差，DNN也能满足约束条件的要求，避免DNN的逼近误差引起的违反约束，能够在几乎不增加计算时间的前提下，保证神经网络输出下的系统状态总是满足约束条件。附图说明图1是本实施例基于深度神经网络逼近MPC的航天器姿态控制方法的实现流程示意图。图2是神经元与神经网络的原理示意图。图3是本实施例中DNN训练原理示意图。图4是在具体应用实施例中使用4层100个神经元/层的网络训练结果示意图。图5是在具体应用实施例中仿真得到的MPC的力矩输出和姿态轨迹示意图。图6是在具体应用实施例中仿真得到的DNN的力矩输出和姿态轨迹示意图。图7是在具体应用实施例中仿真得到的两种控制器的计算时间比较结果示意图。图8是在具体应用实施例中仿真得到的补偿前后的角速率结果示意图。具体实施方式以下结合说明书附图和具体优选的实施例对本发明作进一步描述，但并不因此而限制本发明的保护范围。如图1所示，本实施例基于深度神经网络逼近MPC的航天器姿态控制方法的步骤包括：步骤S01. 训练数据集生成：配置基于模型预测控制方法的MPC控制器，MPC控制器输入航天器的姿态参数、输出对航天器的期望控制力矩，姿态参数包括姿态误差或根据姿态误差转化得到的参数，姿态误差为航天器当前姿态参数值与期望姿态参数值之间的误差值，控制将多个输入姿态参数输入至MPC控制器中，得到对应多个期望控制力矩输出，由各输入的姿态参数、输出的期望控制力矩的对应组合构建形成训练数据集；步骤S02. DNN训练：构建基于深度神经网络的DNN模型，DNN模型的输入层输入航天器的姿态参数、输出层输出预期的控制力矩，使用步骤S01构建形成的训练数据集对DNN模型进行训练，以使得DNN模型逼近MPC控制器，即在相同的姿态输入下，神经网络的输出和模型预测控制算法所得的控制力矩一致，训练完成后得到逼近于MPC控制器的目标DNN模型；步骤S03. 在线姿态控制：使用步骤S02训练后得到的目标DNN模型对目标航天器进行Move-to-rest在线姿态控制，控制过程中获取航天器的实时姿态参数并输入至目标DNN模型中，将目标DNN模型输出的预期的控制力矩提供给航天器中姿态控制执行器。考虑到相对通常的姿态控制问题，Move-to-rest情形下所需的神经网络的输入维度更少，可以简化问题的复杂度，本实施例特定针对Move-to-rest场景下的姿态控制，在姿态控制中，通过先配置MPC控制器，由MPC控制器输入航天器的姿态参数、输出对航天器的控制力矩，同时控制由MPC控制器计算得到多个输入输出组合，生成DNN模型的训练数据，对DNN模型利用MPC生成的训练数据来进行训练，使得DNN能够近似MPC的输出，最终由逼近于MPC的DNN模型实现航天器的姿态控制。由于相比于MPC控制器，DNN在控制回路中占用的计算时间要少的多，同时DNN的训练数据集是利用MPC生成得到，训练数据集预先已由MPC得到了姿态和控制力矩之间的映射对应关系，因而本实施例上述控制方法可以大大提高DNN模型训练所需的时间、降低DNN训练的复杂程度，从而最终能够实现保持和MPC控制器相当的姿态控制性能的同时，还能够有效的减少控制过程中在线计算时间，大大提高航天器姿态的在线控制效率。本实施例进一步分析MPC、深度神经网络的特性，以及DNN逼近MPC的可行性：MPC的特点是在有限的预测长度下重复进行最优控制求解过程，最优控制问题的目标函数大多用二次型表示，并通过优化算法求得其最小值。在每个步骤中，在线收集当前状态并相应地获得一系列控制输出，其中只有第一个时刻的解被应用为当前期望的执行器力矩，伪函数可以写成如下形式：其中，U和x分别为三轴控制力矩和当前系统状态，xr为参考状态，MPC作为状态和力矩之间的映射的伪函数。进一步可以将一个常见的离散MPC问题表示为：其中Q和R为权重矩阵。上述问题的受限条件表示为： 其中，J表示目标函数，Uk表示第k步的控制力矩，U0为初始控制力矩，tk为第k步的时刻，k为执行步数，A、B分别表示动力学系统离散线性化的后转换矩阵和控制矩阵，G表示为等式约束，h表示为不等式约束，T为预测时域长度，N为采样节点数，xk为第k步的姿态参数，所述姿态参数包括欧拉角、角速率。MPC控制的目标即是寻找U0，…，UN-1以最小化J，同时在每一步只更新和使用U0。对于具有采样步长△t和采样数N的预测区间，二次目标函数J可以包含预测系统状态与参考状态的差异以及力矩。MPC中预测方程决定了推导xk+1的方式，其可以根据精度要求而变化。基于MPC实现姿态控制时，上述在第k步，xk包括姿态和角速度，而姿态可以表示为各种形式，包括但不限于四元数或欧拉角；限制条件通常是星载姿态执行器的饱和极限和三轴角速率的上限，这些限制条件在不满足时可能会影响姿态传感器的正常工作。如果姿态机动过程中不需要优先考虑功耗，也可以省略目标函数的力矩项。由于姿态运动学和动力学具有复杂性，使得在线求解基于MPC的姿态控制较为困难，特别是对于低功耗的星载计算机。人工神经网络，如BP网络和Hopfield网络，其具有优越的学习未知系统特征性能，单个神经元的结构如图2A所示。图2中输出yi具体是通过式计算得到，其中非线性激活函数f是神经元学习能力的核心。其中，yi表示第i个神经元的输出，f为非线性激活函数，xi表示第j个输入，ωij表示第i个神经元的第j个输入的权重值，θi表示第i个神经元的偏置值。相比于基于单个神经元或单层网络，深度神经网络能够近似并取代整个传统控制器，而不是仅仅替代其中的一部分。一个多层全连接前馈网络如图2B所示，其中隐藏层的数量以及每层上神经元的数量决定了该深度神经网络的能力。由普适近似定理可知，深度神经网络DNN中只要有一个隐含层和一个适当光滑的隐含层激活函数的多层前馈网络能够任意精确地逼近任意函数及其导数，因而可以使用DNN来近似MPC的输出，即使用DNN逼近MPC是可行的。为实现DNN逼近MPC，本实施例具体通过构建DNN，使得在使用训练好的DNN，输入所有必要状态量，网络可以获得期望力矩，即：比较式和可知两者具有相同的形式，基于此本实施例是先通过MPC生成大量的输入输出数据，由这些输入输出对形成DNN的训练数据集。由于神经网络的另一个基本特性是泛化能力，即使输入不包含在训练数据集中，神经网络也能获得预期的输出。因此，并不需要创建一个包含所有可能数据的无限集合，DNN只需要足够的数据就可以学习隐藏在数据深处的内在原理，最终目的即是使用DNN代替MPC，在保持MPC性能的同时提高计算效率。MPC控制器、DNN模型中是使用航天器的运动学和动力学方程来计算出输入姿态与输出控制力矩之间的映射关系，本实施例中采用的航天器的运动学和动力学方程的构建步骤为：通过先基于航天器的姿态参数构建初始运动学和动力学方程，然后使用姿态误差构建基于姿态误差的运动学和动力学方程，对基于姿态误差的运动学和动力学方程简化后形成最终的运动学和动力学方程，通过使用简化的运动学和动力学方程可以大量减少计算量。上述运动学和动力学方程的构建步骤具体包括：步骤S101. 初始航天器运动学和动力学方程构建。为了避免欧拉角表示的奇异性，使用四元数q表示航天器的姿态，所述四元数q为：其中，r为旋转欧拉轴，φ为旋转角度，qv=T为矢量部分，q0为标量部分，q1~ q3为矢量部分的分量表示；进一步还可以施加一个条件，使该四元数具有唯一表达。由于没有柔性部件的小型航天器可视为刚体，因此本实施例基于四元数q，不考虑执行器模型构建初始运动学和动力学方程为：其中，E3和I分别表示为单位矩阵和转动惯量，ω是物体坐标系相对于惯性坐标系的角速度，U是控制力矩矢量，操作符的定义如下：步骤S102. 构建基于姿态误差的运动学和动力学方程。在姿态机动场景中，定义姿态误差qe以及角速度误差ωe为： 其中，qT为期望姿态，ωT为期望角速度，为四元数乘法，为从期望的本体坐标系到当前本体坐标系的旋转矩阵；根据式的初始航天器运动学和动力学方程，基于所述姿态误差qe、角速度误差ωe构建得到基于姿态误差的运动学和动力学方程为：其中，qev为误差四元数的矢量部分，为，qe0为误差四元数的标量部分。步骤S103.运动学和动力学方程简化。对上述式构建的基于姿态误差的运动学和动力学方程，如果以qe,,ωe,ωT和作为输入和U作为输出，则必须通过以下方式生成我们的训练数据集：上式中输入总共有12个维度，会导致形成一个巨大的数据集，无论是使用MPC生成该数据集，还是后续利用该数据集来训练DNN，都需要花费大量的时间，也需要非常强大的计算机器来完成计算。鉴于此，本实施例进一步对步骤S102中构建的基于姿态误差的运动学和动力学方程进行简化，考虑到对于Move-to-rest的姿态控制问题，目标姿态是静态的，即角速度误差ωT和角加速度误差为零，本实施例将式中角速度误差ωT和角加速度误差简化为零，形成最终的运动学和动力学方程为：通过使用上述简化形成的运动学和动力学方程，可以将输入维数减少一半，大大降低输入数据量，从而大大减小最终所形成的训练数据集，进一步确保DNN训练的效率，显著提高其可行性。本实施例在Move-to-rest的情况下，DNN采用简化的运动学和动力学方程，可表示为： 本实施例具体的DNN的输入层有6个神经元，输出层有3个神经元。每次计算姿态误差并将其输入DNN后，输出层给出预期的控制力矩，该控制力矩应与相同状态下的MPC结果近似，即实现DNN逼近MPC输出。上式中的输入状态为姿态误差qe，本实施例中该姿态误差qe具体使用由姿态误差转换成的3个欧拉角，以便于生成数据集。上述姿态参数具体包括根据航天器的姿态误差转化得到的欧拉角以及角速度。本实施例步骤S01中控制将多个输入姿态参数输入至MPC控制器中，具体在预测姿态数据范围内选择多个均匀分布的姿态数据节点形成多组输入数据，对于每组输入数据，基于模型预测控制方法采用预测方程计算出每组输入姿态数据下的期望控制力矩输出，其中预测方程的目标函数中只考虑姿态误差、忽略能量部分，预测方程具体采用上述式、所示，即：其中，G表示为等式约束，h表示为不等式约束，T为预测时域长度，N为采样节点数，xk为欧拉角、角速率。通常情况下一个包含尽可能多场景的大数据集可以产生一个更强大的网络，进而可以在更大范围内获得良好的性能，但是训练过程会显著变慢，特别是对于一个多维系统，其规模随维数呈指数增长，因而需要对DNN模型训练时需要权衡速度和精度。由于计算机资源有限，本实施例只在部分允许范围内取欧拉角以减少数据量。对于DNN的输入数据，即使超出了训练数据集的输入范围，经过训练的网络也能够处理这种情况，若需要确保更高的精度，则可以在允许范围内创建一个更广的覆盖范围或更密集的取样点，但离线训练时间也会更长。本实施例中输入数据欧拉角、角速率取值具体如表1所述。本实施例中3维欧拉角具体从-60°,-58°,…,-2°,0°,2°,…,58°,60° ,3维角速度具体从-3°/s,-2.7°/s,…,-0.3°/s,0°/s,0.3°/s,…,2.7°/s,3°/s 中选择一个值，组成一个输入组合，共有613·213=2.10207*109组合，每个输入组合都有相应的输出，通过MPC计算出该输出。由于神经元的数量会影响DNN训练时间，DNN深度过深也会使得训练时更新前沿层的参数变得困难，本实施例考虑上述问题，采用如表2所示的网络结构，其中激活函数tanh最适合于输出有上下界的情况。训练时，必须预先设计MPC控制器以用于生成对应输入数据的预期输出值。本实施例步骤S01中配置MPC控制器时，在预测范围内选择多个均匀分布的节点，目标函数中只考虑姿态误差、忽略能量部分，并采用一阶保持器离散系统的运动学和动力学方程，形成MPC控制器的预测方程。本实施例配置的所有MPC参数如表3所示，具体在预测范围内选择10个均匀分布的节点，在如式的目标函数中，忽略能量部分，即R=0，只考虑姿态误差，欧拉角和角速度的权重矩阵分别为Q1和Q2，同时采用一阶保持器离散系统的运动学和动力学，形成MPC的预测方程，系统限制包括执行器输出力矩饱和角速度上下界，然后用SQP算法求解非线性MPC。本实施例步骤S2中使用步骤S01构建形成的训练数据集对所述DNN模型进行训练时，具体采用比例共轭梯度下降法，如图3所示，比例共轭梯度下降法为通过将DNN模型的拟合误差对所需的神经元参数进行求导，按照使得拟合误差减小的方向调节所神经元参数，直至模型误差收敛到满足预设要求。在DNN训练过程中，使用均方误差来评估近似程度，并整批反馈更新隐藏层参数。在整个训练数据集中，85%的数据用于训练网络，其余15%的数据输入训练网络以判断性能。如果连续10个时点MSE不再下降，则视为训练结束，即当采用比例共轭梯度下降法使得DNN模型收敛到满足预设要求时，且连续多次训练后DNN模型的性能参数不再减小，则判定训练完成，训练得到目标DNN模型。DNN模型训练所采用的参数具体如表4所示。本实施例进一步设置不同数据组合来训练DNN，为了检验数据集大小对DNN学习性能的影响，随机从2.10207*109种选取了4个不同的输入数据子集，这些子集的大小是递增的，不同数据集大小训练网络的MSE结果如表5所示，从表5也可以看出，使用更大的数据集可以有助于实现更好的训练效果。在具体应用实施例中使用4层100个神经元/层进行网络训练，即DNN模型具体采用4层隐含层、每层具有100个神经元的结构，单个神经元的输入输出关系满足如式所示，结果图4所示，图中MSE先是快速下降，然后逐渐减缓，最后接近最低水平，即尽管不同网络的最终MSE精度略有差异，但所有的训练案例实际上都具有相似的MSE演化趋势，即总体性能是相当的。本实施例具体选取4层100个神经元/层的网络构建DNN。从上述DNN训练结果可以看出，即便精度再高，DNN与MPC的输出总会存在不可避免的逼近误差。而在实际的姿态控制问题中总会存在约束条件，MPC能够保证约束条件得到满足，但是逼近误差的存在使得DNN无法再确保这一点。即由于近似误差的存在，利用上述控制方法时约束条件有时可能会无法满足，针对该问题，本实施例步骤S03中预期的控制力矩后进一步还包括最优力矩补偿步骤，使得即便存在逼近误差，DNN也能满足约束条件的要求，以保证神经网络输出下的系统状态总是满足约束条件。本实施例中最优力矩补偿步骤包括：对控制力矩Uk施加一个补偿力矩δUk，以使得航天器的状态满足约束条件，经过补偿后的力矩为，其中δ表示为预设的调整系数，根据补偿目的构建关于补偿力矩δUk的多个线性不等式约束条件，基于线性不等式约束条件构建最优问题，并将最优问题转换为求解一组多变量线性方程，对多变量线性方程求解寻找出补偿力矩δUk的最小补偿力矩值，即为最优补偿力矩，按照该最优补偿力矩执行对当前控制力矩的补偿。具体可将补偿力矩后形成的最终输出力矩作为控制指令输出到姿态执行器，实现卫星的姿态控制。具体的，由于近似误差，神经网络的输出可能由于不够精确而使得系统状态无法严格满足约束条件，如将DNN作为控制器时，角速率可能会超过应有的限制，为此本实施例设计补偿转矩δU，δUk的被定义为Uk的基础上一个微小的修正，并期望其能使系统状态始终在允许的范围内。本实施例中，构建最优问题的详细步骤为：构建p个不等式约束，在当前第k步和之后的第k+1步，约束条件为c，则第k+1步的约束可近似表示为:ci；i=1，…，p其中，k为执行步数，i为约束条件数,第i个约束条件对时间的导数，xk为第k步的状态，△t为控制间隔，如果ci≥0，补偿目的是保证；根据动力学方程，定义，得到：其中，z为以状态和控制表示下的约束条件的导数。根据，得到：其中，为补偿后约束条件的导数；最终得到关于δUk的线性不等式约束条件为：其中，uk1、uk2、uk3为控制力矩Uk沿体坐标系的分量。针对于式，可以找到无数的δUk满足c≤0，而在所有可行解中，具有范数最小的是最合适的解，可以纠正对约束条件的违反，同时对原系统的额外影响也最小，即保持性能尽可能接近MPC控制器。基于上述构建的线性不等式约束条件构建得到最优问题具体为： 其中，ωik为第k步时i轴上的角速度，为第k步时i轴上对应的角加速，为待求解的常数，ωmax为约束条件允许的角速度上界。在以深度神经网络为控制时，上述最优问题的解可以保证约束条件ci≤0始终得到满足。对于姿态控制问题，约束条件的具体形式为:上述式具有非线性的目标函数和线性的约束，如果采用一般的非线性规划算法，将占用大量额外的计算资源，据此，本实施例基于Karush-Kuhn-Tucker 条件的解决方案求解上述最优问题，详细的求解过程为：由库恩-塔克定理可知，对于一个非线性规划问题，它受限于不等式约束gi≤0，以及等式约束hj=0，本实施例定义对应的拉格朗日函数： 设x*为函数f的局部最小解，上式成立的必要条件是存在λ*和μ*，使得满足：进一步定义两个集合：I1={i:gi=0}和I2={i:gi＜0}，同时二阶充分条为：对于空间：上的任何向量d，存在λ*和μ*，使得满足：上述*表示最优解。对于上式，是正定的，因此自动满足充分条件，此外式中不包含等式约束，因此只需要处理剩下的必要条件，本实施例中约束条件是，转换为约束：，则可以得到：将所有已知的表达式代入式、，则拉格朗日函数具有如下的形式： 根据上述式、，则可以使用以下方程来获得最优补偿力矩:即将最优问题转换为求解上述式的一组多变量线性方程。要求解上式，可以根据是否等于零来讨论不同的情况。。本实施例中，求解多 变量线性方程时，如果对所有i＝1,2,3都成 立，那么解是和对应场景ci≤0，判定为不需要补偿；如 果对于对应场景为ci＞0，ci为第k+1步姿态参数所对应的约束 条件，求解线性方程得到和根据是否满足判断解是否可行。通过上述步骤，即可以找到最小补偿力矩，基于该最小补偿力矩对控制力矩进行补偿，即可以避免违反约束，使得系统满足约束。上述式关于和是线性的，由于符号的不确定性，如果不等式的数 量是p需要解这个方程最多2p次，但总体计算效率仍然远远高于MPC和DNN，即该求解 所需消耗的时间在控制过程中是可忽略的，同时对于上述，其相应的KKT条件始终是线性的，使得还可以灵活应用于角速率极限之外的其他约束中。本实施例上述通过构建一个最优补偿问题来寻找最小补偿力矩，并最终将原最优问题转化为对不同的线性方程组的求解，不仅可以防止系统状态违反约束条件，同时该补偿方法的耗时非常少，可以忽略不计，因而在几乎不增加计算时间的前提下，保证控制过程中可以满足约束条件。在具体应用实施例中，上述步骤S01、S02可以采用离线执行方式，即离线训练神经网络。首先生成训练集：选取输入的姿态数据的范围，从中选取足够数量的姿态数据组成输入数据集，利用模型预测控制方法，计算出不同姿态数据下对应的输出力矩，由输入的姿态及其对应的输出力矩的集合构成训练集；然后设计一个深度神经网络，利用数据集训练该神经网络，使得在相同的姿态输入下，神经网络的输出和模型预测控制算法所得的控制力矩一致。上述步骤S03则采用在线工作方式，即在线使用训练后的DNN模型执行实时控制。首先将当前航天器的姿态数据输入到DNN模型中，得到神经网络此时的输出力矩；然后针对具体的姿态控制问题中所面对的约束条件，求解所需要的补偿力矩；将补偿力矩和神经网络的输出力矩结合，得到此时期望的姿态执行器的输出力矩作用于航天器的姿态执行器，实现姿态控制。为验证本发明上述DNN逼近MPC的效果，给定所选的训练DNN，分别使用MPC控制器和DNN作为控制器进行Move-to-rest姿态机动仿真，系统参数如表3所示，其中已经给出了其转动惯量和约束条件。采用MPC控制器时，虽然性能较好，但由于在线效率低，实用性较差；采用DNN作为控制器时，由MPC作为训练数据的来源，DNN从中学习原始的MPC输出，使得逼近MPC的输出，通过仿真比较两种控制器的控制精度和计算时间。航天器的初始姿态如表6所示，表5所有状态变量都定义为当前姿态与期望姿态之间的误差。由于是考虑Move-to-rest的姿态问题，角速率误差等于当前实际的角速率，所列的欧拉角定义为3-2-1旋转序列。通过仿真，本发明采用逼近MPC的DNN作为控制器可以实现使姿态误差为0，控制回路采样时间为0.1s。仿真中采用MPC控制器得到的力矩输出和姿态轨迹如图5所示，采用基于图5的MPC输入输出作为训练数据集，得到的DNN控制器的力矩输出和姿态轨迹如图6所示，观察图5、图6可知：首先，两个控制器都在29s左右实现收敛，使姿态误差全部趋近于零，除了基本的控制目标外，MPC和DNN产生了非常相似的姿态轨迹，即表明DNN成功地学习了MPC的内在逻辑。例如，当角速度达到上限3°/s时，DNN自动调整其输出以间接稳定角速度，这与MPC控制器的行为相同。其次，近似误差确实会有影响。比较力矩输出可以发现更明显的差异，如在0横轴附近10s - 30s范围内，MPC的输出u2和u3几乎是平的，而DNN的u2和u3则是波动的。此外，MPC的输出在某些点有一些急转弯，类似于Bang-Bang控制器。相反，DNN则倾向于产生更平滑的拐点，如果训练数据集包含这些转折点的输入数据，则可以改善这种情况。从上述来看，DNN就像一个平滑的插值函数，但DNN与插值的最大区别在于，DNN不需要在非常有限的在线存储中保存庞大的数据集。本实施例进一步验证DNN相对于MPC在计算时间上的优势，在同一台式机上，记录了控制器每一步所消耗的时间，如图7所示，图7中 DNN持续消耗0.013s，而MPC至少消耗了0.055s，甚至消耗达0.45s，由此可知，使用DNN控制器可以显著改进计算时间问题，减少在线计算负担，即本发明通过采用DNN作为控制器能够取得非常近似于MPC的控制性能，而所需消耗的时间又明显更少。当采用DNN逼近于MPC时仍然会有很小的近似误差，从图6中可以看出角速度ω2略微超过了极限并达到-3.04°/s，这违反了约束||ωi||≤3°/s，通过对图6进行动力学方程的 分析，可以推断该误差是由上述转矩在零轴附近的波动引起的，这也即是不可避免的近似误 差。本实施例进一步验证本发明上述最优力矩补偿方法的效果。为了突出违反约束条件的程 度，并进一步验证DNN的泛化能力，仿真时将初始条件ψ从-60°提高到-120°，并保持其 余设置不变，这使得输入数据超出了训练数据集范围，因而必然会导致更大的近似误差，DNN 会设法使系统收敛，但会导致精度显著下降。采用本发明上述最优力矩补偿方法后，得到的 补偿前后的角速率结果如图8所示，其中左图对应为补偿前，右图对应为补偿后。从图8可 知，由于近似误差，在对DNN输出施加任何补偿之前，ω1达到3.16°/s，ω2达到-3.17°/s， 通过以可忽略的时间为代价求解线性方程，补偿后的DNN可以全程保持||ωi||≤3°/s， 即表明本发明最优力矩补偿方法能够有效避免DNN的逼近误差引起的违反约束，使得在几 乎不增加计算时间的前提下，保证满足约束条件。在另一实施例中，本发明存储有计算机程序的计算机可读存储介质，计算机程序执行时实现上述方法。上述只是本发明的较佳实施例，并非对本发明作任何形式上的限制。虽然本发明已以较佳实施例揭露如上，然而并非用以限定本发明。因此，凡是未脱离本发明技术方案的内容，依据本发明技术实质对以上实施例所做的任何简单修改、等同变化及修饰，均应落在本发明技术方案保护的范围内。
