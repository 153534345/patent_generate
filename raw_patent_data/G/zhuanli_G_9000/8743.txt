标题title
一种基于模仿学习的智能路由决策保护方法和装置
摘要abst
本发明提出一种基于模仿学习的智能路由决策保护方法和装置。其中方法包括：步骤S1、基于软件定义网络SDN中各个网络节点的流量状态矩阵，利用Q网络算法对目标智能体进行预训练；具体包括：步骤S1.1、获取第一Q网络的网络结构，构建具有第一Q网络的网络结构的第二Q网络，第一Q网络为Q网络算法中当前状态的Q目标网络，第二Q网络为Q网络算法中构建的Q预测网络；步骤S1.2、基于第一Q网络的Q值和第二Q网络的Q值计算Q值损失函数，当Q值损失函数具有最小值时，预训练过程结束，目标智能体具有将流量以最优策略进行下发的决策能力；步骤S2、利用经预训练的目标智能体执行流量下发任务。
权利要求书clms
1.一种基于模仿学习的智能路由决策保护方法，其特征在于，所述方法包括：步骤S1、基于软件定义网络SDN中各个网络节点的流量状态矩阵，利用Q网络算法对目标智能体进行预训练，所述目标智能体为所述软件定义网络SDN中用于将流量下发至所述各个网络节点的控制器；所述预训练具体包括：步骤S1.1、获取第一Q网络的网络结构，构建具有所述第一Q网络的网络结构的第二Q网络，所述第一Q网络为所述Q网络算法中当前状态的Q目标网络，所述第二Q网络为所述Q网络算法中构建的Q预测网络；步骤S1.2、基于所述第一Q网络的Q值和所述第二Q网络的Q值计算Q值损失函数，当所述Q值损失函数具有最小值时，所述预训练过程结束，所述目标智能体具有将所述流量以最优策略进行下发的决策能力；步骤S2、利用经预训练的目标智能体执行流量下发任务。2.根据权利要求1所述的一种基于模仿学习的智能路由决策保护方法，其特征在于，在所述步骤S1中：将所述软件定义网络SDN中各个网络节点在状态转换过程产生的数据存储在经验回放缓冲区中，以基于经验回放机制将所述经验回放缓冲区中的数据作为所述流量状态矩阵，以随机采样的方式执行批处理的预训练过程；对于所述第一Q网络，利用不断最小化的所述Q值损失函数通过迭代更新所述第一Q网络的网络参数，对于所述第二Q网络，以固定时间间隔获取所述第一Q网络的网络参数，以延时更新的方式更新所述第二Q网络的网络参数。3.根据权利要求2所述的一种基于模仿学习的智能路由决策保护方法，其特征在于，在所述步骤S1.2中：所述第一Q网络的Q值和所述第二Q网络的Q值利用如下方式来确定：当用于接收下发的流量的节点接收到所述下发的流量时，将所述第一Q网络的Q值和所述第二Q网络的Q值置为1；当所述用于接收下发的流量的节点在超过第一阈值的时间后仍未接收到所述下发的流量时，首先将所述第一Q网络的Q值和所述第二Q网络的Q值置为0；进一步判断所述用于接收下发的流量的节点所属的交换机是否连接在线，若是，则维持所述第一Q网络的Q值和所述第二Q网络的Q值为0不变，若否，则将所述第一Q网络的Q值和所述第二Q网络的Q值置为-1；在基于所述第一Q网络的Q值和所述第二Q网络的Q值计算所述Q值损失函数时，限制选取所述第一Q网络的Q值和所述第二Q网络的Q值为-1的情况，以减少所述交换机未连接在线的情况下的无效训练探索。4.根据权利要求3所述的一种基于模仿学习的智能路由决策保护方法，其特征在于，在所述步骤S2中，利用经预训练的目标智能体执行流量下发任务时，同步收集由所述经预训练的目标智能体下发的流量的轨迹数据，作为真实状态策略下的序列轨迹数据，所述真实状态策略下的序列轨迹数据用于训练Gail判别器网络；所述方法还包括：步骤S3、在利用经预训练的目标智能体执行流量下发任务的过程中检测到恶意流量攻击时，利用经训练的Gail判别器网络生成替代决策，所述经预训练的目标智能体以所述替代决策执行后续的流量下发任务。5.根据权利要求4所述的一种基于模仿学习的智能路由决策保护方法，其特征在于，训练所述Gail判别器网络包括：利用未预训练的目标智能体执行所述流量下发任务，同步收集由所述未预训练的目标智能体下发的流量的轨迹数据，作为生成的序列轨迹数据；将所述真实状态策略下的序列轨迹数据和所述生成的序列轨迹数据作为所述Gail判别器网络的输入，通过迭代训练使得所述生成的序列轨迹数据和所述真实状态策略下的序列轨迹数据的判别差距小于阈值。6.根据权利要求5所述的一种基于模仿学习的智能路由决策保护方法，其特征在于，在所述步骤S3中，利用如下方式检测所述恶意流量攻击：获取经预训练的目标智能体执行流量下发任务时的当前节点的流量状态信息，将所述当前节点的流量状态信息与所述真实状态策略下的序列轨迹数据中的流量状态信息进行对比，当超过50%的流量状态存在异常时，判定所述当前节点受到所述恶意流量攻击。7.根据权利要求6所述的一种基于模仿学习的智能路由决策保护方法，其特征在于，在所述步骤S3中，当检测到所述恶意流量攻击时，将受到所述恶意流量攻击的所述当前节点的流量状态信息作为所述经训练的Gail判别器网络的输入，所述经训练的Gail判别器网络通过判别输出所述替代决策。8.一种基于模仿学习的智能路由决策保护装置，其特征在于，所述装置包括：第一处理单元，被控制为，基于软件定义网络SDN中各个网络节点的流量状态矩阵，利用Q网络算法对目标智能体进行预训练，所述目标智能体为所述软件定义网络SDN中用于将流量下发至所述各个网络节点的控制器；所述预训练具体包括：获取第一Q网络的网络结构，构建具有所述第一Q网络的网络结构的第二Q网络，所述第一Q网络为所述Q网络算法中当前状态的Q目标网络，所述第二Q网络为所述Q网络算法中构建的Q预测网络；基于所述第一Q网络的Q值和所述第二Q网络的Q值计算Q值损失函数，当所述Q值损失函数具有最小值时，所述预训练过程结束，所述目标智能体具有将所述流量以最优策略进行下发的决策能力；第二处理单元，被控制为，利用经预训练的目标智能体执行流量下发任务；其中，利用经预训练的目标智能体执行流量下发任务时，同步收集由所述经预训练的目标智能体下发的流量的轨迹数据，作为真实状态策略下的序列轨迹数据，所述真实状态策略下的序列轨迹数据用于训练Gail判别器网络；第三处理单元，被控制为，在利用经预训练的目标智能体执行流量下发任务的过程中检测到恶意流量攻击时，利用经训练的Gail判别器网络生成替代决策，所述经预训练的目标智能体以所述替代决策执行后续的流量下发任务。9.一种电子设备，其特征在于，所述电子设备包括存储器和处理器，所述存储器存储有计算机程序，所述处理器执行所述计算机程序时，实现权利要求1至7中任一项所述的一种基于模仿学习的智能路由决策保护方法中的步骤。10.一种计算机可读存储介质，其特征在于，所述计算机可读存储介质上存储有计算机程序，所述计算机程序被处理器执行时，实现权利要求1至7中任一项所述的一种基于模仿学习的智能路由决策保护方法中的步骤。
说明书desc
技术领域本发明属于智能路由安全技术领域，尤其涉及一种基于模仿学习的智能路由决策保护方法和装置。背景技术近年来，随着互联网技术的不断发展，使网络系统连接对象变得海量化、连接关系复杂化，传统基于人工配置的路由决策方法导致在有限时间内无法配置出最优的路由决策，促使研究人员将人工智能算法引入到智能路由决策过程中。随着强化学习的快速发展和应用，强化学习已在机器人控制、计算机视觉、无人驾驶等领域被广泛使用，而深度强化学习结合了深度学习的感知能力和强化学习的决策能力来优化深度强化学习策略，将深度强化学习算法与路由决策结合，可在很大程度上解决网络传输中丢包率高，网络分配流量不均以及资源利用率不高等问题。已有研究表明，通过恶意状态输入导致的恶意流量攻击，将会改变训练集中的数据，从而使路由决策发生改变，令智能体动作选取失误，最终使智能体达不到学习目的。发明内容目前，互联网应用越来越广泛，随着网络路由决策也面临着越来越大的挑战。而现有的路由协议无法做到采用一定量的带宽传输流量，采用最短路径算法会增加丢包率和恶化资源利用率，还常常导致网络上的流量分布不平衡。此外，真实网络节点容易受到恶意流量的攻击，最后导致路由决策出错。总之，路由决策易受到丢包率高以及流量分布不均等影响，同时还易受到恶意流量的状态输入攻击。针对上述问题，本发明提出一种基于模仿学习的智能路由决策保护方案，以提高模型训练的鲁棒性以及路由决策效率。具体地，该方案是在强化学习训练过程遇到链路过载与恶意流量攻击的场景下的一种策略优化方案，其基于生成式对抗模仿学习来生成训练策略，从而利用生成的序列数据对模型进行优化，达到策略优化目的，进而提高模型的鲁棒性。本发明第一方面公开了一种基于模仿学习的智能路由决策保护方法，所述方法包括：步骤S1、基于软件定义网络SDN中各个网络节点的流量状态矩阵，利用Q网络算法对目标智能体进行预训练，所述目标智能体为所述软件定义网络SDN中用于将流量下发至所述各个网络节点的控制器；所述预训练具体包括：步骤S1.1、获取第一Q网络的网络结构，构建具有所述第一Q网络的网络结构的第二Q网络，所述第一Q网络为所述Q网络算法中当前状态的Q目标网络，所述第二Q网络为所述Q网络算法中构建的Q预测网络；步骤S1.2、基于所述第一Q网络的Q值和所述第二Q网络的Q值计算Q值损失函数，当所述Q值损失函数具有最小值时，所述预训练过程结束，所述目标智能体具有将所述流量以最优策略进行下发的决策能力；步骤S2、利用经预训练的目标智能体执行流量下发任务。根据本发明第一方面的方法，在所述步骤S1中：将所述软件定义网络SDN中各个网络节点在状态转换过程产生的数据存储在经验回放缓冲区中，以基于经验回放机制将所述经验回放缓冲区中的数据作为所述流量状态矩阵，以随机采样的方式执行批处理的预训练过程；对于所述第一Q网络，利用不断最小化的所述Q值损失函数通过迭代更新所述第一Q网络的网络参数，对于所述第二Q网络，以固定时间间隔获取所述第一Q网络的网络参数，以延时更新的方式更新所述第二Q网络的网络参数。根据本发明第一方面的方法，在所述步骤S1.2中：所述第一Q网络的Q值和所述第二Q网络的Q值利用如下方式来确定：当用于接收下发的流量的节点接收到所述下发的流量时，将所述第一Q网络的Q值和所述第二Q网络的Q值置为1；当所述用于接收下发的流量的节点在超过第一阈值的时间后仍未接收到所述下发的流量时，首先将所述第一Q网络的Q值和所述第二Q网络的Q值置为0；进一步判断所述用于接收下发的流量的节点所属的交换机是否连接在线，若是，则维持所述第一Q网络的Q值和所述第二Q网络的Q值为0不变，若否，则将所述第一Q网络的Q值和所述第二Q网络的Q值置为-1；在基于所述第一Q网络的Q值和所述第二Q网络的Q值计算所述Q值损失函数时，限制选取所述第一Q网络的Q值和所述第二Q网络的Q值为-1的情况，以减少所述交换机未连接在线的情况下的无效训练探索。根据本发明第一方面的方法，在所述步骤S2中，利用经预训练的目标智能体执行流量下发任务时，同步收集由所述经预训练的目标智能体下发的流量的轨迹数据，作为真实状态策略下的序列轨迹数据，所述真实状态策略下的序列轨迹数据用于训练Gail判别器网络；所述方法还包括：步骤S3、在利用经预训练的目标智能体执行流量下发任务的过程中检测到恶意流量攻击时，利用经训练的Gail判别器网络生成替代决策，所述经预训练的目标智能体以所述替代决策执行后续的流量下发任务。根据本发明第一方面的方法，训练所述Gail判别器网络包括：利用未预训练的目标智能体执行所述流量下发任务，同步收集由所述未预训练的目标智能体下发的流量的轨迹数据，作为生成的序列轨迹数据；将所述真实状态策略下的序列轨迹数据和所述生成的序列轨迹数据作为所述Gail判别器网络的输入，通过迭代训练使得所述生成的序列轨迹数据和所述真实状态策略下的序列轨迹数据的判别差距小于阈值。根据本发明第一方面的方法，在所述步骤S3中，利用如下方式检测所述恶意流量攻击：获取经预训练的目标智能体执行流量下发任务时的当前节点的流量状态信息，将所述当前节点的流量状态信息与所述真实状态策略下的序列轨迹数据中的流量状态信息进行对比，当超过50%的流量状态存在异常时，判定所述当前节点受到所述恶意流量攻击。根据本发明第一方面的方法，在所述步骤S3中，当检测到所述恶意流量攻击时，将受到所述恶意流量攻击的所述当前节点的流量状态信息作为所述经训练的Gail判别器网络的输入，所述经训练的Gail判别器网络通过判别输出所述替代决策。本发明第二方面公开了一种基于模仿学习的智能路由决策保护装置。所述装置包括：第一处理单元，被控制为，基于软件定义网络SDN中各个网络节点的流量状态矩阵，利用Q网络算法对目标智能体进行预训练，所述目标智能体为所述软件定义网络SDN中用于将流量下发至所述各个网络节点的控制器；所述预训练具体包括：获取第一Q网络的网络结构，构建具有所述第一Q网络的网络结构的第二Q网络，所述第一Q网络为所述Q网络算法中当前状态的Q目标网络，所述第二Q网络为所述Q网络算法中构建的Q预测网络；基于所述第一Q网络的Q值和所述第二Q网络的Q值计算Q值损失函数，当所述Q值损失函数具有最小值时，所述预训练过程结束，所述目标智能体具有将所述流量以最优策略进行下发的决策能力；第二处理单元，被控制为，利用经预训练的目标智能体执行流量下发任务；其中，利用经预训练的目标智能体执行流量下发任务时，同步收集由所述经预训练的目标智能体下发的流量的轨迹数据，作为真实状态策略下的序列轨迹数据，所述真实状态策略下的序列轨迹数据用于训练Gail判别器网络；第三处理单元，被控制为，在利用经预训练的目标智能体执行流量下发任务的过程中检测到恶意流量攻击时，利用经训练的Gail判别器网络生成替代决策，所述经预训练的目标智能体以所述替代决策执行后续的流量下发任务。根据本发明第二方面的装置，所述第一处理单元具体被控制为：将所述软件定义网络SDN中各个网络节点在状态转换过程产生的数据存储在经验回放缓冲区中，以基于经验回放机制将所述经验回放缓冲区中的数据作为所述流量状态矩阵，以随机采样的方式执行批处理的预训练过程；对于所述第一Q网络，利用不断最小化的所述Q值损失函数通过迭代更新所述第一Q网络的网络参数，对于所述第二Q网络，以固定时间间隔获取所述第一Q网络的网络参数，以延时更新的方式更新所述第二Q网络的网络参数。根据本发明第二方面的装置，所述第一处理单元具体被控制为，利用如下方式来确定所述第一Q网络的Q值和所述第二Q网络的Q值：当用于接收下发的流量的节点接收到所述下发的流量时，将所述第一Q网络的Q值和所述第二Q网络的Q值置为1；当所述用于接收下发的流量的节点在超过第一阈值的时间后仍未接收到所述下发的流量时，首先将所述第一Q网络的Q值和所述第二Q网络的Q值置为0；进一步判断所述用于接收下发的流量的节点所属的交换机是否连接在线，若是，则维持所述第一Q网络的Q值和所述第二Q网络的Q值为0不变，若否，则将所述第一Q网络的Q值和所述第二Q网络的Q值置为-1；在基于所述第一Q网络的Q值和所述第二Q网络的Q值计算所述Q值损失函数时，限制选取所述第一Q网络的Q值和所述第二Q网络的Q值为-1的情况，以减少所述交换机未连接在线的情况下的无效训练探索。根据本发明第二方面的装置，训练所述Gail判别器网络包括：利用未预训练的目标智能体执行所述流量下发任务，同步收集由所述未预训练的目标智能体下发的流量的轨迹数据，作为生成的序列轨迹数据；将所述真实状态策略下的序列轨迹数据和所述生成的序列轨迹数据作为所述Gail判别器网络的输入，通过迭代训练使得所述生成的序列轨迹数据和所述真实状态策略下的序列轨迹数据的判别差距小于阈值。根据本发明第二方面的装置，所述第三控制单元具体被配置为，利用如下方式检测所述恶意流量攻击：获取经预训练的目标智能体执行流量下发任务时的当前节点的流量状态信息，将所述当前节点的流量状态信息与所述真实状态策略下的序列轨迹数据中的流量状态信息进行对比，当超过50%的流量状态存在异常时，判定所述当前节点受到所述恶意流量攻击。根据本发明第二方面的装置，所述第三控制单元具体被配置为：当检测到所述恶意流量攻击时，将受到所述恶意流量攻击的所述当前节点的流量状态信息作为所述经训练的Gail判别器网络的输入，所述经训练的Gail判别器网络通过判别输出所述替代决策。本发明第三方面公开了一种电子设备。所述电子设备包括存储器和处理器，所述存储器存储有计算机程序，所述处理器执行所述计算机程序时，实现本发明第一方面所述的一种基于模仿学习的智能路由决策保护方法中的步骤。本发明第四方面公开了一种计算机可读存储介质。所述计算机可读存储介质上存储有计算机程序，所述计算机程序被处理器执行时，实现本发明第一方面所述的一种基于模仿学习的智能路由决策保护方法中的步骤。综上，本发明的目的在于保护基于深度强化学习的路由决策模型不受链路过载或者恶意流量攻击的影响。技术方案包括：首先在流量传输过程中遇到链路过载时，优化后的DQN模型能通过当前节点状态输入后输出的和值来获得此刻网络节点是否拥堵的关键信息，并通过贪婪探索获取最佳的路由决策路径；在流量传输过程中遇恶意流量攻击时，GAIL模型可根据当前输入的节点状态输出一个更贴近真实策略的动作，将该动作作为接下来的路由下发动作，从而阻断了恶意流量的传输。采用本发明既提高了路由决策路径探索的效率，也优化了模型的路由决策，从而提高了整体模型的鲁棒性，进而达到保护智能路由决策模型的目的。附图说明为了更清楚地说明本发明具体实施方式或现有技术中的技术方案下面将对具体实施方式或现有技术描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图是本发明的一些实施方式，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。图1为根据本发明实施例的一种基于模仿学习的智能路由决策保护方法的流程图；图2为根据本发明实施例的DQN算法结构的示意图；图3为根据本发明实施例的基于Gail训练生成替换决策的示意图；图4为根据本发明实施例的针对链路过载和恶意攻击的路由决策保护的流程图；图5为根据本发明实施例的一种基于模仿学习的智能路由决策保护装置的结构图；图6为根据本发明实施例的一种电子设备的结构图。具体实施方式为使本发明实施例的目的、技术方案和优点更加清楚，下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例只是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有做出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。本发明第一方面公开了一种基于模仿学习的智能路由决策保护方法。图1为根据本发明实施例的一种基于模仿学习的智能路由决策保护方法的流程图；如图1所示，所述方法包括：步骤S1、基于软件定义网络SDN中各个网络节点的流量状态矩阵，利用Q网络算法对目标智能体进行预训练，所述目标智能体为所述软件定义网络SDN中用于将流量下发至所述各个网络节点的控制器；所述预训练具体包括：步骤S1.1、获取第一Q网络的网络结构，构建具有所述第一Q网络的网络结构的第二Q网络，所述第一Q网络为所述Q网络算法中当前状态的Q目标网络，所述第二Q网络为所述Q网络算法中构建的Q预测网络；步骤S1.2、基于所述第一Q网络的Q值和所述第二Q网络的Q值计算Q值损失函数，当所述Q值损失函数具有最小值时，所述预训练过程结束，所述目标智能体具有将所述流量以最优策略进行下发的决策能力；步骤S2、利用经预训练的目标智能体执行流量下发任务。在步骤S1中，基于软件定义网络SDN中各个网络节点的流量状态矩阵，利用Q网络算法对目标智能体进行预训练，所述目标智能体为所述软件定义网络SDN中用于将流量下发至所述各个网络节点的控制器。在一些实施例中，所述预训练具体包括：步骤S1.1、获取第一Q网络的网络结构，构建具有所述第一Q网络的网络结构的第二Q网络，所述第一Q网络为所述Q网络算法中当前状态的Q目标网络，所述第二Q网络为所述Q网络算法中构建的Q预测网络；步骤S1.2、基于所述第一Q网络的Q值和所述第二Q网络的Q值计算Q值损失函数，当所述Q值损失函数具有最小值时，所述预训练过程结束，所述目标智能体具有将所述流量以最优策略进行下发的决策能力具体地，对目标智能体进行预训练包括：基于强化学习中的深度Q网络算法训练软件定义网络SDN中的控制器，控制器的目标是将流量正确地、完整地、无延时地下发至各个网络节点，DQN将Q学习与卷积神经网络相结合，构建强化学习训练模型。构建基于OpenFlow协议的软件定义网络SDN拓扑流量模型；基于深度Q网络算法训练目标智能体，此处的智能体是指软件定义网络SDN中的控制器；训练过程中，源节点为初始状态，动作是指从当前节点发送流量至交换机存储在经验回放缓冲区Buff中，作为网络模型的训练数据集；从缓冲区Buff中采样N个训练数据集，通过最小化当前Q网络的预测Q值和目标Q网络的目标Q值的均方差来更新当前Q网络的网络参数，每隔一段时间将当前Q网络的相关参数复制给目标Q网络；在计算和时，若当前链路出现拥塞，和的值都将设为0；若未出现拥堵，则设为1；若交换机没有互相连接，则将和都设为-1。以此来确定当前网络连接状态；在随机动作探索的过程中，执行贪婪探索策略，并对所执行的动作进行限制，限制在某T时刻下可选择的动作为该状态下和值均大于0的动作，以此减少出现下一步动作与当前状态交换机无连接的无效探索，从而在更快找出全局最优动作的同时加快收敛速度。在一些实施例中，DQN通过结合深度神经网络与强化学习的Q学习算法，不仅解决了状态空间过大难以维护的问题，而且由于神经网络强大的特征提取能力，其潜力也远大于人工的特征表示。强化学习中的Q学习通过贝尔曼方程，采用时序差分的方式进行迭代更新状态-动作价值函数Q：其中，为目标Q值，是作为动作出现的下一状态，是状态下的可能动作。为学习率，为折现因子。根据贝尔曼最优方程理论，只要通过不断迭代更新上式，即可使Q函数逼近至真实值Q*，从而最终得到最优策略：在一些实施例中，DQN还使用了目标网络机制，即在当前网络结构基础上，搭建了一个结构完全相同的目标网络组成DQN的整体模型框架，训练过程中，当前网络输出的预测Q值用来选择动作a,另一个目标网络用于计算目标Q值。通过计算预测Q值和目标Q值得均方差来定义损失函数：其中，为目标Q值，通过神经网络的反向梯度传播来更新当前网络的参数。在一些实施例中，在所述步骤S1中，将所述软件定义网络SDN中各个网络节点在状态转换过程产生的数据存储在经验回放缓冲区中，以基于经验回放机制将所述经验回放缓冲区中的数据作为所述流量状态矩阵，以随机采样的方式执行批处理的预训练过程。具体地，训练过程中DQN采用了经验回放机制，图2为根据本发明实施例的DQN算法结构的示意图；如图2所示，将状态转换过程存储在经验回放缓冲区Buff中，作为网络模型的训练数据集，并以随机采样的形式进行批处理学习。从Buff中采样N个训练数据集，通过最小化损失函数来更新当前网络的网络参数，对于目标网络，其网络参数不需要进行迭代更新，而是每隔一段时间从当前网络中将网络参数复制过来，即延时更新，再进行下一轮的学习。这种方法减轻了每次Q值变化对策略参数的影响，即减少了目标Q值与预测Q值之间的相关性，增加了策略训练的稳定性。在一些实施例中，在所述步骤S1.2中：所述第一Q网络的Q值和所述第二Q网络的Q值利用如下方式来确定：当用于接收下发的流量的节点接收到所述下发的流量时，将所述第一Q网络的Q值和所述第二Q网络的Q值置为1；当所述用于接收下发的流量的节点在超过第一阈值的时间后仍未接收到所述下发的流量时，首先将所述第一Q网络的Q值和所述第二Q网络的Q值置为0；进一步判断所述用于接收下发的流量的节点所属的交换机是否连接在线，若是，则维持所述第一Q网络的Q值和所述第二Q网络的Q值为0不变，若否，则将所述第一Q网络的Q值和所述第二Q网络的Q值置为-1。具体地，在计算和时，若当前链路出现拥塞，和的值都将设为0；若未出现拥堵，则设为1；若交换机没有互相连接，则将和都设为-1。以此来确定当前网络连接状态。在一些实施例中，在所述步骤S1.2中：在基于所述第一Q网络的Q值和所述第二Q网络的Q值计算所述Q值损失函数时，限制选取所述第一Q网络的Q值和所述第二Q网络的Q值为-1的情况，以减少所述交换机未连接在线的情况下的无效训练探索。具体地，在所述步骤S1中，对于所述第一Q网络，利用不断最小化的所述Q值损失函数通过迭代更新所述第一Q网络的网络参数，对于所述第二Q网络，以固定时间间隔获取所述第一Q网络的网络参数，以延时更新的方式更新所述第二Q网络的网络参数。在随机动作探索的过程中，执行贪婪探索策略，并对所执行的动作进行限制，限制在某T时刻下可选择的动作为该状态下和值均大于0的动作，即表明此刻网络节点出现拥堵或无连接，以此减少出现下一步动作与当前状态交换机无连接的无效探索。在步骤S2，利用经预训练的目标智能体执行流量下发任务。在一些实施例中，在所述步骤S2中，利用经预训练的目标智能体执行流量下发任务时，同步收集由所述经预训练的目标智能体下发的流量的轨迹数据，作为真实状态策略下的序列轨迹数据，所述真实状态策略下的序列轨迹数据用于训练Gail判别器网络。具体地，收集真实状态策略序列轨迹数据包括，根据深度强度学习预训练模型的策略生成多组T+1个时刻状态动作对轨迹，并取其中获得正奖励的状态动作对作为真实状态策略序列轨迹数据用于与生成的序列轨迹数据进行判别比较，用于判别是否出现恶意流量传输。在一些实施例中，训练所述Gail判别器网络包括：利用未预训练的目标智能体执行所述流量下发任务，同步收集由所述未预训练的目标智能体下发的流量的轨迹数据，作为生成的序列轨迹数据；将所述真实状态策略下的序列轨迹数据和所述生成的序列轨迹数据作为所述Gail判别器网络的输入，通过迭代训练使得所述生成的序列轨迹数据和所述真实状态策略下的序列轨迹数据的判别差距小于阈值。在一些实施例中，训练GAIL的目的旨在当出现恶意流量传输时，利用强化学习的方法来训练GAIL，将GAIL的动作输出来替换上一时刻的动作输出。将预训练好模型经筛选后生成的真实状态策略序列轨迹数据进行拼接，并将拼接后的状态动作置标签1；同时将现生成的序列轨迹数据也进行拼接，并将拼接后的状态动作置标签0。将拼接好的两类数据输入GAIL中的判别器网络进行训练，不断优化真实状态策略序列轨迹数据的损失函数和现生成序列轨迹数据的损失函数，直至判别器网络训练完毕。当判别器网络训练完毕后，将下一个时刻的状态输入GAIL中的actor网络，actor网络由三层线性层组成，actor网络会输出所有动作的概率，选择概率最大的值对应的动作作为最终的动作，并以此替代受到恶意流量攻击后SDN控制器的下一步动作。具体地，根据深度强强化学习预训练模型DRL的策略生成多个T+1个时刻控制器传输流量的序列状态动作对轨迹作为真实状态策略序列轨迹数据用于与生成的序列轨迹数据进行判别比较，如图3所示，图3为根据本发明实施例的基于Gail训练生成替换决策的示意图。基于GAIL训练生成序列状态策略动作,GAIL中使用生成器actor网络来生成序列数据，判别器D用于评估生成的序列与真实的序列之间的差距，从而引导生成器的训练，同时利用强化学习算法来计算奖励值以评估序列数据的生成，其训练过程步骤如下：输入expert trajectories专家样本轨迹；初始化判别器D和actor网络；在每次迭代过程中：利用actor网络生成状态动作轨迹；更新判别器参数，增加，减少，其中表示T时刻内状态动作的分布。判别器D的目的是使生成数据与目标数据分布相接近，其中判别器的目标是使两者的分布最小化，目标函数可表示为：其中，Y表示样本序列数据，Pdata为来自样本序列数据的分布，第一项中的表示判别器对真实序列数据即的判断，第二项则表示对生成的序列数据即的判断。通过对目标函数进行梯度反向求导来更新判别器的参数；训练过程中，使用判别器来评估生成的序列数据，来指导生成器的训练，通过策略梯度更新的方法对生成器的参数进行更新：其中，表示第h步相应的学习率。更新actor网络参数，增加，同时不断更新参数,，为学习率。将actor网络生成的状态动作轨迹输入到判别器网络D中，通过判别器的输出P来判定是否无限接近专家样本。若判别器输出P为1，表明判别器已训练完毕；若P输出为0，则重复上述步骤。所述方法还包括：步骤S3、在利用经预训练的目标智能体执行流量下发任务的过程中检测到恶意流量攻击时，利用经训练的Gail判别器网络生成替代决策，所述经预训练的目标智能体以所述替代决策执行后续的流量下发任务。在一些实施例中，在所述步骤S3中，利用如下方式检测所述恶意流量攻击：获取经预训练的目标智能体执行流量下发任务时的当前节点的流量状态信息，将所述当前节点的流量状态信息与所述真实状态策略下的序列轨迹数据中的流量状态信息进行对比，当超过50%的流量状态存在异常时，判定所述当前节点受到所述恶意流量攻击。在一些实施例中，在所述步骤S3中，当检测到所述恶意流量攻击时，将受到所述恶意流量攻击的所述当前节点的流量状态信息作为所述经训练的Gail判别器网络的输入，所述经训练的Gail判别器网络通过判别输出所述替代决策。图4为根据本发明实施例的针对链路过载和恶意攻击的路由决策保护的流程图；如图4所示，SDN中的控制器在训练过程中，流量传输需首先经过链路信息收集模块，随后开始判断链路是否过载；若链路过载，则将当前的节点信息输入优化后的DQN模型，根据DQN模型输出的Q值大小获取此刻节点是否过载或者封闭等信息，随后开始判断网络拓扑是否存在恶意流量；若同一节点状态信息和输出的对应动作矩阵与真实状态策略轨迹中的动作矩阵存在超过半数的差异，则认为此节点传输的流量为恶意流量，若差异低于50%，则直接进行正常的路由下发；当发现恶意流量存在时，将此刻的节点状态输入至事先训练的GAIL网络，将GAIL网络输出的动作作为控制器将要采取的动作。可见，SDN控制器在基于深度强化学习的训练过程中，遇到链路过载的同时也可能受到恶意流量的攻击，链路过载会使控制器无法在短时间内选择出一条最佳的路由决策路径，而恶意流量的攻击，则将直接导致控制器将流量往错误的节点传输，最终导致整个网络出现故障甚至瘫痪。基于上述情况，当出现链路过载时，先利用优化后的DQN模型对当前节点状态信息进行判断，根据模型输出的和是否大于0来当前网络节点是否出现拥堵，利用贪婪探索策略减少对未与交换机相连的节点进行探索，从而更快得出一条最佳的路由决策路径；与此同时，若发现遭受恶意流量攻击，则将当前的节点状态信息输入预训练好的GAIL网络，将GAIL网络的输出作为最终控制器下发的动作。DRL模型和GAIL模型的引入，既提高了路由决策路径探索的效率也优化了模型的决策，从而提高了整体模型的鲁棒性。本发明第二方面公开了一种基于模仿学习的智能路由决策保护装置。图5为根据本发明实施例的一种基于模仿学习的智能路由决策保护装置的结构图；如图5所示，所述装置500包括：第一处理单元501，被控制为，基于软件定义网络SDN中各个网络节点的流量状态矩阵，利用Q网络算法对目标智能体进行预训练，所述目标智能体为所述软件定义网络SDN中用于将流量下发至所述各个网络节点的控制器；所述预训练具体包括：获取第一Q网络的网络结构，构建具有所述第一Q网络的网络结构的第二Q网络，所述第一Q网络为所述Q网络算法中当前状态的Q目标网络，所述第二Q网络为所述Q网络算法中构建的Q预测网络；基于所述第一Q网络的Q值和所述第二Q网络的Q值计算Q值损失函数，当所述Q值损失函数具有最小值时，所述预训练过程结束，所述目标智能体具有将所述流量以最优策略进行下发的决策能力；第二处理单元503，被控制为，利用经预训练的目标智能体执行流量下发任务；其中，利用经预训练的目标智能体执行流量下发任务时，同步收集由所述经预训练的目标智能体下发的流量的轨迹数据，作为真实状态策略下的序列轨迹数据，所述真实状态策略下的序列轨迹数据用于训练Gail判别器网络；第三处理单元503，被控制为，在利用经预训练的目标智能体执行流量下发任务的过程中检测到恶意流量攻击时，利用经训练的Gail判别器网络生成替代决策，所述经预训练的目标智能体以所述替代决策执行后续的流量下发任务。根据本发明第二方面的装置，所述第一处理单元501具体被控制为：将所述软件定义网络SDN中各个网络节点在状态转换过程产生的数据存储在经验回放缓冲区中，以基于经验回放机制将所述经验回放缓冲区中的数据作为所述流量状态矩阵，以随机采样的方式执行批处理的预训练过程；对于所述第一Q网络，利用不断最小化的所述Q值损失函数通过迭代更新所述第一Q网络的网络参数，对于所述第二Q网络，以固定时间间隔获取所述第一Q网络的网络参数，以延时更新的方式更新所述第二Q网络的网络参数。根据本发明第二方面的装置，所述第一处理单元501具体被控制为，利用如下方式来确定所述第一Q网络的Q值和所述第二Q网络的Q值：当用于接收下发的流量的节点接收到所述下发的流量时，将所述第一Q网络的Q值和所述第二Q网络的Q值置为1；当所述用于接收下发的流量的节点在超过第一阈值的时间后仍未接收到所述下发的流量时，首先将所述第一Q网络的Q值和所述第二Q网络的Q值置为0；进一步判断所述用于接收下发的流量的节点所属的交换机是否连接在线，若是，则维持所述第一Q网络的Q值和所述第二Q网络的Q值为0不变，若否，则将所述第一Q网络的Q值和所述第二Q网络的Q值置为-1；在基于所述第一Q网络的Q值和所述第二Q网络的Q值计算所述Q值损失函数时，限制选取所述第一Q网络的Q值和所述第二Q网络的Q值为-1的情况，以减少所述交换机未连接在线的情况下的无效训练探索。根据本发明第二方面的装置，训练所述Gail判别器网络包括：利用未预训练的目标智能体执行所述流量下发任务，同步收集由所述未预训练的目标智能体下发的流量的轨迹数据，作为生成的序列轨迹数据；将所述真实状态策略下的序列轨迹数据和所述生成的序列轨迹数据作为所述Gail判别器网络的输入，通过迭代训练使得所述生成的序列轨迹数据和所述真实状态策略下的序列轨迹数据的判别差距小于阈值。根据本发明第二方面的装置，所述第三控制单元503具体被配置为，利用如下方式检测所述恶意流量攻击：获取经预训练的目标智能体执行流量下发任务时的当前节点的流量状态信息，将所述当前节点的流量状态信息与所述真实状态策略下的序列轨迹数据中的流量状态信息进行对比，当超过50%的流量状态存在异常时，判定所述当前节点受到所述恶意流量攻击。根据本发明第二方面的装置，所述第三控制单元503具体被配置为：当检测到所述恶意流量攻击时，将受到所述恶意流量攻击的所述当前节点的流量状态信息作为所述经训练的Gail判别器网络的输入，所述经训练的Gail判别器网络通过判别输出所述替代决策。本发明第三方面公开了一种电子设备。所述电子设备包括存储器和处理器，所述存储器存储有计算机程序，所述处理器执行所述计算机程序时，实现本发明第一方面所述的一种基于模仿学习的智能路由决策保护方法中的步骤。图6为根据本发明实施例的一种电子设备的结构图；如图6所示，电子设备包括通过系统总线连接的处理器、存储器、通信接口、显示屏和输入装置。其中，该电子设备的处理器用于提供计算和控制能力。该电子设备的存储器包括非易失性存储介质、内存储器。该非易失性存储介质存储有操作系统和计算机程序。该内存储器为非易失性存储介质中的操作系统和计算机程序的运行提供环境。该电子设备的通信接口用于与外部的终端进行有线或无线方式的通信，无线方式可通过WIFI、运营商网络、近场通信或其他技术实现。该电子设备的显示屏可以是液晶显示屏或者电子墨水显示屏，该电子设备的输入装置可以是显示屏上覆盖的触摸层，也可以是电子设备外壳上设置的按键、轨迹球或触控板，还可以是外接的键盘、触控板或鼠标等。本领域技术人员可以理解，图6中示出的结构，仅仅是与本公开的技术方案相关的部分的结构图，并不构成对本申请方案所应用于其上的电子设备的限定，具体的电子设备可以包括比图中所示更多或更少的部件，或者组合某些部件，或者具有不同的部件布置。本发明第四方面公开了一种计算机可读存储介质。所述计算机可读存储介质上存储有计算机程序，所述计算机程序被处理器执行时，实现本发明第一方面所述的一种基于模仿学习的智能路由决策保护方法中的步骤。综上，本发明的目的在于保护基于深度强化学习的路由决策模型不受链路过载或者恶意流量攻击的影响。技术方案包括：首先在流量传输过程中遇到链路过载时，优化后的DQN模型能通过当前节点状态输入后输出的和值来获得此刻网络节点是否拥堵的关键信息，并通过贪婪探索获取最佳的路由决策路径；在流量传输过程中遇恶意流量攻击时，GAIL模型可根据当前输入的节点状态输出一个更贴近真实策略的动作，将该动作作为接下来的路由下发动作，从而阻断了恶意流量的传输。采用本发明既提高了路由决策路径探索的效率，也优化了模型的路由决策，从而提高了整体模型的鲁棒性，进而达到保护智能路由决策模型的目的。本发明的有益效果主要包括：通过根据DQN模型的和输出，来判断当前网络节点是否存在拥堵，再利用贪婪探索策略减少智能体对未与交换机相连的节点进行不必要探索，从而大大提高最佳路由决策路径的生成效率；在GAIL训练过程中，生成器用来生成序列状态和策略动作轨迹数据，判别器的真实数据输入是采样自预训练的DQN模型筛选后的序列状态动作对数据，训练过程中通过更新生成器和判别器的参数来生成更为贴近真实序列状态策略的数据；在流量传输过程中遇到链路过载时，优化后的DQN模型能通过当前节点状态输入后输出的和值来获得此刻网络节点是否拥堵的关键信息，并通过贪婪探索获取最佳的路由决策路径；在流量传输过程中糟遇恶意流量攻击时，GAIL模型的可根据当前输入的节点状态输出一个更贴近真实策略的动作，将该动作作为接下来的路由下发，从而阻断了恶意流量的传输。既提高了路由决策路径探索的效率，也优化了模型的决策，从而提高了整体模型的鲁棒性。请注意，以上实施例的各技术特征可以进行任意的组合，为使描述简洁，未对上述实施例中的各个技术特征所有可能的组合都进行描述，然而，只要这些技术特征的组合不存在矛盾，都应当认为是本说明书记载的范围。以上所述实施例仅表达了本申请的几种实施方式，其描述较为具体和详细，但并不能因此而理解为对发明专利范围的限制。应当指出的是，对于本领域的普通技术人员来说，在不脱离本申请构思的前提下，还可以做出若干变形和改进，这些都属于本申请的保护范围。因此，本申请专利的保护范围应以所附权利要求为准。
