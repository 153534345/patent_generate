标题title
一种基于边缘重建和密集融合网络的零件位姿计算方法
摘要abst
本发明提供一种基于边缘重建和密集融合网络的零件位姿计算方法，包括采用实例分割对零件场景的RGB图像进行图像分割；采用特征提取网络对分割后的RGB图像的边缘特征进行提取，得到特征图；采用边缘检测器输入H×W×C的特征图，输出单通道特征图H×W×1,单通道特征图随后可视化，得到重建的零件边缘图像；计算特征提取网络的权重，通过密集融合网络回归零件位姿；采用多任务学习策略，建立位姿计算与边缘重建之间的隐式连接，直接回归零件的6D位姿，实现零件位姿计算。本发明解决了对于低纹理甚至无纹理、具有反光表面的零件识别效果较差，难以在实际工业场景中实现高效的零件自动化分拣的问题。
权利要求书clms
1.一种基于边缘重建和密集融合网络的零件位姿计算方法，其特征在于，包括：获取零件场景的RGB图像；采用实例分割对零件场景的RGB图像进行图像分割；采用特征提取网络对分割后的RGB图像的边缘特征进行提取，得到特征图；采用边缘检测器输入H×W×C的特征图，输出单通道特征图H×W×1,单通道特征图随后由sigmoid激活函数可视化，得到重建的零件边缘图像；其中H为特征图的高，W为特征图的长，C为通道数；计算特征提取网络的权重，通过密集融合网络回归零件位姿；采用多任务学习策略，建立位姿计算与边缘重建之间的隐式连接，直接回归零件的6D位姿，实现基于边缘重建和密集融合网络的零件位姿计算。2.根据权利要求1所述的零件位姿计算方法，其特征在于，所述采用特征提取网络对分割后的RGB图像的边缘特征进行提取，得到特征图，包括：输入H×W×3的RGB图像，基于PSPnet通过图像卷积分别得到图像大小的特征图，对应特征维数分别为C1、C2、C3、C4；以图像大小的特征图作为瓶颈层，对瓶颈层进行双线性插值采样，分别得到图像大小的特征图，对应特征维数分别为C’3、C’2、C’1；采用跨层级连接方案，将相同尺寸的特征图特征维度相联接，结合低维具象特征和高维抽象特征，提取高表达性的图像特征，得到与原始图像同尺寸的特征图M，特征图大小为H×W×C0；C0为是特征图M的通道数。3.根据权利要求2所述的零件位姿计算方法，其特征在于，所述计算特征提取网络的权重，通过密集融合网络回归零件位姿，包括：获取零件场景的深度图像；将深度图像转换为点云，在图像掩码上采样N个像素点；输出特征图M提取图像特征，根据深度相机的成像原理寻找RGB图像和点云直接的对应关系，逐像素融合图像光流特征与点云几何特征；基于密集融合策略，每个像素点Ni得到一个变换矩阵和一个置信度ci，最终取最高置信度对应的变换矩阵结果为位姿计算结果。4.根据权利要求3所述的零件位姿计算方法，其特征在于，所述多任务学习策略包括：深度学习网络训练方式为端到端的训练，边缘重建和密集融合网络需要输入同一RGB图像，利用共享权值的特征提取网络建立两者之间的隐式连接；根据两个任务的不同需求定义各自的损失函数，对损失函数求和，指导整体网络训练。5.根据权利要求4所述的零件位姿计算方法，其特征在于，所述根据两个任务的不同需求定义各自的损失函数，对损失函数求和，指导整体网络训练，包括：构建边缘重建的损失函数：其中，β为非边缘部分在整张图像中占的百分比；Egt为用于监督的真值边缘图像中位于的像素值，当该像素位于图像物体边缘时为1，否则为0；Ex为输入图像上位于的像素值；对于非对称物体，构建密集融合网络的位姿计算损失函数：其中，xn为N个像素点中的第n个点；为真值位姿；为与n对应的像素i预测的位姿结果；对于对称物体，消除位姿的模糊性，构建损失函数：构建基于多任务学习的损失函数：Loss＝Lpose+μLedge；其中，Loss为基于多任务学习的损失函数；Lpose为位姿计算损失函数；Ledge为边缘重建损失函数；μ为具有平衡作用的超参数。
说明书desc
技术领域本发明属于计算机视觉技术领域，尤其涉及一种基于边缘重建和密集融合网络的零件位姿计算方法。背景技术计算机视觉技术在机器人非结构化场景感知中占据重要的地位。视觉图像是获取真实世界信息的有效手段，通过视觉感知算法提取对应任务的特征，如物体位置、角度、姿态等信息，从而使机器人能够执行对应操作，完成指定作业任务。对于工业机器人分拣而言，目前已经能够利用视觉传感器获取场景数据，但如何从场景中识别目标物体，并估计其位置和姿态，从而计算工业机器人的抓取位置和抓取路径则成为核心问题。随着深度学习技术的快速发展，基于深度学习的位姿估计技术已经成为位姿估计领域的主流算法。但现有的基于深度学习的主流位姿估计算法大都依赖于物体表面的颜色、纹理等信息，对于工业上低纹理甚至无纹理、具有反光表面的零件识别效果较差，难以在实际工业场景中实现高效的零件自动化分拣。发明内容本发明针对现有技术中的不足，提供一种基于边缘重建和密集融合网络的零件位姿计算方法。本发明提供一种基于边缘重建和密集融合网络的零件位姿计算方法，包括：获取零件场景的RGB图像；采用实例分割对零件场景的RGB图像进行图像分割；采用特征提取网络对分割后的RGB图像的边缘特征进行提取，得到特征图；采用边缘检测器输入H×W×C的特征图，输出单通道特征图H×W×1,单通道特征图随后由sigmoid激活函数可视化，得到重建的零件边缘图像；其中H为特征图的高，W为特征图的长，C为通道数；计算特征提取网络的权重，通过密集融合网络回归零件位姿；采用多任务学习策略，建立位姿计算与边缘重建之间的隐式连接，直接回归零件的6D位姿，实现基于边缘重建和密集融合网络的零件位姿计算。进一步地，所述采用特征提取网络对分割后的RGB图像的边缘特征进行提取，得到特征图，包括：输入H×W×3的RGB图像，基于PSPnet通过图像卷积分别得到图像大小的特征图，对应特征维数分别为C1、C2、C3、C4；以图像大小的特征图作为瓶颈层，对瓶颈层进行双线性插值采样，分别得到图像大小的特征图，对应特征维数分别为C’3、C’2、C’1；采用跨层级连接方案，将相同尺寸的特征图特征维度相联接，结合低维具象特征和高维抽象特征，提取高表达性的图像特征，得到与原始图像同尺寸的特征图M，特征图大小为H×W×C0；C0为是特征图M的通道数。进一步地，所述计算特征提取网络的权重，通过密集融合网络回归零件位姿，包括：获取零件场景的深度图像；将深度图像转换为点云，在图像掩码上采样N个像素点；输出特征图M提取图像特征，根据深度相机的成像原理寻找RGB图像和点云直接的对应关系，逐像素融合图像光流特征与点云几何特征；基于密集融合策略，每个像素点Ni得到一个变换矩阵和一个置信度ci，最终取最高置信度对应的变换矩阵结果为位姿计算结果。进一步地，所述多任务学习策略包括：深度学习网络训练方式为端到端的训练，边缘重建和密集融合网络需要输入同一RGB图像，利用共享权值的特征提取网络建立两者之间的隐式连接；根据两个任务的不同需求定义各自的损失函数，对损失函数求和，指导整体网络训练。进一步地，所述根据两个任务的不同需求定义各自的损失函数，对损失函数求和，指导整体网络训练，包括：构建边缘重建的损失函数：其中，β为非边缘部分在整张图像中占的百分比；Egt为用于监督的真值边缘图像中位于的像素值，当该像素位于图像物体边缘时为1，否则为0；Ex为输入图像上位于的像素值；对于非对称物体，构建密集融合网络的位姿计算损失函数：其中，xn为N个像素点中的第n个点；为真值位姿；为与n对应的像素i预测的位姿结果；对于对称物体，消除位姿的模糊性，构建损失函数：构建基于多任务学习的损失函数：Loss＝Lpose+μLedge；其中，Loss为基于多任务学习的损失函数；Lpose为位姿计算损失函数；Ledge为边缘重建损失函数；μ为具有平衡作用的超参数。本发明提供一种基于边缘重建和密集融合网络的零件位姿计算方法，包括获取零件场景的RGB图像；采用实例分割对零件场景的RGB图像进行图像分割；采用特征提取网络对分割后的RGB图像的边缘特征进行提取，得到特征图；采用边缘检测器输入H×W×C的特征图，输出单通道特征图H×W×1,单通道特征图随后由sigmoid激活函数可视化，得到重建的零件边缘图像；其中H为特征图的高，W为特征图的长，C为通道数；计算特征提取网络的权重，通过密集融合网络回归零件位姿；采用多任务学习策略，建立位姿计算与边缘重建之间的隐式连接，直接回归零件的6D位姿，实现基于边缘重建和密集融合网络的零件位姿计算。本发明采用上述方案，解决了对于工业上低纹理甚至无纹理、具有反光表面的零件识别效果较差，难以在实际工业场景中实现高效的零件自动化分拣的问题。附图说明为了更清楚地说明本发明的技术方案，下面将对实施例中所需要使用的附图作简单地介绍，显而易见地，对于本领域普通技术人员而言，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。图1为本发明实施例提供的一种基于边缘重建和密集融合网络的零件位姿计算方法的工作流程图；图2为本发明实施例提供的零件边缘重建结果示意图；图3为本发明实施例提供的跨层级连接方案的网络结构示意图；图4为本发明实施例提供的位姿计算的结果图；图5为本发明实施例提供的一种基于边缘重建和密集融合网络的零件位姿计算方法的结构示意图。具体实施方式下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整的描述，显然，所描述的实施例仅仅是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有作出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。如背景技术中所述现有的基于深度学习的主流位姿估计算法大都依赖于物体表面的颜色、纹理等信息，对于工业上低纹理甚至无纹理、具有反光表面的零件识别效果较差，难以在实际工业场景中实现高效的零件自动化分拣。因此，为了解决上述问题，本发明实施例部分提供了一种基于边缘重建和密集融合网络的零件位姿计算方法，如图5所示，图5为本发明提供的一种基于边缘重建和密集融合网络的零件位姿计算方法的结构示意图。具体的，如图1所示，本发明实施例部分提供一种基于边缘重建和密集融合网络的零件位姿计算方法，包括：步骤S101，获取零件场景的RGB图像。步骤S102，采用实例分割对零件场景的RGB图像进行图像分割。步骤S103，采用特征提取网络对分割后的RGB图像的边缘特征进行提取，得到特征图。本步骤中，如图2和图3所示，首先输入H×W×3分割后的RGB图像，长为W，高为H，通道数为3，基于PSPnet通过图像卷积分别得到图像大小的特征图，对应特征维数分别为C1＝64、C2＝64、C3＝128、C4＝512。然后以图像大小的特征图作为瓶颈层，对瓶颈层进行双线性插值采样，分别得到图像大小的特征图，对应特征维数分别为C’3＝1024、C’2＝256、C’1＝64；最后采用跨层级连接方案，将相同尺寸的特征图特征维度相联接，结合低维具象特征和高维抽象特征，增强特征图的表达性，提取高表达性的图像特征，得到与原始图像同尺寸的特征图M，特征图大小为H×W×C0，C0为是特征图M的通道数，C0＝32。步骤S104，采用边缘检测器输入H×W×C的特征图M，输出单通道特征图H×W×1,单通道特征图随后由sigmoid激活函数可视化，得到重建的零件边缘图像；其中H为特征图的高，W为特征图的长，C为通道数。步骤S105，计算特征提取网络的权重，通过密集融合网络回归零件位姿。本步骤中，首先获取零件场景的深度图像，将深度图像转换为点云，在图像掩码上采样N个像素点；然后输出特征图M提取图像特征，根据深度相机的成像原理寻找RGB图像和点云直接的对应关系，逐像素融合图像光流特征与点云几何特征；本是实施例中N＝500。基于密集融合策略，每个像素点Ni得到一个变换矩阵和一个置信度ci，最终取最高置信度对应的变换矩阵结果为位姿计算结果。权特征提取网络同时为两个分支使用，每一个分支是独立且权值共享的；其中一个分支为边缘重建服务，另一个分支为位姿计算服务，边缘重建任务引导特征提取网络关注边缘信息；位姿计算共享特征提取网络权重，因此，获得对零件纹理更鲁棒的特征图，然后通过密集融合网络回归零件位姿。步骤S106，采用多任务学习策略，建立位姿计算与边缘重建之间的隐式连接，直接回归零件的6D位姿，实现基于边缘重建和密集融合网络的零件位姿计算。本步骤中，深度学习网络训练方式为端到端的训练，边缘重建和密集融合网络需要输入同一RGB图像，利用共享权值的特征提取网络建立两者之间的隐式连接，需要定义损失函数，根据两个任务的不同需求定义各自的损失函数，对损失函数求和，指导整体网络训练。首先，构建边缘重建的损失函数，以带对数的二元交叉熵的形式呈现，具体为：其中，β为非边缘部分在整张图像中占的百分比；Egt为用于监督的真值边缘图像中位于的像素值，当该像素位于图像物体边缘时为1，否则为0；Ex为输入图像上位于的像素值。其次，构建密集融合网络的位姿计算损失函数，对于非对称物体，损失函数如下：其中，xn为N个像素点中的第n个点；为真值位姿；为与n对应的像素i预测的位姿结果。对于对称物体，为了消除位姿的模糊性，构建损失函数如下：最后，定义边缘重建和密集融合的损失函数后，同时考虑两个任务之间的平衡和性能，构建基于多任务学习的损失函数如下：Loss＝Lpose+μLedge；其中，Loss为基于多任务学习的损失函数；Lpose为位姿计算损失函数；Ledge为边缘重建损失函数；μ为具有平衡作用的超参数。至此，完成整个网络的训练。位姿计算结果可由网络直接回归位姿参数得到。如图4所示，图4展示了该实施例的位姿计算结果，识别的零件用边界框标注。以上结合具体实施方式和范例性实例对本发明进行了详细说明，不过这些说明并不能理解为对本发明的限制。本领域技术人员理解，在不偏离本发明精神和范围的情况下，可以对本发明技术方案及其实施方式进行多种等价替换、修饰或改进，这些均落入本发明的范围内。本发明的保护范围以所附权利要求为准。
