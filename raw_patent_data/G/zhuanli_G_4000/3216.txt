标题title
一种基于循环关联鲁棒学习的主题法条检索方法
摘要abst
本发明公开了一种基于循环关联鲁棒学习的主题法条检索方法，包括以下步骤：利用语言序列模型对主题‑法条成对训练数据进行编码，获得全局特征；构建基于全局特征的循环关联鲁棒学习范式，得到矫正标签；根据矫正标签计算训练批次的损失值，根据损失值使用反向梯度优化语言序列模型并更新模型参数，得到优化后的模型；向优化后的模型输入待检索的主题，计算与测试集中所有法条描述之间的语义相似度；根据计算的语义相似度进行相似性排序，获取法条检索结果，完成检索，本发明利用深度语言序列模型对自然语言描述的主题和法条进行嵌入并获取全局特征用于语义相似度计算，打破了主题‑法条数据间的语义鸿沟，实现跨域数据对的相似度计算。
权利要求书clms
1.一种基于循环关联鲁棒学习的主题法条检索方法，其特征在于，包括以下步骤：S1、利用语言序列模型对主题-法条成对训练数据进行编码，获得全局特征；S2、构建基于全局特征的循环关联鲁棒学习范式，得到矫正标签；S3、根据矫正标签计算训练批次的损失值，根据损失值使用反向梯度优化语言序列模型并更新模型参数，得到优化后的模型；S4、向优化后的模型输入待检索的主题，计算与测试集中所有法条描述之间的语义相似度；S5、根据计算的语义相似度，进行相似性排序，获取法条检索结果，完成检索。2.根据权利要求1所述的基于循环关联鲁棒学习的主题法条检索方法，其特征在于，所述步骤S1包括以下分步骤：S11、采用双向门控循环神经网络作为语言序列模型，并向语言序列模型输入主题-法条成对训练数据，得到双向GRU模型，其表达式为：其中，为输入文本序列第j个分词的嵌入向量，表示主题或者法条描述的分词，/＞和分别为前向GRU和反向GRU的隐藏层，/＞为前向GRU函数，/＞为反向GRU函数，/＞为前一时刻前向GRU的隐藏层，/＞为前一时刻反向GRU的隐藏层；S12、向双向GRU模型输入主题-法条成对训练文本数据获取相应的词级特征，其表达式为：对于一段本文，表示为：；其中，为文本的第j个分词特征，n为文本分词的个数，/＞为维度为d的特征空间；S13、用最大池化策略来将词级特征聚合得到主题或者法条描述的全局特征。3.根据权利要求2所述的基于循环关联鲁棒学习的主题法条检索方法，其特征在于，所述步骤S13中全局特征的表达式为：其中，MaxPooling为最大池化策略。4.根据权利要求3所述的基于循环关联鲁棒学习的主题法条检索方法，其特征在于，所述步骤S2包括以下分步骤：S21、根据全局特征，计算任意主题和法条之间的语义相似度；S22、计算主题匹配到法条的概率和法条匹配到主题的概率，其表达式为：其中，为主题匹配到法条的概率，/＞为法条匹配到主题的概率，exp为以自然常数e为底的指数函数，/＞为一个训练批次中的法条集合，/＞为一个训练批次中的主题集合，/＞为温度系数；/＞为训练批次中的第i个主题，/＞为训练批次中的第k个主题，/＞为训练批次中的第j个法条，/＞为训练集中的第k个法条，k为计数参数，K为训练批次中成对训练数据总数；S23、根据主题匹配到法条的概率和法条匹配到主题的概率计算主题-法条双向匹配的平均概率，其表达式为：其中，为主题-法条双向匹配的概率；S24、使用动量矫正对双向匹配的平均概率进行关联矫正，并通过自循环细化器细化关联矫正，完成循环关联鲁棒学习范式，得到矫正标签。5.根据权利要求4所述的基于循环关联鲁棒学习的主题法条检索方法，其特征在于，所述步骤S21中计算任意主题和法条之间的语义相似度的表达式为：其中，S为相似度函数，为第i个主题，/＞为第j个法条描述，/＞为第i个主题/＞的全局特征，/＞为第j个法条描述/＞的全局特征，/＞表示矩阵的转置。6.根据权利要求5所述的基于循环关联鲁棒学习的主题法条检索方法，其特征在于，所述步骤S24中矫正标签的计算方法为：其中，为矫正标签，若为1则表示训练对为干净对，反之为0则为噪声对，/＞为第t轮第i个训练对的矫正标签，/＞为第t-1轮第i个训练对的矫正标签，/＞为动量参数。7.根据权利要求6所述的基于循环关联鲁棒学习的主题法条检索方法，其特征在于，所述步骤S3包括以下分步骤：S31、计算针对第i对主题-法条对的正学习损失函数和负学习损失函数/＞；/＞S32、根据正学习损失函数和负学习损失，计算针对第i对主题法条的鲁棒跨模态对比损失，其表达式为：S33、根据，计算一个大小为K训练批次的损失值/＞，并根据该损失值使用反向梯度优化语言序列模型并更新模型参数，得到优化后的模型，/＞的表达式为：；反向优化算法的表达式为：其中为优化后的模型参数，/＞为优化前的模型参数，/＞为反向梯度优化器。8.根据权利要求7所述的基于循环关联鲁棒学习的主题法条检索方法，其特征在于，所述步骤S31中第i对主题-法条对的正学习损失函数和负学习损失函数/＞的计算方法为：。/＞
说明书desc
技术领域本发明涉及跨域检索技术，具体涉及一种基于循环关联学习的主题法条检索方法。背景技术现有基于主题检索法条大都基于关键词、目录章节、检索规则等精准检索方法，无法通过一些自然语言描述的主题来智能化检索相应内容的法条。因此，可以将自然语言描述的主题与相应法条当作跨域数据集，并训练深度语言序列模型来构建深度检索模型，实现基于语义的主题-法条跨域检索。与传统的精准定位检索方法相比，深度跨域检索提供了主题-法条上的语义一致性检索，让检索结果在语义上更为合理，并且很容易提供大量具有强语义性的检索候选项。在主题法条跨域检索任务中，主题-法条跨域数据集的构建成本往往是巨大的，这要求大量专业人员进行人工构建，同时也可能会由于人为疏忽引入不成对的主题-法条训练数据对，这些训练对之间的语义关联性较弱或者错误，即噪声关联。然而，现有跨域检索方法几乎都隐式要求跨域训练数据对具有强成对关联性，这些噪声关联会严重影响模型性能。为此，主题法条检索方法面临的主要问题集中在两个方面：如何有效、准确地对主题法条跨域数据集建模，准确衡量自然语言描述的主题与法条之间的语义相似度。如何减小噪声关联问题对于主题法条跨域检索模型的影响，从而实现鲁棒的语义关联学习。发明内容针对现有技术中的上述不足，本发明提供的一种基于循环关联鲁棒学习的主题法条检索方法解决了现有跨域/模态检索模型训练时，数据集中噪声关联引起的语义相似度衡量不准确，进而导致检索精度下降问题。为了达到上述发明目的，本发明采用的技术方案为：提供一种基于循环关联鲁棒学习的主题法条检索方法，包括以下步骤：S1、利用语言序列模型对主题-法条成对训练数据进行编码，获得全局特征；S2、构建基于全局特征的循环关联鲁棒学习范式，得到矫正标签；S3、根据矫正标签计算训练批次的损失值，根据损失值使用反向梯度优化语言序列模型并更新模型参数，得到优化后的模型；S4、向优化后的模型输入待检索的主题，计算与测试集中所有法条描述之间的语义相似度；S5、根据计算的语义相似度，进行相似性排序，获取法条检索结果，完成检索。进一步地：所述步骤S1包括以下分步骤：S11、采用双向门控循环神经网络作为语言序列模型，并向语言序列模型输入主题-法条成对训练数据，得到双向GRU模型，其表达式为：其中，为输入文本序列第j个分词的嵌入向量，表示主题或者法条描述的分词，和/＞分别为前向GRU和反向GRU的隐藏层，/＞为前向GRU函数，/＞为反向GRU函数，为前一时刻前向GRU的隐藏层，/＞为前一时刻反向GRU的隐藏层；S12、向双向GRU模型输入主题-法条成对训练文本数据获取相应的词级特征，其表达式为：对于一段本文，表示为：；其中，为文本的第j个分词特征，n为文本分词的个数，/＞为维度为d的特征空间；S13、用最大池化策略来将词级特征聚合得到主题或者法条描述的全局特征。进一步地：所述步骤S13中全局特征的表达式为：其中，MaxPooling为最大池化策略。上述进一步方案的有益效果为：利用深度语言序列模型对自然语言描述的主题和法条进行嵌入并获取全局特征用于语义相似度计算，打破了主题-法条数据间的语义鸿沟。进一步地：所述步骤S2包括以下分步骤：S21、根据全局特征，计算任意主题和法条之间的语义相似度；S22、计算主题匹配到法条的概率和法条匹配到主题的概率，其表达式为：其中，为主题匹配到法条的概率，/＞为法条匹配到主题的概率，exp为以自然常数e为底的指数函数，/＞为一个训练批次中的法条集合，/＞为一个训练批次中的主题集合，/＞为温度系数；/＞为训练批次中的第i个主题，/＞为训练批次中的第k个主题，/＞为训练批次中的第j个法条，/＞为训练集中的第k个法条，k为计数参数，K为训练批次中成对训练数据总数；S23、根据主题匹配到法条的概率和法条匹配到主题的概率计算主题-法条双向匹配的平均概率，其表达式为：其中，为主题-法条双向匹配的概率；S24、使用动量矫正对双向匹配的平均概率进行关联矫正，并通过自循环细化器细化关联矫正，完成循环关联鲁棒学习范式，得到矫正标签。进一步地：所述步骤S21中计算任意主题和法条之间的语义相似度的表达式为：其中，S为相似度函数，为第i个主题，/＞为第j个法条描述，/＞为第i个主题的全局特征，/＞为第j个法条描述/＞的全局特征，/＞表示矩阵的转置。进一步地：所述步骤S24中矫正标签的计算方法为：其中，为矫正标签，若为1则表示训练对为干净对，反之为0则为噪声对，/＞为第t轮第i个训练对的矫正标签，/＞为第t-1轮第i个训练对的矫正标签，/＞为动量参数。上述进一步方案的有益效果为：通过动量矫正，获取更为准确的成对关联矫正，避免硬性的关联划分；通过自循环细化器，细化关联矫正，并且传递矫正的关联标签作为细化过程的先验知识，避免敏感的训练转折点选择。进一步地：所述步骤S3包括以下分步骤：S31、计算针对第i对主题-法条对的正学习损失函数和负学习损失函数/＞；S32、根据正学习损失函数和负学习损失，计算针对第i对主题法条的鲁棒跨模态对比损失，其表达式为：S33、根据，计算一个大小为K训练批次的损失值/＞，并根据该损失值使用反向梯度优化语言序列模型并更新模型参数，得到优化后的模型，/＞的表达式为：；反向优化算法的表达式为：其中为优化后的模型参数，/＞为优化前的模型参数，/＞为反向梯度优化器。进一步地：所述步骤S31中第i对主题-法条对的正学习损失函数和负学习损失函数/＞的计算方法为：。上述进一步方案的有益效果为：使用正学习损失和负学习损失一起进行优化，在能够挖掘良好的语义关联的同时，还能缓解噪声对带来的负面影响。本发明的有益效果为：1、利用深度语言序列模型对自然语言描述的主题和法条进行嵌入并获取全局特征用于语义相似度计算，打破了主题-法条数据间的语义鸿沟，实现准确跨域数据对的相似度计算；2、构建了用于主题法条检索的循环关联鲁棒学习模型，通过动量矫正，获取更为准确的成对关联矫正，避免硬性的关联划分，并结合自循环矫正器，细化了动量矫正的噪声关联，并且传递矫正的关联标签作为细化过程的先验知识，避免敏感的训练转折点选择；3、设计了一种用于主题法条深度检索模型的鲁棒的跨模态对比损失，该损失平衡了正学习和负学习损失，有效挖掘潜在的语义相关性的同时，缓解了训练过程中噪声关联的影响，实现鲁棒的深度检索模型训练。附图说明图1为本发明所述的主题法条检索方法的流程图。具体实施方式下面对本发明的具体实施方式进行描述，以便于本技术领域的技术人员理解本发明，但应该清楚，本发明不限于具体实施方式的范围，对本技术领域的普通技术人员来讲，只要各种变化在所附的权利要求限定和确定的本发明的精神和范围内，这些变化是显而易见的，一切利用本发明构思的发明创造均在保护之列。如图1所示，在本发明的一个实施例中，提供了一种基于循环关联鲁棒学习的主题法条检索方法，包括以下步骤：S1、利用语言序列模型对主题-法条成对训练数据进行编码，获得全局特征；S2、构建基于全局特征的循环关联鲁棒学习范式，得到矫正标签；S3、根据矫正标签计算训练批次的损失值，根据损失值使用反向梯度优化语言序列模型并更新模型参数，得到优化后的模型；S4、向优化后的模型输入待检索的主题，计算与测试集中所有法条描述之间的语义相似度；S5、根据计算的语义相似度，进行相似性排序，获取法条检索结果，完成检索。在本实施例中，所述步骤S1包括以下分步骤：S11、采用双向门控循环神经网络作为语言序列模型，并向语言序列模型输入主题-法条成对训练数据，得到双向GRU模型，其表达式为：其中，为输入文本序列第j个分词的嵌入向量，表示主题或者法条描述的分词，和/＞分别为前向GRU和反向GRU的隐藏层，/＞为前向GRU函数，/＞为反向GRU函数，为前一时刻前向GRU的隐藏层，/＞为前一时刻反向GRU的隐藏层；S12、向双向GRU模型输入主题-法条成对训练文本数据获取相应的词级特征，其表达式为：对于一段本文，表示为：；其中，为文本的第j个分词特征，n为文本分词的个数，/＞为维度为d的特征空间；S13、用最大池化策略来将词级特征聚合得到主题或者法条描述的全局特征。所述步骤S13中全局特征的表达式为：其中，MaxPooling为最大池化策略。在本实施例中，所述步骤S2包括以下分步骤：S21、根据全局特征，计算任意主题和法条之间的语义相似度；所述步骤S21中计算任意主题和法条之间的语义相似度的表达式为：其中，S为相似度函数，为第i个主题，/＞为第j个法条描述，/＞为第i个主题的全局特征，/＞为第j个法条描述/＞的全局特征，/＞表示矩阵的转置。S22、计算主题匹配到法条的概率和法条匹配到主题的概率，其表达式为：其中，为主题匹配到法条的概率，/＞为法条匹配到主题的概率，exp为以自然常数e为底的指数函数，/＞为一个训练批次中的法条集合，/＞为一个训练批次中的主题集合，/＞为温度系数；/＞为训练批次中的第i个主题，/＞为训练批次中的第k个主题，/＞为训练批次中的第j个法条，/＞为训练集中的第k个法条，k为计数参数，K为训练批次中成对训练数据总数；S23、根据主题匹配到法条的概率和法条匹配到主题的概率计算主题-法条双向匹配的平均概率，其表达式为：其中，为主题-法条双向匹配的概率；S24、使用动量矫正对双向匹配的平均概率进行关联矫正，并通过自循环细化器细化关联矫正，完成循环关联鲁棒学习范式，得到矫正标签。所述步骤S24中矫正标签的计算方法为：其中，为矫正标签，若为1则表示训练对为干净对，反之为0则为噪声对，/＞为第t轮第i个训练对的矫正标签，/＞为第t-1轮第i个训练对的矫正标签，/＞为动量参数。动量矫正使得上述预测关联概率感知更大的样本场，并平滑演化，进而准确地矫正训练数据对关联；为了缓解扰动，自循环器将训练过程划分为多个子过程，每个子过程都将优化的关联标签传递给下一个子过程作为先验知识逐步提高矫正的质量并且这避免了对于训练转折点的人工选择。在本实施例中，所述步骤S3包括以下分步骤：S31、计算针对第i对主题-法条对的正学习损失函数和负学习损失函数/＞；所述步骤S31中第i对主题-法条对的正学习损失函数和负学习损失函数/＞的计算方法为：/＞；S32、根据正学习损失函数和负学习损失，计算针对第i对主题法条的鲁棒跨模态对比损失，其表达式为：S33、根据，计算一个大小为K训练批次的损失值/＞，并根据该损失值使用反向梯度优化语言序列模型并更新模型参数，得到优化后的模型，/＞的表达式为：；反向优化算法的表达式为：其中为优化后的模型参数，/＞为优化前的模型参数，/＞为反向梯度优化器。在本发明的描述中，需要理解的是，术语“中心”、“厚度”、“上”、“下”、“水平”、“顶”、“底”、“内”、“外”、“径向”等指示的方位或位置关系为基于附图所示的方位或位置关系，仅是为了便于描述本发明和简化描述，而不是指示或暗示所指的设备或元件必须具有特定的方位、以特定的方位构造和操作，因此不能理解为对本发明的限制。此外，术语“第一”、“第二”、“第三”仅用于描述目的，而不能理解为指示或暗示相对重要性或隐含指明的技术特征的数量。因此，限定由“第一”、“第二”、“第三”的特征可以明示或隐含地包括一个或者更多个该特征。本发明提供了一种基于循环关联鲁棒学习的主题法条检索方法，利用深度语言序列模型对自然语言描述的主题和法条进行嵌入并获取全局特征用于语义相似度计算，打破了主题-法条数据间的语义鸿沟，实现准确跨域数据对的相似度计算。
