标题title
一种联动智能注采装置的差异化实时注采优化调控方法
摘要abst
本发明提供了一种联动智能注采装置的差异化实时注采优化调控方法，属于油藏注采技术领域，具体包括以下步骤：初始化网络；搭建注采环境模型；智能体读取注采环境初始状态；注采策略网络写入文件；获得t+1时刻的状态数据；读取下一状态及该时间步内经济净现值；将下一状态赋给当前状态，直至完成待优化生产周期；通过批训练模型对环境代理网络参数进行更新；更新动作评价网络参数及注采策略网络参数；重复以上步骤，直至得到最优模型；根据保存的最优模型，与智能注采装置联动，获得油藏的状态信息。本发明的技术方案克服现有技术中的油藏注采优化方法，对已有信息利用率较低，需重复进行优化迭代，不能根据当前生产方案进行实时调整的问题。
权利要求书clms
1.一种联动智能注采装置的差异化实时注采优化调控方法，其特征在于，具体包括以下步骤：S1，初始化注采策略网络、动作评价网络及环境代理网络；S2，根据待优化的油藏数值模拟模型编写注采环境模型，注采环境模型包括：文件读取模块及制度写入模块，其中文件读取模块完成状态的读取功能，制度写入模块实现环境的状态转移；S3，智能体即注采策略网络和动作评价网络，读取注采环境初始状态，即读取时刻油水井注采量数据/＞；S4，将当前状态输入注采策略模型中，输出注采制度/＞，并将注采制度/＞写入油藏数值模拟模型中；S5，注采环境模型根据注采制度，运行油藏数值模拟模型，对油藏下一时间步的状态进行模拟，以获得t+1时刻的状态数据；S6，智能体读取下一状态及该时间步内经济净现值/＞，将{/＞，/＞，/＞，}放入样本数据库；S7，将下一状态赋给当前状态，即，再进行循环，直至完成待优化生产周期；S8，在样本数据库中进行数据采样，根据样本预测误差对样本赋予权值，通过迭代更新对环境代理网络参数进行更新；S9，在样本数据库中进行数据采样，更新动作评价网络参数及注采策略网络参数；S10，重复S4～S9，直至满足迭代收敛条件，得到最优注采策略模型及环境代理模型；S11，根据保存的最优注采策略模型，将智能注采装置所获取的油藏状态信息输入最优注采策略模型中，输出完整的生产制度，实现实时注采优化。2.根据权利要求1所述的一种联动智能注采装置的差异化实时注采优化调控方法，其特征在于，步骤S1具体包括如下步骤：S1.1，使用参数化的神经网络/＞表示注采策略网络，注采策略网络输入为智能体观测到状态/＞，输出为油水井的注采制度/＞；根据油井数量和水井数量设置输入神经元个数和输出神经元个数；输入具体信息包括油水井的日产液量、日产油量、累产液量、累注入量、含水率、日注入量、累注入量、井底压力，以向量形式进行输入，上述信息在油田生产过程中进行实时测量并记录；S1.2，使用参数化的神经网络/＞表示动作评价网络，动作评价网络的输入为状态/＞及油水井的注采制度/＞，输出为状态/＞下根据注采制度/＞生产计算得到的NPV预测值；S1.3，使用参数化的神经网络/＞表示环境代理网络，输入为当前状态/＞及注采制度/＞，输出为下一状态/＞。3.根据权利要求1所述的一种联动智能注采装置的差异化实时注采优化调控方法，其特征在于，步骤S2的注采环境模型包括：状态转移部分、状态读取部分和奖励获取部分：状态转移部分用于将注采环境模型根据注采制度从当前状态转移到下一状态/＞，根据如公式、公式和公式所示的油水两相流动方程，进行状态转移；；；；其中，为毛细管压力，单位为kPa；/＞、/＞为水相、油相的渗流速度，单位为cm/s；、/＞分别为油和水的黏度，单位为mPa·s；/＞和/＞分别为水相和油相的压力，单位为kPa；/＞为岩石的绝对渗透率，单位为um2；/＞和/＞分别为水相和油相的相对渗透率,该步骤以油藏数值模拟模型为基础实现；状态读取结果用于读取智能体观测到的状态，智能体观测到状态采用油田生产现场实时记录的生产日志数据如公式：；其中，为第i口油井的累产油量，/＞为第i口油井的日产油量，/＞为第i口油井的累产液量，/＞为第i口油井的日产液量，/＞为第i口水井的累注入量，为第i口水井的日注入量，/＞为第i口油井的井底压力，/＞为第i口水井的井底压力；奖励获取部分通过读取油藏数值模型在优化阶段内的累产油量、累产水量以及累注水量，进行经济净现值的计算。4.根据权利要求1所述的一种联动智能注采装置的差异化实时注采优化调控方法，其特征在于，步骤S3中，时刻油水井注采量数据/＞包括：油水井的日产液量、日产油量、累产液量、累产油量、含水率、日注入量、累注入量和井底压力，上述数据以向量形式进行输入。5.根据权利要求1所述的一种联动智能注采装置的差异化实时注采优化调控方法，其特征在于，步骤S4具体包括如下步骤：S4.1，注采策略模型输出注采制度，如公式所示：；式中，表示第/＞口生产井的产液量，/＞表示第/＞口注水井的注水速率；S4.2，获取注采制度，并将注采制度/＞输入油藏数值模拟模型。6.根据权利要求1所述的一种联动智能注采装置的差异化实时注采优化调控方法，其特征在于，步骤S6中的为当前时间步的经济净现值，计算如公式所示：；其中，代表当前时间步内累产油量，单位为STB；/＞代表区块累产水量，单位为STB；/＞代表累注入量，单位为STB；/＞为原油价格，单位为元/STB；/＞为油田产出水处理成本，单位为元/STB；/＞为注入水成本，单位为元/STB。7.根据权利要求1所述的一种联动智能注采装置的差异化实时注采优化调控方法，其特征在于，步骤S8，具体包括如下步骤：S8.1,在样本数据库中进行样本数据采样，训练环境代理模型,定义样本采样的概率为；其中，为第i个的优先级，定义/＞，/＞为预测值和实际值之间的偏差，用于防止概率为0，/＞是一个指数超参数，并且/＞对应为均匀采样，/＞为数据样本个数；S8.2，环境代理网络更新采用梯度下降法进行更新，更新所采用的loss函数如公式所示：；其中，为样本/＞模型预测值，/＞为真实值；/＞为采样样本数量。8.根据权利要求1所述的一种联动智能注采装置的差异化实时注采优化调控方法，其特征在于，步骤S9中，具体包括如下步骤：S9.1，在样本数据库中对实时采集的样本进行采样，根据所储存的油藏当前时刻的生产数据、智能体生成的注采制度/＞、经济净现值/＞以及下一时刻油藏状态数据，评估当前注采策略网络执行的策略，并更新动作评价网络的参数/＞：；；其中，为t+1时刻的经济净现值，/＞为折扣因子，/＞表示对状态下执行动作/＞的值函数估计，/＞代表神经网络参数，/＞表示对/＞求梯度，/＞表示学习率，/＞为时序差分误差；S9.2，根据油藏当前时刻的生产数据、智能体生成的注采制度/＞以及动作评价网络返回的/＞，对注采策略网络的参数/＞进行更新，具体公式如下：；其中，表示注采策略网络参数，/＞为动作评价网络输出值，/＞表示对/＞求梯度；表示学习率，/＞表示在状态/＞下执行动作/＞的可能性。9.根据权利要求1所述的一种联动智能注采装置的差异化实时注采优化调控方法，其特征在于，步骤S10中的迭代收敛条件为,迭代次数是否达到预设停止标准的最大上限；如果达到预设的最大上限，则终止计算，输出当前的注采策略模型及环境代理模型，如未满足，则进行重复S4~S9。
说明书desc
技术领域本发明涉及油藏注采技术领域，具体涉及一种联动智能注采装置的差异化实时注采优化调控方法。背景技术在油田实际生产过程中，由于工艺的临时修改或设备发生故障等多种原因，实际生产制度难以完全按照生产计划进行，需要及时根据当前实际生产方案进行优化方案的调整。但现有注采优化方法，对已有信息的利用率较低，优化一次需进行多次迭代，无法实现实时调整，亟待提升油藏注采联动中最优注采制度的制定效率与准确性。强化学习算法能够根据智能体观测到的当前状态进行决策，实现将油藏状态嵌入到注采优化决策过程中，满足现场方案调整后快速重新决策的需求。现有强化学习方法多以现场数据为输入，在应用环境也极大依赖油藏数值模拟模型，无法实现油藏与智能注采工具联动及方案制度的自主修正。因此，现需要一种对已有信息利用率较高，无需重复进行优化迭代，能够根据当前生产方案进行实时调整的油藏注采优化方法。发明内容本发明的主要目的在于提供一种联动智能注采装置的差异化实时注采优化调控方法，以解决现有技术中的注采优化方法，对已有信息的利用率较低，优化一次需要进行多次迭代，无法实现实时调整，亟待提升油藏注采联动中最优注采制度的制度效率与准确性的问题。为实现上述目的，本发明提供了一种联动智能注采装置的差异化实时注采优化调控方法，具体包括以下步骤： S1，初始化注采策略网络、动作评价网络及环境代理网络；S2，根据待优化的油藏数值模拟模型编写注采环境模型，注采环境模型包括：文件读取模块及制度写入模块，其中文件读取模块完成状态的读取功能，制度写入模块实现环境的状态转移；S3，智能体即注采策略网络和动作评价网络，读取注采环境初始状态，即读取时刻油水井注采量数据；S4，将当前状态输入注采策略模型中，输出注采制度，并将注采制度写入油藏数值模拟模型中；S5，注采环境模型根据注采制度，运行油藏数值模拟模型，对油藏下一时间步的状态进行模拟，以获得t+1时刻的状态数据；S6，智能体读取下一状态及该时间步内经济净现值，将{，，，}放入样本数据库；S7，将下一状态赋给当前状态，即，再进行循环，直至完成待优化生产周期；S8，在样本数据库中进行数据采样，根据样本预测误差对样本赋予权值，通过迭代更新对环境代理网络参数进行更新；S9，在样本数据库中进行数据采样，更新动作评价网络参数及注采策略网络参数；S10，重复S4～S9，直至满足迭代收敛条件，得到最优注采策略模型及环境代理模型；S11，根据保存的最优注采策略模型，将智能注采装置所获取的油藏状态信息输入最优注采策略模型中，输出完整的生产制度，实现实时注采优化。进一步地，步骤S1具体包括如下步骤：S1.1，使用参数化的神经网络表示注采策略网络，注采策略网络输入为智能体观测到状态，输出为油水井的注采制度。根据油井数量和水井数量设置输入神经元个数和输出神经元个数。输入具体信息包括油水井的日产液量、日产油量、累产液量、累注入量、含水率、日注入量、累注入量、井底压力，以向量形式进行输入，上述信息在油田生产过程中进行实时测量并记录。S1.2，使用参数化的神经网络表示动作评价网络，动作评价网络的输入为状态及油水井的注采制度，输出为状态下根据注采制度生产计算得到的NPV预测值。S1.3，使用参数化的神经网络表示环境代理网络，输入为当前状态及注采制度，输出为下一状态。进一步地，步骤S2的注采环境模型包括：状态转移部分、状态读取部分和奖励获取部分。状态转移部分用于将注采环境模型根据注采制度从当前状态转移到下一状态，根据如公式、公式和公式所示的油水两相流动方程，进行状态转移。。。。其中，为毛细管压力，单位为kPa；、为水相、油相的渗流速度，单位为cm/s；、分别为油和水的黏度，单位为mPa·s；和分别为水相和油相的压力，单位为kPa；为岩石的绝对渗透率，单位为um2；和分别为水相和油相的相对渗透率,该步骤以油藏数值模拟模型为基础实现。状态读取结果用于读取智能体观测到的状态，智能体观测到状态采用油田生产现场实时记录的生产日志数据如公式：。其中，为第i口油井的累产油量，为第i口油井的日产油量，为第i口油井的累产液量，为第i口油井的日产液量，为第i口水井的累注入量，为第i口水井的日注入量，为第i口油井的井底压力，为第i口水井的井底压力。奖励获取部分通过读取油藏数值模型在优化阶段内的累产油量、累产水量以及累注水量，进行经济净现值的计算。进一步地，步骤S3中，时刻油水井注采量数据包括：油水井的日产液量、日产油量、累产液量、累产油量、含水率、日注入量、累注入量和井底压力，上述数据以向量形式进行输入。进一步地，步骤S4具体包括如下步骤：S4.1，注采策略模型输出注采制度，如公式所示：。式中，表示第口生产井的产液量，表示第口注水井的注水速率；S4.2，获取注采制度，并将注采制度输入油藏数值模拟模型。进一步地，步骤S6中的为当前时间步的经济净现值，计算如公式所示：。其中，代表当前时间步内累产油量，单位为STB；代表区块累产水量，单位为STB；代表累注入量，单位为STB；为原油价格，单位为元/STB；为油田产出水处理成本，单位为元/STB；为注入水成本，单位为元/STB。进一步地，步骤S8，具体包括如下步骤：S8.1,在样本数据库中进行样本数据采样，训练环境代理模型,定义样本采样的概率为。其中，为第i个的优先级，定义，为预测值和实际值之间的偏差，用于防止概率为0，是一个指数超参数，并且对应为均匀采样，为数据样本个数；S8.2，环境代理网络更新采用梯度下降法进行更新，更新所采用的loss函数如公式所示：。其中，为样本模型预测值，为真实值；为采样样本数量。进一步地，步骤S9中，具体包括如下步骤：S9.1，在样本数据库中对实时采集的样本进行采样，根据所储存的油藏当前时刻的生产数据、智能体生成的注采制度、经济净现值以及下一时刻油藏状态数据，评估当前注采策略网络执行的策略，并更新动作评价网络的参数：；；其中，为t+1时刻的经济净现值，为折扣因子，表示对状态下执行动作的值函数估计，代表神经网络参数，表示对求梯度，表示学习率，为时序差分误差。S9.2，根据油藏当前时刻的生产数据、智能体生成的注采制度以及动作评价网络返回的，对注采策略网络的参数进行更新，具体公式如下：；其中，表示注采策略网络参数，为动作评价网络输出值，表示对求梯度；表示学习率，表示在状态下执行动作的可能性。进一步地，步骤S10中的迭代收敛条件为,迭代次数是否达到预设停止标准的最大上限；如果达到预设的最大上限，则终止计算，输出当前的注采策略模型及环境代理模型，如未满足，则进行重复S4~S9。本发明具有如下有益效果：本发明结合强化学习算法，利用实际生产过程中可采集到的开发指标数据，包括累产油量、日产油量、累产液量、日产液量、累注入量和日注入量等实际生产数据，利用注采策略模型，进行未来注采方案策略的优化。同时对强化学习采样过程中的数据加以利用，用于训练环境代理模型，应用时可实现后续的多步离线优化，提高已有信息利用效率。无需重复进行高昂的优化过程；本发明方法的主要用途为进行快速油藏注采优化，具有很好的推广应用价值。附图说明为了更清楚地说明本发明具体实施方式或现有技术中的技术方案，下面将对具体实施方式或现有技术描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图是本发明的一些实施方式，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。在附图中：图1示出了本发明的一种联动智能注采装置的差异化实时注采优化调控方法的注采策略网络示意图。图2示出了本发明的对环境代理模型进行训练过程中的经济净现值曲线。图3是本发明的环境代理模型损失曲线。图4是根据本发明提供的一种联动智能注采装置的差异化实时注采优化调控方法对部分生产井进行优化后产液量的数据示意图；为生产井PRO-01每个时间步的生产制度数据；为生产井PRO-02每个时间步的生产制度数据；为生产井PRO-03每个时间步的生产制度数据；为生产井PRO-04每个时间步的生产制度数据；为生产井PRO-05每个时间步的生产制度数据；为生产井PRO-06每个时间步的生产制度数据；为生产井PRO-07每个时间步的生产制度数据；为生产井PRO-08每个时间步的生产制度数据；为生产井PRO-09每个时间步的生产制度数据。图5是根据本发明提供的一种联动智能注采装置的差异化实时注采优化调控方法对部分注水井进行优化后注入量的数据示意图；为注水井INJ-01每个时间步的生产制度数据；为注水井INJ-02每个时间步的生产制度数据；为注水井INJ-03每个时间步的生产制度数据；为注水井INJ-04每个时间步的生产制度数据。具体实施方式下面将结合附图对本发明的技术方案进行清楚、完整地描述，显然，所描述的实施例是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有做出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。以某油藏模型为例对所本发明所提出的方法进行测试。该模型为油水两相黑油模型，模型网格大小为25*25，该区块共有13口井，九口生产井，四口注入井，井位分布采用了反五点法井网。共生产10个时间步，每个时间步代表180天，对该模型进行注采优化。决策变量为13*10=130维。一种联动智能注采装置的差异化实时注采优化调控方法，具体包括以下步骤：S1， 初始化注采策略网络、动作评价网络及环境代理网络，动作空间为1×13，状态空间为1×66，注采策略网络采用如图1所示的网络，输入层包含66个神经元，隐藏层为2层，其中激活函数为Relu函数，输出层为13个神经元，分别输出各井的生产制度。具体地，步骤S1具体包括如下步骤：S1.1，使用参数化的神经网络表示注采策略网络，注采策略网络输入为智能体观测到状态，输出为油水井的注采制度。根据油井数量和水井数量设置输入神经元个数和输出神经元个数；输入神经元个数为，输出神经元个数为，其中/＞代表油井个数，代表水井个数； 输入具体信息包括油水井的日产液量、日产油量、累产液量、累注入量、含水率、日注入量、累注入量、井底压力，以向量形式进行输入，上述信息在油田生产过程中进行实时测量并记录。S1.2，使用参数化的神经网络/＞表示动作评价网络，动作评价网络的输入为状态/＞及油水井的注采制度/＞，输出为状态/＞下根据注采制度/＞生产计算得到的NPV预测值。S1.3，使用参数化的神经网络/＞表示环境代理网络，输入为当前状态/＞及注采制度/＞，输出为下一状态/＞。在应用阶段，环境代理网络不需要油藏数值模拟模型就可以获得下一时刻的状态。S2，根据待优化的油藏数值模拟模型编写注采环境模型，注采环境模型包括：文件读取模块及制度写入模块，其中文件读取模块完成状态的读取功能，制度写入模块实现环境的状态转移。具体地，步骤S2的注采环境模型包括：状态转移部分、状态读取部分和奖励获取部分：状态转移部分用于将注采环境模型根据注采制度从当前状态转移到下一状态/＞，根据如公式、公式和公式所示的油水两相流动方程，进行状态转移。。 。。其中，为毛细管压力，单位为kPa；/＞、/＞为水相、油相的渗流速度，单位为cm/s；/＞、/＞分别为油和水的黏度，单位为mPa·s；/＞和/＞分别为水相和油相的压力，单位为kPa；/＞为岩石的绝对渗透率，单位为um2；/＞和/＞分别为水相和油相的相对渗透率,该步骤以油藏数值模拟模型为基础实现。状态读取结果用于读取智能体观测到的状态，智能体观测到状态采用油田生产现场实时记录的生产日志数据如公式：。其中，为第i口油井的累产油量，为第i口油井的日产油量，为第i口油井的累产液量，为第i口油井的日产液量，为第i口水井的累注入量，为第i口水井的日注入量，为第i口油井的井底压力，为第i口水井的井底压力。奖励获取部分通过读取油藏数值模型在优化阶段内的累产油量、累产水量以及累注水量，进行经济净现值的计算。S3，智能体即注采策略网络和动作评价网络，读取注采环境初始状态，即读取时刻油水井注采量数据/＞。具体地，步骤S3中，时刻油水井注采量数据/＞包括：油水井的日产液量、日产油量、累产液量、累产油量、含水率、日注入量、累注入量和井底压力，上述数据以向量形式进行输入。上述数据在油田生产过程中进行实时测量记录。S4，将当前状态输入注采策略模型中，输出注采制度，并将注采制度写入油藏数值模拟模型中。具体地，步骤S4具体包括如下步骤：S4.1，注采策略模型输出注采制度，即待优化井的注采量，如公式所示：。式中，表示第/＞口生产井的产液量，/＞表示第/＞口注水井的注水速率。S4.2，获取注采制度，并将注采制度输入油藏数值模拟模型。S5，注采环境模型根据注采制度，运行油藏数值模拟模型，对油藏下一时间步的状态进行模拟，以获得t+1时刻的状态数据。S6，智能体读取下一状态及该时间步内经济净现值/＞，将放入样本数据库。具体地，步骤S6中的为当前时间步的经济净现值，计算如公式所示：。其中，代表当前时间步内累产油量，单位为STB；/＞代表区块累产水量，单位为STB；/＞代表累注入量，单位为STB；/＞为原油价格，单位为元/STB；/＞为油田产出水处理成本，单位为元/STB；/＞为注入水成本，单位为元/STB。S7，将下一状态赋给当前状态，再进行循环，直至完成10个时间步的生产周期。S8，在样本数据库中进行数据采样，根据样本预测误差对样本赋予权值，通过迭代更新对环境代理网络参数进行更新。如图2所示的训练过程中的经济净现值曲线，可以看出，迭代更新可以获取最大化经济净现值。如图3所示，最终环境代理模型训练误差可以达到0.048，测试误差为0.055。具体地，步骤S8，具体包括如下步骤：S8.1，在样本数据库中进行样本数据采样，训练环境代理模型。传统神经网络训练过程中神经网络训练过程中所有样本都以等概率随机抽样，但实际训练过程中，不同数据对于模型来说训练的难度是不同的，有的样本产生的误差比较大，有的比较小，均匀采样会导致学习效率降低。本申请引入优先级经验回放，在训练时重点针对那些产生较大误差的样本进行学习，以使误差函数的收敛速度加快。定义样本采样的概率为：。其中，为第i个的优先级，定义，为预测值和实际值之间的偏差，用于防止概率为0；是一个指数超参数，并且对应为均匀采样，为数据样本个数。S8.2，环境代理网络更新采用梯度下降法进行更新，更新所采用的loss函数如公式所示：。其中，为样本/＞模型预测值，/＞为真实值；/＞为采样样本数量。S9，在样本数据库中进行数据采样，更新动作评价网络参数及注采策略网络参数。具体地，步骤S9中，具体包括如下步骤：S9.1，在样本数据库中对实时采集的样本进行采样，根据所储存的油藏当前时刻的生产数据、智能体生成的注采制度/＞、经济净现值/＞以及下一时刻油藏状态数据/＞，评估当前注采策略网络执行的策略，并更新动作评价网络的参数/＞：。。其中，为t+1时刻的经济净现值，为折扣因子，表示对状态下执行动作的值函数估计，代表神经网络参数，表示对求梯度，表示学习率，为时序差分误差，用于评估当前时刻下的状态-动作对值函数的估计值，与目标值之间的差异。S9.2，根据油藏当前时刻的生产数据、智能体生成的注采制度以及动作评价网络返回的，对注采策略网络的参数进行更新，具体公式如下：。其中，表示注采策略网络参数，为动作评价网络输出值，表示对求梯度；表示学习率，表示在状态下执行动作的可能性。S10，重复S4～S9，直至满足迭代收敛条件，得到最优注采策略模型及环境代理模型。具体地，步骤S10中的迭代收敛条件为,迭代次数是否达到预设停止标准的最大上限；如果达到预设的最大上限，则终止计算，输出当前的注采策略模型及环境代理模型，如未满足，则进行重复S4~S9。S11，根据保存的最优注采策略模型，将智能注采装置所获取的油藏状态信息输入最优注采策略模型中，输出完整的生产制度，实现实时注采优化。如图4和图5所示通过本发明的方法，可以为合理生产配置、注采策略调整提供高质量的决策方案，从而实现油田多产油，少产水的目的。相比启发式算法进行注采优化，可以嵌入生产状态与优化的关联关系；相比以往以场数据为输入的注采优化方法，可以更好的与实际生产状态进行结合；并且模型无需再次进行历史拟合及数值模拟，直接根据实际生产数据进行后续注采策略的调整修正。当然，上述说明并非是对本发明的限制，本发明也并不仅限于上述举例，本技术领域的技术人员在本发明的实质范围内所做出的变化、改型、添加或替换，也应属于本发明的保护范围。
