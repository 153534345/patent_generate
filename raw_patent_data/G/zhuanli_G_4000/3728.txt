标题title
一种多智能体路径规划方法及终端
摘要abst
本发明公开一种多智能体路径规划方法及终端，建立智能体与订单的图神经网络，并建立智能体自身的深度强化学习神经网络；根据所述图神经网络和所述深度强化学习神经网络得到深度强化学习网络；对所述深度强化学习网络进行训练，得到权值更新后的深度强化学习网络；使用所述权值更新后的深度强化学习网络对所述智能体进行订单任务分配以及路径规划，能够使各个智能体之间的数据相互通信，并自动进行任务分配和路径规划，单个智能体出现宕机后，不影响整个仓储管理系统的运行，即使订单变化或智能体数量变动也能够及时重新分配任务和路径规划，从而提高了路径规划的实时性，并确保仓储管理的高效运行。
权利要求书clms
1.一种多智能体路径规划方法，其特征在于，包括步骤：建立智能体与订单的图神经网络，并建立智能体自身的深度强化学习神经网络；根据所述图神经网络和所述深度强化学习神经网络得到深度强化学习网络；对所述深度强化学习网络进行训练，得到权值更新后的深度强化学习网络；使用所述权值更新后的深度强化学习网络对所述智能体进行订单任务分配以及路径规划。2.根据权利要求1所述的一种多智能体路径规划方法，其特征在于，所述建立智能体与订单的图神经网络包括：获取订单信息和智能体信息；根据所述订单信息和智能体信息生成订单顶点和智能体顶点，并对应生成订单与智能体之间的边以及智能体之间的边；根据所述订单顶点、所述智能体顶点、所述订单与智能体之间的边以及所述智能体之间的边构建智能体与订单的图神经网络。3.根据权利要求1所述的一种多智能体路径规划方法，其特征在于，所述对所述深度强化学习网络进行训练，得到权值更新后的深度强化学习网络包括：确定所述深度强化学习网络的迭代轮数、最大步数、衰减因子以及探索率；基于所述迭代轮数、最大步数、衰减因子以及探索率对所述深度强化学习网络进行训练，得到权值更新后的深度强化学习网络。4.根据权利要求2所述的一种多智能体路径规划方法，其特征在于，所述使用所述权值更新后的深度强化学习网络对所述智能体进行订单任务分配以及路径规划包括：根据所述权值更新后的深度强化学习网络中的所述订单与智能体之间的边的权重作为第一Q值生成智能体与订单的第一Q值表，所述第一Q值为所述智能体选取不同动作的奖励；根据所述第一Q值确定所述智能体的订单任务分配，并使用时序差分方式增量式更新所述第一Q值；将时序差分误差作为目标函数更新所述图神经网络；基于所述权值更新后的深度强化学习网络中的所述智能体之间的边的权重作为第二Q值生成智能体与智能体之间的第二Q值表；根据所述第二Q值确定经过同一路段时智能体的优先级；获取当前环境信息，并将所述当前环境信息和所述订单任务分配输入至所述深度强化学习神经网络，输出路径规划结果。5.根据权利要求1所述的一种多智能体路径规划方法，其特征在于，所述建立智能体与订单的图神经网络之前包括：将仓库地图进行栅格化处理，得到处理后的仓库地图；获取障碍物信息，并根据所述障碍物信息对所述处理后的仓库地图进行标记，得到标记后的仓库地图；将所述标记后的仓库地图输入至智能体中。6.根据权利要求1所述的一种多智能体路径规划方法，其特征在于，所述使用所述权值更新后的深度强化学习网络对所述智能体进行订单任务分配以及路径规划之后包括：获取与所述订单任务分配对应的所述智能体的订单完成时间；根据订单完成时间确定所述智能体的奖励值。7.根据权利要求4所述的一种多智能体路径规划方法，其特征在于，所述使用所述权值更新后的深度强化学习网络对所述智能体进行订单任务分配以及路径规划之后包括：将所述订单任务分配中的任务目标地点、所述当前环境信息和所述智能体信息作为状态值输入至所述权值更新后的深度强化学习网络，得到动作值函数；确定随机数，并判断所述随机数是否小于预设探索因子，若是，则随机选择待执行动作，否则，将所述动作值函数最大的动作确定为待执行动作；根据所述智能体的动作确定所述智能体的奖励值。8.根据权利要求4所述的一种多智能体路径规划方法，其特征在于，还包括：使用Q值更新规则对Q值进行更新，所述Q值包括所述第一Q值和/或所述第二Q值；计算损失函数，并根据所述损失函数对所述权值更新后的深度强化学习网络的网络参数进行更新。9.根据权利要求8所述的一种多智能体路径规划方法，其特征在于，所述使用Q值更新规则对Q值进行更新包括： ；式中，s表示状态，a表示动作，α表示对价值更新的步长，r表示奖励值，γ表示衰减因子，A表示智能体的动作空间，s′表示下一时刻状态，a′表示下一时刻动作；所述损失函数Loss为：；式中，N表示训练步数。10.一种多智能体路径规划终端，包括存储器、处理器及存储在存储器上并可在所述处理器上运行的计算机程序，其特征在于，所述处理器执行所述计算机程序时实现权利要求1至9中任一项所述的一种多智能体路径规划方法中的各个步骤。
说明书desc
技术领域本发明涉及路径规划技术领域，尤其涉及一种多智能体路径规划方法及终端。背景技术传统路径规划算法实时性差，规划得到的是一条长时间内没有新运货车加入的路径，并且仅在检测到可能发生碰撞时，进行重新规划避免碰撞发生。实际情况当中，仓储管理常常有较大订单变动，运货车数量也会有不同的变更，如果运货车无法针对实时订单及环境信息做出变更，往往就会导致效率降低，甚至死锁等问题。发明内容本发明所要解决的技术问题是：提供一种多智能体路径规划方法及终端，能够提高路径规划的实时性，并确保仓储管理的高效运行。为了解决上述技术问题，本发明采用的一种技术方案为：一种多智能体路径规划方法，包括步骤：建立智能体与订单的图神经网络，并建立智能体自身的深度强化学习神经网络；根据所述图神经网络和所述深度强化学习神经网络得到深度强化学习网络；对所述深度强化学习网络进行训练，得到权值更新后的深度强化学习网络；使用所述权值更新后的深度强化学习网络对所述智能体进行订单任务分配以及路径规划。为了解决上述技术问题，本发明采用的另一种技术方案为：一种多智能体路径规划终端，包括存储器、处理器及存储在存储器上并可在所述处理器上运行的计算机程序，所述处理器执行所述计算机程序时实现以下步骤：建立智能体与订单的图神经网络，并建立智能体自身的深度强化学习神经网络；根据所述图神经网络和所述深度强化学习神经网络得到深度强化学习网络；对所述深度强化学习网络进行训练，得到权值更新后的深度强化学习网络；使用所述权值更新后的深度强化学习网络对所述智能体进行订单任务分配以及路径规划。本发明的有益效果在于：建立智能体与订单的图神经网络，并建立智能体自身的深度强化学习神经网络，根据图神经网络和深度强化学习神经网络得到深度强化学习网络，对深度强化学习网络进行训练，得到权值更新后的深度强化学习网络，使用其对智能体进行订单任务分配以及路径规划，基于图神经网络的深度强化学习网络能够使各个智能体之间的数据相互通信，并自动进行任务分配和路径规划，单个智能体出现宕机后，深度强化学习网络中的数据也能够进行实时更新，并不影响整个仓储管理系统的运行，即使订单变化或智能体数量变动也能够及时重新分配任务和路径规划，从而提高了路径规划的实时性，并确保仓储管理的高效运行。附图说明图1为本发明实施例的一种多智能体路径规划方法的步骤流程图；图2为本发明实施例的一种多智能体路径规划终端的结构示意图；图3为本发明实施例的一种多智能体路径规划方法的深度强化学习网络结构示意图。具体实施方式为详细说明本发明的技术内容、所实现目的及效果，以下结合实施方式并配合附图予以说明。请参照图1，本发明实施例提供了一种多智能体路径规划方法，包括步骤：建立智能体与订单的图神经网络，并建立智能体自身的深度强化学习神经网络；根据所述图神经网络和所述深度强化学习神经网络得到深度强化学习网络；对所述深度强化学习网络进行训练，得到权值更新后的深度强化学习网络；使用所述权值更新后的深度强化学习网络对所述智能体进行订单任务分配以及路径规划。从上述描述可知，本发明的有益效果在于：建立智能体与订单的图神经网络，并建立智能体自身的深度强化学习神经网络，根据图神经网络和深度强化学习神经网络得到深度强化学习网络，对深度强化学习网络进行训练，得到权值更新后的深度强化学习网络，使用其对智能体进行订单任务分配以及路径规划，基于图神经网络的深度强化学习网络能够使各个智能体之间的数据相互通信，并自动进行任务分配和路径规划，单个智能体出现宕机后，深度强化学习网络中的数据也能够进行实时更新，并不影响整个仓储管理系统的运行，即使订单变化或智能体数量变动也能够及时重新分配任务和路径规划，从而提高了路径规划的实时性，并确保仓储管理的高效运行。进一步地，所述建立智能体与订单的图神经网络包括：获取订单信息和智能体信息；根据所述订单信息和智能体信息生成订单顶点和智能体顶点，并对应生成订单与智能体之间的边以及智能体之间的边；根据所述订单顶点、所述智能体顶点、所述订单与智能体之间的边以及所述智能体之间的边构建智能体与订单的图神经网络。由上述描述可知，根据订单顶点、智能体顶点、订单与智能体之间的边以及智能体之间的边构建智能体与订单的图神经网络，该图神经网络能够根据边的权重来确定智能体所承接的订单以及智能体之间通行的优先级，且根据订单信息和智能体信息生成顶点和边，能够实时根据订单和智能体的最新情况完成订单任务分配，从而提高了仓储管理的运行效率。进一步地，所述对所述深度强化学习网络进行训练，得到权值更新后的深度强化学习网络包括：确定所述深度强化学习网络的迭代轮数、最大步数、衰减因子以及探索率；基于所述迭代轮数、最大步数、衰减因子以及探索率对所述深度强化学习网络进行训练，得到权值更新后的深度强化学习网络。由上述描述可知，通过对深度强化学习网络进行共同训练，避免了单个智能体训练过程中收敛性差且整体上难以达到全局优化的问题，训练后的网络的权值得到了更新，此时即可根据边的权重确定订单任务分配以及路径规划，从而提高路径规划的可靠性。进一步地，所述使用所述权值更新后的深度强化学习网络对所述智能体进行订单任务分配以及路径规划包括：根据所述权值更新后的深度强化学习网络中的所述订单与智能体之间的边的权重作为第一Q值生成智能体与订单的第一Q值表，所述第一Q值为所述智能体选取不同动作的奖励；根据所述第一Q值确定所述智能体的订单任务分配，并使用时序差分方式增量式更新所述第一Q值；将时序差分误差作为目标函数更新所述图神经网络；基于所述权值更新后的深度强化学习网络中的所述智能体之间的边的权重作为第二Q值生成智能体与智能体之间的第二Q值表；根据所述第二Q值确定经过同一路段时智能体的优先级；获取当前环境信息，并将所述当前环境信息和所述订单任务分配输入至所述深度强化学习神经网络，输出路径规划结果。由上述描述可知，根据第一Q值可将不同订单分配给不同的智能体，根据第二Q值可决定通过同一路段时运货车的优先级，最后，将当前环境信息和订单任务分配输入至深度强化学习网络，输出路径规划，即可完成订单任务分配以及路径规划，提高了路径规划的实时性，并确保仓储管理的高效运行。进一步地，所述建立智能体与订单的图神经网络之前包括：将仓库地图进行栅格化处理，得到处理后的仓库地图；获取障碍物信息，并根据所述障碍物信息对所述处理后的仓库地图进行标记，得到标记后的仓库地图；将所述标记后的仓库地图输入至智能体中。由上述描述可知，将标记后的仓库地图输入至智能体中，后续即可根据智能体中的标记后的仓库地图来规划路径，避开障碍物，实现可靠地路径规划。进一步地，所述使用所述权值更新后的深度强化学习网络对所述智能体进行订单任务分配以及路径规划之后包括：获取与所述订单任务分配对应的所述智能体的订单完成时间；根据订单完成时间确定所述智能体的奖励值。由上述描述可知，根据订单完成时间确定智能体的奖励值，在规定时间完成，则奖励值更高，未在规定时间完成，则降低奖励值，能够有效地根据智能体完成订单的情况来调整订单分配，提高仓储管理的运行效率。进一步地，所述使用所述权值更新后的深度强化学习网络对所述智能体进行订单任务分配以及路径规划之后包括：将所述订单任务分配中的任务目标地点、所述当前环境信息和所述智能体信息作为状态值输入至所述权值更新后的深度强化学习网络，得到动作值函数；确定随机数，并判断所述随机数是否小于预设探索因子，若是，则随机选择待执行动作，否则，将所述动作值函数最大的动作确定为待执行动作；根据所述智能体的动作确定所述智能体的奖励值。由上述描述可知，还根据智能体的动作确定智能体的奖励值，以便根据奖励值确定要选择的动作，从而提高智能体的处理效率。进一步地，还包括：使用Q值更新规则对所述Q值进行更新，所述Q值包括所述第一Q值和/或所述第二Q值；计算损失函数，并根据所述损失函数对所述权值更新后的深度强化学习网络的网络参数进行更新。进一步地，所述使用Q值更新规则对Q值进行更新包括：；式中，s表示状态，a表示动作，α表示对价值更新的步长，r表示奖励值，γ表示衰减因子，A表示智能体的动作空间，s′表示下一时刻状态，a′表示下一时刻动作；所述损失函数Loss为：；式中，N表示训练步数。由上述描述可知，使用Q值更新规则对Q值进行更新，能够使智能体更倾向于选择更高奖励的动作，达到提高智能体处理效率的目的，计算损失函数，并根据损失函数对权值更新后的深度强化学习网络的网络参数进行更新，能够不断地优化网络，提高网络的精确度，从而提高路径规划的可靠性。请参照图2，本发明另一实施例提供了一种多智能体路径规划终端，包括存储器、处理器及存储在存储器上并可在所述处理器上运行的计算机程序，所述处理器执行所述计算机程序时实现上述多智能体路径规划方法中的各个步骤。本发明上述的多智能体路径规划方法及终端能够适用于仓储管理系统，以下通过具体实施方式进行说明：实施例一请参照图1和图3，本实施例的一种多智能体路径规划方法，包括步骤：S1、将仓库地图进行栅格化处理，得到处理后的仓库地图；具体的，将仓库地图进行栅格化处理，分为m×n的网格，得到处理后的仓库地图。S2、获取障碍物信息，并根据所述障碍物信息对所述处理后的仓库地图进行标记，得到标记后的仓库地图；具体的，根据障碍物信息若网格内存在障碍物，则记为1，若无障碍物，可以通行，则记为0，另外，若某一个货位没有货架，但被预定，也记为1，视作有障碍物。S3、将所述标记后的仓库地图输入至智能体中。其中，所述智能体包括运货车或机器人，本实施例中，所述智能体为运货车。S4、建立智能体与订单的图神经网络，并建立智能体自身的深度强化学习神经网络，具体包括：S41、获取订单信息和智能体信息；在一种可选的实施方式中，所述订单信息包括起点、终点以及订单状态，所述订单状态包括未被承接、执行中、已完成或已超时，所述智能体信息包括运货车位置和运货车状态，所述运货车状态包括空载、满载或闲置。S42、根据所述订单信息和智能体信息生成订单顶点和智能体顶点，并对应生成订单与智能体之间的边以及智能体之间的边；S43、根据所述订单顶点、所述智能体顶点、所述订单与智能体之间的边以及所述智能体之间的边构建智能体与订单的图神经网络。其中，订单与智能体之间的边的权重作为运货车争取订单的权值，该值越大，其承接订单的概率越高，该边的权重为运货车到订单起点位置的曼哈顿距离的倒数，运货车之间的边的权重作为运货车之间通行同一路段的优先级，以二元组作为运货车之间的边的权值，其中，/＞为重合路段的长度，/＞表示其中一运货车的当前整个规划路径长度，/＞表示另一运货车的当前整个规划路径长度。S44、建立智能体自身的深度强化学习神经网络，用于承接订单后对运货车的行驶路径做出规划。S5、根据所述图神经网络和所述深度强化学习神经网络得到深度强化学习网络；S6、对所述深度强化学习网络进行训练，得到权值更新后的深度强化学习网络，具体包括：S61、确定所述深度强化学习网络的迭代轮数、最大步数、衰减因子以及探索率；S62、基于所述迭代轮数、最大步数、衰减因子以及探索率对所述深度强化学习网络进行训练，得到权值更新后的深度强化学习网络。S7、使用所述权值更新后的深度强化学习网络对所述智能体进行订单任务分配以及路径规划，具体包括：S71、根据所述权值更新后的深度强化学习网络中的所述订单与智能体之间的边的权重作为第一Q值生成智能体与订单的第一Q值表，所述第一Q值为所述智能体选取不同动作的奖励；S72、根据所述第一Q值确定所述智能体的订单任务分配，并使用时序差分方式增量式更新所述第一Q值；S73、将时序差分误差作为目标函数更新所述图神经网络；S74、基于所述权值更新后的深度强化学习网络中的所述智能体之间的边的权重作为第二Q值生成智能体与智能体之间的第二Q值表；S75、根据所述第二Q值确定经过同一路段时智能体的优先级；S76、获取当前环境信息，并将所述当前环境信息和所述订单任务分配输入至所述深度强化学习神经网络，输出路径规划结果。在一种可选的实施方式，使用激光传感器获取当前环境信息以及相较于整个仓库环境的SLAM定位信息，并将当前环境信息和订单任务分配输入至深度强化学习神经网络，输出路径规划结果，该路径规划结果规划了从当前位置抵达目标地点的路径以及运货车各车轮差速。运货车从当前位置出发，到达指定起点货架正下方，通过抬起整个货架，将货架运送到指定终点并卸运视为完成整个任务的流程，运货车的动作空间定义为集合，分别代表运货车向前，向后，向左，向右移动和停在当前位置，运货车采用动作a∈A。对于图神经网络的强化学习训练，考虑将订单完成时间作为奖励值，因此，在一种可选的实施方式中，还包括：获取与所述订单任务分配对应的所述智能体的订单完成时间；根据订单完成时间确定所述智能体的奖励值，比如，订单在规定时长内由运货车完成，其奖励值为ra，若其未在规定时长内完成，其奖励值随超出时长/＞增加而下降，直到降为0；将所述订单任务分配中的任务目标地点、所述当前环境信息和所述智能体信息作为状态值输入至所述权值更新后的深度强化学习网络，得到动作值函数；确定随机数，并判断所述随机数是否小于预设探索因子，若是，则随机选择待执行动作a，否则，将所述动作值函数最大的动作确定为待执行动作；根据所述智能体的动作确定所述智能体的奖励值。其中，所述随机数为0-1之间的随机数，如果运货车的动作为停在原地不动，给予奖励值rs；如果运货车的动作为抵达目的地，给予奖励值rg；如果运货车与目标地点间曼哈顿距离减小，给予奖励值rp；如果运货车与目标地点间曼哈顿距离减小，给予惩罚值rn；如果运货车的动作不可行或发生碰撞，则给予惩罚值rc；如果运货车在最大时长期间仍旧未完成任务，给予惩罚值ru，若当前运货车状态为空载，则取消该运货车承接订单请求，并将订单重新设置为未被承接状态，若当前运货车状态为满载，则运货车将货架就近存放至空闲货位，并重新生成新的未被承接的订单。S8、使用Q值更新规则对Q值进行更新，所述Q值包括所述第一Q值和/或所述第二Q值，具体为：；式中，s表示状态，a表示动作，α表示对价值更新的步长，为常数，r表示奖励值，γ表示衰减因子，A表示智能体的动作空间，s′表示下一时刻状态，a′表示下一时刻动作；S9、计算损失函数，并根据所述损失函数对所述权值更新后的深度强化学习网络的网络参数进行更新。所述损失函数Loss为：；式中，N表示训练步数，用于计算其平均的损失值，Q表示Q值表，需要使其接近后面的时序差分方程。在一种可选的实施方式中，每个agent保存一个Mixing Network模型副本，并根据自身周围的运货车数据作为总数据的子集，通过异步随机梯度下降的方式进行更新，以此可以由agent的硬件设备支持训练，实现分布式的多智能体结构。如图3所示，图3展示了深度强化学习网络结构示意图，MLP为多层感知器，GRU为循环神经网络，实心圆点表示运货车，与实心圆点相连的矩形图标表示订单信息，W1和W2表示网络的一层中的参数，通过更改这些参数来提高网络质量，Ojt表示t时刻的智能体观察到的环境状态，如读入的摄像机信息或激光信息，其作为智能体的状态信息的一部分，ajt-1表示t-1时刻智能体采用的action，如其转向及速度，Q表示函数，该函数根据当前状态，输出其下一步应该采取的动作a的概率，根据概率判断动作a的好坏，进而选取不同的动作。实施例二请参照图2，本实施例的一种多智能体路径规划终端，包括存储器、处理器及存储在存储器上并可在所述处理器上运行的计算机程序，所述处理器执行所述计算机程序时实现实施例一中的多智能体路径规划方法中的各个步骤。综上所述，本发明提供的一种多智能体路径规划方法及终端，建立智能体与订单的图神经网络，并建立智能体自身的深度强化学习神经网络；根据所述图神经网络和所述深度强化学习神经网络得到深度强化学习网络；对所述深度强化学习网络进行训练，得到权值更新后的深度强化学习网络；使用所述权值更新后的深度强化学习网络对所述智能体进行订单任务分配以及路径规划，通过对深度强化学习网络进行共同训练，避免了单个智能体训练过程中收敛性差且整体上难以达到全局优化的问题，训练后的网络的权值得到了更新，此时即可根据边的权重确定订单任务分配以及路径规划，提高路径规划的可靠性；基于图神经网络的深度强化学习网络能够使各个智能体之间的数据相互通信，并自动进行任务分配和路径规划，单个智能体出现宕机后，深度强化学习网络中的数据也能够进行实时更新，并不影响整个仓储管理系统的运行，即使订单变化或智能体数量变动也能够及时重新分配任务和路径规划，从而提高了路径规划的实时性，并确保仓储管理的高效运行。以上所述仅为本发明的实施例，并非因此限制本发明的专利范围，凡是利用本发明说明书及附图内容所作的等同变换，或直接或间接运用在相关的技术领域，均同理包括在本发明的专利保护范围内。
