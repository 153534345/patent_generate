标题title
一种面向对话系统中的自然语言理解方法及装置
摘要abst
本发明属于智能对话技术领域，具体为一种面向对话系统中的自然语言理解方法及装置，包括是词嵌入层、编码表示层和联合学习层，其结构合理，明基于收集的特定领域数据集与1)原始的BERT‑WWM模型2)原始的ERNIE模型3)基于预训练的联合学习模型4)知识蒸馏后的3层BERT‑WWM模型。四个模型进行了对比实验，在特定领域数据集上，3)模型在意图分类准确率和槽位识别F1两个性能指标上均好于1)和2)模型，而经过知识蒸馏后的4)模型参数规模大大减少，推理延迟也有效减低，且性能损失较小。
权利要求书clms
1.一种面向对话系统中的自然语言理解装置，其特征在于：包括是词嵌入层、编码表示层和联合学习层；其中，词嵌入层图1中X1，...，X5表示输入序列的字，而e，...，e表示经过嵌入后的单词表示。所使用的Embedding是通过预训练的方式生成的，本层主要完成由文本到向量的表示；编码表示层我们将嵌入表示后的单词向量输入由堆叠多层Transformer形成的预训练语言模型，进行高层次的特征编码和抽取；经过预训练模型编码后的表示，可以引入模型在预训练过程中学习到的丰富的无监督语法和语义知识，从而提升模型的分类和序列标注能力；联合学习层在图像处理领域，CNN模块是构建网络中不可缺少的模块，且其性能已经被有效证明，在自然语言处理领域同样具有良好的效果。为了从低层到高层地捕获特征，研究人员提出了VDCNN模型，使用多达数十层的卷积块，并且为了缓解梯度消失带来的问题还引入了残差连接，循环神经网络方面，LSTM和GRU都能进一步改善梯度消失和梯度急剧膨胀的问题，在RNN的基础上引入双向建模和最大池化操作可以增强模型在长距离依赖关系的处理能力并保留重要的语义信息。而对于槽位识别来说，双向长短期记忆网络模型对于序列编码具有较大优势，在序列任务中表现出优异的表示学习能力，并且条件随机场具有在序列输出阶段利用标签间信息的优点，所以在槽位识别任务时，使用双向LSTM结合条件随机场共同建模；因此在意图分类时采用由CNN、RCNN和VDCNN三种模型形成的混合网络进行预测，而在槽位识别时则采用BiLSTM+条件随机场的方式进行序列标注。2.根据权利要求1所述的一种面向对话系统中的自然语言理解装置的使用方法，其特征在于：包括如下步骤：步骤一：首先使用在通用领域语料上已经预训练好的预训练语言模型的嵌入层实现对用户输入文本的向量化表示；步骤二：然后使用预训练语言模型的多层Transformer对向量化表示进行高层次的特征抽取和语义表示，融入上下文信息，完成输入的编码表示；在联合学习层中，意图分类使用一个包含TextCNN、RCNN和VDCNN的混合网络模型实现，具体计算过程如下：CNN模块CNN模块采用的是TextCNN模型。在自然语言领域CNN采用一维的形式完成特征抽取，即文本在向量化表示后，在文本序列方向上进行卷积和池化操作，通过滑动窗口的方式，每次卷积选取固定大小的序列进行交互学习；RCNN模块RCNN模型引入上下文同时建模的思想，在进行编码和特征抽取时考虑单词的上文和下文，并且为了捕获单词序列长距离的依赖关系使用RNN作为特征抽取器。实现中使用双向LSTM基本单元。对LSTM编码后的输出进行最大池化操作，可以有效学习句子中单词的语义重要性知识；VDCNN模块VDCNN最初是为计算机视觉领域的图像识别任务而提出的。模型的主要思想是在整个模型的所有卷积层中都使用一个小的卷积核，然后堆叠至一个非常深的深度，最深可以达到19层，记该层输出为R3；步骤三：最后，将三个模型最终得到的编码表示R1，R2，R3进行拼接操作。3.根据权利要求1-2任意一项所述的一种面向对话系统中的自然语言理解装置的使用方法，其特征在于：本发明针对的是中文意图识别和槽位识别，因此在选取合适的编码表示层预训练模型时，选择了更加适合中文场景的两个基于Transformer的预训练语言模型。两个模型分别是百度提出的ERNIE模型和哈工大-讯飞联合提出的BERT-WWM。4.根据权利要求1-3任意一项所述的一种面向对话系统中的自然语言理解装置的使用方法，其特征在于：为增强模型面对教育领域的意图识别和槽位识别性能，本发明通过构建领域词典的方式，提出融入领域词典信息的预训练目标任务，对编码表示层使用的预训练语言模型进行继续预训练。5.根据权利要求1-4任意一项所述的一种面向对话系统中的自然语言理解装置的使用方法，其特征在于：为了进一步提升预训练模型在面对教育领域的两个子任务时的表示学习能力，本发明提出对所基于的预训练模型进行另外两个阶段的进一步训练，即总体来看模型包含通用领域预训练、领域适应预训练、任务适应预训练和微调四个阶段。6.根据权利要求1-4任意一项所述的一种面向对话系统中的自然语言理解装置的使用方法，其特征在于：对基于预训练的联合学习模型进行知识蒸馏，分别蒸馏至三层BERT-WWM。
说明书desc
技术领域本发明涉及智能对话技术领域，具体为一种面向对话系统中的自然语言理解方法及装置。背景技术互联网的高速发展和广泛普及使得21世纪成为一个数据爆炸的时代。人们对于各类信息的需求量急剧增加，需求信息的类别也更加广泛，当用户面对规模庞大且复杂的信息时，如何对海量信息进行有效查找和获取成为了利用信息的关键，这对于信息检索方式提出了更高的要求。传统的检索方式存在： 仅对关键字进行匹配，未考虑用户语义层面的需求；搜索结果一般返回大量文本和网页，需要用户进一步选择。面向特定领域的对话系统、问答系统正是改进传统检索方式的一个研究课题。相较于传统检索方式，对话问答系统可以从语义层面理解问题，而不是简单的关键词匹配。还可以代替用户筛选网页和文档中的内容，返回的结果更加精确，并且返回的是问题对应的答案而不是网页或文档。可以根据应用场景的不同对话问答系统划分为四种：常见问题型：该类型的智能系统一般会给定问题和相应答案，使用模型和算法对用户输入进行解析和处理，并采用某种度量算法找出问题库中相似度最高的问题，返回对应答案。任务型：该类型智能系统的设计目的就是协助用户完成某一任务，对用户输入进行解析，分析用户意图，并在对话策略模型的指导下采取一系列动作完成用户要求。常识型：一般采用知识图谱作为系统的知识库，知识图谱中的三元组包含现实中以自然语言形式存储的常识信息，根据用户输入，从图谱中检索出答案并返回。闲聊型：该类型的对话系统是在开放领域中与用户进行多轮次的对话，目的性较弱，但对于系统的智能性、语义连贯性要求较高。智能对话由于对话系统在应用时存在涉及用户隐私、用户接受度不高、用户体验一般等问题，导致获取大量公开且高质量的对话数据集以及无监督语料十分困难，数据集的缺乏在较大程度上限制了对话系统的发展，带来了挑战。另一方面，对话系统中用户输入往往是口语化的表达，语义多义性、语法随意性程度较高，还具有句子长度分布不固定，内容发散等特点。上述特点都给意图分类任务带来了较大难度。此外，用户输入还可能包含多个意图，且多个意图间存在一定相关性，怎样识别是否存在多意图并将多个意图准确分类，也是意图分类任务面临的一个挑战。系统的关键模块包括语义理解、对话状态追踪、对话管理和对话生成共4个部分。自然语言理解任务一般包含以下三个子任务：领域分类、意图分类和槽位识别。其中，领域分类的目的是使用模型或算法给出用户输入属于的领域类别，意图分类旨在对用户输入的意图进行识别。槽位识别通常按照序列标注任务解决，对用户输入中的实体进行识别和标注。本发明专利所提出的是面向特定领域的自然语言理解方法，因此在自然语言理解部分领域识别和意图识别被建模为一个子任务。即将用户输入总体上分为两部分，一部分为教育领域无关，另一部分为教育领域相关，对领域相关的输入进行更加细化的分类。当前，对话系统在各个领域越来越引起人们的重视，深度学习技术的不断进步极大地推动了对话系统的发展。对于对话系统，深度学习技术可以利用大量的数据来学习特征表示和对用户意图进行分类和识别，这其中仅需要少量的手工操作。发明内容本部分的目的在于概述本发明的实施方式的一些方面以及简要介绍一些较佳实施方式。在本部分以及本申请的说明书摘要和发明名称中可能会做些简化或省略以避免使本部分、说明书摘要和发明名称的目的模糊，而这种简化或省略不能用于限制本发明的范围。鉴于现有智能对话系统中存在的问题，提出了本发明。因此，本发明的目的是提供一种面向对话系统中的自然语言理解方法及装置，能够实现提升智能对话系统中的意图分类和槽位识别的准确率，通过引入预训练模型和使用新的预训练任务提升自然语言理解模块的语义表示能力，通过引入领域适应预训练和任务适应预训练提升自然语言理解模块在特定领域上的表现。同时通过对模型进行知识蒸馏，提升模型推理速度，缓解对话系统的迟滞感。为解决上述技术问题，根据本发明的一个方面，本发明提供了如下技术方案：一种面向对话系统中的自然语言理解方法及装置，其包括是词嵌入层、编码表示层和联合学习层；其中，词嵌入层图1中X1，...，X5表示输入序列的字，而e，...，e表示经过嵌入后的单词表示。所使用的Embedding是通过预训练的方式生成的，本层主要完成由文本到向量的表示；编码表示层我们将嵌入表示后的单词向量输入由堆叠多层Transformer形成的预训练语言模型，进行高层次的特征编码和抽取；将输入序列记作X1，...，X5，在基于Transformer的预训练模型的Encoder 部分中的多头注意力层中，采用缩放点积注意力机制，并引入多头机制使用多个不同的注意力矩阵参与计算，而后输入前馈网络完成非线性变换，这一过程的公式表示如下：e×WQ＝Q e×WK＝K e×WV＝VMultiHead＝ConcatWo FFN＝maxW2+b2式中，dk表示K的维度，FFN表示BERT Encoder中的全连接子层。经过预训练模型编码后的表示，可以引入模型在预训练过程中学习到的丰富的无监督语法和语义知识，从而提升模型的分类和序列标注能力；联合学习层在图像处理领域，CNN模块是构建网络中不可缺少的模块，且其性能已经被有效证明，在自然语言处理领域同样具有良好的效果。为了从低层到高层地捕获特征，研究人员提出了VDCNN模型，使用多达数十层的卷积块，并且为了缓解梯度消失带来的问题还引入了残差连接。循环神经网络方面，LSTM和GRU都能进一步改善梯度消失和梯度急剧膨胀的问题。在RNN的基础上引入双向建模和最大池化操作可以增强模型在长距离依赖关系的处理能力并保留重要的语义信息。而对于槽位识别来说，双向长短期记忆网络模型对于序列编码具有较大优势，在序列任务中表现出优异的表示学习能力，并且条件随机场具有在序列输出阶段利用标签间信息的优点，所以在槽位识别任务时，使用双向LSTM结合条件随机场共同建模；因此在意图分类时采用由CNN、RCNN和VDCNN三种模型形成的混合网络进行预测，而在槽位识别时则采用BiLSTM+条件随机场的方式进行序列标注。作为本发明所述的一种面向对话系统中的自然语言理解方法及装置的一种优选方案，其中：包括如下步骤：步骤一：首先使用在通用领域语料上已经预训练好的预训练语言模型的嵌入层实现对用户输入文本的向量化表示；步骤二：然后使用预训练语言模型的多层Transformer对向量化表示进行高层次的特征抽取和语义表示，融入上下文信息，完成输入的编码表示；在联合学习层中，意图分类使用一个包含TextCNN、RCNN和VDCNN的混合网络模型实现，具体计算过程如下：CNN模块CNN模块采用的是TextCNN模型。在自然语言领域CNN采用一维的形式完成特征抽取，即文本在向量化表示后，在文本序列方向上进行卷积和池化操作，通过滑动窗口的方式，每次卷积选取固定大小的序列进行交互学习，模型的公式表示如下：Ci＝f+b)c＝ R1＝max{c} 其中，w1，...，wn表示输入的文本序列，公式表示对上一层的编码表示进行拼接操作，公式中的w表示卷积核矩阵。公式和表示对卷积后的上下文表示进行拼接和最大池化操作；RCNN模块RCNN模型引入上下文同时建模的思想，在进行编码和特征抽取时考虑单词的上文和下文，并且为了捕获单词序列长距离的依赖关系使用RNN作为特征抽取器。实现中使用双向LSTM基本单元。对LSTM编码后的输出进行最大池化操作，可以有效学习句子中单词的语义重要性知识，RCNN的公式表示如下：cl＝fcl+We)cr＝fci+We)hi＝ 其中，cl表示单词wi的上文，对应地，cr表示单词wi的下文，cl和cr分别按照公式和计算，W表示由上一层到本层的变换矩阵，W矩阵用于融合当前单词的语义表示和下一个单词的上文表示。类似地，W和W也是同样的作用，将上文表示 cl、e和下文表示cr拼接后得到最终的编码表示，按照公式 进行非线性变换，公式表示最大池化操作；VDCNN模块VDCNN最初是为计算机视觉领域的图像识别任务而提出的。模型的主要思想是在整个模型的所有卷积层中都使用一个小的卷积核，然后堆叠至一个非常深的深度，最深可以达到19层，记该层输出为R3；步骤三：最后，将三个模型最终得到的编码表示R1，R2，R3进行拼接操作，如公式式所示；然后经过一次线性变换后，在softmax层计算属于各类别的概率，使用交叉熵作为损失函数：S＝WsR+b 在为槽位识别设计的Bi-LSTM网络中主要包含有Bi-LSTM和CRF两个子层：Bi-LSTM层将经过预训练模型编码后的向量表示送入双向LSTM网络中，向量化表示的文本序列需要经过前向和后向两个方向的学习过程，该过程的形式化描述与公式和一致，最终得到前向编码表示cl 和后向编码表示cr，二者按照公式完成拼接；CRF层CRF层通过学习相邻标签之间的依赖性去约束标签组合，对所有可能的标签路径中的最佳路径进行解码，记H为Bi-LSTM的编码表示，n为句子中的字符数目，m为槽位标签种类数，则Hi，j表示句子中第i个字的第j个标签的得分。在CRF层中，过去的输出转移到当前输入的过程以及该输入所对应的状态共同决定当前输入，分别称为转移得分和状态得分，将H作为CRF 层的状态矩阵。而从状态i转移到状态j的得分有转移矩阵Ti，j表示。按照下列公式进行计算：公式中表示标签的真实值，公式和给出了模型目标函数和Viterbi算法计算最佳标签序列的计算公式；在联合学习模型中，模型整体的损失函数为意图分类模型与槽位识别模型损失加权之和，即：L＝αLintent+Lslot 模型采用带线性预热和权重衰减的Adam优化器最小化目标函数，进行参数更新。作为本发明所述的一种面向对话系统中的自然语言理解方法及装置的一种优选方案，其中：本发明针对的是中文意图识别和槽位识别，因此在选取合适的编码表示层预训练模型时，选择了更加适合中文场景的两个基于Transformer 的预训练语言模型。两个模型分别是百度提出的ERNIE模型和哈工大-讯飞联合提出的BERT-WWM。作为本发明所述的一种面向对话系统中的自然语言理解方法及装置的一种优选方案，其中：为增强模型面对教育领域的意图识别和槽位识别性能，本发明通过构建领域词典的方式，提出融入领域词典信息的预训练目标任务，对编码表示层使用的预训练语言模型进行继续预训练。作为本发明所述的一种面向对话系统中的自然语言理解方法及装置的一种优选方案，其中：为了进一步提升预训练模型在面对教育领域的两个子任务时的表示学习能力，本发明提出对所基于的预训练模型进行另外两个阶段的进一步训练，即总体来看模型包含通用领域预训练、领域适应预训练、任务适应预训练和微调四个阶段。作为本发明所述的一种面向对话系统中的自然语言理解方法及装置的一种优选方案，其中：对基于预训练的联合学习模型进行知识蒸馏，分别蒸馏至三层BERT-WWM。与现有技术相比，本发明的有益效果是：本发明基于收集的特定领域数据集与1)原始的BERT-WWM模型2)原始的ERNIE模型3)基于预训练的联合学习模型4)知识蒸馏后的3层BERT-WWM模型。四个模型进行了对比实验，在特定领域数据集上，3)模型在意图分类准确率和槽位识别F1两个性能指标上均好于1)和2)模型，而经过知识蒸馏后的4)模型参数规模大大减少，推理延迟也有效减低，且性能损失较小。附图说明为了更清楚地说明本发明实施方式的技术方案，下面将结合附图和详细实施方式对本发明进行详细说明，显而易见地，下面描述中的附图仅仅是本发明的一些实施方式，对于本领域普通技术人员来讲，在不付出创造性劳动性的前提下，还可以根据这些附图获得其它的附图。其中：图1为本发明基于预训练的联合学习模型。图2为本发明领域词典构建流程。图3为本发明多阶段预训练流程。图4为本发明3层BERT-WWM蒸馏框架。图5为本发明实施例1的步骤流程图。具体实施方式为使本发明的上述目的、特征和优点能够更加明显易懂，下面结合附图对本发明的具体实施方式做详细的说明。在下面的描述中阐述了很多具体细节以便于充分理解本发明，但是本发明还可以采用其他不同于在此描述的其它方式来实施，本领域技术人员可以在不违背本发明内涵的情况下做类似推广，因此本发明不受下面公开的具体实施方式的限制。为使本发明的目的、技术方案和优点更加清楚，下面将结合附图对本发明的实施方式作进一步地详细描述。实施例1本发明可以被用于面向教育领域的单轮次、问答对、检索式的智能对话系统中，用于对用户输入进行意图分类和槽位识别。1、首先将用户输入文本经过嵌入式表示和语义编码，例如用户输入为：“明天六年一班的课程信息是什么？”，系统收到该用户输入后，将该文本送入本发明提出的基于预训练的联合学习模型中，经过已经预训练好的预训练模型形成由64bit二进制数构成的二维矩阵形式的输入编码表示，预训练语言模型中蕴含的语义语法信息就蕴含在其中。2、对用户意图和槽位进行预测。形成二维矩阵表示后，输入本发明中的联合学习模型，由混合网络模型对输入的意图进行预测，而BiLSTM+CRF则对输入中的槽位进行识别。“明天六年一班的课程信息是什么？”中包含的意图被识别为：“查询课程信息”，而语义槽位共有两个，分别是：“日期：明天”和“班级：六年一班”。3、获得了用户意图和槽位信息之后，对话状态追踪模块用于收集用户输入、历史对话、上下文情况以及用户意图和槽位值，形成当前的对话状态，这一对话状态是可以被对话状态追踪模块所学习的。4、对话策略模块则根据当前的对话状态选择要执行的动作，对话策略是对话系统的核心功能，相当于对话系统的大脑，它负责根据当前用户的反馈，系统根据对话策略模块的输出决定执行哪个具体的动作，以及如何更新对话状态信息等在程序中预设好多种动作，并编程实现，对话策略可以以人工规则的形式给出，也可以通过机器学习、深度学习的方式训练得到策略模型。5、对话响应模块，对话响应模块根据选择的对话动作，从知识库中存储的问答对进行匹配，知识库中存储有结构化的课程信息和非结构化的问答对形式的知识，对于结构化的信息检索之后直接输入，对于非结构化的信息则使用余弦相似度作为匹配程度的度量标准，如果匹配度符合阈值要求则输出对应的回答，否则进行追问，或者返回通用回答。虽然在上文中已经参考实施方式对本发明进行了描述，然而在不脱离本发明的范围的情况下，可以对其进行各种改进并且可以用等效物替换其中的部件。尤其是，只要不存在结构冲突，本发明所披露的实施方式中的各项特征均可通过任意方式相互结合起来使用，在本说明书中未对这些组合的情况进行穷举性的描述仅仅是出于省略篇幅和节约资源的考虑。因此，本发明并不局限于文中公开的特定实施方式，而是包括落入权利要求的范围内的所有技术方案。
