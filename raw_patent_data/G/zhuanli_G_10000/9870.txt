标题title
一种运动控制系统的Q学习扩张状态观测器设计方法
摘要abst
本发明提出一种运动控制系统的Q学习扩张状态观测器设计方法，包括如下步骤：1、根据运动控制系统数学模型的离散形式设计扩张状态观测器：2、设计Q学习算法:3、通过Q学习调节扩张状态观测器的参数。本发明能够针对的系统为实际中常见的对象为连续、输出为采样的混杂系统，直接给出了对应的离散ESO结构和参数设计方法；不需要噪声和扰动的模型来优化扩张状态观测器参数，而是以数据驱动的方式实时地对参数进行调节，与传统的常值ESO相比，具有更精确跟踪内部不确定动态和外部扰动的能力；定量并且显式地给出了Q学习部分的四个参数的调节范围。此参数调节理论能够保证扩张状态观测器的稳定性，并且能够降低实际工程中调节参数所花费的成本。
权利要求书clms
1.一种运动控制系统的Q学习扩张状态观测器设计方法，基于如下的运动控制系统数学模型：其中t表示时间，x1∈R表示t时刻运动物体的位置，x2∈R表示t时刻运动物体的速度，b表示输入增益，u∈R表示t时刻系统的输入，d,t)∈R表示t时刻由系统内部不确定动态和外扰组成的总扰动，c∈R表示t时刻总扰动的导数；x1、x2表示t＝kh时刻运动物体的位置和速度，其中h为采样周期，k表示第k次采样，yi表示xi的量测值，vi为相应通道的量测噪声，i＝1,2；其特征在于，包含如下三个步骤：步骤一：根据式的离散形式设计扩张状态观测器：由于实际系统中采样和控制输入都是离散的，所以需要考虑式的离散逼近形式：x1h)、x2h)表示t＝h时刻运动物体的位置和速度，其中h为采样周期，k+1表示第k+1次采样，yih)表示xih)的量测值，vih)为相应通道的量测噪声；b表示输入增益，u∈R表示t＝kh时刻系统的输入，d,kh)∈R表示t＝kh时刻由系统内部不确定动态和外扰组成的总扰动，c∈R表示t＝kh时刻总扰动的导数；根据式，设计线性扩张状态观测器如下：其中，β1,β2,β3为观测其增益，其设计方法为β1＝3ω,β2＝3ω2,β3＝ω3,ω＞0. 和分别表示x2,d和c的估计值，ω称为t＝kh时刻的观测器带宽，即为需要调节的参数；步骤二：设计Q学习算法:Q学习算法包含状态、行动、奖励和状态-行动值函数四个组成部分：基于系统当前所处状态，根据状态-行动值函数的值选择相应的行动，并得到相应的奖励，再依此更新状态-行动值函数的值；根据实际情况设计状态空间S和动空间Λ，其中Λ是一个有界的实数集，最大、最小值分别记为a；对所有的状态s∈S和行动a∈Λ，初始化相应的状态-行动值函数Q＝0，并选择折扣因子γ∈和满足如下条件的学习率序列对于状态-行动值函数Q，采用如下更新准则：其中，下角标n表示第n次在状态sj选择了行动aj；s'表示选择行动a后转移到的下一个状态；下角标j表示第j次进行Q学习；在系统运行过程中，每Q次采样进行一次Q学习，从t＝jqh时刻到t＝qh时刻之间的带宽是一个常数，记为ωj＝ω,t∈，其中sj,1和sj,2的定义为：行动：第j次进行Q学习时行动aj的选取规则如下：奖励：第j次进行Q学习所获得的奖励计算公式为：其中，λ∈为奖励参数，和分别为sj+1,2和rj,2经过归一化之后的值，sj+1,2与式中定义相同，r2,j表示的方差，即步骤：通过Q学习调节扩张状态观测器的参数当系统运行到t＝jqh，j＝1,2,...时刻时，依照式计算当前状态sj，并根据式选取行动aj，然后通过如下规则对观测器带宽进行调节：和ω为提前设置的带宽上下限；调整带宽后，根据式计算奖励函数rj和下一个状态sj+1，并按式更新Q值函数。2.根据权利要求1所述的一种运动控制系统的Q学习扩张状态观测器设计方法，其特征在于：为保证扩张状态观测器的稳定性，ω、和a需满足如下条件：其中，ε、m、M均为中间变量，其计算方法为：
说明书desc
技术领域本发明属于运动控制系统扩张观测器设计方法的技术领域,具体内容涉及到运动控制系统的扩张状态观测器以及Q学习调参技术。背景技术在过去的几十年中，通过设计观测器对运动控制系统的状态进行估计，并在控制律中实现状态反馈，已经被证明是一种有效的控制方法。但大多数观测器仍有其局限，如依赖于系统模型，只能对状态、单一外扰进行估计而无法处理包含内部未知动态和外部扰动的复杂不确定性，详见参考文献。基于以上问题，我国学者韩京清提出了不依赖于模型、将内部不确定动态和外部扰动视为扩张状态“总扰动”进行估计的扩张状态观测器，详见参考文献。针对ESO的参数调节问题，参考文献提出了针对线性扩张状态观测器的‘带宽法’并得到了广泛应用，参考文献-分别对常值参数的线性ESO和非线性ESO进行了分析，并给出了能够保证ESO稳定性的参数调节方法，详见参考文献利用噪声的统计特性及不确定性变化范围信息提出了ESO的卡尔曼型增益优化调节方法。目前，主要的ESO参数调节主要为常值增益以及依赖于模型信息的优化方法。实际运动控制系统自身的特性和外部环境都会随时间而变化，因此其噪声特性及不确定动态特性都会发生改变，难以假设模型信息已知。因此，运动控制系统的扩张状态观测器设计需要不依赖于模型信息，实现在线利用数据进行学习及优化的增益调节方法，实现状态和不确定性的快速精确估计。发明内容本发明解决的技术问题是：针对运动控制系统，设计了基于Q学习算法调参的扩张状态观测器实现对系统状态和总扰动的有效实时估计，利用系统实时量测数据驱动扩张状态观测器增益的实时优化，从而增强了其估计能力和稳态性能。考虑如下的运动控制系统数学模型：其中t表示时间，x1∈R表示t时刻运动物体的位置，x2∈R表示t时刻运动物体的速度，b表示输入增益，u∈R表示t时刻系统的输入，d,t)∈R表示t时刻由系统内部不确定动态和外扰组成的总扰动，c∈R表示t时刻总扰动的导数；x1、x2表示t＝kh时刻运动物体的位置和速度，其中h为采样周期，k表示第k次采样，yi表示xi的量测值，vi为相应通道的量测噪声。考虑到运动控制系统的状态可以量测得到，所以设计目标是使扩张状态观测器能够根据实时数据调节自身参数，从而快速、准确地跟踪总扰动d,t)，并降低对噪声的敏感程度。本发明的技术解决方案包含如下三个步骤：步骤：根据的离散形式设计扩张状态观测器：由于实际系统中采样和控制输入都是离散的，所以需要考虑的离散逼近形式：x1h)、x2h)表示t＝h时刻运动物体的位置和速度，其中h为采样周期，k+1表示第k+1次采样，yih)表示xih)的量测值，vih)为相应通道的量测噪声。b表示输入增益，u∈R表示t＝kh时刻系统的输入，d,kh)∈R表示t＝kh时刻由系统内部不确定动态和外扰组成的总扰动，c∈R表示t＝kh时刻总扰动的导数。根据，设计线性扩张状态观测器如下：其中，β1,β2,β3为观测其增益，其设计方法为β1＝3ω,β2＝3ω2,β3＝ω3,ω＞0.和分别表示x2,d和c的估计值，ω称为t＝kh时刻的“观测器带宽”，即为需要调节的参数。步骤：设计Q学习算法:Q学习算法包含状态、行动、奖励和状态-行动值函数四个主要组成部分：基于系统当前所处状态，根据状态-行动值函数的值选择相应的行动，并得到相应的奖励，再依此更新状态-行动值函数的值。根据实际情况设计状态空间S和动空间Λ，其中Λ是一个有界的实数集，最大、最小值分别记为a；对所有的状态s∈S和行动a∈Λ，初始化相应的状态-行动值函数Q＝0，并选择折扣因子γ∈和满足如下条件的学习率序列对于状态-行动值函数Q，采用如下更新准则：其中，下角标n表示第n次在状态sj选择了行动aj；s'表示选择行动a后转移到的下一个状态；下角标j表示第j次进行Q学习。在系统运行过程中，每q次采样进行一次Q学习，从t＝jqh时刻到t＝qh时刻之间的带宽是一个常数，记为ωj＝ω,t∈第j次进行Q学习时通过以下公式计算系统所处状态，选取行动并计算奖励：状态：第j次进行Q学习时的状态定义为sj＝，其中sj,1和sj,2的定义为：行动：第j次进行Q学习时行动aj的选取规则如下：奖励：第j次进行Q学习所获得的奖励计算公式为：其中，λ∈为奖励参数，和分别为sj+1,2和rj,2经过归一化之后的值，sj+1,2与中定义相同，r2,j表示的方差，即步骤：通过Q学习调节扩张状态观测器的参数当系统运行到t＝jqh，j＝1,2,...时刻时，依照计算当前状态sj，并根据选取行动aj，然后通过如下规则对观测器带宽进行调节：和ω为提前设置的带宽上下限。调整带宽后，根据计算奖励函数rj和下一个状态sj+1，并按式更新Q值函数。为保证扩张状态观测器的稳定性，ω、和a需满足如下条件：其中，ε、m、M均为中间变量，其计算方法为：本发明与现有技术相比的优点在于：1.针对的系统为实际中常见的对象为连续、输出为采样的混杂系统，直接给出了对应的离散ESO结构和参数设计方法；2.不需要噪声和扰动的模型来优化扩张状态观测器参数，而是以数据驱动的方式实时地对参数进行调节，与传统的常值ESO相比，具有更精确跟踪内部不确定动态和外部扰动的能力；3.定量并且显式地给出了Q学习部分的四个参数的调节范围。此参数调节理论能够保证扩张状态观测器的稳定性，并且能够降低实际工程中调节参数所花费的成本。附图说明图1是本发明方法的流程图。图2是不同带宽扩张状态观测器对扰动的估计误差曲线。图3是不同带宽扩张状态观测器对扰动的估计误差曲线。图4是不同带宽扩张状态观测器对扰动的估计误差曲线。符号说明t：运动控制系统的运行时间，t∈h：运动控制系统的采样周期，h∈R；x1：运动物体在t时刻的位置，x1∈R；x2：运动物体在t时刻的速度，x2∈R；u：运动控制系统在t时刻的控制输入，u∈R；yi：运动控制系统在t＝kh时刻的输出，yi∈R,i＝1,2,k＝1,2,...；vi：运动控制系统在t＝kh时刻的量测噪声，v∈Rn,i＝1,2,k＝1,2,...；d,t)：t时刻运动控制系统内部不确定动态与外部扰动的总和；c：d,t)的导数；x2的估计值；d的估计值；c的估计值；ω：扩张状态观测器的带宽，待调参数；ω：带宽的上、下限；sj：第j次进行Q学习时的状态；aj：第j次进行Q学习时选择的行动；rj：第j次进行Q学习选择行动后得到的奖励；a：行动aj的上、下限。具体实施方式为了检验针对运动控制系统的Q学习扩张状态观测器的适用性，我们进行仿真实验。考虑如下运动控制系统：与如下三类“总扰动”：其中第一类为常值扰动，与时间和系统状态均无关；第二类为分段线性扰动，只依赖于时间；第三类为非线性动态和周期性外扰的复合。根据步骤，设计扩张状态观测器：其中β1＝3ω,β2＝3ω2,β3＝ω3,ω＞0，根据步骤中的式，设计Q学习扩张状态观测器参数为ω＝0.5,a＝-0.5，h＝0.001,q＝100. 在本次仿真中行动集选为{-0.5,0,0.5}，其他参数设置为h＝0.001,q＝100,λ＝0.4，γ＝0.9. 每100次采样按照步骤中的公式计算当前状态sj并按照公式选取行动aj，然后按照调整观测器带宽。其他采样时刻观测器带宽保持不变。在调整带宽的同时，由计算上一次行动的奖励并根据步骤当中的更新Q值，其中计算奖励函数时归一化方法采用当前数值除以其数量级的方法。图2至图4为Q学习扩张状态观测器在三种“总扰动”下的仿真结果，将Q学习扩张状态观测器的初始带宽设置为5，与常值带宽的扩张状态观测器进行对比。第一种情况下，d,t)为常值扰动0.15。带宽取为10时，观测器受噪声影响严重，导致估计误差始终在之间震荡；带宽取为2时，观测器在约10s之后达到稳态，稳态时估计误差处于之间；带宽通过Q学习调整时，观测器在1s时达到稳态，并且在稳态时估计误差处于之间，由于初始阶段数据不足，QESO的调节效果较差，但是在后续阶段通过Q学习的调节，最终得到了良好的性能。第二种情况下，d,t)为分段线性的扰动。带宽取为10时，当扰动变化较快，观测器能够较快地跟踪扰动，但当扰动变化较慢时，噪声的影响也比较明显，估计误差始终在之间；带宽取为2时，对于扰动变化较慢的部分，观测器能取得良好的估计效果，但对于变化较快的部分追踪能力不足，会有较大跟踪误差，整体跟踪误差在之间。带宽通过Q学习调整时，观测器能够在快速追踪扰动和抑制噪声影响之间取得较好的平衡，除去初始阶段由数据不足引起的调节效果较差，在10s时进入稳态后估计误差始终维持在之间，估计效果最好。第三种情况下，系统中的不确定动态为非线性系统动态和周期性外扰的复合。带宽取为10时，噪声的影响比较明显，估计误差始终在之间；带宽取为2时，由于扰动变化较快，跟踪误差较大，整体跟踪误差在之间；带宽通过Q学习调整时，除去初始阶段由数据不足引起的调节效果较差，在1.5s时进入稳态后估计误差始终维持在之间，估计效果最好。参考文献Chi-TsongChen.Linear system theory and design.Holt,Rinchart andWinston,1984.韩京清.一类不确定对象的扩张状态观测器.控制与决策,1995,000:85-88.Gao Z.Scaling and bandwidth-parameterization based controllertuning//IEEE.IEEE,2003.Xue W,Yi H.Performance analysis of active disturbance rejectiontracking control for a class of uncertain LTI systems.Isa Transactions,2015,58:133-154.Guo B Z,Zhao Z L.On the convergence of an extended state observerfor nonlinear systems with uncertainty.Systems&amp;Control Letters,2011,60:420-430.Bai W,Xue W,Huang Y,et al.On extended state based Kalman filterdesign for a class of nonlinear time-varying uncertain systems.ScienceChina Information Sciences,2018,61:1-16.
