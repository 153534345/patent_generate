标题title
神经网络处理单元、神经网络的处理方法及其装置
摘要abst
本申请公开了一种神经网络的处理方法、神经网络处理单元、神经网络的处理方法及其装置，涉及深度学习、语音技术等领域。具体实现方案为：通过神经网络处理单元NPU中的量化单元获取浮点型的输入数据，对浮点型的输入数据进行量化得到量化后的输入数据，并将量化后的输入数据提供至运算单元，从而由NPU中的运算单元对量化后的输入数据执行矩阵向量操作和/或卷积操作，以得到输入数据的运算结果，之后，由量化单元对运算单元输出的运算结果进行反量化，得到反量化结果。由此，通过采用专门的NPU，来实现矩阵计算和/或卷积计算，当该NPU应用于语音芯片中时，可以降低语音芯片中核心的处理负担，提升语音芯片中核心的处理效率。
权利要求书clms
1.一种神经网络处理单元NPU，包括：量化单元，用于获取浮点型的输入数据，对所述浮点型的输入数据进行量化得到量化后的输入数据，并将所述量化后的输入数据提供至运算单元；以及，用于对所述运算单元输出的运算结果进行反量化，得到反量化结果；所述运算单元，用于对所述量化后的输入数据执行矩阵向量操作和/或卷积操作，以得到所述输入数据的运算结果。2.根据权利要求1所述的NPU，其中，所述运算单元，用于执行矩阵向量操作，所述量化单元用于：根据数字信号处理器DSP内部的存储器所存储的浮点型的输入数据，求得用于量化的第一参数和用于反量化的第二参数；对所述浮点型的输入数据中待量化的浮点值乘以第一参数，并求整后转化为数值型，以得到数值型的输入数据；将所述数值型的输入数据发送至所述运算单元；将所述运算单元得到的运算结果转化为浮点型；将浮点型的运算结果乘以所述第二参数后发送至DSP的存储器进行存储。3.根据权利要求2所述的NPU，其中，所述NPU还包括所述总线的主接口；所述主接口，用于通过所述总线向所述DSP发送内存拷贝函数，以访问所述DSP内部的存储器，得到所述DSP内部的存储器所存储的所述浮点型的输入数据。4.根据权利要求1所述的NPU，其中，所述运算单元，用于执行卷积操作，所述量化单元用于：对所述浮点型的输入数据进行浮点转短型的转换操作，以对转换后的短型的输入数据执行卷积操作。5.根据权利要求4所述的NPU，其中，所述NPU通过高速访问接口连接随机存储器RAM；所述RAM，用于将所述短型的输入数据转存至所述RAM中。6.根据权利要求5所述的NPU，其中，所述运算单元包括第一寄存器、第二寄存器和累加器；所述第一寄存器，用于在第一周期从所述RAM中读取所述短型的输入数据；所述第二寄存器，用于在第一周期之后的多个后续周期，读取PSRAM中存储的至少部分网络参数，将每个周期读取的所述至少部分网络参数与所述第一寄存器中对应的输入数据进行点积运算；所述累加器，用于获取点积运算的结果，根据所述点积计算的结果进行累加，以得到卷积操作的运算结果。7.根据权利要求1-6任一项所述的NPU，其中，所述NPU还包括：激活单元，用于根据DSP存储的卷积操作的运算结果采用激活函数进行激活，并将激活结果提供给所述DSP存储。8.一种处理装置，包括：通过总线连接的如权利要求1-7任一项所述的神经网络处理单元NPU、伪静态随机存储器PSRAM和数字信号处理器DSP；其中，所述DSP，用于在内部的存储器中存储待处理的输入数据；以及存储所述NPU对所述输入数据的运算结果；所述PSRAM，用于存储神经网络的网络参数。9.一种神经网络的处理方法，应用于神经网络处理单元NPU，所述NPU包括量化单元和运算单元，所述处理方法包括：所述量化单元获取浮点型的输入数据，对所述浮点型的输入数据进行量化得到量化后的输入数据，并将所述量化后的输入数据提供至运算单元；所述运算单元对所述量化后的输入数据执行矩阵向量操作和/或卷积操作，以得到所述输入数据的运算结果；所述量化单元对所述运算单元输出的运算结果进行反量化，得到反量化结果。10.根据权利要求9所述的方法，其中，所述量化单元根据数字信号处理器DSP内部的存储器所存储的浮点型的输入数据，求得用于量化的第一参数和用于反量化的第二参数，对所述浮点型的输入数据中待量化的浮点值乘以第一参数，并求整后转化为数值型，以得到数值型的输入数据，以及将所述数值型的输入数据发送至所述运算单元；所述运算单元对所述数值型的输入数据执行矩阵向量操作，得到所述运算结果；所述量化单元将所述运算结果转化为浮点型，并将浮点型的运算结果乘以所述第二参数后发送至DSP的存储器进行存储。11.根据权利要求10所述的方法，其中，所述NPU还包括所述总线的主接口；所述主接口，用于通过所述总线向所述DSP发送内存拷贝函数，以访问所述DSP内部的存储器，得到所述DSP内部的存储器所存储的所述浮点型的输入数据。12.根据权利要求9所述的方法，其中，所述量化单元对所述浮点型的输入数据进行浮点转短型的转换操作；所述运算单元对转换后的短型的输入数据执行卷积操作，得到所述运算结果。13.根据权利要求12所述的方法，其中，所述NPU通过高速访问接口连接随机存储器RAM；所述RAM，用于将所述短型的输入数据转存至所述RAM中。14.根据权利要求13所述的方法，其中，所述运算单元包括第一寄存器、第二寄存器和累加器；所述第一寄存器在第一周期从所述RAM中读取所述短型的输入数据；所述第二寄存器在第一周期之后的多个后续周期，读取PSRAM中存储的至少部分网络参数，将每个周期读取的所述至少部分网络参数与所述第一寄存器中对应的输入数据进行点积运算；所述累加器获取点积运算的结果，根据所述点积计算的结果进行累加，以得到卷积操作的运算结果。15.根据权利要求9-14任一项所述的方法，其中，所述NPU还包括激活单元，所述方法还包括：激活单元根据DSP存储的卷积操作的运算结果采用激活函数进行激活，并将激活结果提供给所述DSP存储。16.一种电子设备，包括：至少一个处理器；以及与所述至少一个处理器通信连接的存储器；其中，所述存储器存储有可被所述至少一个处理器执行的指令，所述指令被所述至少一个处理器执行，以使所述至少一个处理器能够执行权利要求9-15任一项所述的神经网络的处理方法。17.一种存储有计算机指令的非瞬时计算机可读存储介质，其中，所述计算机指令用于使所述计算机执行根据权利要求9-15任一项所述的神经网络的处理方法。18.一种计算机程序产品，包括计算机程序，所述计算机程序在被处理器执行时实现根据权利要求9-15任一项所述的神经网络的处理方法。
说明书desc
技术领域本申请涉及深度学习、语音技术等AI领域，尤其涉及神经网络处理单元、神经网络的处理方法及其装置。背景技术目前对于智能音箱等电子设备中的语音芯片，双核架构中的一个核心用于语音处理，一个核心用于实现主控MCU的功能。然而，通过单核心处理所有语音，处理负担较大。发明内容本申请提供了一种用于神经网络处理单元、神经网络的处理方法及其装置。根据本申请的一方面，提供了一种NPU，包括：量化单元，用于获取浮点型的输入数据，对所述浮点型的输入数据进行量化得到量化后的输入数据，并将所述量化后的输入数据提供至运算单元；以及，用于对所述运算单元输出的运算结果进行反量化，得到反量化结果；所述运算单元，用于对所述量化后的输入数据执行矩阵向量操作和/或卷积操作，以得到所述输入数据的运算结果。根据本申请的另一方面，提供了一种处理装置，包括：通过总线连接的如上述一方面提出的神经网络处理单元NPU、伪静态随机存储器PSRAM和数字信号处理器DSP；其中，所述DSP，用于在内部的存储器中存储待处理的输入数据；以及存储所述NPU对所述输入数据的运算结果；所述PSRAM，用于存储神经网络的网络参数。根据本申请的又一方面，提供了一种神经网络的处理方法，应用于神经网络处理单元NPU，所述NPU包括量化单元和运算单元，所述处理方法包括：所述量化单元获取浮点型的输入数据，对所述浮点型的输入数据进行量化得到量化后的输入数据，并将所述量化后的输入数据提供至运算单元；所述运算单元对所述量化后的输入数据执行矩阵向量操作和/或卷积操作，以得到所述输入数据的运算结果；所述量化单元对所述运算单元输出的运算结果进行反量化，得到反量化结果。根据本申请的又一方面，提供了一种电子设备，包括：至少一个处理器；以及与所述至少一个处理器通信连接的存储器；其中，所述存储器存储有可被所述至少一个处理器执行的指令，所述指令被所述至少一个处理器执行，以使所述至少一个处理器能够执行本申请上述提出的神经网络的处理方法。根据本申请的再一方面，提供了一种计算机指令的非瞬时计算机可读存储介质，所述计算机指令用于使所述计算机执行本申请上述提出的神经网络的处理方法。根据本申请的还一方面，提供了一种计算机程序产品，包括计算机程序，所述计算机程序在被处理器执行时实现本申请上述提出的神经网络的处理方法。应当理解，本部分所描述的内容并非旨在标识本申请的实施例的关键或重要特征，也不用于限制本申请的范围。本申请的其它特征将通过以下的说明书而变得容易理解。附图说明附图用于更好地理解本方案，不构成对本申请的限定。其中：图1为本申请实施例一所提供的NPU的结构示意图；图2为本申请实施例二所提供的NPU的结构示意图；图3为本申请实施例中卷积计算过程示意图；图4为本申请实施例三所提供的处理装置的结构示意图；图5为本申请实施例四所提供的处理装置的结构示意图；图6为本申请实施例五所提供的神经网络的处理方法的流程示意图；图7示出了可以用来实施本申请的实施例的示例电子设备的示意性框图。具体实施方式以下结合附图对本申请的示范性实施例做出说明，其中包括本申请实施例的各种细节以助于理解，应当将它们认为仅仅是示范性的。因此，本领域普通技术人员应当认识到，可以对这里描述的实施例做出各种改变和修改，而不会背离本申请的范围和精神。同样，为了清楚和简明，以下的描述中省略了对公知功能和结构的描述。为了节省语音芯片的成本，以及满足平衡算法需求，可以降低语音芯片的片内内存，再使用SIP封装PSRAM扩展内存的方法，将原语音芯片通过ESP32外挂PSRAM的方案的成本降低。即，现有方案中，是将PSRAM放在ESP32的主控芯片端，且外置于板级，需要额外的成本，因此，可以将PSRAM封装至语音芯片内，配合片内内存的降低，节省了外挂PSRAM的成本。然而，随着片内内存的降低，高带宽的内部内存减少，数据加载的速度会降低，从而带来了AI计算和模型数据并行加载的风险，因此，如何提升PSRAM的带宽利用率至关重要。并且，为了节省语音芯片的面积，可以将语音芯片中主控MCU的功能从ESP32中挪到语音芯片中，语音芯片的双核架构中只有一个核心留给语音处理。然而，将双核心的计算量全部放到一个核心之后，8x8，16x8乘加运算的算力不足，单核心处理所有语音处理的压力较大。因此针对上述存在的问题，本申请提出一种神经网络处理单元、神经网络的处理方法及其装置。下面参考附图描述本申请实施例的神经网络处理单元、神经网络的处理方法及其装置。图1为本申请实施例一所提供的NPU的结构示意图。如图1所示，该NPU100可以包括：量化单元110和运算单元120。其中，量化单元110，用于获取浮点型的输入数据，对浮点型的输入数据进行量化得到量化后的输入数据，并将量化后的输入数据提供至运算单元120；以及，用于对运算单元120输出的运算结果进行反量化，得到反量化结果。运算单元120，用于对量化后的输入数据执行矩阵向量操作和/或卷积操作，以得到输入数据的运算结果。在本申请实施例中，当NPU应用于语音芯片中时，浮点型的输入数据可以根据用户输入的语音数据的特征向量确定。相应的，反量化结果，用于确定语音数据对应的语音识别结果。应当理解的是，NPU也可以应用于其他芯片中，此时，浮点型的输入数据可以根据其他数据确定，比如可以根据图像的特征向量、视频帧的特征向量、文本的特征向量等确定，本申请对此并不作限制。在本申请实施例中，可以通过量化单元110获取浮点型的输入数据，对浮点型的输入数据进行量化得到量化后的输入数据，并将量化后的输入数据提供至运算单元120，相应的，运算单元120在接收到量化后的输入数据后，可以对量化后的输入数据执行矩阵向量操作和/或卷积操作，以得到输入数据的运算结果，并将运算结果输出至量化单元110，量化单元110在接收到运算结果后，可以对运算结果进行反量化，得到反量化结果。由此，通过采用专门的硬件NPU，来实现矩阵计算和/或卷积计算，当该NPU应用于语音芯片中时，可以降低语音芯片中核心的处理负担，提升语音芯片中核心的处理效率。本申请实施例的NPU，通过量化单元获取浮点型的输入数据，对浮点型的输入数据进行量化得到量化后的输入数据，并将量化后的输入数据提供至运算单元，从而由运算单元对量化后的输入数据执行矩阵向量操作和/或卷积操作，以得到输入数据的运算结果，之后，由量化单元对运算单元输出的运算结果进行反量化，得到反量化结果。由此，通过采用专门的NPU，来实现矩阵计算和/或卷积计算，当该NPU应用于语音芯片中时，可以降低语音芯片中核心的处理负担，提升语音芯片中核心的处理效率。为了清楚说明本申请上述实施例中是如何对输入数据进行量化，以及如何对运算单元120输出的运算结果进行反量化的，下面以运算单元120执行矩阵向量操作进行示例说明。当运算单元120执行矩阵向量操作时，量化单元110可以用于根据DSP内部的存储器所存储的浮点型的输入数据，求得用于量化的第一参数和用于反量化的第二参数；对浮点型的输入数据中待量化的浮点值乘以第一参数，并求整后转化为数值型char，以得到数值型的输入数据；将数值型的输入数据发送至运算单元120；将运算单元120得到的运算结果转化为浮点型；将浮点型的运算结果乘以第二参数后发送至DSP的存储器进行存储。在本申请实施例中，用于量化的第一参数和用于反量化的第二参数是根据浮点型的输入数据确定的。作为一种示例，可以确定浮点型的输入数据对应的向量最大值，标记向量最大值为fmax，第一参数为B，第二参数为A，则B可以为127.0f/fmax，A可以为fmax/127.0f。其中，一个char的取值范围是-128-127，量化时，可以将fmax映射为127这个量化值，以获得最大的精度；f是指float。在本申请实施例中，NPU100中的量化单元110可以根据DSP内部的存储器所存储的浮点型的输入数据，求得用于量化的第一参数和用于反量化的第二参数，将浮点型的输入数据中待量化的浮点值乘以第一参数，并求整后转化为数值型的输入数据，将数值型的输入数据发送至运算单元120，由运算单元120对数值型的输入数据执行矩阵向量操作，以得到输入数据的运算结果，运算单元120将运算结果发送至量化单元110，由量化单元110将运算单元120计算得到的运算结果转化为浮点型，并将浮点型的运算结果乘以第二参数后，得到反量化结果，并将反量化结果发送至DSP的存储器进行存储，从而后续操作可以由DSP的软件执行。由此，一方面，可以实现通过专门的量化单元，来实现量化过程，可以保证NPU100有效执行矩阵计算过程。另一方面，通过将浮点型的输入数据存储在DSP的存储器中，同时，将矩阵向量操作的运算结果存储在DSP的存储器中，从而DSP无需和NPU进行Cache一致性的设计，可以极大地简化硬件设计，解决DSP和NPU的数据一致性问题。其中，数据一致性，是指DSP访问NPU的RAM时，访问数据会映射到Cache中，如果NPU修改NPURAM中的数据，DSP是无法看见NPURAM中的修改数据的，而只能看到Cache中的数据，从而造成了数据一致性问题。而NPU访问DSP内部的存储器时，DSP内部的存储器对于DSP和NPU同时可见，不会出现数据一致性问题。作为一种示例，NPU100中的量化单元110可以确定浮点型的输入数据对应的向量最大值fmax，根据fmax确定用于量化的第一参数B和用于反量化的第二参数A，在执行矩阵向量操作中，可以将输入数据中的所有浮点值乘以B，然后求整并转换为浮点型char，将char型的输入数据发送至运算单元120，由运算单元120对char型的输入数据与char型的神经网络参数weight执行8x8的矩阵向量操作，将矩阵向量操作的结果输出至累加器ACC，ACC输出的结果即为运算结果，可以将ACC输出的运算结果转化为浮点型，并将浮点型的运算结果乘以A后，发送至DSP的存储器)中进行存储。在本申请实施例的一种可能的实现方式中，可以通过PSRAM存储神经网络的网络参数，运算单元120可以读取PSRAM中存储的至少部分网络参数，根据读取到的至少部分网络参数对数值型的输入数据执行矩阵向量操作，并同步继续读取PSRAM中的其余网络参数。由此，可以实现边读取网络参数，边执行矩阵向量操作，即可以实现数据读取/加载和计算的并行，以提升计算效率。作为一种应用场景，以神经网络应用于语音识别场景进行示例，上述输入数据可以根据用户输入的语音数据的特征向量确定，运算单元输出的运算结果，用于确定语音数据对应的语音识别结果。作为另一种应用场景，以神经网络应用于图像识别场景或视频识别场景中进行示例，上述输入数据可以根据图像或视频帧的特征向量确定，相应的，运算单元输出的运算结果，用于确定图像或视频帧的分类结果。一种示例，以神经网络用于身份识别进行示例性说明，上述输入数据可以根据图像或视频帧的特征向量确定，相应的，上述运算结果用于确定图像或视频帧中目标对象的身份信息。另一种示例，以神经网络用于活体检测进行示例性说明，上述输入数据可以根据图像或视频帧的特征向量确定，相应的，上述运算结果用于确定图像或视频帧中是否存在活体。比如当神经网络输出的概率值大于或者等于预设阈值时，分类结果为存在活体，而当神经网络输出的概率值小于预设阈值时，分类结果为未存在活体。又一种示例，以神经网络用于违禁图片检测进行示例性说明，上述输入数据可以根据图像或视频帧的特征向量确定，相应的，上述运算结果用于确定图像或视频帧是否为违禁图片。比如当神经网络输出的概率值大于或者等于预设阈值时，分类结果为：图像或视频帧为违禁图片，而当神经网络输出的概率值小于预设阈值时，分类结果为：图像或视频帧为正常图片。作为又一种应用场景，以神经网络应用于语音翻译场景中进行示例，上述输入数据可以根据用户输入的语音数据的特征向量确定。相应的，运算单元输出的运算结果，用于确定语音翻译结果。举例而言，以神经网络应用于中英互译场景中进行示例性说明，上述输入数据可以根据中文的语音数据的特征向量确定，相应的，上述运算结果用于确定语音数据对应的英文翻译结果，该英文翻译结果可以为语音形式，或者也可以为文本形式，对此不作限制。在本申请实施例的一种可能的实现方式中，NPU100可以通过总线访问DSP内部的存储器，具体地，NPU100还可以包括总线的主接口，其中，主接口用于通过总线向DSP发送内存拷贝函数memcpy，以访问DSP内部的存储器，得到DSP内部的存储器所存储的浮点型的输入数据。由此，可以实现有效读取DSP内部的存储器所存储的输入数据，从而可以保证NPU100有效执行计算过程。并且，DSP内部的存储器对于DSP和NPU同时可见，通过总线访问DSP内部的存储器，还可以避免出现数据一致性问题。在本申请实施例的一种可能的实现方式中，当运算单元120执行卷积操作时，量化单元110可以用于：对浮点型的输入数据进行浮点转短型的转换操作，以由运算单元120对转换后的短型的输入数据执行卷积操作。由此，可以实现将量化过程简化为浮点型转换为短型定点的过程，不仅可以保证卷积过程的精度，还可以减少量化过程的计算开销。其中，浮点型的输入数据可以存储在DSP内部的存储器中。在本申请实施例的一种可能的实现方式中，NPU100可以通过高速访问接口连接RAM，RAM可以从NPU获取短型的输入数据，并将短型的输入数据转存至RAM中，从而后续在计算过程中，运算单元120可以从RAM中有效获取短型的输入数据，对短型的输入数据执行卷积操作。即，本申请中，可以通过RAM，对量化单元110输出的短型的输入数据进行存储。其中，上述RAM为NPU的RAM，简称为NPURAM。为了清楚说明本申请上述实施例中是如何对短型的输入数据执行卷积操作的，本申请提供另一种NPU。图2为本申请实施例二所提供的NPU的结构示意图。如图2所示，该NPU200可以包括：量化单元210和运算单元220，其中，运算单元220包括第一寄存器221、第二寄存器222和累加器223。其中，量化单元210，用于对浮点型的输入数据进行浮点转短型的转换操作，以对转换后的短型的输入数据执行卷积操作。其中，NPU200通过高速访问接口连接RAM；RAM，用于将短型的输入数据转存至RAM中。第一寄存器221，用于在第一周期从RAM中读取短型的输入数据。第二寄存器222，用于在第一周期之后的多个后续周期，读取PSRAM中存储的至少部分网络参数，将每个周期读取的至少部分网络参数与第一寄存器221中对应的输入数据进行点积运算。累加器223，用于获取点积运算的结果，根据点积计算的结果进行累加，以得到卷积操作的运算结果。举例而言，标记网络参数为weight’，可以将网络参数weight’分成8份weight”，每一份weight”通过总线读取，卷积操作，只针对短型的输入数据和weight”，在某个周期获取到某一份weight”时，利用该weight”和短型的输入数据执行卷积操作过程中，运算单元可以读取下一个weight”，从而可以实现读取/加载过程和卷积计算过程的并行，提升卷积计算的效率。例如，标记输入数据为I，神经网络的网络参数为W，以输入数据为128字节bytes进行示例，第一周期可以读取输入数据中的前4个byte，在第二个周期至第三十三个周期时，读取32周期的网络参数，即读取128byte的网络参数，如图3所示，可以同时将输入数据的前4个byte与网络参数的128byte进行点积运算，累加器ACC一共累加32个周期的点积运算的结果。例如，图3中的ACC1的输出为：W×I+W×I+W×I+W×I，同理，ACC2的输出为：W×I+W×I+W×I+W×I，以此类推，ACC32的输出为：W×I+W×I+W×I+W×I。之后再次读取输入数据中的4个byte以及读取32周期的网络参数，并执行点积运算，将点积运算的结果发送至累加器进行累加，直到输入数据中的所有byte消耗完，即直到输入数据中的所有byte均参与运算，矩阵运算结束。由此，可以实现在网络参数的加载或读取过程中，利用已读取的网络参数执行卷积操作，可以实现数据读取/加载和卷积计算的并行，提升卷积计算效率。在本申请实施例的一种可能的实现方式中，当该NPU应用于语音芯片中时，为了进一步降低语音芯片中核心的处理负担，NPU中还可以包括高性能的激活单元，通过激活单元对卷积操作的运算结果进行激活。具体地，卷积操作的运算结果可以发送至DSP的存储器进行存储，激活单元可以通过总线访问DSP内部的存储器，获取DSP存储的卷积操作的运算结果，根据卷积操作的运算结果采用激活函数进行激活，并将激活结果提供给DSP存储，从而后续操作可以由DSP的软件执行。上述实施例为NPU的结构，本申请还提供一种处理装置的结构。图4为本申请实施例三所提供的处理装置的结构示意图。如图4所示，该处理装置可以包括：通过总线连接的如上述任一实施例提出的NPU410、PSRAM420和DSP430。其中，DSP430，用于在内部的存储器中存储待处理的输入数据；以及存储NPU对输入数据的运算结果。PSRAM420，用于存储神经网络的网络参数。在本申请实施例中，NPU410可以通过总线访问DSP430内部的存储器，以读取得到待处理的输入数据，以及通过总线访问PSRAM420得到至少部分网络参数，根据读取到的至少部分网络参数对输入数据执行矩阵向量操作和卷积操作中的至少一个，并同步继续读取PSRAM420中的其余网络参数，从而可以根据继续读取的其余网络参数对输入数据执行矩阵向量操作和卷积操作中的至少一个，以得到输入数据的运算结果。由此，可以实现一边读取或加载数据，一边执行计算过程，即可以实现数据读取/加载和计算的并行，从而可以提升计算效率。需要说明的是，相关技术中，PSRAM的数据需要靠Cache进行加载，Cache加载时DSP处于待机状态，当数据加载完成后，才能利用已加载的数据执行计算过程，计算效率较低。而本申请中，PSRAM420中网络参数的加载过程和NPU410的计算过程并行化执行，可以实现既提升数据加载的利用率，又大幅提升计算效率。以神经网络应用于语音识别场景中进行示例性说明，在计算效率大幅提升的情况下，可以使得该处理装置更适用于神经网络化的语音唤醒和识别任务。作为一种示例，以DSP为HiFiDSP进行示例，处理装置的结构可以如图5所示，NPU可以包括总线的主接口，该主接口通过总线访问HiFi DSP内部的存储器，另外，NPU还具有高速访问接口，通过该高速访问接口连接NPURAM。通过将浮点型的输入数据、矩阵向量操作的运算结果和卷积操作的运算结果存储至HiFi DSP内部的存储器中，HiFi DSP不需要和NPU进行Cache一致性的设计，即不必修改Cache结构或添加一致性总线，可以简化硬件的设计。在计算能力上，NPU内置128个8x8的乘加运算，支持4x32，8x16，16x8三种矩阵运算模式。同时兼容64个16x8的乘加运算，支持2x32，4x16，8x8三种卷积运算模式。其中，4x32是指，128个元素分为32组，每组的4个元素和输入数据的4个元素做点积，点积结果送至32个累加器中。如果输入数据的向量维度为N，则共需要N/4个周期完成1xN和Nx32的矩阵运算。8x16，16x8类似。矩阵运算，即矩阵向量操作，输入数据或输入向量量化为8bit，8bit乘8bit的向量乘矩阵运算，矩阵运算结果乘以输入数据的量化scale值。其中，神经网络的网络参数weight也是需要量化的，网络参数的量化过程可以由HiFi DSP的软件完成，即weight的缩放系数和偏移系数的操作可以由HiFi DSP的软件完成，因为这部分的计算量占比较低。上述操作，在64x64元素的8x8矩阵运算过程中，量化的算力占用约30％，8x8矩阵运算约67％，乘scale占3％。量化过程的占比较高，主要原因是浮点转短型定点的过程中，需要判断浮点的符号位，然后±0.5，再转换为int8整数，而这个操作HiFiDSP没有特定的加速指令，只能一个一个的执行。而通过本申请上述硬件加速的方式，可以采用专用电路的方法，即通过NPU执行矩阵运算可以将这部分的占比由30％降低至5％。配合矩阵运算每个周期8个乘加运算提升到128个乘加运算，大大的提升了计算效率。对于卷积操作，其输入采用16bit，从而将量化过程简化为浮点型*1024转换为short型定点的过程。原量化过程是求输入数据或输入向量的最大值absmax,所有值除max再乘127，该计算需要三个步骤，而浮点型*1024转short型定点只是其中第三个步骤。由此，既保证了卷积过程的精度，又减少了量化过程的计算开销。NPU中具有高性能的激活单元，实现sigmoid/tanh/log/exp等操作，精度和单精度浮点数学库接近，一个周期可以完成一个单元的计算，大大减少了使用HiFi DSP计算这些函数的时间，每个单元计算约需要400-1000周期。上述使用专用的量化单元减少量化的时间开销，本申请还可以通过对内存的极限使用，来提升计算效率。在不损失性能的前提下，可以尽量减少片内的SRAM的大小。相对于相关技术中的语音芯片，将有1MB+的存储放在PSRAM上，对于PSRAM只有166MB/s的带宽，如果10ms调用一次的话，仅读取这1MB的存储就需要占到60％的理论带宽，当计算效率为80％时，该占比会增加至75％。所以，首先需要将调用次数少的模型放到PSRAM中，例如放在PSRAM中的模型有30ms调用一次的模型。另外，需要在数据加载的时候，同时进行计算，且在片内进行模型Layer级别的缓冲，以减少重复加载。使用NPU硬件加速时，可以将网络参数的加载，存储到片内RAM，以及计算过程完全并行化，去除了等待加载后再进行计算的限制，从而使带宽利用率最大化，这个是HiFi DSP系统做不到的。因此，本申请中，使用硬件实现加载和计算的并行化，NPU既加载PSRAM中的网络参数，又同时进行矩阵运算。硬件加速对片内RAM进行每个周期128Bytes的读取，其带宽比HiFi DSP的64bits提升了16倍。前面介绍的输入过程有量化过程，或者浮点型转短型的过程，因为NPU硬件加速单元面积的考虑，这两个过程的硬件单元不能摆放128个，所以不需要128Bytes的读取速率。最终确定为总线64bit的读取带宽，摆放2个执行单元。所以对于浮点型的输入数据或输入向量，其存放位置需要放在HiFi DSP的核内；同时矩阵运算和卷积运算的结果也需要存回HiFi DSP的核内。这样HiFi DSP不需要和NPU进行Cache一致性的设计，从而大大简化了设计。而使用该处理装置的结构后，将计算密集型的部分使用NPU计算，HiFi DSP进行通用型的计算和语音信号处理的计算，从而达到了各种语音任务的最优计算效率，以及计算和加载的并行。本申请实施例的处理装置，通过采用专门的NPU，来实现矩阵计算和/或卷积计算，当该NPU应用于语音芯片中时，可以降低语音芯片中核心的处理负担，提升语音芯片中核心的处理效率。为了实现上述实施例，本申请还提出一种神经网络的处理方法。图6为本申请实施例五所提供的神经网络的处理方法的流程示意图。本申请实施例以该神经网络的处理方法，应用于神经网络处理单元NPU，NPU包括量化单元和运算单元。如图6所示，该神经网络的处理方法可以包括以下步骤：步骤601，量化单元获取浮点型的输入数据，对浮点型的输入数据进行量化得到量化后的输入数据，并将量化后的输入数据提供至运算单元。步骤602，运算单元对量化后的输入数据执行矩阵向量操作和/或卷积操作，以得到输入数据的运算结果。步骤603，量化单元对运算单元输出的运算结果进行反量化，得到反量化结果。在本申请实施例的一种可能的实现方式中，当运算单元执行矩阵向量操作时，量化单元根据数字信号处理器DSP内部的存储器所存储的浮点型的输入数据，求得用于量化的第一参数和用于反量化的第二参数，对浮点型的输入数据中待量化的浮点值乘以第一参数，并求整后转化为数值型，以得到数值型的输入数据，以及将数值型的输入数据发送至运算单元；运算单元对数值型的输入数据执行矩阵向量操作，得到运算结果；量化单元将运算结果转化为浮点型，并将浮点型的运算结果乘以第二参数后发送至DSP的存储器进行存储。作为一种可能的实现方式，NPU还包括总线的主接口；主接口，用于通过总线向DSP发送内存拷贝函数，以访问DSP内部的存储器，得到DSP内部的存储器所存储的浮点型的输入数据。在本申请实施例的另一种可能的实现方式中，当运算单元执行卷积操作时，量化单元对浮点型的输入数据进行浮点转短型的转换操作；运算单元对转换后的短型的输入数据执行卷积操作，得到运算结果。作为一种可能的实现方式，NPU通过高速访问接口连接RAM；RAM，用于将短型的输入数据转存至RAM中。作为一种可能的实现方式，运算单元包括第一寄存器、第二寄存器和累加器；第一寄存器在第一周期从RAM中读取短型的输入数据；第二寄存器在第一周期之后的多个后续周期，读取PSRAM中存储的至少部分网络参数，将每个周期读取的至少部分网络参数与第一寄存器中对应的输入数据进行点积运算；累加器获取点积运算的结果，根据点积计算的结果进行累加，以得到卷积操作的运算结果。作为一种可能的实现方式，NPU还包括激活单元，激活单元根据DSP存储的卷积操作的运算结果采用激活函数进行激活，并将激活结果提供给DSP存储。需要说明的是，前述任一实施例中对NPU的解释说明，以及对处理装置的解释说明，也适用于该实施例，其实现原理类似，在此不做赘述。本申请实施例的神经网络的处理方法，通过量化单元获取浮点型的输入数据，对浮点型的输入数据进行量化得到量化后的输入数据，并将量化后的输入数据提供至运算单元，从而由运算单元对量化后的输入数据执行矩阵向量操作和/或卷积操作，以得到输入数据的运算结果，之后，由量化单元对运算单元输出的运算结果进行反量化，得到反量化结果。由此，通过采用专门的NPU，来实现矩阵计算和/或卷积计算，当该NPU应用于语音芯片中时，可以降低语音芯片中核心的处理负担，提升语音芯片中核心的处理效率。为了实现上述实施例，本申请还提供一种电子设备，该电子设备可以包括至少一个处理器；以及与至少一个处理器通信连接的存储器；其中，存储器存储有可被至少一个处理器执行的指令，指令被至少一个处理器执行，以使至少一个处理器能够执行本申请上述任一实施例提出的神经网络的处理方法。为了实现上述实施例，本申请还提供一种存储有计算机指令的非瞬时计算机可读存储介质，其中，计算机指令用于使计算机执行本申请上述任一实施例提出的神经网络的处理方法。为了实现上述实施例，本申请还提供一种计算机程序产品，该计算机程序产品包括计算机程序，计算机程序在被处理器执行时实现本申请上述任一实施例提出的神经网络的处理方法。根据本申请的实施例，本申请还提供了一种电子设备、一种可读存储介质和一种计算机程序产品。图7示出了可以用来实施本申请的实施例的示例电子设备的示意性框图。电子设备旨在表示各种形式的数字计算机，诸如，膝上型计算机、台式计算机、工作台、个人数字助理、服务器、刀片式服务器、大型计算机、和其它适合的计算机。电子设备还可以表示各种形式的移动装置，诸如，个人数字处理、蜂窝电话、智能电话、可穿戴设备和其它类似的计算装置。本文所示的部件、它们的连接和关系、以及它们的功能仅仅作为示例，并且不意在限制本文中描述的和/或者要求的本申请的实现。如图7所示，设备700包括计算单元701，其可以根据存储在ROM702中的计算机程序或者从存储单元707加载到RAM703中的计算机程序，来执行各种适当的动作和处理。在RAM703中，还可存储设备700操作所需的各种程序和数据。计算单元701、ROM 702以及RAM 703通过总线704彼此相连。I/O接口705也连接至总线704。设备700中的多个部件连接至I/O接口705，包括：输入单元706，例如键盘、鼠标等；输出单元707，例如各种类型的显示器、扬声器等；存储单元708，例如磁盘、光盘等；以及通信单元709，例如网卡、调制解调器、无线通信收发机等。通信单元709允许设备700通过诸如因特网的计算机网络和/或各种电信网络与其他设备交换信息/数据。计算单元701可以是各种具有处理和计算能力的通用和/或专用处理组件。计算单元701的一些示例包括但不限于CPU、GPU、各种专用的AI计算芯片、各种运行机器学习模型算法的计算单元、DSP、以及任何适当的处理器、控制器、微控制器等。计算单元701执行上文所描述的各个方法和处理，例如上述神经网络的处理方法。例如，在一些实施例中，上述神经网络的处理方法可被实现为计算机软件程序，其被有形地包含于机器可读介质，例如存储单元708。在一些实施例中，计算机程序的部分或者全部可以经由ROM 702和/或通信单元709而被载入和/或安装到设备700上。当计算机程序加载到RAM 703并由计算单元701执行时，可以执行上文描述的神经网络的处理方法的一个或多个步骤。备选地，在其他实施例中，计算单元701可以通过其他任何适当的方式而被配置为执行上述神经网络的处理方法。本文中以上描述的系统和技术的各种实施方式可以在数字电子电路系统、集成电路系统、FPGA、ASIC、ASSP、SOC、CPLD、计算机硬件、固件、软件、和/或它们的组合中实现。这些各种实施方式可以包括：实施在一个或者多个计算机程序中，该一个或者多个计算机程序可在包括至少一个可编程处理器的可编程系统上执行和/或解释，该可编程处理器可以是专用或者通用可编程处理器，可以从存储系统、至少一个输入装置、和至少一个输出装置接收数据和指令，并且将数据和指令传输至该存储系统、该至少一个输入装置、和该至少一个输出装置。用于实施本申请的方法的程序代码可以采用一个或多个编程语言的任何组合来编写。这些程序代码可以提供给通用计算机、专用计算机或其他可编程数据处理装置的处理器或控制器，使得程序代码当由处理器或控制器执行时使流程图和/或框图中所规定的功能/操作被实施。程序代码可以完全在机器上执行、部分地在机器上执行，作为独立软件包部分地在机器上执行且部分地在远程机器上执行或完全在远程机器或服务器上执行。在本申请的上下文中，机器可读介质可以是有形的介质，其可以包含或存储以供指令执行系统、装置或设备使用或与指令执行系统、装置或设备结合地使用的程序。机器可读介质可以是机器可读信号介质或机器可读储存介质。机器可读介质可以包括但不限于电子的、磁性的、光学的、电磁的、红外的、或半导体系统、装置或设备，或者上述内容的任何合适组合。机器可读存储介质的更具体示例会包括基于一个或多个线的电气连接、便携式计算机盘、硬盘、RAM、ROM、EPROM或快闪存储器、光纤、CD-ROM、光学储存设备、磁储存设备、或上述内容的任何合适组合。为了提供与用户的交互，可以在计算机上实施此处描述的系统和技术，该计算机具有：用于向用户显示信息的显示装置或者LCD监视器)；以及键盘和指向装置，用户可以通过该键盘和该指向装置来将输入提供给计算机。其它种类的装置还可以用于提供与用户的交互；例如，提供给用户的反馈可以是任何形式的传感反馈；并且可以用任何形式来接收来自用户的输入。可以将此处描述的系统和技术实施在包括后台部件的计算系统、或者包括中间件部件的计算系统、或者包括前端部件的计算系统、或者包括这种后台部件、中间件部件、或者前端部件的任何组合的计算系统中。可以通过任何形式或者介质的数字数据通信来将系统的部件相互连接。通信网络的示例包括：LAN、WAN、互联网和区块链网络。计算机系统可以包括客户端和服务器。客户端和服务器一般远离彼此并且通常通过通信网络进行交互。通过在相应的计算机上运行并且彼此具有客户端-服务器关系的计算机程序来产生客户端和服务器的关系。服务器可以是云服务器，又称为云计算服务器或云主机，是云计算服务体系中的一项主机产品，以解决了传统物理主机与VPS服务中，存在的管理难度大，业务扩展性弱的缺陷。服务器也可以为分布式系统的服务器，或者是结合了区块链的服务器。其中，需要说明的是，人工智能是研究使计算机来模拟人的某些思维过程和智能行为的学科，既有硬件层面的技术也有软件层面的技术。人工智能硬件技术一般包括如传感器、专用人工智能芯片、云计算、分布式存储、大数据处理等技术；人工智能软件技术主要包括计算机视觉技术、语音识别技术、自然语言处理技术以及机器学习/深度学习、大数据处理技术、知识图谱技术等几大方向。根据本申请实施例的技术方案，通过量化单元获取浮点型的输入数据，对浮点型的输入数据进行量化得到量化后的输入数据，并将量化后的输入数据提供至运算单元，从而由运算单元对量化后的输入数据执行矩阵向量操作和/或卷积操作，以得到输入数据的运算结果，之后，由量化单元对运算单元输出的运算结果进行反量化，得到反量化结果。由此，通过采用专门的NPU，来实现矩阵计算和/或卷积计算，当该NPU应用于语音芯片中时，可以降低语音芯片中核心的处理负担，提升语音芯片中核心的处理效率。应该理解，可以使用上面所示的各种形式的流程，重新排序、增加或删除步骤。例如，本发公开中记载的各步骤可以并行地执行也可以顺序地执行也可以不同的次序执行，只要能够实现本申请公开的技术方案所期望的结果，本文在此不进行限制。上述具体实施方式，并不构成对本申请保护范围的限制。本领域技术人员应该明白的是，根据设计要求和其他因素，可以进行各种修改、组合、子组合和替代。任何在本申请的精神和原则之内所作的修改、等同替换和改进等，均应包含在本申请保护范围之内。
