标题title
一种融合韵律和个人信息的中文语音合成方法
摘要abst
本发明提供了一种融合韵律和个人信息的中文语音合成方法，中文语音合成方法包括如下步骤：步骤1：将输入文本、输入文本对应的拼音和输入文本的词法句法特征输入层次化韵律预测模型，得到输入文本的多层韵律信息；步骤2：将输入文本对应的拼音、声调等输入到声学模型，得到输入文本对应的语音特征谱图；步骤3：将多层韵律信息引入声学模型，将层次化韵律预测模型和声学模型进行联合，形成新的声学模型；步骤4：在新的声学模型中引入说话人信息，形成个性化语音合成模型，支持多人个性化语音合成。本发明有益效果：本发明在目前端到端合成模型的基础上，提高音频质量、速度；单人和多人的应用场景下，探索一种联合韵律预测任务和梅尔谱图生成任务的多任务学习方法，使得合成音频的停顿节奏感更自然，更贴近原始音频。
权利要求书clms
1.一种融合韵律和个人信息的中文语音合成方法，其特征在于，包括如下步骤：步骤1：将输入文本、输入文本对应的拼音和输入文本的词法句法特征输入层次化韵律预测模型，得到输入文本的多层韵律信息；步骤2：将输入文本对应的拼音、声调输入到声学模型，得到输入文本对应的语音特征谱图；步骤3：将多层韵律信息引入声学模型，将层次化韵律预测模型和声学模型进行联合形成新的声学模型；步骤4：在新的声学模型中引入说话人信息，形成个性化语音合成模型，支持多人个性化语音合成。2.根据权利要求1所述的中文语音合成方法，其特征在于，在所述步骤1中，所述层次化韵律预测模型包括编码模块和多任务学习模块，所述编码模块：将输入文本输入预训练的BERT语言模型，得到文本的上下文表示，将其与输入文本对应的拼音、词法句法特征表示进行拼接，然后经多层全连接网络进行编码；所述多任务学习模块是一个层次化的基于双向门控循环网络的条件随机场模型：将文本韵律分为四层，对于每一层的韵律，采用将低层韵律预测结果和编码层输出经全连接网络转换后输入条件随机场模型BiGRU-CRF预测下一层的方式实现。3.根据权利要求2所述的中文语音合成方法，其特征在于，在所述步骤2中，所述声学模型包括编码单元和解码模块，所述编码单元：将输入语句编码成上下文语义表示，在Transformer模块的基础上引入一个门控线性单元来增强对信息流的控制和三个卷积模块来建模局部信息；所述解码模块：自回归生成最终的语音特征谱图；包括一个注意力机制模块，通过注意力机制学习输入序列和输出序列的映射关系。4.根据权利要求3所述的中文语音合成方法，其特征在于，所述门控线性单元用于对信息流进行控制；所述三个卷积模块分别为两个仿Inception网络分支结构和深度可分离卷积模块；所述仿Inception网络分支结构：用于对特征层面进行多粒度融合；所述深度可分离卷积模块：用于对特征层面的深度和空间的信息进行解耦。5.根据权利要求3所述的中文语音合成方法，其特征在于，在所述注意力机制模块中，采用基于前向的注意力机制保持声学模型的注意力对齐路径单调递进，同时使用对角注意力损失来促进声学模型的收敛速度。6.根据权利要求5所述的中文语音合成方法，其特征在于，在所述注意力机制模块中，前向注意力机制保证当前解码时刻的注意力状态只能由前一时刻前一编码步位置或者相同编码步位置的注意力状态转移而来，确保了声学模型在学习对齐时，注意力权重对齐路径保持单调递进。7.根据权利要求1-6任一项所述的中文语音合成方法，其特征在于，通过三种优化步骤对声学模型进行优化，三种优化步骤分别为差分损失优化步骤、波形损失优化步骤、混合输入优化步骤，所述差分损失优化步骤：首先将真实谱图和合成谱图经过一阶差分计算得到对应处理图像，然后再对其两者进行均方差损失操作；所述波形损失优化步骤：首先通过声码器将真实谱图和语音特征谱图转换成对应的波形，然后计算两个波形之间的失真程度当作波形损失，使得声学模型生成的谱图经过相位重建之后的音频更接近原始音频；所述混合输入优化步骤：将预测信息和真实信息混合作为解码模块的输入。8.根据权利要求7所述的中文语音合成方法，其特征在于，在所述混合输入优化步骤中，在训练时刻，在每一个解码时刻采取上一时刻的预测值和该时刻的真实值的拼接作为解码模块的输入；在推理时刻，在每一个解码时刻，将上一时刻的预测值进行复制拼接的值作为解码模块的输入。9.一种中文语音合成系统，其特征在于，包括：存储器、处理器以及存储在所述存储器上的计算机程序，所述计算机程序配置为由所述处理器调用时实现权利要求1－8中任一项所述的中文语音合成方法的步骤。10.一种计算机可读存储介质，其特征在于：所述计算机可读存储介质存储有计算机程序，所述计算机程序配置为由处理器调用时实现权利要求1－8中任一项所述的中文语音合成方法的步骤。
说明书desc
技术领域本发明涉及语音处理技术领域，尤其涉及一种融合韵律和个人信息的中文语音合成方法。背景技术随着手机、平板、智能家居及可穿戴设备等都开始接入语音功能，人机交互方式逐渐走入语音时代。与传统的人机交互不同的是，语音交互具有便捷性、智能性，可以使得机器具有像人一样听说读写的综合能力。语音合成是智能语音交互系统的最后一环，负责让机器说出特点文本、特定说话人的语音音频，其分为文本分析和声学模型建模两个部分。文本分析主要是对文本进行特征提取，为后端提供发音、韵律等文本相关的信息；后端的工作是基于前端提取的语言特征来进行声学建模，从而获得自然可懂的语音输出。语音合成技术经过几十年的研究，从最早通过机械组件来模拟人体发声，逐渐发展成基于单元波形拼接和统计参数合成两个主要技术流派，虽然该两种技术模型产生的语音质量基本上满足可懂度的要求，但针对不同的语言，其模型设计方式和语言特征提取的方式也不同，需要开发人员具有较强的语言学背景，极大的限制了模型的通用性，同时复杂的组件设计导致错误不断的被传递累加，从而限制着合成语音的自然度。随着人工智能浪潮的兴起，越来越多的研究人员都开始将神经网络技术应用到语音合成领域，使得语音合成技术进入了一个新的时代。目前的主流语音合成系统一般都是采取基于端到端的声学模型加神经网络声码器的串联架构，该架构生成音频的主观测试分数十分接近原始音频。同时由于深度学习技术的蓬勃发展，复杂的声码器技术也得到不断优化提速，让语音合成工程化应用成为可能。随着合成音频音质的提升及合成速度的加快，公众的需求从最初的“可懂度”逐步发展到赋能场景的能力、对产品体验的提升上。目前市场上的大多数语音合成产品都是针对单人定制的，不能灵活的满足用户合成多个人声音的个性化需求。为了更好的迎合用户，探索多人合成技术是十分有必要的。因为多人语音合成技术可以提供一个通用模型，使得可以高效率、低成本的进行多人语音合成，具有很强的实用性。此外用户往往都希望获得自然流利的语音输出，促使了研究者将更多的注意力集中在文本的韵律结构预测任务中。正确的韵律结构信息引导合成系统学习到更恰当的停顿发音特点，尤其是针对中文这种极具韵律美的语言，用户更希望能够得到具有更自然的情感起伏、抑扬顿挫的语音。发明内容为了解决端到端中文语音合成中的长难句合成不稳定，音质发散、停顿韵律不自然的问题，本发明提供了一种中文语音合成方法，从用户的个性化需求出发，在目前端到端合成模型的基础上，提高音频质量、速度；同时在单人和多人的应用场景下，探索一种联合韵律预测任务和梅尔谱图生成任务的多任务学习方法，使得合成音频的停顿节奏感更自然，更贴近原始音频，并能支持多人语音合成。本发明提供了一种融合韵律和个人信息的中文语音合成方法，包括如下步骤：步骤1：将输入文本、输入文本对应的拼音和输入文本的词法句法特征输入层次化韵律预测模型，得到输入文本的多层韵律信息；步骤2：将输入文本对应的拼音、声调输入到声学模型，得到输入文本对应的语音特征谱图；步骤3：将多层韵律信息引入声学模型，将层次化韵律预测模型和声学模型进行联合形成新的声学模型；步步骤4：在新的声学模型中引入说话人信息，形成个性化语音合成模型，支持多人个性化语音合成。作为本发明的进一步改进，在所述步骤1中，所述层次化韵律预测模型包括编码模块和多任务学习模块，所述编码模块：将输入文本输入预训练的BERT语言模型，得到文本的上下文表示，将其与输入文本对应的拼音、词法句法特征表示进行拼接，然后经多层全连接网络进行编码；所述多任务学习模块是一个层次化的基于双向门控循环网络的条件随机场模型：将文本韵律分为四层，对于每一层的韵律，采用将低层韵律预测结果和编码层输出经全连接网络转换后输入条件随机场模型BiGRU-CRF预测下一层的方式实现。作为本发明的进一步改进，在所述步骤2中，所述声学模型包括编码单元和解码模块，所述编码单元：将输入语句编码成上下文语义表示，在Transformer模块的基础上引入一个门控线性单元来增强对信息流的控制和三个卷积模块来建模局部信息；所述解码模块：自回归生成最终的语音特征谱图；包括一个注意力机制模块，通过注意力机制学习输入序列和输出序列的映射关系。作为本发明的进一步改进，所述门控线性单元用于对信息流进行控制；所述三个卷积模块分别为两个仿Inception网络分支结构和深度可分离卷积模块；所述仿Inception网络分支结构：用于对特征层面进行多粒度融合；所述深度可分离卷积模块：用于对特征层面的深度和空间的信息进行解耦。作为本发明的进一步改进，在所述注意力机制模块中，采用基于前向的注意力机制保持声学模型的注意力对齐路径单调递进，同时使用对角注意力损失来促进声学模型的收敛速度。作为本发明的进一步改进，在所述注意力机制模块中，前向注意力机制保证当前解码时刻的注意力状态只能由前一时刻前一编码步位置或者相同编码步位置的注意力状态转移而来，确保了声学模型在学习对齐时，注意力权重对齐路径保持单调递进。作为本发明的进一步改进，通过三种优化步骤对声学模型进行优化，三种优化步骤分别为差分损失优化步骤、波形损失优化步骤、混合输入优化步骤，所述差分损失优化步骤：首先将真实谱图和合成谱图经过一阶差分计算得到对应处理图像，然后再对其两者进行均方差损失操作；所述波形损失优化步骤：首先通过声码器将真实谱图和语音特征谱图转换成对应的波形，然后计算两个波形之间的失真程度当作波形损失，使得声学模型生成的谱图经过相位重建之后的音频更接近原始音频；所述混合输入优化步骤：将预测信息和真实信息混合作为解码模块的输入。作为本发明的进一步改进，在所述混合输入优化步骤中，在训练时刻，在每一个解码时刻采取上一时刻的预测值和该时刻的真实值的拼接作为解码模块的输入；在推理时刻，在每一个解码时刻，将上一时刻的预测值进行复制拼接的值作为解码模块的输入。本发明还提供了一种中文语音合成系统，包括：存储器、处理器以及存储在所述存储器上的计算机程序，所述计算机程序配置为由所述处理器调用时实现本发明所述的中文语音合成方法的步骤。本发明还提供了一种计算机可读存储介质，所述计算机可读存储介质存储有计算机程序，所述计算机程序配置为由处理器调用时实现本发明所述的中文语音合成方法的步骤。本发明的有益效果是：本发明的中文语音合成方法从用户的个性化需求出发，在目前端到端合成模型的基础上，提高音频质量、速度；同时在单人和多人的应用场景下，探索一种联合韵律预测任务和梅尔谱图生成任务的多任务学习方法，使得合成音频的停顿节奏感更自然，更贴近原始音频。附图说明图1是本发明的层次化韵律停顿序列转化示意图；图2是本发明的韵律预测模型架构图；图3是本发明的声学模型架构图；图4是本发明的波形损失示意图；图5是本发明的混合输入示意图；图6是本发明的韵律预测与谱图预测联合学习框架图；图7是多人场景的韵律预测与谱图预测联合学习框架图。具体实施方式本发明公开了一种融合韵律和个人信息的中文语音合成方法，主要应用于智能人机交互产品中，比如智能音箱、手机助手、直播互动等。步骤1：将输入文本、输入文本对应的拼音和输入文本的词法句法特征输入层次化韵律预测模型，得到输入文本的多层韵律信息。将中文的四级韵律停顿转化成图1所示的层次化韵律停顿序列的形式。所述韵律预测模型对层次化的韵律停顿序列进行建模，包括编码模块和多任务学习模块。所述编码模块，预训练的BERT等语言模型将输入文本s转换成上下文表示rs，将rs与输入文本对应的拼音特征表示rp和词法句法特征表示rf进行拼接得到蕴含更丰富信息的表示rc。所述多任务学习模块是一个四层的BiGRU-CRF模型，所述编码模块输出rc经多层全连接网络进行转换后变成rm并输入第一层BiGRU-CRF，得到第一层韵律停顿及其表示l1。将rc和l1经全连接网络转换后输入第二层BiGRU-CRF，得到第二层韵律停顿及其表示l2。将rc和l2经全连接网络转换后输入第二层BiGRU-CRF，得到第三层韵律停顿及其表示l3。将rc和l3经全连接网络转换后输入第四层BiGRU-CRF，得到第四层韵律停顿及其表示l4。步骤2：将输入文本对应的拼音、声调等输入到声学模型，得到输入文本对应的语音特征谱图。如图3所示，所述声学模型包括编码单元和解码模块。所述编码单元：将输入语句编码成上下文语义表示，在Transformer模块的基础上引入一个门控线性单元用于增强对信息流的控制，减小梯度消失概率，引入三个卷积模块来分别增强对信息流的控制和提升局部信息建模能力。所述三个卷积模块分别为两个仿Inception网络分支结构和深度可分离卷积模块。所述仿Inception网络分支结构，相当于在特征层面做了一个多粒度融合。所述深度可分离卷积模块，用于对特征层面的深度和空间的信息进行解耦。所述解码模块：根据编码单元的输出和注意力机制计算得到的上下文信息，自回归生成最终的语音特征谱图；通过所述注意力机制学习输入序列和输出序列的映射关系；在所述注意力机制模块中，采用基于前向的注意力机制保持声学模型的注意力对齐路径单调递进，同时使用对角注意力损失来促进语音合成模型的收敛速度。所述前向注意力机制保证当前解码时刻的注意力状态只能由前一时刻前一编码步位置或者相同编码步位置的注意力状态转移而来，确保了声学模型在学习对齐时，注意力权重对齐路径保持单调递进，避免了出现“回头看”等情况。同时也保证了注意对齐的快速收敛和稳定性。通过引入对角引导注意力损失，可以使得模型快速的得到收敛且极大的降低了训练成本。通过三种优化步骤对语音合成模型进行优化，三种优化步骤分别为差分损失优化步骤、波形损失优化步骤、混合输入优化步骤。步骤3：将多层韵律信息引入声学模型，采用如图6所示韵律预测和谱图生成的多任务框架将韵律预测模型和声学模型进行联合形成新的声学模型。韵律预测模型和声学模型相互促进，分别提高两者的性能，提升语音合成效果。将韵律预测模型的输入部分也作为声学模型输入的一部分，将韵律预测模型预测的韵律停顿经过FCN转换后作为声学模型中注意力机制模块输入的一部分。对声学模型的损失函数Lp和谱图生成任务损失函数Lm的进行线性组合：L＝αLm+Lp，其中，α是一个组合系数，用于调节每部分的权重，α越大，表示谱图生成任务对模型的影响越大，反之亦然。Lp和Lm均采用均方差和极大似然损失等。步骤4：在新的声学模型中引入说话人信息，形成个性化语音合成模型，支持多人个性化语音合成。在步骤3的基础上引入基于说话人编码，作为声学模型中注意力机制模块输入的一部分，实现支持多人语音合成的功能。本发明提出一种中文语音合成方法，在原有的声学模型基础上，提出根据目前主流模型的优缺点，设计了一种新的语音合成模型新架构Evotron，并引入了三种优化技巧，提升了系统的鲁棒性、泛化性及音频质量。同时和神经网络声码器搭建级联语音合成系统吗，达到了实时合成的效果。为了进一步的提升中文合成音频中的停顿节奏感，提出了层次韵律预测框架和联合韵律学习策略，提升了合成音频的停顿节奏感，在多人场景下有助于捕捉到独特的发音特点。在本发明中，将韵律信息引入声学模型，合成停顿感更强的语音；将韵律模型与声学模型进行联合训练，提高两者的性能。在新的声学模型中引入说话人信息，建模说话人的音色特点，支持多人个性化语音合成。以上内容是结合具体的优选实施方式对本发明所作的进一步详细说明，不能认定本发明的具体实施只局限于这些说明。对于本发明所属技术领域的普通技术人员来说，在不脱离本发明构思的前提下，还可以做出若干简单推演或替换，都应当视为属于本发明的保护范围。
