标题title
一种基于注意力模型的视频摘要描述生成方法及装置
摘要abst
本发明公开了一种基于注意力模型的视频摘要描述生成方法及装置，其中方法包括：获取原始视频数据集及对应的视频摘要数据集并处理，提取对应视频帧的时序特征序列；将所述视频帧的时序特征序列输入到视频摘要模型中进行处理，生成对应的语义特征；对所述语义特征利用损失函数进行评价；本发明实现对视频摘要数据的处理，同时这种还能有效的保持了摘要与原视频之间语义的一致性。
权利要求书clms
1.一种基于注意力模型的视频摘要描述生成方法，其特征在于，包括：获取原始视频数据集及对应的视频摘要数据集并处理，提取对应视频帧的时序特征序列；将所述视频帧的时序特征序列输入到视频摘要模型中进行处理，生成对应的语义特征；对所述语义特征利用损失函数进行评价。2.根据权利要求1所述的一种基于注意力模型的视频摘要描述生成方法，其特征在于，所述视频摘要模型包括：摘要器、第一编码器、第一解码器、第二编码器及第二解码器；所述摘要器、所述第一编码器、所述第一解码器、所述第二编码器及所述第二解码器依次连接。3.根据权利要求2所述的一种基于注意力模型的视频摘要描述生成方法，其特征在于，生成对应的语义特征具体过程包括：将所述视频帧的时序特征序列输入至所述摘要器，对所述时序特征序列中的每一帧进行预测，得到对应的重要分数；将所述重要分数权重化，利用所述第一编码器进行编码生成潜在特征序列及每一帧对应的隐藏状态，所述第一解码器处理所述潜在特征序列及所述隐藏状态，生成语义特征。4.根据权利要求3所述的一种基于注意力模型的视频摘要描述生成方法，其特征在于，在利用所述第一编码器进行编码生成潜在特征序列及每一帧对应的隐藏状态时，获取注意力权重图。5.根据权利要求2所述的一种基于注意力模型的视频摘要描述生成方法，其特征在于，生成对应的语义特征具体过程还包括：将所述视频帧的时序特征序列输入至所述第二编码器及所述第二解码器，得到对应的原始视频数据集的语义描述。6.根据权利要求5所述的一种基于注意力模型的视频摘要描述生成方法，其特征在于，所述第一解码器还将原始视频数据集及对应的视频摘要数据集映射到相同的潜在语义空间中生成语义信息，引入语义一致性损失函数评价所述语义信息与所述语义特征的一致性。7.根据权利要求3所述的一种基于注意力模型的视频摘要描述生成方法，其特征在于，对所述重要分数进行归一化处理。8.一种基于注意力模型的视频摘要描述生成装置，其特征在于，包括：提取模块，所述提取模块用于生成视频帧的时序特征序列；处理模块，所述处理模块将所述视频帧的时序特征序列输入到视频摘要模型中进行处理，生成对应的语义特征；评价模块，所述评价模块用于对所述语义特征利用损失函数进行评价。9.根据权利要求8所述的一种基于注意力模型的视频摘要描述生成装置，其特征在于，所述处理模块包括：模型建立单元，所述模型建立单元用于建立视频摘要模型；潜在特征序列生成单元，所述潜在特征序列生成单元用于生成潜在特征序列及每一帧对应的隐藏状态；语义特征生成单元，所述语义特征生成单元用于处理潜在特征序列及每一帧对应的隐藏状态，生成对应的语义特征。
说明书desc
技术领域本发明涉及视频摘要技术领域，更具体的说是涉及一种基于注意力模型的视频摘要描述生成方法及装置。背景技术目前，视频是继文字之后的重要信息载体，承载了各式各样的视频图像信息，然而用户在面对大量的视频信息时，如何做到快速检索用户感兴趣的视频则成为比较关键的问题。因此当下急需要一种能够在不完全观看视频内容的情况下获得视频所要表达的核心内容的方法，进而为用户节省选择和浏览的时间。但是，虽然现有的视频摘要技术已经取得巨大的成功，但是它们的研究侧重还是聚焦于视频画面上，即关注视频帧之间的多样性和代表性。虽然近些年也有不少的方法开始关注于对视频的语义分析，但这类也大都关注于用户的需求，即依据用户的爱好和查询，选取出与之匹配的摘要，这种做法虽然在某种角度推动视频摘要研究的推进，但是这些方法也仅仅考虑了图片与文本之间的关系，而忽略了视觉信息在长时间跨度范围内的时序信息和语义连续性。同时在现有数据下，并不存在具有大规模的文本标注，这些问题都影响了视频摘要描述的发展，也不能满足用户的实际需求。因此，如何提供一种能够解决上述问题的视频摘要描述生成方法是本领域技术人员亟需解决的问题。发明内容有鉴于此，本发明提供了一种基于注意力模型的视频摘要描述生成方法及装置，实现对视频摘要数据的处理，同时这种还能有效的保持了摘要与原视频之间语义的一致性。为了实现上述目的，本发明采用如下技术方案：一种基于注意力模型的视频摘要描述生成方法，包括：获取原始视频数据集及对应的视频摘要数据集并处理，提取对应视频帧的时序特征序列；将所述视频帧的时序特征序列输入到视频摘要模型中进行处理，生成对应的语义特征；对所述语义特征利用损失函数进行评价。优选的，所述视频摘要模型包括：摘要器、第一编码器、第一解码器、第二编码器及第二解码器；所述摘要器、所述第一编码器、所述第一解码器、所述第二编码器及所述第二解码器依次连接。优选的，生成对应的语义特征具体过程包括：将所述视频帧的时序特征序列输入至所述摘要器，对所述时序特征序列中的每一帧进行预测，得到对应的重要分数；将所述重要分数权重化，利用所述第一编码器进行编码生成潜在特征序列及每一帧对应的隐藏状态，所述第一解码器处理所述潜在特征序列及所述隐藏状态，生成语义特征。优选的，在利用所述第一编码器进行编码生成潜在特征序列及每一帧对应的隐藏状态时，获取注意力权重图。优选的，生成对应的语义特征具体过程还包括：将所述视频帧的时序特征序列输入至所述第二编码器及所述第二解码器，得到对应的原始视频数据集的语义描述，能够在获得原视频的描述之后，就可以进行伪监督学习，避免了使用人工描述带来的影响。优选的，所述第一解码器还将原始视频数据集及对应的视频摘要数据集映射到相同的潜在语义空间中生成语义信息，引入语义一致性损失函数评价所述语义信息与所述语义特征的一致性。优选的，对所述重要分数进行归一化处理。进一步，本发明提供一种基于注意力模型的视频摘要描述生成装置，包括：提取模块，所述提取模块用于生成视频帧的时序特征序列处理模块，所述处理模块将所述视频帧的时序特征序列输入到视频摘要模型中进行处理，生成对应的语义特征；评价模块，所述评价模块用于对所述语义特征利用损失函数进行评价。优选的，所述处理模块包括：模型建立单元，所述模型建立单元用于建立视频摘要模型；潜在特征序列生成单元，所述潜在特征序列生成单元用于生成潜在特征序列及每一帧对应的隐藏状态；语义特征生成单元，所述语义特征生成单元用于处理潜在特征序列及每一帧对应的隐藏状态，生成对应的语义特征。经由上述的技术方案可知，与现有技术相比，本发明公开提供了一种基于注意力模型的视频摘要描述生成方法及装置，实现对视频摘要数据的处理，同时这种还能有效的保持了摘要与原视频之间语义的一致性。附图说明为了更清楚地说明本发明实施例或现有技术中的技术方案，下面将对实施例或现有技术描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本发明的实施例，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据提供的附图获得其他的附图。图1附图为本发明提供的一种基于注意力模型的视频摘要描述生成方法的流程图图2附图为本发明提供的一种基于注意力模型的视频摘要描述生成装置的结构原理框图；图3附图为本发明实施例2提供的视频关键帧图。具体实施方式下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例仅仅是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有做出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。实施例1参见附图1所示，本发明实施例1公开了一种基于注意力模型的视频摘要描述生成方法，包括：获取原始视频数据集及对应的视频摘要数据集并处理，提取对应视频帧的时序特征序列，时序特征序列可以表示为X＝{x1,x2,...,xT}，T表示视频的帧总数；将视频帧的时序特征序列输入到视频摘要模型中进行处理，生成对应的语义特征，语义特征可以表示为Z＝{z1,z2,...,zM}，M表示将会预测出的单词数；对语义特征利用损失函数进行评价。在一个具体的实施例中，视频摘要模型包括：摘要器、第一编码器、第一解码器、第二编码器及第二解码器；摘要器、第一编码器、第一解码器、第二编码器及第二解码器依次连接。摘要器的作用就是对视频帧进行评分，第一编码器、第一解码器则用于生成对视频摘要的描述，还要负责对原视频进行描述。具体的，摘要器、第一编码器、第一解码器、第二编码器及第二解码器均可以采用双向循环神经网络，摘要器记为sLSTM，第一编码器记为eLSTM，第一解码器记为dLSTM。在一个具体的实施例中，生成对应的语义特征具体过程包括：将视频帧的时序特征序列输入至摘要器，对时序特征序列中的每一帧进行预测，得到对应的重要分数s＝{st:st∈|t＝1,2...,T}，重要分数表示某一帧在视频中的重要性。在训练时，本实施例1将视频摘要看作经过重要性分数权重化的原视频特征，而在最终的测试时我们会将这些分数归一化为作为指示符；将重要分数权重化，作为我们的摘要特征，利用第一编码器进行编码生成潜在特征序列及每一帧对应的隐藏状态，潜在特征序列可以表示为E＝{e1,e2,...,eT}，隐藏状态可以表示为第一解码器处理潜在特征序列及隐藏状态，生成语义特征Z。在一个具体的实施例中，在利用第一编码器进行编码生成潜在特征序列E及每一帧对应的隐藏状态时，获取注意力权重图。具体的，当我们要计算一个语义信息zm时，我们首先要计算此刻的输入特征。为此我们使用编码器的输出与提取前一刻语义信息时伴随产生的隐藏状态计算注意力权重图。这个权重图将会权重化编码器的输出，并与前一刻预测的单词的嵌入特征级联，作为此刻的输入去获得此刻的语义信息。该处理过程如下所示：et,ht＝eLSTM注意力权重图的具体表达式如下：式中，⊙表示逐元素乘法，W表示单词序列，B表示注意力模块的偏量。在获得解码的语义特征Z之后，将其转换为单词序列W＝{w1,w2,...,wM}，其中w代表着一个被预测的单词，M表示这段描述的长度。因此，本发明实施例1将长视频序列直接送到语义信息一致网络，然后将视频的视觉信息转换为文本表示形式。通过这种方式，我们的框架可以逐步学习视频内容与其长时间范围内语义之间的映射关系。在一个具体的实施例中，生成对应的语义特征具体过程还包括：将视频帧的时序特征序列输入至第二编码器及第二解码器，得到对应的原始视频数据集的语义描述。具体的，由于现有的数据集中缺少对每个视频的描述，而唯一可作为语义分析的描述也仅仅是简短的标题，这些都不足以完成视频描述器对先验知识的获取。即使拥有描述，使用这些人工标注的描述也会影响模型的处理精度，降低可靠性。为此，本发明实施例1采用描述器作为原始视频的客观描述手段，这样既能产生用训练的标签又能避免人工描述带来的负面影响。同时这是一种能够保留原视频语义信息的可行性方法。在获得原视频的描述之后，就可以进行伪监督学习，避免了使用人工描述带来的影响。具体的，给予一段视频的特征向量V和一段经过编码过的句子{w1,w2,...,wM}，第一解码器会根据描述的前t个单词而调节自身，并学习预测下一个单词。在给定序列特征的情况下，应用相同的原理将其“翻译”成为其描述。本发明实施例1通过如下公式直接最大化给定特征的正确描述的概率：其中，θ是我们模型的参数，V是输入的视频特征，W是这段视频的正确描述。因为W代表了任何句子，所以他的长度是不定的。为此，普遍采用链式法则模拟{w1,w2,...,wM}的联合概率，如下：在一个具体的实施例中，第一解码器还将原始视频数据集及对应的视频摘要数据集映射到相同的潜在语义空间中生成语义信息，引入语义一致性损失函数评价语义信息与语义特征的一致性。具体的，在视频摘要工作中，需要摘要视频内能够尽可能地存留原视频的语义信息，生成的摘要视频才能正确地以简短的形式表达出原视频的内容。为此，摘要的语义特征应与原视频在潜在的特征空间相近。此处引入语义一致性损失函数，具体表达式为：式中，M表示视频预测出的句子长度，zm与z'm分别表示第mth个单词对应的语义特征。在使用摘要器、第一编码器和第一解码器预测视频描述时，第一解码器提取了有关视频内容的语义特征。同时，第二编码器和第二解码器同时还将原视频与摘要视频映射到相同的潜在语义空间中，方便了对二者语义信息的比较。在衡量原视频与摘要视频之间的语义信息是否匹配时，我们使用二者在语义空间中的距离作为参考，所以我们使用如下公式作为最终的损失计算：式中，T表示整个视频的帧数目，σ表示超参数，其主要的目的就是约束最后被选择的关键帧，约束其在整个视频帧的所占的比重。当数据提供关键帧的真实标注时，此时引入有监督的目标函数，该函数定义为：式中，表示的数据集中对关键帧的真实值，st尽可能呈现两极分化，因此采用的是二值交叉熵损失函数。在一个具体的实施例中，对重要分数进行归一化处理。进一步，参见附图2所示，本发明实施例1还提供一种基于注意力模型的视频摘要描述生成装置，包括：提取模块1，提取模块1用于生成视频帧的时序特征序列处理模块2，处理模块2将视频帧的时序特征序列输入到视频摘要模型中进行处理，生成对应的语义特征；评价模块3，评价模块3用于对语义特征利用损失函数进行评价。具体的，处理模块2包括：模型建立单元21，模型建立单元21用于建立视频摘要模型；潜在特征序列生成单元22，潜在特征序列生成单元22用于生成潜在特征序列及每一帧对应的隐藏状态；语义特征生成单元23，语义特征生成单元23用于处理潜在特征序列及每一帧对应的隐藏状态，生成对应的语义特征。实施例2为了验证本发明实施例1所提出的方法，具体的实验过程如下：1、数据准备本发明实施例2采用视频摘要中常用数据集，还增加了两个数据集。SumMe数据集：该数据集收集了25个视频，这些视频的主题涵盖了假日、活动以及体育等。这些视频是较为原始的或者经过少量的编辑，同时与已编辑的视频相比，它们的播放量也是很高的。数据集中的每个视频的时间长在1分钟到6分钟不等，且有15到18个人提供的帧级别的打分。TVSum数据集：该数据集从YouTube中收集了50视频，并且这些视频的类别选自TRECVid Multimedia Event Detection任务。整个数据集共有10个类别，涵盖了交通工具、动物、运动以及饮食等内容，且每种类别都挑选了5个视频。此外，这些视频的长短是不一样，但所有的视频时长在2分钟到5分钟之间，且每个视频都含有至少一个镜头。数据集中的视频还提供了由20个志愿者打出的帧级别的重要性分数。OVP数据集与YouTube数据集：这两个数据集并不是视频摘要任务中用作指标检测的数据，而是备用做数据增强使用的。OVP包含50个视频，通常是一些新闻以及文献记录等；YouTube也包含有50个视频，这些视频的长度在1分钟到10分钟之间。为了能够对我们所提出的模型做出有效的评估，如表1所示，我们对数据集做出如下调整：将数据分为未增强型与增强型。未增强型就在两个传统的数据对模型训练，分别抽取80％的视频作为训练集，剩下的20％作为测试集；增强型是在未增强型的训练集中添加了其他视频，如在80％的SumMe中添加了TVSum、OVP以及YouTube。在对训练集和测试集的划分中，我们随机划分五次，并取这五次划分所产生结果的均值，作为最后评价分数。此外，我们又引入一个在视频描述任务常用的数据集MSR-VTT。该数据集一共有10000个视频片段且共计20个类别，被分为训练、验证和测试集三部分。在我们的使用中，将三部分共同用作训练集，加强描述器的学习。同时，MSR-VTT中每个视频都被标注了大约20条英文句子。表1数据集的设置2、实验条件设置在对视频的特征提取上，本发明实施例2采用经由ImageNet训练过的ResNet152网络提取的深度特征，以2FPS的频率采样。模型中所涉及的编码器采用1024个隐藏单元的双向递归网络，解码器采用512个隐藏单元的单向递归网络；要求编码器最后一个隐藏状态初始化解码器的第一个隐藏状态，摘要器我们采用的是1024个隐藏单元的双向递归神经网络。在该实验中，根据MSR-VTT中视频描述的长度，将生成句子的最大长度设置为30。3、实验结果分析将会所提出的摘要模型进行实验分析，分别通过对所引入的损失函数做分析，此外还与视频摘要中出色的算法做出比较。最后展示我们模型在常规数据设置和增强设置下的表现，具体结果如表2所示。表2模型在不同条件下的性能表2所示的结果就是在不同情况下的模型产生结果以及相互之间的比较。在观察时发现，ScSUMsparsity的表现要优于ScSUMcaption，在两个数据集分别超出了0.4％和1.7％。这是因为加上长度稀疏性约束，网络将会被迫从其子集中重建原视频中较高一级的上下文。然后，模型会将更多的注意力放在最具有语义代表性的帧上，从而易于区分关键帧和非关键帧。没有上述约束，ScSUM很难保证对关键帧的选择。而语义一致损失使得模型的效果得到了进一步提升，ScSUMconsistent在两个数据集上分别获得0.8％和2.2％的提升。这是因为本章所设计的一致性损失函数成功地在摘要视频中保留了原视频的语义精华，而视频中精华的成分几乎就是视频中语义所呈现出的内容，所以摘要视频能够与原视频取得相近的内容呈现。这也佐证了一段好的摘要视频应当保证视频的语义内涵不丢失而不是仅仅考虑视觉内容的差异与丰富。ScSUM再将所有目标函数融入后取得更好的表现。另外，加入有监督的损失函数后，ScSUMsup再一次得到了提升。这是因为利用了人工注释标签作为监督，生成的摘要比ScSUM更符合人们对摘要的要求和喜好。为了更好的展示不同条件下模型的选择结果，在附图3中展示了在示例视频中所产生的结果，该结果以片段中的代表性关键帧形式表现。本说明书中各个实施例采用递进的方式描述，每个实施例重点说明的都是与其他实施例的不同之处，各个实施例之间相同相似部分互相参见即可。对于实施例公开的装置而言，由于其与实施例公开的方法相对应，所以描述的比较简单，相关之处参见方法部分说明即可。对所公开的实施例的上述说明，使本领域专业技术人员能够实现或使用本发明。对这些实施例的多种修改对本领域的专业技术人员来说将是显而易见的，本文中所定义的一般原理可以在不脱离本发明的精神或范围的情况下，在其它实施例中实现。因此，本发明将不会被限制于本文所示的这些实施例，而是要符合与本文所公开的原理和新颖特点相一致的最宽的范围。
