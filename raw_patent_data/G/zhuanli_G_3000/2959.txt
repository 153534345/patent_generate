标题title
基于自适应深度修正的纯视觉三维目标检测方法和系统
摘要abst
本发明公开了一种基于自适应深度修正的纯视觉三维目标检测方法，包括：获取图像并将该图像输入训练好的深度预测模型DORN中，以实时生成深度图其中图像的大小为深度图的大小为利用双线性差值方法对深度图进行4倍的下采样处理，以得到下采样后的深度图大小为W×H，其中将下采样后的深度图和图像一起输入预先训练好的三维目标检测模型中，以得到最终的三维目标检测结果。本发明能够解决现有特征视角转换的模型在同一个网络内部同时实现深度预测任务和三位目标检测任务，会加重网络学习的负担，对深度信息学习的不准确，从而影响最终的三维目标检测性能的技术问题。
权利要求书clms
1.一种基于自适应深度修正的纯视觉三维目标检测方法，其特征在于，包括以下步骤：获取图像并将该图像/＞输入训练好的深度预测模型DORN中，以实时生成深度图其中图像/＞的大小为/＞深度图/＞的大小为/＞利用双线性差值方法对步骤得到的深度图进行4倍的下采样处理，以得到下采样后的深度图/＞大小为W×H，其中/＞将步骤得到的下采样后的深度图和步骤获取的图像/＞一起输入预先训练好的三维目标检测模型中，以得到最终的三维目标检测结果。2.根据权利要求1所述的基于自适应深度修正的纯视觉三维目标检测方法，其特征在于，三维目标检测模型包括依次相连的图像特征编码网络、自适应深度修正网络、自适应视角转换网络、以及鸟瞰特征解码网络四个部分。3.根据权利要求1或2所述的基于自适应深度修正的纯视觉三维目标检测方法，其特征在于，对于图像特征编码网络而言，其具体结构为：第一层是特征抽取层，其从骨干网络提取下采样4倍的图像特征；具体而言，是从ResNet-101的block1阶段获取下采样四倍的图像特征大小为W×H×C，其中代表图像特征的分辨率，C代表通道数。第二层为采样层，对第一层中得到的下采样4倍的图像特征进行1×1的卷积操作，使该图像特征/＞的通道维度从C＝256降到C＝64，从而得到最终的图像特征F，其大小也为W×H×C。4.根据权利要求1至3中任意一项所述的基于自适应深度修正的纯视觉三维目标检测方法，其特征在于，自适应深度修正网络的具体结构为：第一层是Sigmoid层，其输入为深度图，利用Sigmoid函数对深度图进行归一化，以得到归一化后的深度图，大小为W×H。第二层是BroadcastAdd层，其输入是第一层得到的大小为W×H的归一化后的深度图和大小为W×H×C的图像特征F，对深度图和图像特征F进行广播式相加，以得到初步的深度融合特征，大小为W×H×C。第三层为卷积层，其对第二层得到的初步的融合特征进行3×3的卷积操作，之后接上BatchNorm层进行归一化，然后再经过ReLU激活函数，得到图像与深度图最终的融合特征，大小为W×H×C。第四层为深度偏差感知层，其利用1×1的卷积操作对第三层得到的融合特征进行卷积，以得到大小为W×H的深度偏差∈。第五层为深度偏差修正层，其将步骤得到的深度图与第四层得到的深度偏差∈进行逐像素相加，得到修正后的深度图，大小为W×H。5.根据权利要求4所述的基于自适应深度修正的纯视觉三维目标检测方法，其特征在于，自适应视角转换网络的具体结构为：第一层为深度分布参数图生成网络，其输入是自适应深度修正网络中第三层得到的融合特征，大小为W×H×C，利用1×1的卷积操作对其进行卷积，得到大小为W×H深度分布参数图，深度分布参数图中的每一个像素值代表这个像素对应的高斯分布标准差；第二层为高斯深度分布生成网络，输入是自适应深度修正网络中的第五层得到的修正后的深度图，以及第一层得到的大小为W×H的深度分布参数图，输出是W×H×D的高斯深度分布。第三层为特征视角转换层，输入是大小为W×H×C的图像特征和第二层得到的大小为W×H×D的高斯深度分布，经过相机转换矩阵与求和池化操作得到鸟瞰特征，大小为X×Y×C，通道数C＝256。6.根据权利要求5所述的基于自适应深度修正的纯视觉三维目标检测方法，其特征在于，鸟瞰特征解码网络的具体结构为：第一层为鸟瞰特征抽取网络，输入是大小为X×Y×C的图像特征，经过三层卷积进行特征抽取得到最终的鸟瞰特征，形状为X×Y×C。第二层为检测网络，输入是第一层得到的鸟瞰特征，形状为Y×Y×C，将鸟瞰特征输入PointPillar的检测头，输出最终的三维目标的类别、长、宽、高、旋转角等结果。7.根据权利要求6所述的基于自适应深度修正的纯视觉三维目标检测方法，其特征在于，三维目标检测模型是通过以下步骤训练得到的：获取图像将图像/＞输入训练好的深度预测模型，以得到该图像对应的深度图并对该深度图进行下采样，以得到下采样后的深度图/＞将步骤得到的图像输入图像特征编码网络的第一层，提取抽象的图像特征/＞大小为W×H×C，代表图像特征的分辨率，C代表通道数；将步骤得到的图像特征输入到图像特征编码网络的第二层中对其进行维度缩减，通道维度从C＝256降到C＝64，以得到最终的图像特征F，大小为W×H×C。将步骤得到的深度图D输入到自适应深度修正网络的第一层Sigmoid层进行归一化，并后将归一化后的深度图与步骤中得到的图像特征F的每个通道进行逐像素的相加，以得到初步的深度图与图像的融合特征Finit。将步骤得到初步的融合特征Finit输入到自适应深度修正网络的第三层进行3×3的卷积操作，之后接上BatchNorm层进行归一化，然后再经过ReLU激活函数，得到最终的图像与深度图的融合特征Ffus，大小为W×H×C。将步骤中得到的融合特征Ffus输入到自适应深度修正网络的第四层进行1×1的卷积操作，以获取深度偏差∈，其大小为W×H；将步骤得到的深度偏差∈与步骤中下采样后的深度图D输入到自适应深度修正网络的第五层，进行逐像素的相加，以获取修正后的深度图其大小为W×H。将步骤中得到的融合特征Ffus输入到自适应视角转换网络的第一层进行1×1的卷积操作，以获取分布参数图σ，其大小为W×H；将步骤得到的修正后的深度图以及步骤得到的分布参数图σ一起输入到自适应视角转换网络的第二层，经过Dist转换操作得到每个像素特征对应的高斯深度分布τ，其大小W×H×D。将步骤中得到的图像特征F和步骤得到的高斯深度分布τ作为输入，对两者进行外积，以获取加权后的像素特征；将步骤得到的加权后的像素特征投射到三维空间，利用PointPillar的方法生成柱状体素网格，利用生成的柱状体素网格对投射后的像素特征进行划分，并对划分结果进行求和池化操作，以获取鸟瞰特征其大小为X×Y×C，通道数C＝256。将得到的鸟瞰特征输入鸟瞰特征解码网络的第一层，经过三层卷积进行特征抽取，以获取最终的鸟瞰特征B，其形状为X×Y×C。将得到的最终的鸟瞰特征B输入输入鸟瞰特征解码网络的第二层，输出最终的三维目标的类别、长、宽、高和旋转角结果。根据步骤得到的三维目标的类别、长、宽、高和旋转角计算损失函数，并利用该损失函数对三维目标检测模型进行迭代训练，直到该三维目标检测模型收敛为止，从而得到训练好的三维目标检测模型。8.根据权利要求7所述的基于自适应深度修正的纯视觉三维目标检测方法，其特征在于，步骤中Dist转换操作的计算公式如下：其中代表像素坐标深度为li的概率，duv代表像素位置为的深度值，σuv代表像素位置为的高斯分布标准差，li代表固定的离散深度，exp代表e为底的指数函数，ε代表最小数用来避免分母为0的情况，exp为指数函数，公式表示如下：exP＝ex步骤中的操作是按照如下公式：其中cuv为像素坐标为的像素特征，代表像素坐标深度为li时的概率，/＞为加权后的像素特征。9.根据权利要求8所述的基于自适应深度修正的纯视觉三维目标检测方法，其特征在于，损失函数是由三部分组成：第一个为三维目标框回归损失，第二个为目标类别预测损失，第三个为目标框朝向预测损失；三维目标框回归损失采用Fast R-CNN中提出的smooth-L1损失函数，损失函数公式如下：其中回归模型预测得到的三维目标预测框的参数值为，三维标注框的参数值为，x,y,z,w,l,h,θ分别代表三维框的中心点、长宽高以及方向角，p代表预测框的参数，g代表标注狂的参数，那么三维预测框和标注框之间的中心点回归残差可以表示为：其中长宽高的回归残差可以表示为：角度的回归残差可以表示为：Δθ＝sin,则三维目标框回归损失可以表示为:目标类别预测损失采用Focal Loss作为损失函数，具体的损失函数公式如下：Lclass＝-αtγlogpt，其中系数αt为权重因子，用来控制正样本和负样本的权重，γ为调制系数，用来控制易分类样本和难分类样本的权重，pt为预测框类别的概率。目标框朝向预测损失沿用PointPillars提出的方法回归两个维度值对方向进行预测，并用交叉熵损失进行约束得到损失函数为Ldirection，最终的损失函数可表示为：其中Npos代表正样本框的数量，λ1,λ2,λ3分表代表各类损失的权重。10.一种基于自适应深度修正的纯视觉三维目标检测系统，其特征在于，包括：第一模块，用于获取图像并将该图像/＞输入训练好的深度预测模型DORN中，以实时生成深度图/＞其中图像/＞的大小为/＞深度图/＞的大小为/＞第二模块，用于利用双线性差值方法对第一模块得到的深度图进行4倍的下采样处理，以得到下采样后的深度图/＞大小为W×H，其中/＞第三模块，用于将第二模块得到的下采样后的深度图和第一模块获取的图像/＞一起输入预先训练好的三维目标检测模型中，以得到最终的三维目标检测结果。
说明书desc
技术领域本发明属于计算机视觉和自动驾驶技术领域，更具体地，涉及一种基于自适应深度修正的纯视觉三维目标检测方法和系统。背景技术三维目标检测在自动驾驶感知层上的具体实现主要分为单模态和多模态。单模态指只利用相机或者激光雷达的数据进行检测，虽然单一数据可以使模型建模简单，但是两种单模态的感知算法都存在一定的固有缺陷。目前基于激光点云和基于多模态融合的三维目标检测算法经过不断地发展，已经达到了很高的准确度，而基于纯视觉的方法的效果与前者的对比依然存在比较大的差距，但是由于很多现实原因寻找能够替代激光雷达进行准确的三维目标检测的方法依然是非常必要的。第一，目前激光雷达的造价非常昂贵，市面上供汽车使用的激光雷达传感器基本都在万元以上，无论是对车企的成本，还是用户后期的维修代价都非常大，这必然会影响自动驾驶技术在现实生活中的普及；第二，激光雷达一般要放置在车身顶部，这也会影响自动驾驶车辆本身设计的美观性，从而影响其销量。而对于相机来说，目前供汽车使用的相机传感器只需几百元，更容易替换，这便可大大地降低车辆的制造成本与用户的后期维修费用，更利于自动驾驶技术的普及，并且相机可以嵌入在车身内部，不会对车的美观造成影响。所以，研究利用相机替代激光雷达来进行三维目标检测，具有非常大的现实价值和应用前景，将更有利于推动自动驾驶技术在现实生活中的普及。以往基于纯视觉的三维目标检测方案通过拓展先进的二维目标检测方法来实现对三维目标的检测，这种方法是利用相机视角的二维特征去预测三维空间中的目标，视角的不统一会导致性能的受限。因此，最近很多方法开始尝试将RGB图像特征先转成鸟瞰特征，再在鸟瞰特征的基础上对三维目标进行检测，如CaDDN，BEVDet等模型，这种方案达到了相比以往方案更为优秀的检测性能，成为目前的主流方案。然而，上述现有的三维目标检测方法均存在一些不可忽略的缺陷。第一，以往进行特征视角转换的模型在同一个网络内部同时实现深度预测任务和三位目标检测任务，会加重网络学习的负担，对深度信息学习的不准确，从而影响最终的三维目标检测性能；第二，特征的视角转换需要准确的深度信息，但目前的方法在网络的内部为低分辨率的图像特征预测深度信息，直接通过预测出来的深度分布去实现特征的视角转换，这种方法其实将深度预测与三维目标检测这两个不同的任务集成在同一个网络中去实现，会加重网络学习的负担，对深度的预测准确性较低，从而会影响最终的三维目标检测器的性能；第三，以往模型中对于深度分布的预测并没有先验分布的指引，未知的分布形式可能会导致RGB图像特征在转换到鸟瞰视角后产生大量的噪音特征，从而会对最终的三维目标检测产生影响。发明内容针对现有技术的以上缺陷或改进需求，本发明提供了一种基于自适应深度修正的纯视觉三维目标检测方法。其目的在于，解决现有特征视角转换的模型在同一个网络内部同时实现深度预测任务和三位目标检测任务，会加重网络学习的负担，对深度信息学习的不准确，从而影响最终的三维目标检测性能的技术问题，以及现有特征的视角转换需要准确的深度信息，但目前的方法在网络的内部为低分辨率的图像特征预测深度信息，直接通过预测出来的深度分布去实现特征的视角转换，导致这种方法会加重网络学习的负担，对深度的预测准确性较低，从而会影响最终的三维目标检测器的性能的技术问题，以及现有模型中对于深度分布的预测并没有先验分布的指引，未知的分布形式可能会导致RGB图像特征在转换到鸟瞰视角后产生大量的噪音特征，从而会对最终的三维目标检测产生影响的技术问题。为实现上述目的，按照本发明的一个方面，提供了一种基于自适应深度修正的纯视觉三维目标检测方法，包括以下步骤：获取图像并将该图像/＞输入训练好的深度预测模型DORN中，以实时生成深度图/＞其中图像/＞的大小为/＞深度图/＞的大小为/＞利用双线性差值方法对步骤得到的深度图进行4倍的下采样处理，以得到下采样后的深度图/＞大小为W×H，其中/＞将步骤得到的下采样后的深度图和步骤获取的图像/＞一起输入预先训练好的三维目标检测模型中，以得到最终的三维目标检测结果。优选地，三维目标检测模型包括依次相连的图像特征编码网络、自适应深度修正网络、自适应视角转换网络、以及鸟瞰特征解码网络四个部分。优选地，对于图像特征编码网络而言，其具体结构为：第一层是特征抽取层，其从骨干网络提取下采样4倍的图像特征；具体而言，是从ResNet-101的block1阶段获取下采样四倍的图像特征大小为W×H×C，其中代表图像特征的分辨率，C代表通道数。第二层为采样层，对第一层中得到的下采样4倍的图像特征进行1×1的卷积操作，使该图像特征/＞的通道维度从C＝256降到C＝64，从而得到最终的图像特征F，其大小也为W×H×C。优选地，自适应深度修正网络的具体结构为：第一层是Sigmoid层，其输入为深度图，利用Sigmoid函数对深度图进行归一化，以得到归一化后的深度图，大小为W×H。第二层是BroadcastAdd层，其输入是第一层得到的大小为W×H的归一化后的深度图和大小为W×H×C的图像特征F，对深度图和图像特征F进行广播式相加，以得到初步的深度融合特征，大小为W×H×C。第三层为卷积层，其对第二层得到的初步的融合特征进行3×3的卷积操作，之后接上BatchNorm层进行归一化，然后再经过ReLU激活函数，得到图像与深度图最终的融合特征，大小为W×H×C。第四层为深度偏差感知层，其利用1×1的卷积操作对第三层得到的融合特征进行卷积，以得到大小为W×H的深度偏差∈。第五层为深度偏差修正层，其将步骤得到的深度图与第四层得到的深度偏差∈进行逐像素相加，得到修正后的深度图，大小为W×H。优选地，自适应视角转换网络的具体结构为：第一层为深度分布参数图生成网络，其输入是自适应深度修正网络中第三层得到的融合特征，大小为W×H×C，利用1×1的卷积操作对其进行卷积，得到大小为W×H深度分布参数图，深度分布参数图中的每一个像素值代表这个像素对应的高斯分布标准差；第二层为高斯深度分布生成网络，输入是自适应深度修正网络中的第五层得到的修正后的深度图，以及第一层得到的大小为W×H的深度分布参数图，输出是W×H×D的高斯深度分布。第三层为特征视角转换层，输入是大小为W×H×C的图像特征和第二层得到的大小为W×H×D的高斯深度分布，经过相机转换矩阵与求和池化操作得到鸟瞰特征，大小为X×Y×C，通道数C＝256。优选地，鸟瞰特征解码网络的具体结构为：第一层为鸟瞰特征抽取网络，输入是大小为X×Y×C的图像特征，经过三层卷积进行特征抽取得到最终的鸟瞰特征，形状为X×Y×C。第二层为检测网络，输入是第一层得到的鸟瞰特征，形状为X×Y×C，将鸟瞰特征输入PointPillar的检测头，输出最终的三维目标的类别、长、宽、高、旋转角等结果。优选地，三维目标检测模型是通过以下步骤训练得到的：获取图像将图像/＞输入训练好的深度预测模型，以得到该图像对应的深度图/＞并对该深度图进行下采样，以得到下采样后的深度图/＞将步骤得到的图像输入图像特征编码网络的第一层，提取抽象的图像特征/＞大小为W×H×C，代表图像特征的分辨率，C代表通道数；将步骤得到的图像特征输入到图像特征编码网络的第二层中对其进行维度缩减，通道维度从C＝256降到C＝64，以得到最终的图像特征F，大小为W×H×C。将步骤得到的深度图输入到自适应深度修正网络的第一层Sigmoid层进行归一化，并后将归一化后的深度图与步骤中得到的图像特征F的每个通道进行逐像素的相加，以得到初步的深度图与图像的融合特征Finit。将步骤得到初步的融合特征Finit输入到自适应深度修正网络的第三层进行3×3的卷积操作，之后接上BatchNorm层进行归一化，然后再经过ReLU激活函数，得到最终的图像与深度图的融合特征Ffus，大小为W×H×C。将步骤中得到的融合特征Ffus输入到自适应深度修正网络的第四层进行1×1的卷积操作，以获取深度偏差∈，其大小为W×H；将步骤得到的深度偏差∈与步骤中下采样后的深度图输入到自适应深度修正网络的第五层，进行逐像素的相加，以获取修正后的深度图/＞其大小为W×H。将步骤中得到的融合特征Ffus输入到自适应视角转换网络的第一层进行1×1的卷积操作，以获取分布参数图σ，其大小为W×H；将步骤得到的修正后的深度图以及步骤得到的分布参数图σ一起输入到自适应视角转换网络的第二层，经过Dist转换操作得到每个像素特征对应的高斯深度分布τ，其大小W×H×D。将步骤中得到的图像特征F和步骤得到的高斯深度分布τ作为输入，对两者进行外积，以获取加权后的像素特征；将步骤得到的加权后的像素特征投射到三维空间，利用PointPillar的方法生成柱状体素网格，利用生成的柱状体素网格对投射后的像素特征进行划分，并对划分结果进行求和池化操作，以获取鸟瞰特征其大小为X×Y×C，通道数C＝256。将得到的鸟瞰特征输入鸟瞰特征解码网络的第一层，经过三层卷积进行特征抽取，以获取最终的鸟瞰特征B，其形状为X×Y×C。将得到的最终的鸟瞰特征B输入输入鸟瞰特征解码网络的第二层，输出最终的三维目标的类别、长、宽、高和旋转角结果。根据步骤得到的三维目标的类别、长、宽、高和旋转角计算损失函数，并利用该损失函数对三维目标检测模型进行迭代训练，直到该三维目标检测模型收敛为止，从而得到训练好的三维目标检测模型。优选地，步骤中Dist转换操作的计算公式如下：其中代表像素坐标深度为li的概率，duv代表像素位置为的深度值，σuv代表像素位置为的高斯分布标准差，li代表固定的离散深度，exp代表e为底的指数函数，ε代表最小数用来避免分母为0的情况，exp为指数函数，公式表示如下：exp＝ex步骤中的操作是按照如下公式：其中cuv为像素坐标为的像素特征，代表像素坐标深度为li时的概率，/＞为加权后的像素特征。优选地，损失函数是由三部分组成：第一个为三维目标框回归损失，第二个为目标类别预测损失，第三个为目标框朝向预测损失；三维目标框回归损失采用Fast R-CNN中提出的smooth-L1损失函数，损失函数公式如下：其中回归模型预测得到的三维目标预测框的参数值为，三维标注框的参数值为，x,y,z,w,l,h,θ分别代表三维框的中心点、长宽高以及方向角，p代表预测框的参数，g代表标注狂的参数，那么三维预测框和标注框之间的中心点回归残差可以表示为：其中长宽高的回归残差可以表示为：角度的回归残差可以表示为：Δθ＝sin,则三维目标框回归损失可以表示为:目标类别预测损失采用Focal Loss作为损失函数，具体的损失函数公式如下：Lclass＝-αtγlogpt，其中系数αt为权重因子，用来控制正样本和负样本的权重，γ为调制系数，用来控制易分类样本和难分类样本的权重，pt为预测框类别的概率。目标框朝向预测损失沿用PointPillars提出的方法回归两个维度值对方向进行预测，并用交叉熵损失进行约束得到损失函数为Ldirection，最终的损失函数可表示为：其中Npos代表正样本框的数量，λ1,λ2,λ3分表代表各类损失的权重。按照本发明的另一方面，提供了一种基于自适应深度修正的纯视觉三维目标检测系统，包括：第一模块，用于获取图像并将该图像/＞输入训练好的深度预测模型DORN中，以实时生成深度图/＞其中图像/＞的大小为/＞深度图/＞的大小为/＞第二模块，用于利用双线性差值方法对第一模块得到的深度图进行4倍的下采样处理，以得到下采样后的深度图/＞大小为W×H，其中/＞第三模块，用于将第二模块得到的下采样后的深度图和第一模块获取的图像/＞一起输入预先训练好的三维目标检测模型中，以得到最终的三维目标检测结果。总体而言，通过本发明所构思的以上技术方案与现有技术相比，能够取得下列有益效果：本发明由于采用了步骤，利用专门的深度预测模型去预测深度，可以将深度预测任务与主体检测网络解耦，使主体网络专注于三维目标检测任务的学习，减轻网络的学习负担，因此能够解决现有方法中存在的网络学习负担过重的问题；本发明由于采用了步骤到，其通过一个新颖的自适应深度修正模块，在主体网络的内部对图像特征与深度图进行融合，去自适应的感知一阶段深度与真实深度之间的偏差来对一阶段深度进行修正，更准确的深度信息可以显著提升三维目标检测器性能，因此能够解决现有方法由于深度预测不准而影响最终三维目标检测效果的技术问题；本发明由于采用了步骤到，其设计了一个自适应视角转换模块，可以实现图像的相机视角特征到鸟瞰视角特征的有效转换。该模块的思想是以高斯分布作为先验分布指导的深度分布的生成，因此能够解决现有方法由于未知分布导致产生大量噪音特征的技术问题，可以有效地减少特征视角转换过程中噪音特征的生成；本发明具有非常好的通用性和有效性，在单目场景和多相机环视场景这两个场景下的实验都取得了比基准模型更优秀的检测效果，且可以无缝嵌入进许多具有视角转换过程的方法中。附图说明图1是本发明基于自适应深度修正的纯视觉三维目标检测方法的流程图；图2是深度预测模型生成的深度图示例；图3是本发明基于自适应深度修正的纯视觉三维目标检测方法的整体框架示意图；图4是本发明自适应深度修正网络的框架示意图；图5是本发明自适应视角转换网络的框架示意图。具体实施方式为了使本发明的目的、技术方案及优点更加清楚明白，以下结合附图及实施例，对本发明进行进一步详细说明。应当理解，此处所描述的具体实施例仅仅用以解释本发明，并不用于限定本发明。此外，下面所描述的本发明各个实施方式中所涉及到的技术特征只要彼此之间未构成冲突就可以相互组合。本发明的基本思路在于，从三个方面提升纯视觉场景下三维目标检测的效果。第一，考虑到目前主流方法将深度预测任务与三维目标检测任务统一在一个网络里实现，会加重网络学习的负担，本发明提出将深度预测任务与三维目标检测任务解耦，利用专门的深度预测模型去预测深度，让主体网络专注于对三维目标检测任务的学习，减轻主体网络的学习负担。第二，考虑到不准确的预测深度会对模型的性能产生影响，本发明设计了一个自适应深度修正模块对深度偏差进行感知，以获取更为准确的深度信息。第三，考虑到以往模型在进行特征视角转换的过程中会产生不确定的噪音特征，本发明提出了一个自适应视角转换模块来实现相机视角特征到鸟瞰视角特征的有效转换，以高斯分布作为先验分布指导深度分布的生成，减小噪音特征对模型性能的影响。最后在鸟瞰特征的基础上对三维物体的最终检测结果进行预测。本发明目的在于，将深度预测任务与三维目标检测任务解耦，利用专门的深度预测模型去预测深度，减轻网络的学习负担，同时考虑到不准确的预测深度会影响模型的性能，设计了一个自适应深度修正模块对深度偏差进行感知，以获得更为准确的深度信息，考虑到以往模型的特征视角转换过程会产生不确定的噪音特征，提出了一个自适应视角转换模块来实现相机视角特征到鸟瞰视角特征的有效转换，以高斯分布作为先验分布指导深度分布的生成，减小噪音特征对模型性能的影响。本发明分别在单目场景和多相机环视场景下进行了实验，都得到了不错的性能表现。在KITTI数据集上与基准模型相比，本发明在AP3D指标下的三种模式分别提升10.0％，3.02％和2.18％，在APBEV指标下的三种模式分别提升15.84％，7.58％和5.36％，在更大规模的NuScenes数据集上与基准模型相比，本方案在mAP和NDS指标上分别提升0.22％和0.06％，实验结果验证了本发明的有效性和通用性。如图1所示，本发明提供了一种基于自适应深度修正的纯视觉三维目标检测方法，包括以下步骤：获取图像并将该图像/＞输入训练好的深度预测模型DORN中，以实时生成深度图/＞本发明需要获得数据集每一张图像的深度图作为模型的一阶段深度。对于KITTI数据集，本发明利用提前训练好的单目深度预测模型DORN来对每张图像的深度进行实时预测，图2展示了KITTI数据集生成的深度图示例。具体而言，设输入的图像为大小为/＞将图像/＞输入训练好的深度预测模型DORN中，得到大小为/＞的深度图/＞利用双线性差值方法对步骤得到的深度图进行4倍的下采样处理，以得到下采样后的深度图/＞大小为W×H，其中/＞将步骤得到的下采样后的深度图和步骤获取的图像/＞一起输入预先训练好的三维目标检测模型中，以得到最终的三维目标检测结果。如图3所示，本发明的三维目标检测模型包括依次相连的图像特征编码网络、自适应深度修正网络、自适应视角转换网络、以及鸟瞰特征解码网络四个部分。对于图像特征编码网络而言，其具体结构为：第一层是特征抽取层，其从骨干网络提取下采样4倍的图像特征。具体而言，为保持较高的图像特征分辨率，从ResNet-101的block1阶段获取下采样四倍的图像特征大小为W×H×C，其中代表图像特征的分辨率，C代表通道数。第二层为采样层，对第一层中得到的下采样4倍的图像特征进行1×1的卷积操作，使该图像特征/＞的通道维度从C＝256降到C＝64，从而得到最终的图像特征F，其大小也为W×H×C。图4展示了自适应深度修正网络，其具体结构为：第一层是Sigmoid层，其输入为深度图下采样后的深度图完全相同)，利用Sigmoid函数对深度图进行归一化，以得到归一化后的深度图，大小为W×H。第二层是BroadcastAdd层，其输入是第一层得到的大小为W×H的归一化后的深度图和大小为W×H×C的图像特征F，对深度图和图像特征F进行广播式相加，以得到初步的深度融合特征，大小为W×H×C。第三层为卷积层，其对第二层得到的初步的融合特征进行3×3的卷积操作，之后接上BatchNorm层进行归一化，然后再经过ReLU激活函数，得到图像与深度图最终的融合特征，大小为W×H×C。第四层为深度偏差感知层，其利用1×1的卷积操作对第三层得到的融合特征进行卷积，以得到大小为W×H的深度偏差∈。第五层为深度偏差修正层，其将步骤得到的深度图与第四层得到的深度偏差∈进行逐像素相加，得到修正后的深度图，大小为W×H。经过自适应深度修正网络后，会得到更为准确的深度信息，减少预测深度与真实深度之间的偏差。图5展示了自适应视角转换网络的基本框架，其具体结构为：第一层为深度分布参数图生成网络，其输入是自适应深度修正网络中第三层得到的融合特征，大小为W×H×C，利用1×1的卷积操作对其进行卷积，得到大小为W×H深度分布参数图，对于深度分布参数图中的每一个像素值代表这个像素对应的高斯分布标准差。第二层为高斯深度分布生成网络，输入是自适应深度修正网络中的第五层得到的修正后的深度图，以及第一层得到的大小为W×H的深度分布参数图，输出是W×H×D的高斯深度分布。本层通过将高斯分布作为先验分布，指导深度分布的生成，可以减少噪音特征的产生。第三层为特征视角转换层，输入是大小为W×H×C的图像特征和第二层得到的大小为W×H×D的高斯深度分布，经过相机转换矩阵与求和池化操作得到鸟瞰特征，大小为X×Y×C，通道数C＝256。对于鸟瞰特征解码网络而言，其具体结构为：第一层为鸟瞰特征抽取网络，输入是大小为X×Y×C的图像特征，经过三层卷积进行特征抽取得到最终的鸟瞰特征，形状为X×Y×C。第二层为检测网络，输入是第一层得到的鸟瞰特征，形状为X×Y×C，将鸟瞰特征输入PointPillar的检测头，输出最终的三维目标的类别、长、宽、高、旋转角等结果。具体而言，本发明的三维目标检测模型是通过以下步骤训练得到的：获取图像将图像/＞输入训练好的深度预测模型，以得到该图像对应的深度图/＞并对该深度图进行下采样，以得到下采样后的深度图/＞本步骤中为图像数据集生成深度图的过程和上述步骤完全相同，深度图降采样过程和上述步骤完全相同，在此不再赘述；将步骤得到的图像输入图像特征编码网络的第一层，提取抽象的图像特征/＞大小为W×H×C，代表图像特征的分辨率，C代表通道数；经过本步骤以后，的分辨率大小相比于图像/＞缩小了四倍。步骤的优点在于，可以保持较高的图像特征分辨率，在之后步骤的视角转换过程后，鸟瞰特征会有更丰富的特征信息。将步骤得到的图像特征输入到图像特征编码网络的第二层中对其进行维度缩减，通道维度从C＝256降到C＝64，以得到最终的图像特征F，大小为W×H×C。将步骤得到的深度图D输入到自适应深度修正网络的第一层Sigmoid层进行归一化，并后将归一化后的深度图与步骤中得到的图像特征F的每个通道进行逐像素的相加，以得到初步的深度图与图像的融合特征Finit。将步骤得到初步的融合特征Finit输入到自适应深度修正网络的第三层进行3×3的卷积操作，之后接上BatchNorm层进行归一化，然后再经过ReLU激活函数，得到最终的图像与深度图的融合特征Ffus，大小为W×H×C。步骤的优点在于，经过以上深度信息融合步骤，就可以得到深度与图像抽象的融合特征。将步骤中得到的融合特征Ffus输入到自适应深度修正网络的第四层进行1×1的卷积操作，以获取深度偏差∈，其大小为W×H；本步骤中得到的深度偏差与深度图的大小相同。步骤的优点在于，没有任何先验条件和额外的损失对深度偏差的预测进行约束，可以让网络自适应的去学习预测深度与真实深度之间的偏差。将步骤得到的深度偏差∈与步骤中下采样后的深度图输入到自适应深度修正网络的第五层，进行逐像素的相加，以获取修正后的深度图/＞其大小为W×H。上述步骤到步骤的优点在于，可以让网络自适应的去感知预测深度和真实深度之间的偏差情况，对深度进一步修正，得到更准确的深度信息。将步骤中得到的融合特征Ffus输入到自适应视角转换网络的第一层进行1×1的卷积操作，以获取分布参数图σ，其大小为W×H；本步骤中得到的分布参数图与步骤得到的修正后的深度图的大小相同。将步骤得到的修正后的深度图以及步骤得到的分布参数图σ一起输入到自适应视角转换网络的第二层，经过Dist转换操作得到每个像素特征对应的高斯深度分布τ，其大小W×H×D。具体而言，本步骤中Dist转换操作的计算公式如下：其中代表像素坐标深度为li的概率，duv代表像素位置为的深度值，σuv代表像素位置为的高斯分布标准差，li代表固定的离散深度，exp代表e为底的指数函数，ε代表最小数用来避免分母为0的情况，exp为指数函数，公式表示如下：exp＝ex.步骤的优点在于，以高斯分布作为先验分布，指导深度分布的生成，可以有效减少噪音特征的产生，减少噪音特征对检测性能的影响。将步骤中得到的图像特征F和步骤得到的高斯深度分布τ作为输入，对两者进行外积，以获取加权后的像素特征；具体而言，本步骤的操作是按照如下公式：其中cuv为像素坐标为的像素特征，代表像素坐标深度为li时的概率，/＞为加权后的像素特征。将步骤得到的加权后的像素特征投射到三维空间，利用PointPillar的方法生成柱状体素网格，利用生成的柱状体素网格对投射后的像素特征进行划分，并对划分结果进行求和池化操作，以获取鸟瞰特征其大小为X×Y×C，通道数C＝256。上述步骤到步骤的优点在于，利用高斯分布来指导深度分布的生产，可以有效减少特征的视角转换过程中噪音特征的产生。将得到的鸟瞰特征输入鸟瞰特征解码网络的第一层，经过三层卷积进行特征抽取，以获取最终的鸟瞰特征B，其形状为X×Y×C。将得到的最终的鸟瞰特征B输入输入鸟瞰特征解码网络的第二层，输出最终的三维目标的类别、长、宽、高和旋转角结果。根据步骤得到的三维目标的类别、长、宽、高和旋转角计算损失函数，并利用该损失函数对三维目标检测模型进行迭代训练，直到该三维目标检测模型收敛为止，从而得到训练好的三维目标检测模型。本步骤使用的损失函数由三部分组成：第一个为三维目标框回归损失，第二个为目标类别预测损失，第三个为目标框朝向预测损失。对于三维目标框回归损失采用Fast R-CNN中提出的smooth-L1损失函数，损失函数公式如下：设回归模型预测得到的三维目标预测框的参数值为，三维标注框的参数值为，其中x,y,z,w,l,h,θ分别代表三维框的中心点、长宽高以及方向角，p代表预测框的参数，g代表标注狂的参数，那么三维预测框和标注框之间的中心点回归残差可以表示为:其中长宽高的回归残差可以表示为：角度的回归残差可以表示为：Δθ＝sin,则三维目标框回归损失可以表示为:对于目标类别预测损失，采用Focal Loss作为损失函数，具体的损失函数公式如下：Lclass＝-αtγlogpt，其中系数αt为权重因子，用来控制正样本和负样本的权重，γ为调制系数，用来控制易分类样本和难分类样本的权重，pt为预测框类别的概率。对于目标框朝向预测损失，本发明沿用PointPillars提出的方法回归两个维度值对方向进行预测，并用交叉熵损失进行约束得到损失函数为Ldirection，则最终的损失函数可表示为其中Npos代表正样本框的数量，λ1,λ2,λ3分表代表各类损失的权重。实验结果表1展示了本发明中的模型ADCNet与CaDDN在KITTI数据集下的实验结果对比，设定IoU阈值为0.7。可以发现在骨干网络、回归头，超参数设置和训练策略对齐的情况下，ADCNet的预测性能比CaDDN有了大幅的提升。在AP3D指标下，“简单easy”，“适中moderate”，“困难hard”三种困难模式分别提升10％、3.02％和2.18％，特别是在APBEV指标下，“简单easy”，“适中moderate”，“困难hard”三种模式分别提升15.84％、7.58％和5.36％，这充分展示了本方法的有效性，更准确的深度信息使得物体的定位更为的准确。表1表2展示了本发明中的模型ADCNet与BEVDet在NuScenes数据集下的实验结果对比，其中Size代表输入模型的图像分辨率，Param代表模型参数量。可以发现在骨干网络、回归头、损失函数、超参数设置、训练策略以及输入分辨率对齐的情况下，与BEVDet模型相比ADCNet获得了更好的性能，模型参数量只略微增加2.03M。表2本领域的技术人员容易理解，以上所述仅为本发明的较佳实施例而已，并不用以限制本发明，凡在本发明的精神和原则之内所作的任何修改、等同替换和改进等，均应包含在本发明的保护范围之内。
