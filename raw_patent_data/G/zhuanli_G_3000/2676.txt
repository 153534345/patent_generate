标题title
一种虚拟谈话数字人生成方法
摘要abst
一种虚拟谈话数字人生成方法，包括：S1，基于获取的真实录像的谈话视频，通过编码器结合语音特征分离出身份特征和脸部表情特征；S2，将脸部表情特征编码到隐变量空间中，得到谈话视频中的每一帧图像对应的表情隐变量表示；S3，在得到表情隐变量表示后，通过基于注意力机制的Transformer神经网络拟合出谈话音频对应的表情隐变量；S4，解码器基于身份特征和谈话音频对应的表情隐变量生成谈话头像视频；S5，根据谈话视频中的音频特征从全身的身体动作库选取对应的动作组；S6，对谈话头像视频和动作组进行融合，得到全身的虚拟谈话数字人。本发明能够得到形象更加逼真、更接近现实生活中人物的谈话虚拟数字人。
权利要求书clms
1.一种虚拟谈话数字人生成方法，其特征在于，包括以下步骤：S1，基于获取的真实录像的谈话视频，通过编码器结合语音特征分离出身份特征和脸部表情特征；S2，将脸部表情特征编码到隐变量空间中，得到谈话视频中的每一帧图像对应的表情隐变量表示，所述隐变量空间是一个可学习的面部表情表征字典，代表面部运动的合集；S3，在得到表情隐变量表示后，通过基于注意力机制的Transformer神经网络拟合出谈话音频对应的表情隐变量；S4，解码器基于身份特征和谈话音频对应的表情隐变量生成谈话头像视频，通过基于图像分块的对抗网络来提升图像质量；S5，根据谈话视频中的音频特征从全身的身体动作库选取对应的动作组；S6，对谈话头像视频和动作组进行融合，得到全身的虚拟谈话数字人。2.根据权利要求1所述的虚拟谈话数字人生成方法，其特征在于，步骤S1具体包括：S11，将获取的真实录像的谈话视频分离出每一帧图像，将每一帧图像中的人像根据眼部位置及嘴部位置裁剪并对齐头像位置，以得到多个谈话头像图片；S12，对得到的谈话头像图片提取对应的音频信号特征，根据音频信号特征和对应身份的无表情头像图片获取表情掩码特征；S13，编码器根据获取的表情掩码特征分离谈话头像图片的身份特征和脸部表情特征。3.根据权利要求2所述的虚拟谈话数字人生成方法，其特征在于，步骤S2具体包括：S21，设计一个可学习的面部表情表征字典，将获取到的脸部表情特征投影到该面部表情表征字典的线性组合空间中；S22，联合优化面部表情表征字典及其线性组合系数，使面部表情表征字典能最大程度表示脸部表情特征，从而得到谈话视频中的每一帧图像对应的表情隐变量表示，进而得到包含了所有面部表情的动作合集。4.根据权利要求3所述的虚拟谈话数字人生成方法，其特征在于，步骤S3具体包括：S31，根据步骤S2获取到的每一帧图像对应的表情隐变量表示，将其聚合到谈话视频的面部表情表征字典空间中作为训练时的监督；S32，获取谈话视频中的每一帧图像的对应的音频信号特征，设计一个基于注意力机制的Transformer神经网络，其输入为每一帧和其前面所有的图像对齐的音频信号特征，输出为这一帧图像的面部表情表征字典空间的拟合结果，迭代训练该神经网络，使拟合结果与步骤S31中的结果误差达到最小；S33，将谈话视频中的谈话音频输入到循环神经网络中，获取到谈话音频的表情隐变量，在测试时，将任意一段音频输入到神经网络中，获取到该音频未在面部表情表征字典空间中的表情隐变量。5.根据权利要求4所述的虚拟谈话数字人生成方法，其特征在于，步骤S4具体包括：S41，根据步骤S3的谈话音频对应的表情隐变量，结合步骤S2的面部表情表征字典重建高维表情特征；S42，解码器将步骤S41得到的高维表情特征结合步骤S1获取到的身份特征，生成出说话者在一段谈话音频下的对应视频；S43，采用基于图像块的对抗网络，提升说话人图像的图像质量，并基于提升图像质量后的说话人图像生成谈话头像视频。6.根据权利要求5所述的虚拟谈话数字人生成方法，其特征在于，步骤S5具体包括：S51，拍摄人体躯干运动视频，将运动视频分离出每一帧运动图像，并从每一帧运动图像中分离出头部与躯干，建立人体驱干运动动作库；S52，分析谈话视频中的每一帧图像的对应的音频信号特征，根据音频信号特征中的时长特征、说话的语气情绪特征，从人体驱干运动动作库中选取对应的动作组。7.根据权利要求6所述的虚拟谈话数字人生成方法，其特征在于，步骤S6具体包括：S61，将步骤S4获取到的谈话头像视频和步骤S5获取到的动作组对齐头部位置和躯干位置，将谈话头像视频对应的图片和动作组对应的图片进行融合，得到拼接图片；S62，对拼接图片做拼接后处理，利用图像混合技术对拼接位置的色差和位置偏差进行消除，从而得到全身的虚拟谈话数字人。
说明书desc
技术领域本发明涉及数据处理技术领域，特别是涉及一种虚拟谈话数字人生成方法。背景技术随着人工智能技术的迅速发展与普及，虚拟数字人技术逐渐成熟，并慢慢地进入人们的日常生活中。然而目前大量的虚拟数字人基本上以人造形象为主，该形象不是动漫角色就是对真人的模拟。目前以现实生活中的真人形象设计的虚拟谈话数字人主要应用在主持人播报场景下。传统的虚拟谈话数字人形象往往倾向于先建立数字人的三维形象，接着对该三维形象上贴上不同的纹理，再根据不同的音频条件驱动三维结构的变形，然后渲染出不同的图像。该方案往往在渲染出的图像上失真，无法达到现实生活中人物图像那般复杂的纹理结构，只对动漫形象较为简单化的形象具有较好的效果。近年来，随着深度学习神经网络在虚拟数字人领域不断的探索应用，传统方案的保真性得到了很大程度的解决，但随之带来的超大计算量，使得传统方案无法做到实时驱动，这对人机交互应用就带来巨大挑战。另一条深度学习数字人路线为驱动图像的变形，在实时驱动和形象保真两方面取得了平衡。本发明沿用该路线发明出语音驱动下形象逼真的实时谈话虚拟数字人。发明内容本发明的目的在于提供一种虚拟谈话数字人生成方法，能够得到形象更加逼真、更接近现实生活中人物的谈话虚拟数字人。一种虚拟谈话数字人生成方法，包括以下步骤：S1，基于获取的真实录像的谈话视频，通过编码器结合语音特征分离出身份特征和脸部表情特征；S2，将脸部表情特征编码到隐变量空间中，得到谈话视频中的每一帧图像对应的表情隐变量表示，所述隐变量空间是一个可学习的面部表情表征字典，代表面部运动的合集；S3，在得到表情隐变量表示后，通过基于注意力机制的Transformer神经网络拟合出谈话音频对应的表情隐变量；S4，解码器基于身份特征和谈话音频对应的表情隐变量生成谈话头像视频，通过基于图像分块的对抗网络来提升图像质量；S5，根据谈话视频中的音频特征从全身的身体动作库选取对应的动作组；S6，对谈话头像视频和动作组进行融合，得到全身的虚拟谈话数字人。根据本发明提供的虚拟谈话数字人生成方法，利用编码器自适应分离出身份特征与脸部表情特征，相对于人工标注的特征点表情或脸部运动肌肉特征，具有更丰富的语义表达能力；本发明将高维的脸部表情特征编码到低维的隐变量空间中，能够在最少地牺牲表示精度的情况下，压缩表情特征维度，为利用谈话音频拟合出表情隐变量提供了更好的条件；然后解码器基于身份特征和谈话音频对应的表情隐变量生成谈话头像视频，再根据谈话视频中的音频特征从动作库选取对应的动作组，并对谈话头像视频和动作组进行融合，得到虚拟谈话数字人，使得本发明能够得到形象更加逼真、更接近现实生活中人物的谈话虚拟数字人，具有实时性，形象逼真，音唇同步率高的特点。此外，上述的虚拟谈话数字人生成方法，还具有以下技术特征：进一步的，步骤S1具体包括：S11，将获取的真实录像的谈话视频分离出每一帧图像，将每一帧图像中的人像根据眼部位置及嘴部位置裁剪并对齐头像位置，以得到多个谈话头像图片；S12，对得到的谈话头像图片提取对应的音频信号特征，根据音频信号特征和对应身份的无表情头像图片获取表情掩码特征；S13，编码器根据获取的表情掩码特征分离谈话头像图片的身份特征和脸部表情特征。进一步的，步骤S2具体包括：S21，设计一个可学习的面部表情表征字典，将获取到的脸部表情特征投影到该面部表情表征字典的线性组合空间中；S22，联合优化面部表情表征字典及其线性组合系数，使面部表情表征字典能最大程度表示脸部表情特征，从而得到谈话视频中的每一帧图像对应的表情隐变量表示，进而得到包含了所有面部表情的动作合集。进一步的，步骤S3具体包括：S31，根据步骤S2获取到的每一帧图像对应的表情隐变量表示，将其聚合到谈话视频的面部表情表征字典空间中作为训练时的监督；S32，获取谈话视频中的每一帧图像的对应的音频信号特征，设计一个基于注意力机制的Transformer神经网络，其输入为每一帧和其前面所有的图像对齐的音频信号特征，输出为这一帧图像的面部表情表征字典空间的拟合结果，迭代训练该神经网络，使拟合结果与步骤S31中的结果误差达到最小；S33，将谈话视频中的谈话音频输入到循环神经网络中，获取到谈话音频的表情隐变量，在测试时，将任意一段音频输入到神经网络中，获取到该音频未在面部表情表征字典空间中的表情隐变量。进一步的，步骤S4具体包括：S41，根据步骤S3的谈话音频对应的表情隐变量，结合步骤S2的面部表情表征字典重建高维表情特征；S42，解码器将步骤S41得到的高维表情特征结合步骤S1获取到的身份特征，生成出说话者在一段谈话音频下的对应视频；S43，采用基于图像块的对抗网络，提升说话人图像的图像质量，并基于提升图像质量后的说话人图像生成谈话头像视频。进一步的，步骤S5具体包括：S51，拍摄人体躯干运动视频，将运动视频分离出每一帧运动图像，并从每一帧运动图像中分离出头部与躯干，建立人体驱干运动动作库；S52，分析谈话视频中的每一帧图像的对应的音频信号特征，根据音频信号特征中的时长特征、说话的语气情绪特征，从人体驱干运动动作库中选取对应的动作组。进一步的，步骤S6具体包括：S61，将步骤S4获取到的谈话头像视频和步骤S5获取到的动作组对齐头部位置和躯干位置，将谈话头像视频对应的图片和动作组对应的图片进行融合，得到拼接图片；S62，对拼接图片做拼接后处理，利用图像混合技术对拼接位置的色差和位置偏差进行消除，从而得到全身的虚拟谈话数字人。附图说明图1为本发明实施例的虚拟谈话数字人生成方法的流程图。具体实施方式为使本发明实施例的目的、技术方案和优点更加清楚，下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有做出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。本发明采用编码器结合语音信号特征分离头像图片的身份特征和脸部表情特征，比起传统人工设计的脸部表情运动具有更高的语义表达能力；接着将高维表情特征编码到低维的隐变量空间，使得后续通过语音信号特征来拟合表情特征空间成为可能；设计循环神经网络架构，输入语音信号特征来拟合表情隐变量空间，在最大程度上减少拟合误差；解码器将上述取得的身份特征和音频信号拟合出的表情隐变量空间重新恢复出谈话数字头像视频，进一步利用图像块对抗网络提高生成的视频质量；建立躯干运动动作库，分析语音信号从动作库中取得最符合的动作序列；对齐拼接谈话头像视频与躯干动作序列并利用图像混合技术混合拼接位置，使得人眼难以区分，进一步产生栩栩如生的虚拟谈话数字人形象。下面对本发明的技术方案进行详细说明。请参阅图1，本发明的实施例提供了一种虚拟谈话数字人生成方法，包括步骤S1~步骤S6。S1，基于获取的真实录像的谈话视频，通过编码器结合语音特征分离出身份特征和脸部表情特征。其中，编码器结合语音特征分离身份特征和脸部表情特征，是编码器结合无表情头像和语音信号特征获取表情掩码，通过表情掩码来分离，步骤S1具体包括：S11，将获取的真实录像的谈话视频分离出每一帧图像，将每一帧图像中的人像根据眼部位置及嘴部位置裁剪并对齐头像位置，以得到多个谈话头像图片；S12，对得到的谈话头像图片提取对应的音频信号特征，根据音频信号特征和对应身份的无表情头像图片获取表情掩码特征；S13，编码器根据获取的表情掩码特征分离谈话头像图片的身份特征和脸部表情特征。S2，将脸部表情特征编码到隐变量空间中，得到谈话视频中的每一帧图像对应的表情隐变量表示，所述隐变量空间是一个可学习的面部表情表征字典，代表面部运动的合集。本实施例中，隐变量空间是一个可学习的面部表情表征字典，代表面部运动的合集，这一步的目的是为了训练这个可学习的动作字典，只在网络训练的时候有这一步，测试的时候没有。而现有技术是通过面部关键点确定目标对象的表情特征。其中，将脸部表情特征编码到隐变量空间中时需要先建立一个表征字典，所有的高维表情特征都可通过该面部表情表征字典的线性组合来表示，具体的，步骤S2包括：S21，设计一个可学习的面部表情表征字典，将获取到的脸部表情特征投影到该面部表情表征字典的线性组合空间中；S22，联合优化面部表情表征字典及其线性组合系数，使面部表情表征字典能最大程度表示脸部表情特征，从而得到谈话视频中的每一帧图像对应的表情隐变量表示，进而得到包含了所有面部表情的动作合集。S3，在得到表情隐变量表示后，通过基于注意力机制的Transformer神经网络拟合出谈话音频对应的表情隐变量。其中，将步骤S2中得到的谈话视频中的每一帧图像对应的表情隐变量表示组合到一起，设计的循环神经网络输入为音频信号特征，输出为表情隐变量，迭代训练网络直至误差收敛。在上一步训练动作库时用到的脸部表情特征是从视频图像中得到的，这一步要得到语音和面部动作字典的对应。在这里可以把循环神经网络换成基于注意力机制的Transformer神经网络。Transformer擅长处理长序列数据，同时它是一个自回归模型，可以根据上文数据来推断下文，可以很好地处理音频数据。步骤S3具体包括：S31，根据步骤S2获取到的每一帧图像对应的表情隐变量表示，将其聚合到谈话视频的面部表情表征字典空间中作为训练时的监督；S32，获取谈话视频中的每一帧图像的对应的音频信号特征，设计一个基于注意力机制的Transformer神经网络，其输入为每一帧和其前面所有的图像对齐的音频信号特征，输出为这一帧图像的面部表情表征字典空间的拟合结果，迭代训练该神经网络，使拟合结果与步骤S31中的结果误差达到最小；S33，将谈话视频中的谈话音频输入到循环神经网络中，获取到谈话音频的表情隐变量，在测试时，将任意一段音频输入到神经网络中，获取到该音频未在面部表情表征字典空间中的表情隐变量。S4，解码器基于身份特征和谈话音频对应的表情隐变量生成谈话头像视频，通过基于图像分块的对抗网络来提升图像质量。其中，解码器先将步骤S3中拟合的表情隐变量结合步骤S2中的表征字典来恢复高维的表情空间，接着将高维表情与步骤S1中获得的身份特征融合后将融合结果解码成真实图像，该步骤需要编码器结构协同训练，直至网络训练损失最小，并采用基于图像块的对抗网络，进一步提升生成视频质量，编码器和解码器需协同工作。步骤S4具体包括：S41，根据步骤S3的谈话音频对应的表情隐变量，结合步骤S2的面部表情表征字典重建高维表情特征；S42，解码器将步骤S41得到的高维表情特征结合步骤S1获取到的身份特征，生成出说话者在一段谈话音频下的对应视频；S43，采用基于图像块的对抗网络，提升说话人图像的图像质量，并基于提升图像质量后的说话人图像生成谈话头像视频。S5，根据谈话视频中的音频特征从全身的身体动作库选取对应的动作组。本实施例中，做出了全身的动画视频，能够做到即使是语音停了，动作还没做完，这个动作会做完后才结束整个动画。其中，在以上步骤利用音频信号驱动面部运动外，本发明设计躯干运动模块，生成整体的真人形象；谈话时带有躯干运动，使得虚拟数字人形象更加生动、逼真，步骤S5的实施步骤如下：S51，拍摄人体躯干运动视频，将运动视频分离出每一帧运动图像，并从每一帧运动图像中分离出头部与躯干，建立人体驱干运动动作库；S52，分析谈话视频中的每一帧图像的对应的音频信号特征，根据音频信号特征中的时长特征、说话的语气情绪特征，从人体驱干运动动作库中选取对应的动作组。S6，对谈话头像视频和动作组进行融合，得到全身的虚拟谈话数字人。其中，本步骤将对头像与躯干动作进行融合，对其头部与躯干位置，并消除拼接位置差异，其具体实施步骤如下：S61，将步骤S4获取到的谈话头像视频和步骤S5获取到的动作组对齐头部位置和躯干位置，将谈话头像视频对应的图片和动作组对应的图片进行融合，得到拼接图片；S62，对拼接图片做拼接后处理，利用图像混合技术对拼接位置的色差和位置偏差进行消除，从而得到全身的虚拟谈话数字人。综上，根据本发明提供的虚拟谈话数字人生成方法，利用编码器自适应分离出身份特征与脸部表情特征，相对于人工标注的特征点表情或脸部运动肌肉特征，具有更丰富的语义表达能力；本发明将高维的脸部表情特征编码到低维的隐变量空间中，能够在最少地牺牲表示精度的情况下，压缩表情特征维度，为利用谈话音频拟合出表情隐变量提供了更好的条件；然后解码器基于身份特征和谈话音频对应的表情隐变量生成谈话头像视频，再根据谈话视频中的音频特征从动作库选取对应的动作组，并对谈话头像视频和动作组进行融合，得到虚拟谈话数字人，使得本发明能够得到形象更加逼真、更接近现实生活中人物的谈话虚拟数字人，具有实时性，形象逼真，音唇同步率高的特点。在本说明书的描述中，参考术语“一个实施例”、“一些实施例”、 “示例”、“具体示例”、或“一些示例”等的描述意指结合该实施例或示例描述的具体特征、结构、材料或者特点包含于本发明的至少一个实施例或示例中。在本说明书中，对上述术语的示意性表述不一定指的是相同的实施例或示例。而且，描述的具体特征、结构、材料或者特点可以在任何的一个或多个实施例或示例中以合适的方式结合。尽管已经示出和描述了本发明的实施例，本领域的普通技术人员可以理解：在不脱离本发明的原理和宗旨的情况下可以对这些实施例进行多种变化、修改、替换和变型，本发明的范围由权利要求及其等同物限定。
