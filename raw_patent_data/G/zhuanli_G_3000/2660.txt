标题title
一种基于强化学习的打击序列智能规划方法
摘要abst
本发明提供一种基于强化学习的打击序列智能规划方法，包括以下步骤：S1、建立大规模交战序列规划问题的PPO强化学习网络模型；S2、根据建立的所述网络模型进行模型训练，生成大规模交战序列规划问题的PPO强化学习网络模型结果；S3、应用得到的训练结果求解大规模交战序列规划问题，并根据应用场景变化进行大规模交战序列规划问题PPO强化学习网络的优化，完成所述PPO强化学习网络模型的自学习与在线升级。本发明的基于强化学习的打击序列智能规划方法，通过设计大规模交战序列规划问题强化学习求解环境的状态、动作和奖励，实现了大规模交战序列规划问题的快速、高效、自动求解。
权利要求书clms
1.一种基于强化学习的打击序列智能规划方法，其特征在于，包括如下步骤：S1、建立交战序列规划的PPO强化学习网络模型；S2、对建立的网络模型进行模型训练，得到交战序列规划的PPO强化学习网络模型的训练结果；S3、应用得到的训练结果进行交战序列规划，并根据应用场景变化进行交战序列规划PPO强化学习网络的优化，完成所述PPO强化学习网络模型的自学习与在线升级。2.根据权利要求1所述的基于强化学习的打击序列智能规划方法，其特征在于，在所述步骤S1之前，还包括，设计大规模交战序列规划问题的PPO强化学习算法求解环境的当前状态St，t表示当前时刻；设计大规模交战序列规划问题的PPO强化学习算法求解环境的动作Mt；设计大规模交战序列规划问题的PPO强化学习算法求解环境的奖励评价函数。3.根据权利要求2所述的基于强化学习的打击序列智能规划方法，其特征在于，在所述设计大规模交战序列规划问题的PPO强化学习算法求解环境的当前状态St的步骤中，包括，a)武器编号Ni：整型，表示求解问题中每个武器的代号，范围为的整数，N≤500；b)发射点编号Pi：整型，表示求解问题中武器Ni所在发射点的代号，范围为，K≤10；c)发射点优先级次序pi：整型，表示求解问题中发射点Pi的发射优先级顺序，范围为，K≤10，1表示发射优先级最低的发射点，10表示发射优先级最高的发射点；d)武器发射时间间隔sij；整型，表示求解问题中武器Ni和Nj之间的最小发射时间间隔要求，sij表示从编号为Ni的武器发射后经过该时间间隔编号为Nj的武器才可以发射，sji表示从编号为Nj的武器发射后经过该时间间隔编号为Ni的武器才可以发射；e)当前保存武器发射次序列表Qt：列表类型，表示求解问题中对应特定当前状态St下已经生成的武器发射序列；f)武器可选择标签wi：整型，表示求解问题中武器Ni在当前状态St下是否已发射并加入至当前保存武器发射次序列表Qt中，取值为0、1，0表示该武器尚未发射，1表示该武器已经完成发射。4.根据权利要求3所述的基于强化学习的打击序列智能规划方法，其特征在于，在设计大规模交战序列规划问题的PPO强化学习算法求解环境的动作Mt的步骤中，包括，Mt＝i,i∈其中，Mt为武器发射动作，i为区间的离散整数值，表示当前状态下选择了编号为Ni的武器发射并加入当前保存武器发射次序列表Qt，N表示问题域中武器的最大数量。5.根据权利要求2所述的基于强化学习的打击序列智能规划方法，其特征在于，在所述设计大规模交战序列规划问题的PPO强化学习算法求解环境的奖励评价函数的步骤中，包括，构建基于约束条件泛化的大规模交战序列规划问题求解奖励评价函数R：其中，R为奖励函数的奖励值；w0为武器发射用时评价因子权重，为武器发射用时评价因子；w1为发射点综合优先级评价因子权重，/＞为发射点综合优先级评价因子；w2为泛化约束评价因子权重，/＞为泛化约束评价因子，实现对多个不确定性复杂约束的同时评价奖励；根据问题规模与当前状态环境自定义设置/＞w0、w1、w2；Nt为当前状态环境下已经完成发射的武器数量；Tt表示当前状态环境下从第1枚到第Nt枚武器的发射用时；pi表示每个发射点的优先级等级，优先等级越高pi数值越大；Wi为每个发射点优先级所占的权重，/＞n为全部发射点总数量；Nk表示当前求解问题中相关约束的数量；nk表示泛化约束的奖励值，若当前状态环境的解不能满足某一项泛化约束时，将其对应的约束奖励值设置为-108，反之，则设置为正数。6.根据权利要求1所述的基于强化学习的打击序列智能规划方法，其特征在于，在所述建立交战序列规划的PPO强化学习网络模型的步骤中，包括，建立PPO算法Actor网络，当前状态s0为Actor网络的输入，Pi为Actor网络的输出；所述当前状态s0＝{Qt,wi,sij}，Qt为当前武器发射次序列表一维状态向量，wi为武器可选择标签一维状态向量，sij为武器发射时间间隔二维状态向量，其中武器发射时间间隔sij为二维向量；所述输出Pi＝π，即当前状态下可选择武器的发射概率，at为根据动作概率做出的决策，即下一架发射的武器编号。7.根据权利要求6所述的基于强化学习的打击序列智能规划方法，其特征在于，还包括，建立PPO算法Critic网络，所述Critic网络的输入为当前状态s0，输出为该当前状态的价值v；所述当前状态s0包括当前武器发射次序列表Qt，武器可选择标签wi和武器发射时间间隔sij等状态信息，其中武器发射时间间隔sij为二维向量，其它均为一维向量。8.根据权利要求1所述的基于强化学习的打击序列智能规划方法，其特征在于，在所述根据建立的所述网络模型进行模型训练，生成大规模交战序列规划问题的PPO强化学习网络模型结果的步骤中，包括，S21、构建大规模交战序列规划问题环境，初始化问题规模以及发射时间间隔矩阵与发射点优先级数据；S22、构建大规模交战序列规划问题的PPO强化学习网络，根据构建的规划问题环境定义在交战序列规划过程中武器发射决策的状态空间、动作空间以及奖励函数值，同时初始化网络参数；S23、大规模交战序列规划问题下的PPO强化学习网络通过当前状态St完成当前交战序列规划决策，输出并执行决策动作at，根据当前决策以及环境综合发射时间，发射点优先级以及相关约束满足情况计算并输出当前交战序列规划决策奖励Rt；S24、大规模交战序列规划问题下的PPO强化学习网络根据当前决策动作at更新武器发射状态，作为强化学习求解网络模型的下一次训练输入，同时更新当前保存武器发射次序列表Qt与当前步骤策略所选武器的可选择标签wi；S25、将当前步骤执行前环境当前状态St，当前步骤决策动作at，当前交战序列规划决策奖励Rt以及当前决策执行后的环境状态St+1作为样本存入样本序列，并将决策后环境状态St+1与当前交战序列规划决策奖励Rt返回大规模交战序列规划问题下的PPO强化学习网络模型；S26、利用强化学习算法加载所述样本序列，完成大规模交战序列规划问题下的PPO强化学习网络模型的训练与优化；利用大规模交战序列规划问题下的PPO强化学习网络计算当前决策的动作价值Q，采用梯度下降法更新对应网络完成训练；S27、当求解问题中的所有武器均已完成交战序列规划任务时，转入步骤S28，否则循环执行步骤S23至S26，直至完成求解问题中所有武器的序列规划任务；S28、达到指定训练次数时终止训练，输出大规模交战序列规划问题下的PPO强化学习网络模型成果。9.一种电子设备，其特征在于，包括存储器和处理器，所述存储器上储存有在所述处理器上运行的程序，所述处理器运行所述程序时执行权利要求1-8任一项所述的基于强化学习的打击序列智能规划方法的步骤。10.一种计算机可读存储介质，其上存储有计算机指令，其特征在于，所述计算机指令运行时执行权利要求1-8任一项所述的基于强化学习的打击序列智能规划方法的步骤。
说明书desc
技术领域发明涉及军事决策技术领域，特别涉及一种基于强化学习的打击序列智能规划方法、电子设备和存储介质。背景技术现有技术中，导弹、火箭弹、火炮等武器发射后按预定航路空域飞行，由于不同发射点出发的武器可能存在航路交叉导致的武器碰撞或互扰等危险事件的发生，因此，如何合理规划武器交战序列一直是武器作战筹划须解决的关键问题之一。武器交战序列规划，主要是针对多种、多个武器打击单个或多个目标，合理规划每个武器的发射时间序列，在满足武器发射安全、飞行安全等约束条件的同时，在尽可能短的时间窗口内完成全部武器发射，提高武器快速反应能力和打击效能。而大规模作战条件下，因发射武器种类多、数量多、打击目标多、发射点位和目标点位分布散，导致大规模作战条件下的武器交战序列规划问题变量规模大、约束条件复杂，问题求解难度巨大，目前尚无有效解决方法。该问题的求解模型可近似看作一种大规模的ATSP。以往对非对称旅行商问题的求解方法主要包括精确算法、近似方法两大类。其中，精确算法主要包括分支界定法、逐次最短法或动态规划法等，这些算法理论上能够获得问题的精确最优解，通常会和特殊设计的启发式方法结合使用来降低求解的时间复杂度，是大部分求解器的最基本方法，能在小规模问题上取得较好的效果，但是当问题规模扩大时，算法将消耗巨大的计算量，难以求解大规模问题，无法用于大规模交战序列问题的求解；近似方法主要包括近似算法和启发式算法，近似算法以贪心算法、局部搜索算法和序列算法等为主要代表，启发式算法以粒子群算法、禁忌搜索算法、蚁群算法等为主要代表，这些方法虽然能在一定时间内给出较好的可行解，但对大规模问题的搜索时间慢、易陷入局部最优，且算法随机性强难以工程应用，也无法满足大规模交战序列问题的求解使用需求。随着深度强化学习技术的发展，在围棋、机器人等领域得到了成功应用，并广泛用于各个行业领域的探索实践，在这一背景下，基于深度强化学习的组合优化方法因其具有求解速度快、模型泛化能力强等优势，逐步成为近年来的研究热点，涌现出了一系列相关研究和案例，为大规模交战序列问题的求解提供了一条可行的新思路。发明内容为了解决现有技术存在的问题，本发明提供一种基于强化学习的打击序列智能规划方法，通过设计大规模交战序列规划问题强化学习求解环境的状态、动作和奖励，实现了大规模交战序列规划问题的快速、高效、自动求解。为实现上述目的，本发明提供的一种基于强化学习的打击序列智能规划方法，包括以下步骤：S1、建立交战序列规划的PPO强化学习网络模型；S2、对建立的网络模型进行模型训练，得到交战序列规划的PPO强化学习网络模型的训练结果；S3、应用得到的训练结果进行交战序列规划，并根据应用场景变化进行交战序列规划PPO强化学习网络的优化，完成所述PPO强化学习网络模型的自学习与在线升级。进一步地，在所述步骤S1之前，还包括，设计大规模交战序列规划问题的PPO强化学习算法求解环境的当前状态St，t表示当前时刻；设计大规模交战序列规划问题的PPO强化学习算法求解环境的动作Mt；设计大规模交战序列规划问题的PPO强化学习算法求解环境的奖励评价函数。进一步地，在所述设计大规模交战序列规划问题的PPO强化学习算法求解环境的当前状态St的步骤中，包括，a)武器编号Ni：整型，表示求解问题中每个武器的代号，范围为的整数，N≤500；b)发射点编号Pi：整型，表示求解问题中武器Ni所在发射点的代号，范围为，K≤10；c)发射点优先级次序pi：整型，表示求解问题中发射点Pi的发射优先级顺序，范围为，K≤10，1表示发射优先级最低的发射点，10表示发射优先级最高的发射点；d)武器发射时间间隔sij；整型，表示求解问题中武器Ni和Nj之间的最小发射时间间隔要求，sij表示从编号为Ni的武器发射后经过该时间间隔编号为Nj的武器才可以发射，sji表示从编号为Nj的武器发射后经过该时间间隔编号为Ni的武器才可以发射；e)当前保存武器发射次序列表Qt：列表类型，表示求解问题中对应特定当前状态St下已经生成的武器发射序列；f)武器可选择标签wi：整型，表示求解问题中武器Ni在当前状态St下是否已发射并加入至当前保存武器发射次序列表Qt中，取值为0、1，0表示该武器尚未发射，1表示该武器已经完成发射。进一步地，在设计大规模交战序列规划问题的PPO强化学习算法求解环境的动作Mt的步骤中，包括，Mt＝i,i∈其中，Mt为武器发射动作，i为区间的离散整数值，表示当前状态下选择了编号为Ni的武器发射并加入当前保存武器发射次序列表Qt，N表示问题域中武器的最大数量。进一步地，在所述设计大规模交战序列规划问题的PPO强化学习算法求解环境的奖励评价函数的步骤中，包括，构建基于约束条件泛化的大规模交战序列规划问题求解奖励评价函数R：其中，R为奖励函数的奖励值；w0为武器发射用时评价因子权重，为武器发射用时评价因子；w1为发射点综合优先级评价因子权重，/＞为发射点综合优先级评价因子；w2为泛化约束评价因子权重，/＞为泛化约束评价因子，实现对多个不确定性复杂约束的同时评价奖励；根据问题规模与当前状态环境自定义设置/＞w0、w1、w2；Nt为当前状态环境下已经完成发射的武器数量；Tt表示当前状态环境下从第1枚到第Nt枚武器的发射用时；pi表示每个发射点的优先级等级，优先等级越高pi数值越大；Wi为每个发射点优先级所占的权重，/＞n为全部发射点总数量；Nk表示当前求解问题中相关约束的数量；nk表示泛化约束的奖励值，若当前状态环境的解不能满足某一项泛化约束时，将其对应的约束奖励值设置为-108，反之，则设置为正数。进一步地，在所述建立交战序列规划的PPO强化学习网络模型的步骤中，包括，建立PPO算法Actor网络，当前状态s0为Actor网络的输入，Pi为Actor网络的输出；所述当前状态s0＝{Qt,wi,sij}，Qt为当前武器发射次序列表一维状态向量，wi为武器可选择标签一维状态向量，sij为武器发射时间间隔二维状态向量，其中武器发射时间间隔sij为二维向量；所述输出Pi＝π，即当前状态下可选择武器的发射概率，at为根据动作概率做出的决策，即下一架发射的武器编号。进一步地，还包括，建立PPO算法Critic网络，所述Critic网络的输入为当前状态s0，输出为该当前状态的价值v；所述当前状态s0包括当前武器发射次序列表Qt，武器可选择标签wi和武器发射时间间隔sij等状态信息，其中武器发射时间间隔sij为二维向量，其它均为一维向量。进一步地，在所述根据建立的所述网络模型进行模型训练，生成大规模交战序列规划问题的PPO强化学习网络模型结果的步骤中，包括，S21、构建大规模交战序列规划问题环境，初始化问题规模以及发射时间间隔矩阵与发射点优先级数据；S22、构建大规模交战序列规划问题的PPO强化学习网络，根据构建的规划问题环境定义在交战序列规划过程中武器发射决策的状态空间、动作空间以及奖励函数值，同时初始化网络参数；S23、大规模交战序列规划问题下的PPO强化学习网络通过当前状态St完成当前交战序列规划决策，输出并执行决策动作at，根据当前决策以及环境综合发射时间，发射点优先级以及相关约束满足情况计算并输出当前交战序列规划决策奖励Rt；S24、大规模交战序列规划问题下的PPO强化学习网络根据当前决策动作at更新武器发射状态，作为强化学习求解网络模型的下一次训练输入，同时更新当前保存武器发射次序列表Qt与当前步骤策略所选武器的可选择标签wi；S25、将当前步骤执行前环境当前状态St，当前步骤决策动作at，当前交战序列规划决策奖励Rt以及当前决策执行后的环境状态St+1作为样本存入样本序列，并将决策后环境状态St+1与当前交战序列规划决策奖励Rt返回大规模交战序列规划问题下的PPO强化学习网络模型；S26、利用强化学习算法加载所述样本序列，完成大规模交战序列规划问题下的PPO强化学习网络模型的训练与优化；利用大规模交战序列规划问题下的PPO强化学习网络计算当前决策的动作价值Q，采用梯度下降法更新对应网络完成训练；S27、当求解问题中的所有武器均已完成交战序列规划任务时，转入步骤S28，否则循环执行步骤S23至S26，直至完成求解问题中所有武器的序列规划任务；S28、达到指定训练次数时终止训练，输出大规模交战序列规划问题下的PPO强化学习网络模型成果。为实现上述目的，本发明还提供一种电子设备，包括存储器和处理器，所述存储器上储存有在所述处理器上运行的程序，所述处理器运行所述程序时执行上述的基于强化学习的打击序列智能规划方法的步骤。为实现上述目的，本发明还提供一种计算机可读存储介质，其上存储有计算机指令，所述计算机指令运行时执行上述的基于强化学习的打击序列智能规划方法的步骤。本发明的基于强化学习的打击序列智能规划方法，具有以下有益效果：与现有技术相比，本发明提出的一种基于强化学习的打击序列智能规划方法，具有求解速度快、优化效果佳、鲁棒性好和具备自学习能力等优点，且通过约束条件泛化设计实现了对不同问题场景的弹性自适应，应用范围广。本发明的其它特征和优点将在随后的说明书中阐述，并且，部分地从说明书中变得显而易见，或者通过实施本发明而了解。附图说明附图用来提供对本发明的进一步理解，并且构成说明书的一部分，并与本发明的实施例一起，用于解释本发明，并不构成对本发明的限制。在附图中：图1为本发明的基于强化学习的打击序列智能规划方法的流程图；图2为本发明实施例的大规模交战序列规划任务动作网络示意图；图3为本发明实施例的大规模交战序列规划任务价值网络示意图；图4为本发明实施例的PPO算法实现流程示意图；图5为本发明实施例的大规模交战序列规划智能算法综合运用环境示意图；图6为本发明实施例的基于强化学习的打击序列智能规划方法整体实现流程图；图7为本发明实施例的大规模交战序列规划任务环境流程图。具体实施方式以下结合附图对本发明的优选实施例进行说明，应当理解，此处所描述的优选实施例仅用于说明和解释本发明，并不用于限定本发明。PPO算法：为强化学习算法的一种，PPO即Proximal Policy Optimization，是一种新型的Policy Gradient算法。ATSP问题：是旅行商问题的一种，ATSP即Asymmetric Traveling SalesmanProblem。实施例1图1为根据本发明的基于强化学习的打击序列智能规划方法流程图，下面将参考图1，对本发明的基于强化学习的打击序列智能规划方法进行详细描述。在步骤101，设计大规模交战序列规划问题的PPO强化学习算法求解环境。优选地，主要完成大规模交战序列规划问题强化学习算法中每个步骤的状态S、动作A和奖励R的设计，其中奖励综合了当前发射总用时，发射点优先级顺序以及是否满足约束条件这三个指标进行建模，每个步骤根据当前状态进行动作，选择武器编号加入武器发射次序列表，并检查是否满足当前问题约束，当不满足问题约束时将给是否满足约束指标赋负值，根据当前状态奖励值生成新的状态，完成大规模交战序列规划问题。基于强化学习网络的大规模交战序列规划问题逻辑处理流程图如图6所示。优选地，设计大规模交战序列规划问题的PPO强化学习算法求解环境的当前状态St，t表示当前时刻，主要包括：a)武器编号Ni：整型，表示求解问题中每个武器的代号，范围为的整数，N≤500；b)发射点编号Pi：整型，表示求解问题中武器Ni所在发射点的代号，范围为，K≤10；c)发射点优先级次序pi：整型，表示求解问题中发射点Pi的发射优先级顺序，范围为，K≤10，1表示发射优先级最低的发射点，10表示发射优先级最高的发射点；d)武器发射时间间隔sij；整型，表示求解问题中武器Ni和Nj之间的最小发射时间间隔要求，sij表示从编号为Ni的武器发射后经过该时间间隔编号为Nj的武器才可以发射，sji表示从编号为Nj的武器发射后经过该时间间隔编号为Ni的武器才可以发射；e)当前保存武器发射次序列表Qt：列表类型，表示求解问题中对应特定当前状态St下已经生成的武器发射序列；f)武器可选择标签wi：整型，表示求解问题中武器Ni在当前状态St下是否已发射并加入至当前保存武器发射次序列表Qt中，取值为0、1，0表示该武器尚未发射，1表示该武器已经完成发射。优选地，设计大规模交战序列规划问题的PPO强化学习算法求解环境的动作Mt：Mt＝i,i∈其中，Mt为武器发射动作，i为区间的离散整数值，表示当前状态下选择了编号为Ni的武器发射并加入当前保存武器发射次序列表Qt，N表示问题域中武器的最大数量。优选地，设计大规模交战序列规划问题的PPO强化学习算法求解环境的奖励评价函数。构建基于约束条件泛化的大规模交战序列规划问题求解奖励评价函数R：其中，R为奖励函数的奖励值；w0为武器发射用时评价因子权重，是武器发射用时评价因子；w1为发射点综合优先级评价因子权重，/＞为发射点综合优先级评价因子；w2为泛化约束评价因子权重，/＞为泛化约束评价因子，实现对多个不确定性复杂约束的同时评价奖励。/＞w0、w1、w2根据问题规模与当前状态环境人为设置与调整；Nt为当前状态环境下已经完成发射的武器数量；Tt表示当前状态环境下从第1枚到第Nt枚武器的发射用时；pi表示的是每个发射点的优先级等级，优先等级越高pi数值越大；Wi是每个发射点优先级所占的权重，/＞n为全部发射点总数量；Nk表示当前求解问题中相关约束的数量；nk表示泛化约束的奖励值，若当前状态环境的解不能满足某一项泛化约束时，将其对应的约束奖励值设置为极大负数-108，反之，则设置为正数。本实施例中，如下表1所示，举例说明Tt的计算方法，表1就是sij数据，表示第i个武器发射后至少要过sij这么久时间第j个武器才能可以发射，当i＝j时sij为0。在表1所示的数据里，假设当前确定的发射顺序是2-＞3-＞1，那么因为s23＝1，也就是第二个武器发射至少1秒后3才能发射，s31＝3，第三个武器发射后3秒第一个武器才能发射。看起来如果以2-＞3-＞1的顺序发射的话，等待时间就应该是1+3＝4，但是同时也要考虑s21的数值，s21＝9，也就是说第二个武器发射至少9秒之后第一个武器才能发射，前面算出来的4不满足所以应该取大的值，最后算出来当发射顺序是231时总的发射用时Tt为9秒。表1武器发射时间间隔示例本实施例中，对于泛化约束评价因子由于武器发射时序规划并不只是与发射间隔相关，根据问题环境可能也会存在一些其他的约束条件，比如要求最后一个武器的发射时间要小于特定值，或者某个发射点在第一个武器发射之后的特定时间之内须完成所有武器发射，由于作战场景的复杂性，导致事前往往很难准确界定全部约束条件，因此，这里设计一种泛化的约束条件表示方式，将其作为一部分放到强化学习的评价奖励函数中。如果当前环境的解不能满足某一条约束条件时，将其对应的约束奖励值nk设置为极大负数-108，从而终止强化学习网路的学习，引导其朝着能够满足约束的方向学习，而当能够满足约束时则将其设置为某一特定正值，作为奖励给网络结构。问题环境中存在多个需要同时考虑的复杂约束条件时，分别判断每一个约束条件是否满足并对其赋值，综合累加后得到泛化约束评价因子，作为大规模交战序列规划问题的奖励评价函数组成部分。在步骤102，建立大规模交战序列规划问题的PPO强化学习网络模型。优选地，建立PPO算法Actor网络，如图2所示，当前状态s0为Actor网络的输入，Pi为Actor网络的输出。s0＝{Qt,wi,sij}，Qt为当前武器发射次序列表一维状态向量，wi为武器可选择标签一维状态向量，sij为武器发射时间间隔二维状态向量，其中武器发射时间间隔sij为二维向量。Pi＝π，即当前状态下可选择武器的发射概率；at为根据动作概率做出的决策，即下一架发射的武器编号。优选地，建立PPO算法Critic网络，如图3所示，其中网络的输入为当前状态s0，主要包括当前武器发射次序列表Qt，武器可选择标签wi和武器发射时间间隔sij等状态信息，其中武器发射时间间隔sij为二维向量，其它均为一维向量。输出为v，表示该状态的价值。优选地，如图4所示，建立PPO算法实现流程，at为根据动作概率做出的决策，即下一个发射的武器编号，rt为将at反馈给环境得到的奖励，用于综合评价当前状态下已安排发射的武器发射总用时，发射点优先级次序以及求解问题中的相关约束等指标。vt为按时间衰减后累加的奖励。V为将所有s组合输入后得到的对应状态价值。At为优势函数，用于表示该动作相较于平均的优势。在步骤103，训练大规模交战序列规划问题的PPO强化学习网络。优选地，搭建大规模交战序列规划问题环境并完成PPO强化学习算法的训练，具体过程包括：步骤301，构建大规模交战序列规划问题环境，初始化问题规模以及发射时间间隔矩阵与发射点优先级数据。步骤302，构建大规模交战序列规划问题的PPO强化学习求解网络，依托环境定义在交战序列规划过程中武器发射决策的状态空间、动作空间以及奖励函数值，同时初始化网络参数。步骤303，大规模交战序列规划问题下的PPO强化学习网络通过当前状态St完成当前交战序列规划决策，输出并执行决策动作at，根据当前决策以及环境综合发射时间，发射点优先级以及相关约束满足情况计算并输出当前交战序列规划决策奖励Rt。步骤304，大规模交战序列规划问题下的PPO强化学习网络根据当前决策动作at更新武器发射状态，作为强化学习求解网络模型的下一次训练输入，同时更新当前保存武器发射次序列表Qt与当前步骤策略所选武器的可选择标签wi。步骤305，将当前步骤执行前当前状态St，当前步骤决策动作at，当前决策奖励Rt以及当前决策执行后的环境状态St+1作为样本暂时存入样本序列，t表示当前时刻，t+1表示下一时刻，并将决策后状态St+1与奖励Rt返回大规模交战序列规划问题下的PPO强化学习网络模型。步骤306，利用强化学习算法加载样本序列，完成大规模交战序列规划问题下的PPO强化学习网络模型的训练与优化。首先利用大规模交战序列规划问题下的PPO强化学习网络计算当前决策的动作价值Q，然后采用梯度下降法更新对应网络完成训练。步骤307，当求解问题中的所有武器均已完成交战序列规划任务时，转入步骤308，否则循环执行步骤303至306，直至完成求解问题中所有武器的序列规划任务。步骤308，达到指定训练次数时终止训练，输出大规模交战序列规划问题下的PPO强化学习网络模型成果。在步骤104，应用步骤103所得训练结果智能求解大规模交战序列规划问题，并根据应用场景变化开展大规模交战序列规划问题PPO强化学习网络的优化，完成算法的自学习与在线升级。优选地，将训练好的大规模交战序列规划问题下的PPO强化学习网络模型接入真实相关应用场景中，提供大规模交战序列规划后台智能算法支撑，对现实场景中大规模交战序列规划问题以及其他相关可迁移场景提供问题求解。当应用场景发生变化时，可由应用环境提供新的训练场景以及训练数据来进行场景迁移，通过执行步骤103来实现算法的自学习与在线升级，解决新场景下的问题。本发明提出一种基于PPO强化学习的大规模交战序列智能规划方法，通过设计大规模交战序列规划问题强化学习求解环境的状态、动作和奖励，实现了大规模交战序列规划问题的快速、高效、自动求解。本发明还提供一种电子设备，包括存储器和处理器，所述存储器上储存有在所述处理器上运行的程序，所述处理器运行所述程序时执行上述的基于强化学习的打击序列智能规划方法的步骤。本发明还提供了一种计算机可读存储介质，其上存储有计算机指令，所述计算机指令运行时执行上述的基于强化学习的打击序列智能规划方法的步骤，所述基于强化学习的打击序列智能规划方法参见前述部分的介绍，不再赘述。本领域普通技术人员可以理解：以上所述仅为本发明的优选实施例而已，并不用于限制本发明，尽管参照前述实施例对本发明进行了详细的说明，对于本领域的技术人员来说，其依然可以对前述各实施例记载的技术方案进行修改，或者对其中部分技术特征进行等同替换。凡在本发明的精神和原则之内，所作的任何修改、等同替换、改进等，均应包含在本发明的保护范围之内。
