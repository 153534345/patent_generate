标题title
基于目标生成式回应语言模型的回应方法和装置
摘要abst
本申请涉及一种基于目标生成式回应语言模型的回应方法和装置。其中，该方法包括：基于教育设备中的提示数据集，训练得到初始生成式提示语言模型和初始生成式回应语言模型；并利用评分模型对二者的预测结果进行评分；基于对评分值的加权计算结果，通过强化学习和对抗学习进一步训练初始生成式提示语言模型和初始生成式回应语言模型，得到目标生成式回应语言模型；将教育设备采集的待测文本数据输入目标生成式回应语言模型，目标生成式回应语言模型将待测文本数据与对话数据进行拼接，得到相应的回应。采用本方法能生成多样的新提示，并加强生成式提示语言模型和生成式回应语言模型之间的交互，从而进一步改善生成式语言模型的意料外行为问题。
权利要求书clms
1.一种基于目标生成式回应语言模型的回应方法，其特征在于，所述方法包括：基于教育设备中的提示数据集，训练得到初始生成式提示语言模型和初始生成式回应语言模型；所述初始生成式提示语言模型具备根据提示生成新提示的能力，所述初始生成式回应语言模型具备根据提示生成回应的能力；基于采样所述教育设备中成对的提示数据，训练得到提示评分模型；基于采样所述教育设备中成对的回应数据，训练得到回应评分模型；利用所述提示评分模型对所述初始生成式提示语言模型输出的预测提示进行评分，得到提示评分值；利用所述回应评分模型对所述初始生成式回应语言模型输出的预测回应进行评分，得到回应评分值；对所述提示评分值和所述回应评分值进行加权计算，基于加权计算结果，通过强化学习和对抗学习进一步训练所述初始生成式提示语言模型和所述初始生成式回应语言模型，得到目标生成式回应语言模型；将所述教育设备采集的待测文本数据输入所述目标生成式回应语言模型，所述目标生成式回应语言模型将待测文本数据与对话数据进行拼接，得到相应的回应。2.根据权利要求1所述的基于目标生成式回应语言模型的回应方法，其特征在于，所述基于教育设备中的提示数据集，训练得到初始生成式提示语言模型和初始生成式回应语言模型，包括：获取教育设备中的提示数据集，并基于从所述提示数据集中采样的提示，获取根据所述提示预先设置的新提示和回应；将所述提示作为模型输入，并将所述预先设置的新提示为训练目标，使用监督学习训练得到初始生成式提示语言模型；将所述提示作为模型输入，并将所述预先设置的回应为训练目标，使用监督学习训练得到初始生成式回应语言模型。3.根据权利要求1所述的基于目标生成式回应语言模型的回应方法，其特征在于，所述基于采样所述教育设备中成对的提示数据，训练得到提示评分模型，包括：采样所述教育设备中的提示数据集中的一条所述提示，将所述提示输入所述初始生成式提示语言模型，得到模型生成的新提示；获取所述教育设备中的根据所述模型生成的新提示预先设置的第一评分值；基于所述提示、所述模型生成的新提示以及所述第一评分值，训练得到提示评分模型。4.根据权利要求1所述的基于目标生成式回应语言模型的回应方法，其特征在于，所述基于采样所述教育设备中成对的回应数据，训练得到回应评分模型，包括：采样所述教育设备中的提示数据集中的一条所述提示，将所述提示输入所述初始生成式回应语言模型，得到模型生成的回应；获取所述教育设备中的根据所述模型生成的回应预先设置的第二评分值；基于所述提示、所述模型生成的回应以及所述第二评分值，训练得到回应评分模型。5.根据权利要求1所述的基于目标生成式回应语言模型的回应方法，其特征在于，所述利用所述回应评分模型对所述初始生成式回应语言模型输出的预测回应进行评分，得到回应评分值，包括：获取一个新的提示数据集；将所述新的提示数据集输入至所述初始生成式回应语言模型，得到第一预测回应，利用所述回应评分模型对所述第一预测回应进行评分，得到第一回应评分值；将所述新的提示数据集输入至所述初始生成式提示语言模型，得到新生成的新提示，将所述新生成的新提示输入至所述初始生成式回应语言模型，得到第二预测回应，利用所述回应评分模型对所述第二预测回应进行评分，得到第二回应评分值。6.根据权利要求1所述的基于目标生成式回应语言模型的回应方法，其特征在于，所述对所述提示评分值和所述回应评分值进行加权计算，基于加权计算结果，通过强化学习和对抗学习进一步训练所述初始生成式提示语言模型和所述初始生成式回应语言模型，得到目标生成式回应语言模型，包括：基于对所述提示评分值和所述回应评分值进行的不同的加权计算，分别得到所述初始生成式提示语言模型的评分以及所述初始生成式回应语言模型的评分；基于所述初始生成式提示语言模型的评分，更新所述初始生成式提示语言模型的参数，得到目标生成式提示语言模型；基于所述初始生成式回应语言模型的评分，更新所述初始生成式回应语言模型的参数，得到目标生成式回应语言模型。7.一种基于目标生成式回应语言模型的回应装置，其特征在于，所述装置包括：训练初始模型模块，用于基于教育设备中的提示数据集，训练得到初始生成式提示语言模型和初始生成式回应语言模型；所述初始生成式提示语言模型具备根据提示生成新提示的能力，所述初始生成式回应语言模型具备根据提示生成回应的能力；训练评分模型模块，用于基于采样所述教育设备中成对的提示数据，训练得到提示评分模型；基于采样所述教育设备中成对的回应数据，训练得到回应评分模型；应用评分模型模块，用于利用所述提示评分模型对初始生成式提示语言模型输出的预测提示进行评分，得到提示评分值；利用所述回应评分模型对初始生成式回应语言模型输出的预测回应进行评分，得到回应评分值；训练目标模型模块，用于对所述提示评分值和所述回应评分值进行加权计算，基于加权计算结果，通过强化学习和对抗学习进一步训练所述初始生成式提示语言模型和所述初始生成式回应语言模型，得到目标生成式回应语言模型；应用目标模型模块，用于将所述教育设备采集的待测文本数据输入所述目标生成式回应语言模型，所述目标生成式回应语言模型将待测文本数据与对话数据进行拼接，得到相应的回应。8.一种计算机设备，包括存储器和处理器，所述存储器存储有计算机程序，其特征在于，所述处理器执行所述计算机程序时实现权利要求1至权利要求6中任一项所述的方法的步骤。9.一种计算机可读存储介质，其上存储有计算机程序，其特征在于，所述计算机程序被处理器执行时实现权利要求1至权利要求6中任一项所述的方法的步骤。10.一种计算机程序产品，包括计算机程序，其特征在于，所述计算机程序被处理器执行时实现权利要求1至权利要求6中任一项所述的方法的步骤。
说明书desc
技术领域本申请涉及人工智能与深度学习技术领域，特别是涉及一种基于目标生成式回应语言模型的回应方法和装置。背景技术随着技术的发展，智能对话技术已经不局限于人工规则的使用，朝着更加智能化的方向发展，这一发展带来了智能对话效果的提升，展现出更加拟人化，更加多元化的趋势，也使得用户更愿意使用这项技术。基于生成式语言模型的智能对话技术是当前的热门，已经十分普及，很多教育类产品使用了该项技术。生成式语言模型可以通过用户输入的提示来执行一系列自然语言处理任务，如命名实体识别，关系抽取，问答等。但是，这些模型经常做出意想不到的行为，比如编造不实信息，生成带有偏见的文本以及不遵守用户的意图等，导致对孩童身心发育产生不利影响。出现此类问题的原因主要是常用的生成式语言模型的预训练方法没有引导生成式语言模型遵循基本规则。为了解决上述问题，现有技术中引入强化学习来引导生成式语言模型遵循基本规则以避免意料外行为，首先通过监督学习训练生成式语言模型使其具备根据提示生成回应的能力，其次利用模型生成的回应以及人类专家对回应的反馈训练评分模型，最后借助评分模型替代人类专家对生成式语言模型生成的回应评分并使用强化学习更新生成式语言模型的参数。但是，由于语言表达的多样性，预训练所用的数据集不可能覆盖所有提示表达，生成式语言模型的意料外行为问题依旧存在。发明内容基于此，有必要针对上述技术问题，提供一种能够生成多样的新提示，进一步改善生成式回应语言模型的意料外行为问题的基于目标生成式回应语言模型的回应方法和装置。第一方面，本申请提供了一种基于目标生成式回应语言模型的回应方法。所述方法包括：基于教育设备中的提示数据集，训练得到初始生成式提示语言模型和初始生成式回应语言模型；所述初始生成式提示语言模型具备根据提示生成新提示的能力，所述初始生成式回应语言模型具备根据提示生成回应的能力；基于采样所述教育设备中成对的提示数据，训练得到提示评分模型；基于采样所述教育设备中成对的回应数据，训练得到回应评分模型；利用所述提示评分模型对所述初始生成式提示语言模型输出的预测提示进行评分，得到提示评分值；利用所述回应评分模型对所述初始生成式回应语言模型输出的预测回应进行评分，得到回应评分值；对所述提示评分值和所述回应评分值进行加权计算，基于加权计算结果，通过强化学习和对抗学习进一步训练所述初始生成式提示语言模型和所述初始生成式回应语言模型，得到目标生成式回应语言模型；将所述教育设备采集的待测文本数据输入所述目标生成式回应语言模型，所述目标生成式回应语言模型将待测文本数据与对话数据进行拼接，得到相应的回应。在其中一个实施例中，所述基于教育设备中的提示数据集，训练得到初始生成式提示语言模型和初始生成式回应语言模型，包括：获取教育设备中的提示数据集，并基于从所述提示数据集中采样的提示，获取根据所述提示预先设置的新提示和回应；将所述提示作为模型输入，并将所述预先设置的新提示为训练目标，使用监督学习训练得到初始生成式提示语言模型；将所述提示作为模型输入，并将所述预先设置的回应为训练目标，使用监督学习训练得到初始生成式回应语言模型。在其中一个实施例中，所述基于采样所述教育设备中成对的提示数据，训练得到提示评分模型，包括：采样所述教育设备中的提示数据集中的一条所述提示，将所述提示输入所述初始生成式提示语言模型，得到模型生成的新提示；获取所述教育设备中的根据所述模型生成的新提示预先设置的第一评分值；基于所述提示、所述模型生成的新提示以及所述第一评分值，训练得到提示评分模型。在其中一个实施例中，所述基于采样所述教育设备中成对的回应数据，训练得到回应评分模型，包括：采样所述教育设备中的提示数据集中的一条所述提示，将所述提示输入所述初始生成式回应语言模型，得到模型生成的回应；获取所述教育设备中的根据所述模型生成的回应预先设置的第二评分值；基于所述提示、所述模型生成的回应以及所述第二评分值，训练得到回应评分模型。在其中一个实施例中，所述利用所述回应评分模型对所述初始生成式回应语言模型输出的预测回应进行评分，得到回应评分值，包括：获取一个新的提示数据集；将所述新的提示数据集输入至所述初始生成式回应语言模型，得到第一预测回应，利用所述回应评分模型对所述第一预测回应进行评分，得到第一回应评分值；将所述新的提示数据集输入至所述初始生成式提示语言模型，得到新生成的新提示，将所述新生成的新提示输入至所述初始生成式回应语言模型，得到第二预测回应，利用所述回应评分模型对所述第二预测回应进行评分，得到第二回应评分值。在其中一个实施例中，所述对所述提示评分值和所述回应评分值进行加权计算，基于加权计算结果，通过强化学习和对抗学习进一步训练所述初始生成式提示语言模型和所述初始生成式回应语言模型，得到目标生成式回应语言模型，包括：基于对所述提示评分值和所述回应评分值进行的不同的加权计算，分别得到所述初始生成式提示语言模型的评分以及所述初始生成式回应语言模型的评分；基于所述初始生成式提示语言模型的评分，更新所述初始生成式提示语言模型的参数，得到目标生成式提示语言模型；基于所述初始生成式回应语言模型的评分，更新所述初始生成式回应语言模型的参数，得到目标生成式回应语言模型。第二方面，本申请还提供了一种基于目标生成式回应语言模型的回应装置。所述装置包括：训练初始模型模块，用于基于教育设备中的提示数据集，训练得到初始生成式提示语言模型和初始生成式回应语言模型；所述初始生成式提示语言模型具备根据提示生成新提示的能力，所述初始生成式回应语言模型具备根据提示生成回应的能力；训练评分模型模块，用于基于采样所述教育设备中成对的提示数据，训练得到提示评分模型；基于采样所述教育设备中成对的回应数据，训练得到回应评分模型；应用评分模型模块，用于利用所述提示评分模型对初始生成式提示语言模型输出的预测提示进行评分，得到提示评分值；利用所述回应评分模型对初始生成式回应语言模型输出的预测回应进行评分，得到回应评分值；训练目标模型模块，用于对所述提示评分值和所述回应评分值进行加权计算，基于加权计算结果，通过强化学习和对抗学习进一步训练所述初始生成式提示语言模型和所述初始生成式回应语言模型，得到目标生成式回应语言模型；应用目标模型模块，用于将所述教育设备采集的待测文本数据输入所述目标生成式回应语言模型，所述目标生成式回应语言模型将待测文本数据与对话数据进行拼接，得到相应的回应。第三方面，本申请还提供了一种计算机设备。所述计算机设备包括存储器和处理器，所述存储器存储有计算机程序，所述处理器执行所述计算机程序时实现上述第一方面所述的基于目标生成式回应语言模型的回应方法的步骤。第四方面，本申请还提供了一种计算机可读存储介质。所述计算机可读存储介质，其上存储有计算机程序，所述计算机程序被处理器执行时实现上述第一方面所述的基于目标生成式回应语言模型的回应方法的步骤。第五方面，本申请还提供了一种计算机程序产品，包括计算机程序，所述计算机程序被处理器执行时实现上述第一方面所述的基于目标生成式回应语言模型的回应方法的步骤。上述基于目标生成式回应语言模型的回应方法和装置，通过引入生成式提示语言模型以生成多样的新提示，引入一个提示评分模型以评价生成的新提示的优劣，并通过对抗学习加强两个生成式语言模型之间的交互，得到遵循基本规则的目标生成式回应语言模型，从而进一步改善教育设备中生成式语言模型的意料外行为问题。附图说明此处所说明的附图用来提供对本申请的进一步理解，构成本申请的一部分，本申请的示意性实施例及其说明用于解释本申请，并不构成对本申请的不当限定。在附图中：图1为一个实施例中基于目标生成式回应语言模型的回应方法的硬件结构框图；图2为一个实施例中基于目标生成式回应语言模型的回应方法的流程示意图；图3为初始生成式提示语言模型和初始生成式回应语言模型训练过程的示意图；图4为提示评分模型和回应评分模型训练过程的示意图；图5为基于强化学习和对抗学习训练目标生成式提示语言模型和目标生成式回应语言模型过程的示意图；图6为一个实施例中基于目标生成式回应语言模型的回应装置的结构框图。具体实施方式为了使本申请的目的、技术方案及优点更加清楚明白，以下结合附图及实施例，对本申请进行进一步详细说明。应当理解，此处描述的具体实施例仅仅用以解释本申请，并不用于限定本申请。基于本申请提供的实施例，本领域普通技术人员在没有作出创造性劳动的前提下所获得的所有其他实施例，都属于本申请保护的范围。显而易见地，下面描述中的附图仅仅是本申请的一些示例或实施例，对于本领域的普通技术人员而言，在不付出创造性劳动的前提下，还可以根据这些附图将本申请应用于其他类似情景。此外，还可以理解的是，虽然这种开发过程中所作出的努力可能是复杂并且冗长的，然而对于与本申请公开的内容相关的本领域的普通技术人员而言，在本申请揭露的技术内容的基础上进行的一些设计，制造或者生产等变更只是常规的技术手段，不应当理解为本申请公开的内容不充分。在本申请中提及“实施例”意味着，结合实施例描述的特定特征、结构或特性可以包含在本申请的至少一个实施例中。在说明书中的各个位置出现该短语并不一定均是指相同的实施例，也不是与其它实施例互斥的独立的或备选的实施例。本领域普通技术人员显式地和隐式地理解的是，本申请所描述的实施例在不冲突的情况下，可以与其它实施例相结合。除非另作定义，本申请所涉及的技术术语或者科学术语应当为本申请所属技术领域内具有一般技能的人士所理解的通常意义。本申请所涉及的“一”、“一个”、“一种”、“该”等类似词语并不表示数量限制，可表示单数或复数。本申请所涉及的术语“包括”、“包含”、“具有”以及它们任何变形，意图在于覆盖不排他的包含；例如包含了一系列步骤或模块的过程、方法、系统、产品或设备没有限定于已列出的步骤或单元，而是可以还包括没有列出的步骤或单元，或可以还包括对于这些过程、方法、产品或设备固有的其它步骤或单元。本申请所涉及的“连接”、“相连”、“耦接”等类似的词语并非限定于物理的或者机械的连接，而是可以包括电气的连接，不管是直接的还是间接的。本申请所涉及的“多个”是指两个或两个以上。“和/或”描述关联对象的关联关系，表示可以存在三种关系，例如，“A和/或B”可以表示：单独存在A，同时存在A和B，单独存在B这三种情况。字符“/”一般表示前后关联对象是一种“或”的关系。本申请所涉及的术语“第一”、“第二”、“第三”等仅仅是区别类似的对象，不代表针对对象的特定排序。在本实施例中提供的方法实施例可以在终端、计算机或者类似的运算装置中执行。比如在终端上运行，图1是本实施例的基于目标生成式回应语言模型的回应方法的硬件结构框图。如图1所示，终端可以包括一个或多个处理器102和用于存储数据的存储器104，其中，处理器102可以包括但不限于微处理器MCU或可编程逻辑器件FPGA等的处理装置。上述终端还可以包括用于通信功能的传输设备106以及输入输出设备108。本领域普通技术人员可以理解，图1所示的结构仅为示意，其并不对上述终端的结构造成限制。例如，终端还可包括比图1中所示更多或者更少的组件，或者具有与图1所示出的不同配置。存储器104可用于存储计算机程序，例如，应用软件的软件程序以及模块，如在本实施例中的基于目标生成式回应语言模型的回应方法对应的计算机程序，处理器102通过运行存储在存储器104内的计算机程序，从而执行各种功能应用以及数据处理，即实现上述的方法。存储器104可包括高速随机存储器，还可包括非易失性存储器，如一个或者多个磁性存储装置、闪存、或者其他非易失性固态存储器。在一些实例中，存储器104可进一步包括相对于处理器102远程设置的存储器，这些远程存储器可以通过网络连接至终端。上述网络的实例包括但不限于互联网、企业内部网、局域网、移动通信网及其组合。传输设备106用于经由一个网络接收或者发送数据。上述的网络包括终端的通信供应商提供的无线网络。在一个实例中，传输设备106包括一个网络适配器，其可通过基站与其他网络设备相连从而可与互联网进行通讯。在一个实例中，传输设备106可以为射频模块，其用于通过无线方式与互联网进行通讯。在本实施例中提供了一种基于目标生成式回应语言模型的回应方法，图2是本实施例的基于目标生成式回应语言模型的回应方法的流程示意图，如图2所示，该流程包括如下步骤：步骤S210，基于教育设备中的提示数据集，训练得到初始生成式提示语言模型和初始生成式回应语言模型；初始生成式提示语言模型具备根据提示生成新提示的能力，初始生成式回应语言模型具备根据提示生成回应的能力。其中，提示数据集指预先收集的若干条提示的集合，提示指输入到模型的一段话，例如，“今天天气真好！”。新提示指与提示语义一致的另一种表达，例如，模型针对提示“今天天气真好”生成“今天万里无云，艳阳高照！”。回应指与提示语义连贯的一段话，例如，模型针对提示“今天天气真好”生成“是啊，适合出去散步。” 为了更好的改善语言模型的意料外行为问题，教育设备中的提示数据集包括更多探索性质的内容，比如一些孩童经常问的问题。具体的，训练初始生成式提示语言模型和初始生成式回应语言模型时可以使用监督学习。以提示数据集中的提示作为模型的输入，以根据该提示预先设置的新提示作为模型的输出目标，训练得到初始生成式提示语言模型。并且，以上述提示数据集中的提示作为模型的输入，以根据该提示预先设置的回应作为模型的输出目标，训练得到初始生成式回应语言模型。其中，根据提示预先设置的新提示和回应可以由数据处理人员编写，也可以使用现有的语言模型生成。训练过后的初始生成式提示语言模型和初始生成式回应语言模型已经具备了一定的根据提示生成新提示或回应的能力，为了使用强化学习和对抗学习继续提升生成式提示语言模型和生成式回应语言模型的能力，需要先训练评分模型，评分模型可以根据模型的输入和模型的输出给出评分。步骤S220，基于采样教育设备中成对的提示数据，训练得到提示评分模型；基于采样教育设备中成对的回应数据，训练得到回应评分模型。具体的，采样提示数据集中的一条提示，将提示输入初始生成式提示语言模型，生成若干条与提示对应的新提示；从模型输出中采样一条新提示，并将新提示和对应的提示组成成对的提示数据，可以得到若干组成对的提示数据。基于成对的提示数据，获取预存的第一评分值，基于第一评分值和对应的成对的提示数据训练得到提示评分模型。其中，第一评分值可通过人工评价或自动评价得到。人工评价是有由数据处理人员针对成对的提示数据的相对优劣进行评分；自动评价指借助统计评价指标或者评价模型针对成对的提示数据进行评分。具体的，采样提示数据集中的一条提示，将提示输入初始生成式回应语言模型，生成若干条与提示对应的回应；从模型输出中采样一条回应，并将回应和对应的提示组成成对的回应数据，可以得到若干组成对的回应数据。基于成对的回应数据，获取预存的第二评分值，基于第二评分值和对应的成对的回应数据训练得到提示评分模型。同样的，第二评分值也可通过人工评价或自动评价得到。步骤S230，利用提示评分模型对初始生成式提示语言模型输出的预测提示进行评分，得到提示评分值；利用回应评分模型对初始生成式回应语言模型输出的预测回应进行评分，得到回应评分值。具体的，获取教育设备中的一个新的提示数据集，将新的提示数据集作为初始生成式提示语言模型的输入数据，得到新生成的新提示，并利用提示评分模型对新生成的新提示进行评分，得到提示评分值；将新的提示数据集，以及新生成的新提示作为初始生成式回应语言模型的输入数据，得到新生成的回应，并利用回应评分模型对新生成的回应进行评分，得到回应评分值。步骤S240，对提示评分值和回应评分值进行加权计算，基于加权计算结果，通过强化学习和对抗学习进一步训练初始生成式提示语言模型和初始生成式回应语言模型，得到目标生成式回应语言模型。具体的，基于对提示评分值和回应评分值进行的不同的加权计算，分别得到初始生成式提示语言模型的评分以及初始生成式回应语言模型的评分；基于初始生成式提示语言模型的评分，更新初始生成式提示语言模型的参数，得到目标生成式提示语言模型；基于初始生成式回应语言模型的评分，更新初始生成式回应语言模型的参数，得到目标生成式回应语言模型。其中，强化学习和对抗学习为训练模型的方法。强化学习的方法给定输入并对模型输出评分，旨在训练模型根据输入生成评分更高的输出。对抗学习的方法涉及到两个模型，旨在通过两个模型的对抗，同步提升模型效果。步骤S250，将教育设备采集的待测文本数据输入目标生成式回应语言模型，目标生成式回应语言模型将待测文本数据与对话数据进行拼接，得到相应的回应。其中，对话数据包括起始符和目标生成式回应语言模型的输入输出数据，输入输出数据主要是多轮对话中某一次预测前所保存输入输出的数据，应用对话数据的目的在于保证目标生成式回应语言模型能够在多轮对话中保持对话的连续性。示例性的，在对话过程中，比如教育设备采集到的待测文本数据为“你好”，则将 “你好”与起始符“start”进行拼接，得到“start”+“你好”，以“start”+“你好”为目标生成式回应语言模型中前向网络中的输入数据，并得到模型的预测结果“你好~”，如果继续采集到文本数据“今天天气怎么样？”，则模型输入为“start”+“你好”+“你好~”+“今天天气怎么样？”，模型返回“今天天气不错”，如果教育设备采集到输入，则以此类推。在更复杂的场景中，比如，用户1尝试跟教育设备通过语音对话进行互动，在用户1说完“你好”之后，教育设备采集到用户1的输入数据“你好”，并利用目标生成式回应语言模型返回“你好~”，此时用户2跟用户1对话“你的PPT写的怎么样了”，用户1回复用户2“还没开始写呢”，由于用户1和用户2之间也是语音对话，这段对话被教育设备识别并输入到目标生成式回应语言模型，具体为“start”+“你好”+“你好~”+“你那个PPT写的怎么样了”+“还没开始写呢”，此时，目标生成式回应语言模型分析到“你那个PPT写的怎么样了”+“还没开始写呢”这一部分内容和前文“start”+“你好”+“你好~”没有任何逻辑关联，也不像是对模型的对话，于是不输出任何回应。在上述方法中，通过教育设备中的提示数据集首先训练得到初始生成式提示语言模型和初始生成式回应语言模型，再训练得到提示评分模型和提示评分模型以对上述两个生成式语言模型进行评分，基于评分使用强化学习和对抗学习进一步训练初始生成式提示语言模型和初始生成式回应语言模型，加强两个生成式语言模型之间的交互，得到遵循基本规则的目标生成式回应语言模型，从而进一步改善教育设备中生成式语言模型的意料外行为问题。在一个实施例中，基于上述步骤S210，基于教育设备中的提示数据集，训练得到初始生成式提示语言模型和初始生成式回应语言模型，具体可以包括以下步骤：步骤S211，获取教育设备中的提示数据集，并基于从提示数据集中采样的提示，获取根据提示预先设置的新提示和回应。其中，预先设置的新提示是根据提示的语义来编写的另一种与提示语义一致的表达，可以由数据处理人员编写，也可以使用现有的语言模型生成；预先设置的回应是根据提示的语义来编写的与提示语义连贯的一段话，可以由数据处理人员编写，也可以使用现有的语言模型生成。步骤S212，将所述提示作为模型输入，并将所述预先设置的新提示为训练目标，使用监督学习训练得到初始生成式提示语言模型。步骤S213，将提示作为模型输入，并将预先设置的回应为训练目标，使用监督学习训练得到初始生成式回应语言模型。通过上述步骤S211至步骤S213，通过提示数据集中的样本作为输入，预先设置的新提示和回应作为训练目标，使用监督学习的方法，训练得到一个具备根据提示生成新提示能力的生成式提示语言模型和一个具备根据提示生成新提回应的生成式回应语言模型，为后续得到效果更好的生成式语言模型提供预训练基础。在一个实施例中，基于上述步骤S220，基于采样教育设备中的成对的提示数据，训练得到提示评分模型，具体可以包括以下步骤：步骤S221，采样教育设备中的提示数据集中的一条提示，将提示输入初始生成式提示语言模型，得到模型生成的新提示。步骤S222，获取教育设备中的根据模型生成的新提示预先设置的第一评分值。具体的，在得到模型生成的新提示后，可以对模型生成的新提示的优劣进行评分，并将得到的第一评分值预先存储在计算机设备中，待训练提示评分模型时进行读取。示例性的，评价过程借助统计评价指标或者评价模型对模型生成的新提示进行评分。此外，生成在得到模型的新提示后，也可以发送给数据处理人员，由数据处理人员对模型生成的新提示的优劣进行评分。步骤S223，基于提示、模型生成的新提示以及第一评分值，训练得到提示评分模型。上述步骤S221至步骤S223，通过参考新提示对应的第一评分值，训练得到一个预测更精准的提示评分模型，以提高生成式语言模型的预训练效果。在一个实施例中，基于上述步骤S220，基于采样教育设备中的成对的回应数据，训练得到回应评分模型，具体可以包括以下步骤：步骤S224，采样教育设备中提示数据集中的一条提示，将提示输入初始生成式回应语言模型，得到模型生成的回应。步骤S225，获取教育设备中的根据模型生成的回应预先设置的第二评分值。具体的，在得到模型生成的回应后，可以对模型生成的回应的优劣进行评分，并将得到的第二评分值预先存储在计算机设备中，待训练提示评分模型时进行读取。示例性的，评价过程借助统计评价指标或者评价模型对模型生成的回应进行评分。此外，生成在得到模型的回应后，也可以发送给数据处理人员，由数据处理人员对模型生成的回应的优劣进行评分。步骤S226，基于提示、模型生成的回应以及第二评分值，训练得到回应评分模型。上述步骤S224至步骤S226，通过参考回应对应的第二评分值，训练得到一个预测更精准的回应评分模型，以提高生成式语言模型的预训练效果。在一个实施例中，基于上述步骤S230，利用回应评分模型对初始生成式回应语言模型输出的预测回应进行评分，得到回应评分值，具体可以包括以下步骤：步骤S231，获取一个新的提示数据集；步骤S232，将新的提示数据集输入至初始生成式回应语言模型，得到第一预测回应，利用回应评分模型对第一预测回应进行评分，得到第一回应评分值；步骤S233，将新的提示数据集输入至初始生成式提示语言模型，得到新生成的新提示，将新生成的新提示输入至初始生成式回应语言模型，得到第二预测回应，利用回应评分模型对第二预测回应进行评分，得到第二回应评分值。为增加对抗学习的效果，优选的，通过提示评分模型对上述步骤S233中新生成的新提示进行评分，将高评分值的新生成的新提示作为步骤S233中初始生成式回应语言模型。只保留高提示评分值对应的新提示，可以更好的引导两个生成式语言模型的输出质量，利于两个语言模型之间互相提升文本生成效果。上述步骤S231至步骤S233，通过利用训练好的初始生成式提示语言模型得到新生成的新提示，为初始生成式回应语言模型提供了更丰富的训练数据，并基于不同的输入数据集得到不同的回应评分值，便于对各评分值灵活的设置权重参数。在一个实施例中，基于上述步骤S240，对提示评分值和回应评分值进行加权计算，基于加权计算结果，通过强化学习和对抗学习进一步训练初始生成式提示语言模型和初始生成式回应语言模型，得到目标生成式回应语言模型，具体可以包括以下步骤：步骤S241，基于对提示评分值和回应评分值进行的不同的加权计算，分别得到初始生成式提示语言模型的评分以及初始生成式回应语言模型的评分。其中，回应评分值可通过上述实施例中的步骤S231至步骤S233得到，设回应评分值中的第一回应评分值为rr1，第二回应评分值为rr2，另外，设提示评分值为rp，示例性的，将rp+α作为初始生成式提示语言模型的评分，将rr1+βrr2作为初始生成式回应语言模型的评分。训练中可根据模型生成效果通过调节参数α、β选择性引入对抗。具体的，当α=0且β=0时，初始生成式提示语言模型和初始生成式回应语言模型分别进行基于强化学习的训练。当α、β其中一个不为0时，两个模型同时进行基于强化学习和对抗学习的训练。在实际训练过程中，可以先进行一定程度的单独训练，然后逐步增加对抗训练过程。步骤S242，基于初始生成式提示语言模型的评分，更新初始生成式提示语言模型的参数，得到目标生成式提示语言模型。其中，目标生成式提示语言模型用来探索更多的表达方式，以便为生成式回应语言模型提供多样的输入。步骤S243，基于初始生成式回应语言模型的评分，更新初始生成式回应语言模型的参数，得到目标生成式回应语言模型。上述步骤S241至步骤S243，分别设置初始生成式提示语言模型和初始生成式回应语言模型的评分，并可以通过调节权重值来调节强化学习和对抗学习的程度，及时根据模型生成效果更新模型参数，完成进一步的训练，使得初始生成式提示语言模型探索对于初始生成式回应语言模型更加困难的新提示，使得初始生成式回应语言模型以表达更丰富的提示作为输入，两个模型之间互相提升文本生成效果，得到可以输出更规范的文本的目标生成式提示语言模型和目标生成式回应语言模型。下面通过优选实施例对本实施例进行描述和说明。图3为初始生成式提示语言模型和初始生成式回应语言模型训练过程的示意图。步骤1.1，在教育设备中的提示数据集中采样一条提示“今天天气真好！”；步骤1.2，获取根据提示预先设置的新提示“今天万里无云，艳阳高照！”；步骤1.3，将步骤1.1中采样得到的提示作为模型输入，将步骤1.2中预先设置的新提示作为模型训练的目标，使用监督学习训练得到初始生成式提示语言模型。步骤2.1，在教育设备中的提示数据集中采样一条提示“今天天气真好！”；步骤2.2，获取根据提示预先设置的回应“是啊，适合出去散步。”；步骤2.3，将步骤2.1中采样得到的提示作为模型输入，将步骤2.2中预先设置的回应作为模型训练的目标，使用监督学习训练得到初始生成式回应语言模型。其中，预先设置的新提示和回应可以由数据处理人员编写，也可以使用现有的语言模型生成。图4为提示评分模型和回应评分模型训练过程的示意图。步骤3.1，在教育设备中的提示数据集中采样一条提示“今天天气真好！”；步骤3.2，将步骤3.1中采样得到的提示输入至初始生成式提示语言模型，以获得模型输出“今天好闷啊！”、“今天秋高气爽凉风习习！”等等；步骤3.3，在步骤3.2的模型输出中采样多条生成的新提示“今天好闷啊！”、“今天秋高气爽凉风习习！”；步骤3.4，获取步骤3.3中的新提示所对应的预先设置的第一评分值；其中，第一评分值可通过人工评价或自动评价得到，“今天好闷啊！”和“今天秋高气爽凉风习习！”各自对应有第一评分值，因为“今天秋高气爽凉风习习！”比“今天好闷啊！”更贴合步骤3.1中的提示的语义，所以“今天秋高气爽凉风习习！”对应的第一评分值比“今天好闷啊！”对应的第一评分值分数更高；步骤3.5，根据步骤3.1中采样得到的提示、步骤3.2中采样的新提示、以及步骤3.4中获得的第一评分，训练得到提示评分模型。步骤4.1，在教育设备中的提示数据集中采样一条提示“今天天气真好！”；步骤4.2，将步骤4.1中采样得到的提示输入至初始生成式回应语言模型，以获得模型输出“真想出去走走！”、“足球丢了！”等等；步骤4.3，在步骤4.2的模型输出中采样多条生成的回应“真想出去走走！”、“足球丢了！”；步骤4.4，获取步骤4.3中的回应所对应的预先设置的第二评分值；其中，第二评分值可通过人工评价或自动评价得到，“真想出去走走！”和“足球丢了！”各自对应有第二评分值，因为“真想出去走走！”比“足球丢了！”更贴合步骤4.1中的提示的语义，所以“真想出去走走！”对应的第二评分值比“足球丢了！”对应的第二评分值分数更高；步骤4.5，根据步骤4.1中采样得到的提示、步骤4.2中采样的回应、以及步骤4.4中获得的第一评分，训练得到回应评分模型。图5为基于强化学习和对抗学习训练目标生成式提示语言模型和目标生成式回应语言模型过程的示意图。步骤5.1，从一个新的提示数据集中采样一条提示“今天天气真好！”；步骤5.2，将步骤5.1中采样的提示“今天天气真好！”输入至初始生成式提示语言模型，得到模型生成的新提示“今天好闷啊！”；步骤5.3，将步骤5.2中生成的新提示“今天好闷啊！”输入至提示评分模型，得到该新提示的提示评分值rp；步骤5.4，将步骤5.1中的提示“今天天气真好！”输入至初始生成式回应语言模型，得到模型生成的回应“真想出去走走！”；步骤5.5，将步骤5.4中生成的回应“真想出去走走！”输入至回应评分模型，得到该回应的第一回应评分值rr1；步骤5.6，若步骤5.3中的提示评分值rp高于阈值，则将步骤5.2中生成的新提示“今天好闷啊！”输入至初始生成式回应语言模型，得到模型生成的回应；步骤5.7，将步骤5.6中生成的回应输入至回应评分模型，得到该回应的第二回应评分值rr2；步骤5.8，将rp+α作为初始生成式提示语言模型的评分，将rr1+βrr2作为初始生成式回应语言模型的评分，利用评分更新对应模型的参数，得到目标生成式提示语言模型和目标生成式回应语言模型。步骤6，将教育设备采集的待测文本数据输入所述目标生成式回应语言模型，目标生成式回应语言模型将待测文本数据与对话数据进行拼接，得到相应的回应。本优选实施例与现有技术相比，训练模型用的提示表达更加多样，初始生成式提示语言模型可以根据已有的提示生成不同表达形式的新提示，并将新提示作为初始生成式回应语言模型的输入，丰富了训练数据集。并且，模型的鲁棒性更强，通过引入对抗学习，促使生成式提示语言模型探索对于生成式回应语言模型更加困难的提示，促使生成式回应语言模型使用更多形式的提示，使得初始生成式提示语言模型和初始生成式回应语言模型二者之间互相提升文本生成效果。应该理解的是，虽然如上的各实施例所涉及的流程图中的各个步骤按照箭头的指示依次显示，但是这些步骤并不是必然按照箭头指示的顺序依次执行。除非本文中有明确的说明，这些步骤的执行并没有严格的顺序限制，这些步骤可以以其它的顺序执行。而且，如上的各实施例所涉及的流程图中的至少一部分步骤可以包括多个步骤或者多个阶段，这些步骤或者阶段并不必然是在同一时刻执行完成，而是可以在不同的时刻执行，这些步骤或者阶段的执行顺序也不必然是依次进行，而是可以与其它步骤或者其它步骤中的步骤或者阶段的至少一部分轮流或者交替地执行。基于同样的发明构思，在本实施例中还提供了一种基于目标生成式回应语言模型的回应装置，该系统用于实现上述实施例及优选实施方式，已经进行过说明的不再赘述。以下所使用的术语“模块”、“单元”、“子单元”等可以实现预定功能的软件和/或硬件的组合。尽管在以下实施例中所描述的系统较佳地以软件来实现，但是硬件，或者软件和硬件的组合的实现也是可能并被构想的。在一个实施例中，如图6所示，提供了一种基于目标生成式回应语言模型的回应装置，包括：训练初始模型模块61、训练评分模型模块62、应用评分模型模块63、训练目标模型模块64和应用目标模型模块65，其中：训练初始模型模块61，用于基于教育设备中的提示数据集，训练得到初始生成式提示语言模型和初始生成式回应语言模型；初始生成式提示语言模型具备根据提示生成新提示的能力，初始生成式回应语言模型具备根据提示生成回应的能力。训练评分模型模块62，用于基于采样教育设备中成对的提示数据，训练得到提示评分模型；基于采样教育设备中成对的回应数据，训练得到回应评分模型。应用评分模型模块63，用于利用提示评分模型对初始生成式提示语言模型输出的预测提示进行评分，得到提示评分值；利用回应评分模型对初始生成式回应语言模型输出的预测回应进行评分，得到回应评分值。训练目标模型模块64，用于对提示评分值和回应评分值进行加权计算，基于加权计算结果，通过强化学习和对抗学习进一步训练初始生成式提示语言模型和初始生成式回应语言模型，得到目标生成式回应语言模型。应用目标模型模块65，用于将教育设备采集的待测文本数据输入目标生成式回应语言模型，目标生成式回应语言模型将待测文本数据与对话数据进行拼接，得到相应的回应。在一个实施例中，训练初始模型模块61还包括获取教育设备中的提示数据集，并基于从提示数据集中采样的提示，获取根据提示预先设置的新提示和回应；将提示作为模型输入，并将预先设置的新提示为训练目标，使用监督学习训练得到初始生成式提示语言模型；将提示作为模型输入，并将预先设置的回应为训练目标，使用监督学习训练得到初始生成式回应语言模型。在一个实施例中，训练评分模型模块62还包括采样教育设备中的提示数据集中的一条提示，将提示输入初始生成式提示语言模型，得到模型生成的新提示；获取教育设备中的根据模型生成的新提示预先设置的第一评分值；基于提示、模型生成的新提示以及第一评分值，训练得到提示评分模型。在一个实施例中，训练评分模型模块62还包括采样教育设备中的提示数据集中的一条提示，将提示输入初始生成式回应语言模型，得到模型生成的回应；获取教育设备中的根据模型生成的回应预先设置的第二评分值；基于提示、模型生成的回应以及第二评分值，训练得到回应评分模型。在一个实施例中，应用评分模型模块63还包括获取一个新的提示数据集；将新的提示数据集输入至初始生成式回应语言模型，得到第一预测回应，利用回应评分模型对第一预测回应进行评分，得到第一回应评分值；将新的提示数据集输入至初始生成式提示语言模型，得到新生成的新提示，将新生成的新提示输入至初始生成式回应语言模型，得到第二预测回应，利用回应评分模型对第二预测回应进行评分，得到第二回应评分值。在一个实施例中，训练目标模型模块64还包括基于对提示评分值和回应评分值进行的不同的加权计算，分别得到初始生成式提示语言模型的评分以及初始生成式回应语言模型的评分；基于初始生成式提示语言模型的评分，更新初始生成式提示语言模型的参数，得到目标生成式提示语言模型；基于初始生成式回应语言模型的评分，更新初始生成式回应语言模型的参数，得到目标生成式回应语言模型。上述基于目标生成式回应语言模型的回应装置中的各个模块可全部或部分通过软件、硬件及其组合来实现。上述各模块可以硬件形式内嵌于或独立于计算机设备中的处理器中，也可以以软件形式存储于计算机设备中的存储器中，以便于处理器调用执行以上各个模块对应的操作。在一个实施例中，提供了一种计算机设备，包括存储器和处理器，存储器中存储有计算机程序，该处理器执行计算机程序时实现上述任一项基于目标生成式回应语言模型的回应方法实施例中的步骤。在一个实施例中，提供了一种计算机可读存储介质，其上存储有计算机程序，计算机程序被处理器执行时实现上述任一项基于目标生成式回应语言模型的回应方法实施例中的步骤。在一个实施例中，提供了一种计算机程序产品，包括计算机程序，该计算机程序被处理器执行时实现上述任一项基于目标生成式回应语言模型的回应方法实施例中的步骤。需要说明的是，本申请所涉及的用户信息和数据，均为经用户授权或者经过各方充分授权的信息和数据。本领域普通技术人员可以理解实现上述实施例方法中的全部或部分流程，是可以通过计算机程序来指令相关的硬件来完成，所述的计算机程序可存储于一非易失性计算机可读取存储介质中，该计算机程序在执行时，可包括如上述各方法的实施例的流程。其中，本申请所提供的各实施例中所使用的对存储器、数据库或其它介质的任何引用，均可包括非易失性和易失性存储器中的至少一种。非易失性存储器可包括只读存储器、磁带、软盘、闪存、光存储器、高密度嵌入式非易失性存储器、阻变存储器、磁变存储器、铁电存储器、相变存储器、石墨烯存储器等。易失性存储器可包括随机存取存储器或外部高速缓冲存储器等。作为说明而非局限，RAM可以是多种形式，比如静态随机存取存储器或动态随机存取存储器等。本申请所提供的各实施例中所涉及的数据库可包括关系型数据库和非关系型数据库中至少一种。非关系型数据库可包括基于区块链的分布式数据库等，不限于此。本申请所提供的各实施例中所涉及的处理器可为通用处理器、中央处理器、图形处理器、数字信号处理器、可编程逻辑器、基于量子计算的数据处理逻辑器等，不限于此。以上实施例的各技术特征可以进行任意的组合，为使描述简洁，未对上述实施例中的各个技术特征所有可能的组合都进行描述，然而，只要这些技术特征的组合不存在矛盾，都应当认为是本说明书记载的范围。以上所述实施例仅表达了本申请的几种实施方式，其描述较为具体和详细，但并不能因此而理解为对本申请专利范围的限制。应当指出的是，对于本领域的普通技术人员来说，在不脱离本申请构思的前提下，还可以做出若干变形和改进，这些都属于本申请的保护范围。因此，本申请的保护范围应以所附权利要求为准。
