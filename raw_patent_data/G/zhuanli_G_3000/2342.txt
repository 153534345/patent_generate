标题title
一种对话模型的优化部署方法、装置、设备及介质
摘要abst
本发明公开了一种对话模型的优化部署方法、装置、设备及介质。对话模型的优化部署方法，包括：获取当前自助对话模型、当前自助对话模型网络参数以及模型训练关联数据集；根据当前自助对话模型的硬件关联特征数据以及目标优化指标，对当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，得到待训练自助对话模型；基于模型训练关联数据集以及模型综合部署指标，对待训练自助对话模型进行训练优化，得到符合模型综合部署指标的待部署自助对话模型；对待部署自助对话模型进行优化部署。本发明实施例的技术方案能够在低存储量、低能耗的前提下对模型快速高效部署。
权利要求书clms
1.一种对话模型的优化部署方法，其特征在于，包括：获取当前自助对话模型、当前自助对话模型网络参数以及模型训练关联数据集；根据所述当前自助对话模型的硬件关联特征数据以及目标优化指标，对所述当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，得到待训练自助对话模型；基于所述模型训练关联数据集以及模型综合部署指标，对所述待训练自助对话模型进行训练优化，得到符合所述模型综合部署指标的待部署自助对话模型；对所述待部署自助对话模型进行优化部署。2.根据权利要求1所述的方法，其特征在于，所述根据所述当前自助对话模型的硬件关联特征数据以及目标优化指标，对所述当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，得到待训练自助对话模型，包括：根据所述当前自助对话模型的硬件关联特征数据以及目标优化指标，对所述当前自助对话模型的注意力机制层和前馈神经网络层进行计算图优化，并在所述注意力机制层的输入向量进行计算图优化，得到预处理自助对话模型；确定所述预处理自助对话模型的注意力机制层和前馈神经网络层中模型压缩计算节点位置；根据所述模型压缩计算节点位置，对所述预处理自助对话模型进行模型压缩计算节点插入处理，得到所述待训练自助对话模型。3.根据权利要求1所述的方法，其特征在于，所述基于所述模型训练关联数据集以及模型综合部署指标，对所述待训练自助对话模型进行训练优化，得到符合所述模型综合部署指标的待部署自助对话模型，包括：根据所述模型训练关联数据集，分析所述待训练自助对话模型的模型精度损失数据，并根据所述模型精度损失数据，确定重训练超参数；根据所述重训练超参数对所述待训练自助对话模型进行重训练；若重训练后的待训练自助对话模型达到所述模型综合部署指标，则将重训练后的所述待训练自助对话模型，作为所述待部署自助对话模型。4.根据权利要求3所述的方法，其特征在于，在确定重训练超参数之后，还包括：若重训练后的所述待训练自助对话模型精度达不到所述当前自助对话模型的模型精度，则对所述重训练后的待训练自助对话模型进行优化训练，得到符合所述当前自助对话模型的模型精度的第一待训练自助对话模型；根据所述第一待训练自助对话模型以及所述模型综合部署指标，确定所述待部署自助对话模型。5.根据权利要求4所述的方法，其特征在于，所述根据所述第一待训练自助对话模型以及所述模型综合部署指标，确定所述待部署自助对话模型，包括：若所述第一待训练自助对话模型不符合所述模型综合部署指标，则对所述第一待训练自助对话模型中插入的低比特量化节点和/或稀疏化节点进行调整，得到符合所述模型综合部署指标的待部署自助对话模型。6.根据权利要求4所述的方法，其特征在于，所述若重训练后的所述待训练自助对话模型精度达不到所述当前自助对话模型的模型精度，则对所述重训练后的待训练自助对话模型进行优化训练，得到符合所述当前自助对话模型的模型精度的第一待训练自助对话模型，包括：将所述当前自助对话模型作为自助对话教师模型；根据所述自助对话教师模型，对重训练后的所述待训练自助对话模型进行优化训练，得到符合所述当前自助对话模型的模型精度的第一待训练自助对话模型。7.根据权利要求1所述的方法，其特征在于，所述对所述待部署自助对话模型进行优化部署，包括：对所述待部署自助对话模型进行冗余节点消除、图融合以及张量稀疏化存储，得到目标待部署自助对话模型；在目标平台对所述目标待部署自助对话模型进行部署。8.一种对话模型的优化部署装置，其特征在于，包括：数据获取模块，用于获取当前自助对话模型、当前自助对话模型网络参数以及模型训练关联数据集；待训练自助对话模型确定模块，用于根据所述当前自助对话模型的硬件关联特征数据以及目标优化指标，对所述当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，得到待训练自助对话模型；待部署自助对话模型获取模块，用于基于所述模型训练关联数据集以及模型综合部署指标，对所述待训练自助对话模型进行训练优化，得到符合所述模型综合部署指标的待部署自助对话模型；优化部署模块，用于对所述待部署自助对话模型进行优化部署。9.一种电子设备，其特征在于，所述电子设备包括：至少一个处理器；以及与所述至少一个处理器通信连接的存储器；其中，所述存储器存储有可被所述至少一个处理器执行的计算机程序，所述计算机程序被所述至少一个处理器执行，以使所述至少一个处理器能够执行权利要求1-7中任一项所述的对话模型的优化部署方法。10.一种计算机可读存储介质，其特征在于，所述计算机可读存储介质存储有计算机指令，所述计算机指令用于使处理器执行时实现权利要求1-7中任一项所述的对话模型的优化部署方法。
说明书desc
技术领域本发明涉及深度学习技术领域，尤其涉及一种对话模型的优化部署方法、装置、设备及介质。背景技术随着深度学习算法的发展以及芯片算力与存储的增加，深度学习模型中对话模型的模型参数数量能达到几十亿、几百亿、几千亿甚至上万亿级别，导致对话模型部署时能耗以及存储等消耗日益增大，然而终端与边缘端设备因芯片的存储与算力能耗受限，对于需要部署的模型的算力与参数量要求更为严苛。发明内容本发明提供了一种对话模型的优化部署方法、装置、设备及介质，以解决现有对话模型部署受限严重以及存储消耗大的问题。根据本发明的一方面，提供了一种对话模型的优化部署方法，包括：获取当前自助对话模型、当前自助对话模型网络参数以及模型训练关联数据集；根据当前自助对话模型的硬件关联特征数据以及目标优化指标，对当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，得到待训练自助对话模型；基于模型训练关联数据集以及模型综合部署指标，对待训练自助对话模型进行训练优化，得到符合模型综合部署指标的待部署自助对话模型；对待部署自助对话模型进行优化部署。根据本发明的另一方面，提供了一种对话模型的优化部署装置，包括：数据获取模块，用于获取当前自助对话模型、当前自助对话模型网络参数以及模型训练关联数据集；待训练自助对话模型确定模块，用于根据当前自助对话模型的硬件关联特征数据以及目标优化指标，对当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，得到待训练自助对话模型；待部署自助对话模型获取模块，用于基于模型训练关联数据集以及模型综合部署指标，对待训练自助对话模型进行训练优化，得到符合模型综合部署指标的待部署自助对话模型；优化部署模块，用于对待部署自助对话模型进行优化部署。根据本发明的另一方面，提供了一种电子设备，所述电子设备包括：至少一个处理器；以及与所述至少一个处理器通信连接的存储器；其中，所述存储器存储有可被所述至少一个处理器执行的计算机程序，所述计算机程序被所述至少一个处理器执行，以使所述至少一个处理器能够执行本发明任一实施例所述的对话模型的优化部署方法。根据本发明的另一方面，提供了一种计算机可读存储介质，所述计算机可读存储介质存储有计算机指令，所述计算机指令用于使处理器执行时实现本发明任一实施例所述的对话模型的优化部署方法。本发明实施例的技术方案，通过获取当前自助对话模型、当前自助对话模型网络参数以及模型训练关联数据集，从而根据当前自助对话模型的硬件关联特征数据以及目标优化指标，对当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，得到待训练自助对话模型，进而基于模型训练关联数据集以及模型综合部署指标，对待训练自助对话模型进行训练优化，得到符合模型综合部署指标的待部署自助对话模型，最终对待部署自助对话模型进行优化部署。在本方案中，对当前自助对话模型的优化处理考虑了硬件关联特征数据，能够大幅度避免模型部署时存储与算力能耗不支持的问题，而通过对当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，实现对模型的存储与算力的压缩，节省模型部署空间，降低算力需求，对待训练自助对话模型进行训练优化，使其达到最终的部署要求时再进行部署，解决了现有对话模型部署受限严重以及存储消耗大的问题，能够在低存储量、低能耗的前提下对模型快速高效部署。应当理解，本部分所描述的内容并非旨在标识本发明的实施例的关键或重要特征，也不用于限制本发明的范围。本发明的其它特征将通过以下的说明书而变得容易理解。附图说明为了更清楚地说明本发明实施例中的技术方案，下面将对实施例描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本发明的一些实施例，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。图1为本发明实施例一提供的一种对话模型的优化部署方法的流程图；图2为本发明实施例二提供的一种对话模型的优化部署方法的流程图；图3为本发明实施例二提供的一种对话模型的优化部署方法的逻辑图；图4为本发明实施例二提供的一个未插入节点前的计算图；图5为本发明实施例二提供的一个插入节点后的计算图；图6为本发明实施例二提供的一种部署于目标平台的计算图；图7为本发明实施例三提供的一种对话模型的优化部署装置的结构示意图；图8示出了可以用来实施本发明的实施例的电子设备的结构示意图。具体实施方式为了使本技术领域的人员更好地理解本发明方案，下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例仅仅是本发明一部分的实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有做出创造性劳动前提下所获得的所有其他实施例，都应当属于本发明保护的范围。需要说明的是，本发明的说明书和权利要求书及上述附图中的术语“第一”等是用于区别类似的对象，而不必用于描述特定的顺序或先后次序。应该理解这样使用的数据在适当情况下可以互换，以便这里描述的本发明的实施例能够以除了在这里图示或描述的那些以外的顺序实施。此外，术语“包括”和“具有”以及他们的任何变形，意图在于覆盖不排他的包含，例如，包含了一系列步骤或单元的过程、方法、系统、产品或设备不必限于清楚地列出的那些步骤或单元，而是可包括没有清楚地列出的或对于这些过程、方法、产品或设备固有的其它步骤或单元。实施例一图1为本发明实施例一提供的一种对话模型的优化部署方法的流程图，本实施例可适用于低存储量消耗快速部署对话模型的情况，该方法可以由对话模型的优化部署装置来执行，该对话模型的优化部署装置可以采用硬件和/或软件的形式实现，该对话模型的优化部署装置可配置于电子设备中。如图1所示，该方法包括：S110、获取当前自助对话模型、当前自助对话模型网络参数以及模型训练关联数据集。其中，当前自助对话模型可以是具备自助对话功能的已完成训练的深度学习模型，需要在芯片上进行优化部署。当前自助对话模型可以用于与对话对象进行对话交互。当前自助对话模型网络参数可以是当前自助对话模型的网络参数。模型训练关联数据集可以用于训练深度学习模型使训练后的模型具备自助对话功能。示例性的，模型训练关联数据集可以包括但不限于公开的对话数据集，或者当前自助对话模型的训练集等。在本发明实施例中，可以获取需要优化部署的当前自助对话模型，进而获取与当前自助对话模型对应的当前自助对话模型网络参数以及模型训练关联数据集。S120、根据当前自助对话模型的硬件关联特征数据以及目标优化指标，对当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，得到待训练自助对话模型。其中，硬件关联特征数据可以是根据当前自助对话模型中的计算节点匹配的硬件指令集确定的硬件特征数据。硬件关联特征数据可以包括执行前述硬件指令集的硬件的存储容量以及算力等数据。目标优化指标可以是预先设置的优化指标，用于指导当前自助对话模型进行优化和存储空间压缩。目标优化指标可以包括但不限于能耗、时延以及存储需求等。计算图优化可以是已知的任意的计算图的优化处理操作。可选的，计算图优化可以包括但不限于计算节点融合以及冗余节点删除等。模型压缩计算节点插入处理，用于对计算图优化后的当前自助对话模型进行低比特量化和稀疏化。模型压缩计算节点插入处理可以包括对计算图优化后的当前自助对话模型插入低比特量化节点以及稀疏化节点。低比特量化节点可以包括模型可参数量化节点以及模型计算特征图量化节点。待训练自助对话模型可以是完成计算图优化以及模型压缩计算节点插入处理的当前自助对话模型。在本发明实施例中，可以确定当前自助对话模型的计算节点所需使用的硬件指令集，从而获取执行硬件指令集的对应硬件的硬件关联特征数据，并根据模型性能优化需求配置目标优化指标，进而基于当前自助对话模型的硬件关联特征数据以及目标优化指标，对当前自助对话模型进行计算图优化，进一步对计算图优化后的当前自助对话模型进行模型压缩，即在相关计算节点进行节点插入处理完成模型压缩计算节点插入处理，得到待训练自助对话模型。S130、基于模型训练关联数据集以及模型综合部署指标，对待训练自助对话模型进行训练优化，得到符合模型综合部署指标的待部署自助对话模型。其中，模型综合部署指标可以是待训练自助对话模型优化训练后需要达到的部署指标。模型综合部署指标可以包括但不限于模型精度指标以及性能指标。待部署自助对话模型可以是对待训练自助对话模型进行训练优化得到的，符合模型综合部署指标要求的自助对话模型。在本发明实施例中，可以根据当前自助对话模型所需部署的芯片的性能，设置模型综合部署指标，从而利用模型训练关联数据集对待训练自助对话模型进行训练，并利用模型综合部署指标对训练优化后的待训练自助对话模型进行评估优化，得到符合模型综合部署指标的待部署自助对话模型。可选的，若训练优化后的待训练自助对话模型，满足模型综合部署指标，则可将训练后的待训练自助对话模型作为符合模型综合部署指标的待部署自助对话模型。若训练优化后的待训练自助对话模型不满足模型综合部署指标，则根据模型综合部署指标调整训练参数，对待训练自助对话模型再次进行优化训练，直至得到符合模型综合部署指标的待部署自助对话模型。S140、对待部署自助对话模型进行优化部署。在本发明实施例中，得到待部署自助对话模型之后，进一步按照预先设置的优化处理算法，对待部署自助对话模型进行优化处理，并将优化处理后的待部署自助对话模型进行部署。本发明实施例的技术方案，通过获取当前自助对话模型、当前自助对话模型网络参数以及模型训练关联数据集，从而根据当前自助对话模型的硬件关联特征数据以及目标优化指标，对当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，得到待训练自助对话模型，进而基于模型训练关联数据集以及模型综合部署指标，对待训练自助对话模型进行训练优化，得到符合模型综合部署指标的待部署自助对话模型，最终对待部署自助对话模型进行优化部署。在本方案中，对当前自助对话模型的优化处理考虑了硬件关联特征数据，能够大幅度避免模型部署时存储与算力能耗不支持的问题，而通过对当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，实现对模型的存储与算力的压缩，降低算力需求，对待训练自助对话模型进行训练优化，使其达到最终的部署要求时再进行部署，解决了现有对话模型部署受限严重以及存储消耗大的问题，能够在低存储量、低能耗的前提下对模型快速高效部署。实施例二图2为本发明实施例二提供的一种对话模型的优化部署方法的流程图，本实施例以上述实施例为基础进行具体化，给出了根据当前自助对话模型的硬件关联特征数据以及目标优化指标，对当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，得到待训练自助对话模型的具体的可选的实施方式。如图2所示，该方法包括：S210、获取当前自助对话模型、当前自助对话模型网络参数以及模型训练关联数据集。S220、根据当前自助对话模型的硬件关联特征数据以及目标优化指标，对当前自助对话模型的注意力机制层和前馈神经网络层进行计算图优化，并在注意力机制层的输入向量进行计算图优化，得到预处理自助对话模型。其中，预处理自助对话模型可以是对当前自助对话模型进行计算图优化后得到的自助对话模型。在本发明实施例中，可以根据当前自助对话模型的硬件关联特征数据以及目标优化指标，对当前自助对话模型的注意力机制层和前馈神经网络层进行计算操作融合，并在注意力机制层的输入向量按照输入向量的权重形状进行向量的计算融合，得到预处理自助对话模型。S230、确定预处理自助对话模型的注意力机制层和前馈神经网络层中模型压缩计算节点位置。其中，模型压缩计算节点位置可以是预处理自助对话模型中插入节点的位置。在本发明实施例中，可以根据硬件关联特征数据以及目标优化指标，确定基于低比特化和稀疏化实现模型压缩时，在预处理自助对话模型的注意力机制层和前馈神经网络层中进行节点插入的位置，从而将确定的节点插入位置作为模型压缩计算节点位置。S240、根据模型压缩计算节点位置，对预处理自助对话模型进行模型压缩计算节点插入处理，得到待训练自助对话模型。在本发明实施例中，可以在预处理自助对话模型的模型压缩计算节点位置，进行模型压缩计算节点插入处理，得到待训练自助对话模型。S250、基于模型训练关联数据集以及模型综合部署指标，对待训练自助对话模型进行训练优化，得到符合模型综合部署指标的待部署自助对话模型。在本发明的一个可选实施例中，基于模型训练关联数据集以及模型综合部署指标，对待训练自助对话模型进行训练优化，得到符合模型综合部署指标的待部署自助对话模型，可以包括：根据模型训练关联数据集，分析待训练自助对话模型的模型精度损失数据，并根据模型精度损失数据，确定重训练超参数；根据重训练超参数对待训练自助对话模型进行重训练；若重训练后的待训练自助对话模型达到模型综合部署指标，则将重训练后的待训练自助对话模型，作为待部署自助对话模型。其中，模型精度损失数据可以是描述待训练自助对话模型的精度相较于当前自助对话模型损失程度的数据，用于描述模型压缩计算节点插入处理对当前自助对话模型造成的精度损失。重训练超参数可以是根据模型精度损失数据确定的，对待训练自助对话模型重训练使用的超参数。重训练超参数可以包括但不限于学习率以及训练周期时长等。在本发明实施例中，可以将模型训练关联数据集输入至待训练自助对话模型，从而根据待训练自助对话模型的输出结果，确定待训练自助对话模型的模型精度，进而确定待训练自助对话模型相较于当前自助对话模型的模型精度损失数据，进一步根据模型精度损失数据，对当前自助对话模型网络参数进行调整，得到重训练超参数，从而根据重训练超参数对待训练自助对话模型进行重训练，并判断重训练后的待训练自助对话模型是否满足模型综合部署指标，若重训练后的待训练自助对话模型达到模型综合部署指标，则将重训练后的待训练自助对话模型，作为待部署自助对话模型。在本发明的一个可选实施例中，在确定重训练超参数之后，还可以包括：若重训练后的待训练自助对话模型精度达不到当前自助对话模型的模型精度，则对重训练后的待训练自助对话模型进行优化训练，得到符合当前自助对话模型的模型精度的第一待训练自助对话模型；根据第一待训练自助对话模型以及模型综合部署指标，确定待部署自助对话模型。其中，第一待训练自助对话模型可以是对重训练后的待训练自助对话模型进行优化训练后，得到的符合当前自助对话模型的模型精度的模型。相应的，在待训练自助对话模型重训练后，将重训练后的待训练自助对话模型精度，与当前自助对话模型的模型精度进行对比，若重训练后的待训练自助对话模型精度达不到当前自助对话模型的模型精度，则对重训练后的待训练自助对话模型进行优化训练，得到符合当前自助对话模型的模型精度的第一待训练自助对话模型，从而基于模型综合部署指标，对第一待训练自助对话模型进行评估训练，得到待部署自助对话模型。在本发明的一个可选实施例中，根据第一待训练自助对话模型以及模型综合部署指标，确定待部署自助对话模型，可以包括：若第一待训练自助对话模型不符合模型综合部署指标，则对第一待训练自助对话模型中插入的低比特量化节点和/或稀疏化节点进行调整，得到符合模型综合部署指标的待部署自助对话模型。在本发明实施例中，可以评估第一待训练自助对话模型是否满足模型综合部署指标，若第一待训练自助对话模型达不到模型综合部署指标中的性能要求，则增加第一待训练自助对话模型中插入的低比特量化节点和/或稀疏化节点，若第一待训练自助对话模型达不到模型综合部署指标中的模型精度要求，则删除第一待训练自助对话模型中插入的部分低比特量化节点和/或稀疏化节点，最终得到符合模型综合部署指标的待部署自助对话模型。在本发明的一个可选实施例中，若重训练后的待训练自助对话模型精度达不到当前自助对话模型的模型精度，则对重训练后的待训练自助对话模型进行优化训练，得到符合当前自助对话模型的模型精度的第一待训练自助对话模型，可以包括：将当前自助对话模型作为自助对话教师模型；根据自助对话教师模型，对重训练后的待训练自助对话模型进行优化训练，得到符合当前自助对话模型的模型精度的第一待训练自助对话模型。其中，自助对话教师模型可以用于对重训练后的待训练自助对话模型进行优化训练。在本发明实施例中，可以将当前自助对话模型作为自助对话教师模型，并将重训练后的待训练自助对话模型作为学生模型，从而利用自助对话教师模型以及特征图损失函数，对重训练后的待训练自助对话模型进行优化训练，得到符合当前自助对话模型的模型精度的第一待训练自助对话模型。在本发明的一个可选实施例中，对待部署自助对话模型进行优化部署，可以包括：对待部署自助对话模型进行冗余节点消除、图融合以及张量稀疏化存储，得到目标待部署自助对话模型；在目标平台对目标待部署自助对话模型进行部署。其中，目标待部署自助对话模型可以是对待部署自助对话模型进行冗余节点消除以及图融合处理后得到的模型。张量稀疏化存储可以用于存储低比特量化节点和/或稀疏化节点相关数据。目标平台可以是部署目标待部署自助对话模型的平台。在本发明实施例中，可以对待部署自助对话模型进行冗余节点消除、图融合以及张量稀疏化存储，得到目标待部署自助对话模型，并根据部署需求确定目标平台，从而将目标待部署自助对话模型部署于目标平台。在一个具体的例子中，对话模型的优化部署方法的逻辑图如图3所示，首先加载当前自助对话模型，考虑到特定硬件的指令与存储带宽等硬件关联特征数据，对当前自助对话模型进行量化与稀疏重构，重构过程包括考虑性能计算图融合等特征，识别与分析节点插入位置，进行低比特量化节点以及稀疏化节点插入，并训练当前自助对话模型的网络参数，进而保存量化参数，并将重训练的模型导出，并进行模型优化以及模型业务部署，用于后续模型推理。具体的过程如下：随着算力提升以及大数据的规模越来越大，在自然语言处理有GPT3 175B的模型参数的规模在多模态的flamingo有80B，PaLM-E有560B的模型参数量。而且这些模型结构都是在transformer的模型架构基础上进行的建模。当前自助对话模型可以在transformer主体结构上进行模型量化与稀疏等轻量化操作。具体在post-training阶段对transformer类模型进行量化稀疏训练。获取当前自助对话模型网络参数，并加载当前自助对话模型，进一步获取模型训练关联数据集。对当前自助对话模型上的计算节点进行分析优化，依据硬件关联特征数据以及目标优化指标等，筛选出待量化与权重参数稀疏的计算节点的插入位置：a、针对的transformer的attention层与FFN层进行elementwise计算操作融合，在相关的QxX,KxX,VxX依据相关权重形状，进行Q,K,V融合等。b、对attention层与FFN层的矩阵乘部分进行低比特量化节点以及稀疏化节点的插入，其他计算节点也可进行低比特量化节点插入，针对矩阵乘部分的计算输入也进行相应的低比特量化节点插入。c、在模型压缩计算节点位置中待量化的位置插入低比特量化节点，并在模型压缩计算节点位置中待稀疏的权重位置插入稀疏化节点。低比特量化节点与稀疏化节点支持不同的量化与稀疏计算方法。稀疏化节点包括对模拟量化后的权重张量进行稀疏化，同时支持相应的权重张量同步进行稀疏与低比特量化。对低比特量化与稀疏重构后的待训练自助对话模型进行重训练，根据模型训练关联数据集，分析稀疏量化等轻量化操作对模型精度造成的损失，从而选取相应的训练超参包括学习率以训练周期等。a、对于低比特与稀疏造成误差较大的增大epoch以及调整增大学习率等进行训练超参数调整，消除由于低比特量化与稀疏引入的误差，达到与当前自助对话模型的模型精度。b、在a）后若达不到当前自助对话模型的模型精度，还可以将未经低比特量化与稀疏重构的当前自助对话模型作为自助对话教师模型，将重训练后的待训练自助对话模型作为学生网络，并基于相关的特征图损失函数对进行优化训练，从而降低低比特量化与稀疏引入的误差，达到与当前自助对话模型同等的模型精度。根据待训练自助对话模型是否能达到模型综合部署指标，调整在插入节点的数量。对于性能要求较高的，可以进行稀疏度更高的压缩或者更低比特的模型权重的量化；对于精度要求更高的场景，可以对相关量化或者稀疏计算更为敏感的不再进行相关量化操作，然后进行相关的训练。对于达到模型综合部署指标的待部署自助对话模型对应的计算图，导出在目标平台。对中导出的计算图进行优化。将优化后的计算图包含模型参数部分等保存，在目标平台进行模型部署。在芯片架构上进行当前自助对话模型的推理时可以充分利用硬件的算力，存储以及带宽等硬件资源。对于推理当前自助对话模型进行优化融合等，降低模型推理计算时的延时与能耗，从而提升模型部署整体效率。低比特量化以及模型参数稀疏化可以有效降低模型存储需求，并且可以更有效的利用芯片的算力同时降低芯片的能耗，从而有效提升芯片资源利用率，提升模型整体部署时的效率。低比特量化模型推理时由于低比特表示的动态范围与精度降低会引入模型部署较大的误差，再引入稀疏表示会更加限制模型表示引入更大的误差。通过少量数据集对深度学习模型进行重训练可以有效提升模型部署时的精度减少消除稀疏与量化引入的误差。此稀疏低比特量化感知训练优化部署方法适用于不同的深度学习训练框架包括pytorch，tensorflow，paddlepaddle等。此方法同时也适用于不同的深度学习网络部署平台。图4为本发明实施例二提供的一个未插入节点前的计算图，插入低比特量化节点以及稀疏化节点后如图5所示，最终部署在目标平台的模型的计算图如图6所示。图4中的W和X分别为矩阵乘运算的参数节点和输入节点，Y为输出节点。图5中W和X插入的低比特量化节点包括量化计算节点以及反量化计算节点。量化比特数以及量化算法，用户可以通过API选取。稀疏化节点会额外添加mask掩码对相应位置的权重执行置零操作。图6中的W’是完成待量化节点插入、稀疏化节点插入等处理后，达到模型部署精度以及性能要求处理后的参数节点，X’为完成待量化节点插入等处理后，满足模型部署精度以及性能要求处理后的输入节点，Y’为完成待量化节点插入以及满足模型部署精度以及性能要求的输出节点，X’和Y’满足低比特矩阵乘运算。可选的，低比特矩阵乘运算可以支持int8、fp8以及混合精度等，如W8X8、W4X8以及W4X16等。本发明实施例的技术方案，通过获取当前自助对话模型、当前自助对话模型网络参数以及模型训练关联数据集，从而根据当前自助对话模型的硬件关联特征数据以及目标优化指标，对当前自助对话模型的注意力机制层和前馈神经网络层进行计算图优化，并在注意力机制层的输入向量进行计算图优化，得到预处理自助对话模型，进而确定预处理自助对话模型的注意力机制层和前馈神经网络层中模型压缩计算节点位置，进一步根据模型压缩计算节点位置，对预处理自助对话模型进行模型压缩计算节点插入处理，得到待训练自助对话模型，并基于模型训练关联数据集以及模型综合部署指标，对待训练自助对话模型进行训练优化，得到符合模型综合部署指标的待部署自助对话模型。在本方案中，对当前自助对话模型的优化处理考虑了硬件关联特征数据，能够大幅度避免模型部署时存储与算力能耗不支持的问题，而通过对当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，不仅实现了模型优化还对模型进行了压缩，节省模型部署空间，降低算力需求，对待训练自助对话模型进行训练优化，使其达到最终的部署要求时再进行部署，解决了现有对话模型部署受限严重以及存储消耗大的问题，能够在低存储量消耗的前提下对模型快速高效部署。本发明实施例的技术方案中，所涉及用户个人信息的获取，存储和应用等，均符合相关法律法规的规定，且不违背公序良俗。实施例三图7为本发明实施例三提供的一种对话模型的优化部署装置的结构示意图。如图7所示，该装置包括：数据获取模块310、待训练自助对话模型确定模块320、待部署自助对话模型获取模块330以及优化部署模块340，其中，数据获取模块310，用于获取当前自助对话模型、当前自助对话模型网络参数以及模型训练关联数据集；待训练自助对话模型确定模块320，用于根据当前自助对话模型的硬件关联特征数据以及目标优化指标，对当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，得到待训练自助对话模型；待部署自助对话模型获取模块330，用于基于模型训练关联数据集以及模型综合部署指标，对待训练自助对话模型进行训练优化，得到符合模型综合部署指标的待部署自助对话模型；优化部署模块340，用于对待部署自助对话模型进行优化部署。本发明实施例的技术方案，通过获取当前自助对话模型、当前自助对话模型网络参数以及模型训练关联数据集，从而根据当前自助对话模型的硬件关联特征数据以及目标优化指标，对当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，得到待训练自助对话模型，进而基于模型训练关联数据集以及模型综合部署指标，对待训练自助对话模型进行训练优化，得到符合模型综合部署指标的待部署自助对话模型，最终对待部署自助对话模型进行优化部署。在本方案中，对当前自助对话模型的优化处理考虑了硬件关联特征数据，能够大幅度避免模型部署时存储与算力能耗不支持的问题，而通过对当前自助对话模型进行计算图优化以及模型压缩计算节点插入处理，实现对模型的存储与算力的压缩，节省模型部署空间，降低算力需求，对待训练自助对话模型进行训练优化，使其达到最终的部署要求时再进行部署，解决了现有对话模型部署受限严重以及存储消耗大的问题，能够在低存储量、低能耗的前提下对模型快速高效部署。可选的，待训练自助对话模型确定模块320，用于根据所述当前自助对话模型的硬件关联特征数据以及目标优化指标，对所述当前自助对话模型的注意力机制层和前馈神经网络层进行计算图优化，并在所述注意力机制层的输入向量进行计算图优化，得到预处理自助对话模型；确定所述预处理自助对话模型的注意力机制层和前馈神经网络层中模型压缩计算节点位置；根据所述模型压缩计算节点位置，对所述预处理自助对话模型进行模型压缩计算节点插入处理，得到所述待训练自助对话模型。可选的，待部署自助对话模型获取330包括第一待部署自助对话模型获取单元以及第二待部署自助对话模型获取单元，用于根据所述模型训练关联数据集，分析所述待训练自助对话模型的模型精度损失数据，并根据所述模型精度损失数据，确定重训练超参数；根据所述重训练超参数对所述待训练自助对话模型进行重训练；若重训练后的待训练自助对话模型达到所述模型综合部署指标，则将重训练后的所述待训练自助对话模型，作为所述待部署自助对话模型。可选的，第二待部署自助对话模型获取单元，包括第一待训练自助对话模型获取子单元以及第二待部署自助对话模型获取子单元，第一待训练自助对话模型获取子单元用于若重训练后的所述待训练自助对话模型精度达不到所述当前自助对话模型的模型精度，则对所述重训练后的待训练自助对话模型进行优化训练，得到符合所述当前自助对话模型的模型精度的第一待训练自助对话模型。第二待部署自助对话模型获取子单元，用于根据所述第一待训练自助对话模型以及所述模型综合部署指标，确定所述待部署自助对话模型。可选的，第二待部署自助对话模型获取子单元，用于若所述第一待训练自助对话模型不符合所述模型综合部署指标，则对所述第一待训练自助对话模型中插入的低比特量化节点和/或稀疏化节点进行调整，得到符合所述模型综合部署指标的待部署自助对话模型。可选的，第一待训练自助对话模型获取子单元，用于将所述当前自助对话模型作为自助对话教师模型；根据所述自助对话教师模型，对重训练后的所述待训练自助对话模型进行优化训练，得到符合所述当前自助对话模型的模型精度的第一待训练自助对话模型。可选的，优化部署模块340，用于对所述待部署自助对话模型进行冗余节点消除、图融合以及张量稀疏化存储，得到目标待部署自助对话模型；在目标平台对所述目标待部署自助对话模型进行部署。本发明实施例所提供的对话模型的优化部署装置可执行本发明任意实施例所提供的对话模型的优化部署方法，具备执行方法相应的功能模块和有益效果。实施例四图8示出了可以用来实施本发明的实施例的电子设备的结构示意图。电子设备旨在表示各种形式的数字计算机，诸如，膝上型计算机、台式计算机、工作台、个人数字助理、服务器、刀片式服务器、大型计算机、和其它适合的计算机。电子设备还可以表示各种形式的移动装置，诸如，个人数字处理、蜂窝电话、智能电话、可穿戴设备和其它类似的计算装置。本文所示的部件、它们的连接和关系、以及它们的功能仅仅作为示例，并且不意在限制本文中描述的和/或者要求的本发明的实现。如图8所示，电子设备10包括至少一个处理器11，以及与至少一个处理器11通信连接的存储器，如只读存储器12、随机访问存储器13等，其中，存储器存储有可被至少一个处理器执行的计算机程序，处理器11可以根据存储在只读存储器12中的计算机程序或者从存储单元18加载到随机访问存储器13中的计算机程序，来执行各种适当的动作和处理。在RAM 13中，还可存储电子设备10操作所需的各种程序和数据。处理器11、ROM 12以及RAM 13通过总线14彼此相连。输入/输出接口15也连接至总线14。电子设备10中的多个部件连接至I/O接口15，包括：输入单元16，例如键盘、鼠标等；输出单元17，例如各种类型的显示器、扬声器等；存储单元18，例如磁盘、光盘等；以及通信单元19，例如网卡、调制解调器、无线通信收发机等。通信单元19允许电子设备10通过诸如因特网的计算机网络和/或各种电信网络与其他设备交换信息/数据。处理器11可以是各种具有处理和计算能力的通用和/或专用处理组件。处理器11的一些示例包括但不限于中央处理单元、图形处理单元、各种专用的人工智能计算芯片、各种运行机器学习模型算法的处理器、数字信号处理器、以及任何适当的处理器、控制器、微控制器等。处理器11执行上文所描述的各个方法和处理，例如对话模型的优化部署方法。在一些实施例中，对话模型的优化部署方法可被实现为计算机程序，其被有形地包含于计算机可读存储介质，例如存储单元18。在一些实施例中，计算机程序的部分或者全部可以经由ROM 12和/或通信单元19而被载入和/或安装到电子设备10上。当计算机程序加载到RAM 13并由处理器11执行时，可以执行上文描述的对话模型的优化部署方法的一个或多个步骤。备选地，在其他实施例中，处理器11可以通过其他任何适当的方式而被配置为执行对话模型的优化部署方法。本文中以上描述的系统和技术的各种实施方式可以在数字电子电路系统、集成电路系统、场可编程门阵列、专用集成电路、专用标准产品、芯片上系统的系统、负载可编程逻辑设备、计算机硬件、固件、软件、和/或它们的组合中实现。这些各种实施方式可以包括：实施在一个或者多个计算机程序中，该一个或者多个计算机程序可在包括至少一个可编程处理器的可编程系统上执行和/或解释，该可编程处理器可以是专用或者通用可编程处理器，可以从存储系统、至少一个输入装置、和至少一个输出装置接收数据和指令，并且将数据和指令传输至该存储系统、该至少一个输入装置、和该至少一个输出装置。用于实施本发明的方法的计算机程序可以采用一个或多个编程语言的任何组合来编写。这些计算机程序可以提供给通用计算机、专用计算机或其他可编程数据处理装置的处理器，使得计算机程序当由处理器执行时使流程图和/或框图中所规定的功能/操作被实施。计算机程序可以完全在机器上执行、部分地在机器上执行，作为独立软件包部分地在机器上执行且部分地在远程机器上执行或完全在远程机器或服务器上执行。在本发明的上下文中，计算机可读存储介质可以是有形的介质，其可以包含或存储以供指令执行系统、装置或设备使用或与指令执行系统、装置或设备结合地使用的计算机程序。计算机可读存储介质可以包括但不限于电子的、磁性的、光学的、电磁的、红外的、或半导体系统、装置或设备，或者上述内容的任何合适组合。备选地，计算机可读存储介质可以是机器可读信号介质。机器可读存储介质的更具体示例会包括基于一个或多个线的电气连接、便携式计算机盘、硬盘、随机存取存储器、只读存储器、可擦除可编程只读存储器、光纤、便捷式紧凑盘只读存储器、光学储存设备、磁储存设备、或上述内容的任何合适组合。为了提供与用户的交互，可以在电子设备上实施此处描述的系统和技术，该电子设备具有：用于向用户显示信息的显示装置或者LCD监视器）；以及键盘和指向装置，用户可以通过该键盘和该指向装置来将输入提供给电子设备。其它种类的装置还可以用于提供与用户的交互；例如，提供给用户的反馈可以是任何形式的传感反馈；并且可以用任何形式来接收来自用户的输入。可以将此处描述的系统和技术实施在包括后台部件的计算系统、或者包括中间件部件的计算系统、或者包括前端部件的计算系统、或者包括这种后台部件、中间件部件、或者前端部件的任何组合的计算系统中。可以通过任何形式或者介质的数字数据通信来将系统的部件相互连接。通信网络的示例包括：局域网、广域网、区块链网络和互联网。计算系统可以包括客户端和服务器。客户端和服务器一般远离彼此并且通常通过通信网络进行交互。通过在相应的计算机上运行并且彼此具有客户端-服务器关系的计算机程序来产生客户端和服务器的关系。服务器可以是云服务器，又称为云计算服务器或云主机，是云计算服务体系中的一项主机产品，以解决了传统物理主机与VPS服务中，存在的管理难度大，业务扩展性弱的缺陷。应该理解，可以使用上面所示的各种形式的流程，重新排序、增加或删除步骤。例如，本发明中记载的各步骤可以并行地执行也可以顺序地执行也可以不同的次序执行，只要能够实现本发明的技术方案所期望的结果，本文在此不进行限制。上述具体实施方式，并不构成对本发明保护范围的限制。本领域技术人员应该明白的是，根据设计要求和其他因素，可以进行各种修改、组合、子组合和替代。任何在本发明的精神和原则之内所作的修改、等同替换和改进等，均应包含在本发明保护范围之内。
