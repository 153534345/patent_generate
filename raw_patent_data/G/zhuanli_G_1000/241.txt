标题title
策略网络训练方法及人形双足机器人步态控制方法
摘要abst
本发明涉及机器人自动化技术领域，提供一种策略网络训练方法及人形双足机器人步态控制方法，该训练方法采用深度强化学习方法得到目标策略网络，无需构建复杂的步态库，不需要增加额外的数据收集成本和计算代价，在不借助任何步态先验知识引导的情况下，通过引入包含有周期步态奖励、膝关节位置跟踪奖励和上肢关节位置跟踪奖励中的至少一个的步态奖励，可以使目标策略网络具有控制人形双足机器人实现自然、平稳、对称、协调的行走步态的功能，提升了目标策略网络的鲁棒性和抗干扰性。
权利要求书clms
1.一种策略网络训练方法，其特征在于，包括：获取人形双足机器人样本在当前时刻的样本信息，所述样本信息包括样本状态观测值以及样本速度命令；基于所述样本信息，应用价值网络，对策略网络进行深度强化学习，得到目标策略网络；其中，所述目标策略网络用于对行走过程中的人形双足机器人进行步态控制；所述深度强化学习中采用的奖励函数包括步态奖励，所述步态奖励包括周期步态奖励、膝关节位置跟踪奖励和上肢关节位置跟踪奖励中的至少一个。2.根据权利要求1所述的策略网络训练方法，其特征在于，所述样本信息还包括步态相位和额外观测值，所述额外观测值包括双脚的受力；所述样本状态观测值包括所述双脚的速度、双腿的膝关节位置以及双臂的肩关节位置；所述周期步态奖励基于步态周期内所述双脚的受力、速度以及所述步态相位确定；所述膝关节位置跟踪奖励基于所述双腿的膝关节位置以及膝关节参考位置确定；所述上肢关节位置跟踪奖励基于所述双臂的肩关节位置以及肩关节参考位置确定。3.根据权利要求2所述的策略网络训练方法，其特征在于，所述周期步态奖励的确定步骤包括：基于所述步态周期内各时刻的步态相位，分别计算所述双脚的受力对应的第一相位指示器函数的取值，以及所述双脚的速度对应的第二相位指示器函数的取值；基于所述第一相位指示器函数的取值，对所述双脚的受力进行调节，得到所述周期步态奖励中的受力奖励；基于所述第二相位指示器函数的取值，对所述双脚的速度进行调节，得到所述周期步态奖励中的速度奖励。4.根据权利要求2所述的策略网络训练方法，其特征在于，所述步态周期包括两个双脚支撑阶段和两个单脚支撑阶段；所述双腿的膝关节参考位置基于如下步骤确定：基于所述步态周期内各时刻的步态相位、单个所述双脚支撑阶段的时长在所述步态周期内的相位占比，以及支撑状态时的膝关节位置，计算所述双腿的膝关节参考位置。5.根据权利要求2所述的策略网络训练方法，其特征在于，所述步态周期包括两个双脚支撑阶段和两个单脚支撑阶段；所述双臂的肩关节参考位置基于如下步骤确定：基于所述步态周期内各时刻的步态相位，以及单个所述双脚支撑阶段的时长在所述步态周期内的相位占比，计算所述双臂的肩关节参考位置。6.根据权利要求1-5中任一项所述的策略网络训练方法，其特征在于，所述样本状态观测值具体包括所述人形双足机器人样本的关节位置、关节速度、机身姿态以及前一时刻的期望动作；所述基于所述样本信息，应用价值网络，对策略网络进行深度强化学习，得到目标策略网络，包括：将所述样本状态观测值以及所述样本速度命令作为输入，基于所述策略网络得到所述当前时刻的期望动作，并基于所述当前时刻的期望动作与环境交互，得到新的样本状态观测值以及综合奖励；将与环境交互前后的样本状态观测值作为输入，基于所述价值网络得到所述策略网络的价值；基于所述综合奖励以及所述价值，计算损失，并基于所述损失，对所述价值网络和所述策略网络的结构参数进行更新，得到所述目标策略网络。7.根据权利要求6所述的策略网络训练方法，其特征在于，所述样本信息还包括额外观测值，所述额外观测值包括双脚的受力以及地形高度；所述将与环境交互前后的样本状态观测值作为输入，基于所述价值网络得到所述策略网络的价值，包括：将与环境交互前后的样本状态观测值以及所述额外观测值作为输入，基于所述价值网络得到所述策略网络的价值。8.根据权利要求6所述的策略网络训练方法，其特征在于，还包括：在所述深度强化学习中执行如下至少一个步骤：在所述将所述样本状态观测值以及所述样本速度命令作为输入，基于所述策略网络得到所述当前时刻的期望动作之前，对所述人形双足机器人样本的状态空间进行加噪，并对所述人形双足机器人样本的动力学参数进行随机化处理；在所述基于所述当前时刻的期望动作与环境交互，得到新的样本状态观测值以及综合奖励之前，将所述环境进行随机化处理；对所述人形双足机器人样本的任意位置随机施加预设时长的推力。9.一种人形双足机器人步态控制方法，其特征在于，包括：获取目标人形双足机器人在当前时刻的目标信息，所述目标信息包括目标状态观测值以及目标速度命令；将所述目标状态观测值以及所述目标速度命令输入至目标策略网络，得到所述目标策略网络输出的所述当前时刻的预测动作；基于所述当前时刻的预测动作，对所述目标人形双足机器人进行步态控制；其中，所述目标策略网络基于如权利要求1-8中任一项所述的策略网络训练方法得到。10.一种策略网络训练装置，其特征在于，包括：样本信息获取模块，用于获取人形双足机器人样本在当前时刻的样本信息，所述样本信息包括样本状态观测值以及样本速度命令；强化学习模块，用于基于所述样本信息，应用价值网络，对策略网络进行深度强化学习，得到目标策略网络；其中，所述深度强化学习中采用的奖励函数包括步态奖励，所述步态奖励包括周期步态奖励、膝关节位置跟踪奖励和上肢关节位置跟踪奖励中的至少一个。11.一种人形双足机器人步态控制装置，其特征在于，包括：目标信息获取模块，用于获取目标人形双足机器人在当前时刻的目标信息，所述目标信息包括目标状态观测值以及目标速度命令；策略输出模块，用于将所述目标状态观测值以及所述目标速度命令输入至目标策略网络，得到所述目标策略网络输出的所述当前时刻的预测动作；步态控制模块，用于基于所述当前时刻的预测动作，对所述目标人形双足机器人进行步态控制；其中，所述目标策略网络基于如权利要求1-8中任一项所述的策略网络训练方法得到。12.一种电子设备，包括存储器、处理器及存储在所述存储器上并可在所述处理器上运行的计算机程序，其特征在于，所述处理器执行所述计算机程序时实现如权利要求1-8中任一项所述的策略网络训练方法，或如权利要求9所述的人形双足机器人步态控制方法。13.一种非暂态计算机可读存储介质，其上存储有计算机程序，其特征在于，所述计算机程序被处理器执行时实现如权利要求1-8中任一项所述的策略网络训练方法，或如权利要求9所述的人形双足机器人步态控制方法。
说明书desc
技术领域本发明涉及机器人自动化技术领域，尤其涉及一种策略网络训练方法及人形双足机器人步态控制方法。背景技术人形双足机器人与传统的履带式、轮式移动机器人相比，具有复杂的腿部关节结构，不仅可以获得更高的灵活度与适应性，而且能够实现爬楼梯、非平整地面等复杂路面情况下的正常行走。然而，由于非线性、高自由度、强耦合和多结构等特点，双足机器人步态控制算法的设计比较困难。传统基于模型的双足机器人步态控制算法需要依赖于复杂的动力学模型和数学工程，很难适应多变的环境，鲁棒性和抗干扰性较差，且需要事先建立环境动力学模型，还将引入额外的训练误差。基于深度强化学习的方法通常需要构建步态库，引入运动参考轨迹作为步态先验引导策略网络模仿学习生成相似步态，这增加了额外的数据收集成本和计算代价，且导致学习到的策略生成的步态鲁棒性和抗干扰性较差。发明内容本发明提供一种策略网络训练方法及人形双足机器人步态控制方法，用以解决现有技术中存在的缺陷。本发明提供一种策略网络训练方法，包括：获取人形双足机器人样本在当前时刻的样本信息，所述样本信息包括样本状态观测值以及样本速度命令；基于所述样本信息，应用价值网络，对策略网络进行深度强化学习，得到目标策略网络；其中，所述目标策略网络用于对行走过程中的人形双足机器人进行步态控制；所述深度强化学习中采用的奖励函数包括步态奖励，所述步态奖励包括周期步态奖励、膝关节位置跟踪奖励和上肢关节位置跟踪奖励中的至少一个。根据本发明提供的一种策略网络训练方法，所述样本信息还包括步态相位和额外观测值，所述额外观测值包括双脚的受力；所述样本状态观测值包括所述双脚的速度、双腿的膝关节位置以及双臂的肩关节位置；所述周期步态奖励基于步态周期内所述双脚的受力、速度以及所述步态相位确定；所述膝关节位置跟踪奖励基于所述双腿的膝关节位置以及膝关节参考位置确定；所述上肢关节位置跟踪奖励基于所述双臂的肩关节位置以及肩关节参考位置确定。根据本发明提供的一种策略网络训练方法，所述周期步态奖励的确定步骤包括：基于所述步态周期内各时刻的步态相位，分别计算所述双脚的受力对应的第一相位指示器函数的取值，以及所述双脚的速度对应的第二相位指示器函数的取值；基于所述第一相位指示器函数的取值，对所述双脚的受力进行调节，得到所述周期步态奖励中的受力奖励；基于所述第二相位指示器函数的取值，对所述双脚的速度进行调节，得到所述周期步态奖励中的速度奖励。根据本发明提供的一种策略网络训练方法，所述步态周期包括两个双脚支撑阶段和两个单脚支撑阶段；所述双腿的膝关节参考位置基于如下步骤确定：基于所述步态周期内各时刻的步态相位、单个所述双脚支撑阶段的时长在所述步态周期内的相位占比，以及支撑状态时的膝关节位置，计算所述双腿的膝关节参考位置。根据本发明提供的一种策略网络训练方法，所述步态周期包括两个双脚支撑阶段和两个单脚支撑阶段；所述双臂的肩关节参考位置基于如下步骤确定：基于所述步态周期内各时刻的步态相位，以及单个所述双脚支撑阶段的时长在所述步态周期内的相位占比，计算所述双臂的肩关节参考位置。根据本发明提供的一种策略网络训练方法，所述样本状态观测值具体包括所述人形双足机器人样本的关节位置、关节速度、机身姿态以及前一时刻的期望动作；所述基于所述样本信息，应用价值网络，对策略网络进行深度强化学习，得到目标策略网络，包括：将所述样本状态观测值以及所述样本速度命令作为输入，基于所述策略网络得到所述当前时刻的期望动作，并基于所述当前时刻的期望动作与环境交互，得到新的样本状态观测值以及综合奖励；将与环境交互前后的样本状态观测值作为输入，基于所述价值网络得到所述策略网络的价值；基于所述综合奖励以及所述价值，计算损失，并基于所述损失，对所述价值网络和所述策略网络的结构参数进行更新，得到所述目标策略网络。根据本发明提供的一种策略网络训练方法，所述样本信息还包括额外观测值，所述额外观测值包括双脚的受力以及地形高度；所述将与环境交互前后的样本状态观测值作为输入，基于所述价值网络得到所述策略网络的价值，包括：将与环境交互前后的样本状态观测值以及所述额外观测值作为输入，基于所述价值网络得到所述策略网络的价值。根据本发明提供的一种策略网络训练方法，还包括：在所述深度强化学习中执行如下至少一个步骤：在所述将所述样本状态观测值以及所述样本速度命令作为输入，基于所述策略网络得到所述当前时刻的期望动作之前，对所述人形双足机器人样本的状态空间进行加噪，并对所述人形双足机器人样本的动力学参数进行随机化处理；在所述基于所述当前时刻的期望动作与环境交互，得到新的样本状态观测值以及综合奖励之前，将所述环境进行随机化处理；对所述人形双足机器人样本的任意位置随机施加预设时长的推力。本发明还提供一种人形双足机器人步态控制方法，包括：获取目标人形双足机器人在当前时刻的目标信息，所述目标信息包括目标状态观测值以及目标速度命令；将所述目标状态观测值以及所述目标速度命令输入至目标策略网络，得到所述目标策略网络输出的所述当前时刻的预测动作；基于所述当前时刻的预测动作，对所述目标人形双足机器人进行步态控制；其中，所述目标策略网络基于上述的策略网络训练方法得到。本发明还提供一种策略网络训练装置，包括：样本信息获取模块，用于获取人形双足机器人样本在当前时刻的样本信息，所述样本信息包括样本状态观测值以及样本速度命令；强化学习模块，用于基于所述样本信息，应用价值网络，对策略网络进行深度强化学习，得到目标策略网络；其中，所述深度强化学习中采用的奖励函数包括步态奖励，所述步态奖励包括周期步态奖励、膝关节位置跟踪奖励和上肢关节位置跟踪奖励中的至少一个。本发明还提供一种人形双足机器人步态控制装置，包括：目标信息获取模块，用于获取目标人形双足机器人在当前时刻的目标信息，所述目标信息包括目标状态观测值以及目标速度命令；策略输出模块，用于将所述目标状态观测值以及所述目标速度命令输入至目标策略网络，得到所述目标策略网络输出的所述当前时刻的预测动作；步态控制模块，用于基于所述当前时刻的预测动作，对所述目标人形双足机器人进行步态控制；其中，所述目标策略网络基于上述的策略网络训练方法得到。本发明还提供一种电子设备，包括存储器、处理器及存储在存储器上并可在处理器上运行的计算机程序，所述处理器执行所述计算机程序时实现如上述任一种所述的策略网络训练方法，或人形双足机器人步态控制方法。本发明还提供一种非暂态计算机可读存储介质，其上存储有计算机程序，该计算机程序被处理器执行时实现如上述任一种所述的策略网络训练方法，或人形双足机器人步态控制方法。本发明还提供一种计算机程序产品，包括计算机程序，所述计算机程序被处理器执行时实现如上述任一种所述的策略网络训练方法，或人形双足机器人步态控制方法。本发明提供的策略网络训练方法及人形双足机器人步态控制方法，该训练方法首先获取人形双足机器人样本在当前时刻的样本信息，样本信息包括样本状态观测值以及样本速度命令；然后基于样本信息，应用价值网络，对策略网络进行深度强化学习，得到目标策略网络。该深度强化学习中采用的奖励函数包括步态奖励，所述步态奖励包括周期步态奖励、膝关节位置跟踪奖励和上肢关节位置跟踪奖励中的至少一个。该训练方法采用深度强化学习方法得到目标策略网络，无需构建复杂的步态库，不需要增加额外的数据收集成本和计算代价，在不借助任何步态先验知识引导的情况下，通过引入包含有周期步态奖励、膝关节位置跟踪奖励和上肢关节位置跟踪奖励中的至少一个的步态奖励，可以使目标策略网络具有控制人形双足机器人实现自然、平稳、对称、协调的行走步态的功能，提升了目标策略网络的鲁棒性和抗干扰性。附图说明为了更清楚地说明本发明或现有技术中的技术方案，下面将对实施例或现有技术描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。图1是本发明提供的策略网络训练方法的流程示意图；图2是本发明提供的策略网络训练方法中的深度强化学习架构的结构示意图；图3是本发明提供的策略网络训练方法中一个完整的步态周期的结构示意图；图4是本发明提供的策略网络训练方法中第一相位指示器函数随步态相位的变化曲线示意图；图5是本发明提供的策略网络训练方法中第二相位指示器函数随步态相位的变化曲线示意图；图6是本发明提供的人形双足机器人步态控制方法的流程示意图；图7是本发明提供的策略网络训练装置的结构示意图；图8是本发明提供的人形双足机器人步态控制装置的结构示意图；图9是本发明提供的电子设备的结构示意图。具体实施方式为使本发明的目的、技术方案和优点更加清楚，下面将结合本发明中的附图，对本发明中的技术方案进行清楚、完整地描述，显然，所描述的实施例是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有做出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。本发明的说明书和权利要求书中的术语“第一”、“第二”的特征可以明示或者隐含地包括一个或者更多个该特征。在发明的描述中，除非另有说明，“多个”的含义是两个或两个以上。此外，说明书以及权利要求中“和/或”表示所连接对象的至少其中之一，字符“/”，一般表示前后关联对象是一种“或”的关系。人形双足机器人与传统的履带式、轮式移动机器人相比，具有复杂的腿部关节结构，不仅可以获得更高的灵活度与适应性，而且能够实现爬楼梯、非平整地面等复杂路面情况下的正常行走。人形双足机器人具有与人类相似的外形特征，能更好地融入社会，在医疗保健、家庭陪护等服务业有着广泛的应用前景。人形双足机器人在实际应用过程中，步行的稳定性与步行能耗是其走向实际应用的关键。因此实现稳定的步态控制是人形双足机器人的研究重点。然而，由于非线性、高自由度、强耦合和多结构等特点，双足机器人步态控制系统的设计比较困难。传统基于模型的双足机器人步态控制算法，通常针对特定的步态运动，采用开环控制方法，通过预先设定的各关节位置轨迹，对各关节进行位置控制，从而实现运动。或者，利用零力矩点的方法进行双足机器人的步态控制。这些传统的方法依赖于复杂的动力学模型和数学工程，很难适应多变的环境，且鲁棒性和适应性较差。基于深度强化学习的方法属于无模型方法，相比于传统的基于模型的方法，不需要事先建立环境动力学模型，智能体直接与环境进行交互，获取经验样本优化策略网络，避免了引入额外的训练误差，学习到的控制策略更加准确，对环境的适应性更好。但是，这种方法通常需要构建步态库，引入运动参考轨迹作为步态先验引导策略网络模仿学习生成相似步态，这增加了额外的数据收集成本和计算代价，且导致学习到的策略生成的步态鲁棒性和抗干扰性较差。为此，本发明实施例中提供了一种用于对行走过程中的人形双足机器人进行步态控制的策略网络进行训练的策略网络训练方法以及应用训练得到的目标策略网络实现的人形双足机器人步态控制方法。图1为本发明实施例中提供的一种策略网络训练方法的流程示意图，如图1所示，该训练方法包括：S11，获取人形双足机器人样本在当前时刻的样本信息，所述样本信息包括样本状态观测值以及样本速度命令；S12，基于所述样本信息，应用价值网络，对策略网络进行深度强化学习，得到目标策略网络；其中，所述目标策略网络用于对行走过程中的人形双足机器人进行步态控制；所述深度强化学习中采用的奖励函数包括步态奖励，所述步态奖励包括周期步态奖励、膝关节位置跟踪奖励和上肢关节位置跟踪奖励中的至少一个。具体地，本发明实施例中提供的策略网络训练方法，其执行主体为策略网络训练装置，该装置可以配置于计算机内，该计算机可以为本地计算机或云计算机，本地计算机可以是电脑、平板等，此处不作具体限定。首先执行步骤S11，获取人形双足机器人样本在当前时刻的样本信息。该人形双足机器人样本可以是在物理仿真环境中构建的人形双足机器人模型，可以包括头部、四肢以及各关节等。该人形双足机器人样本作为智能体，可以执行相应的动作，例如行走、弯腰等。人形双足机器人样本的样本信息可以包括样本状态观测值以及样本速度命令，该样本状态观测值可以包括人形双足机器人样本的关节位置、关节速度、机身姿态以及前一时刻的期望动作。该样本状态观测值可以通过人形双足机器人样本的本体传感器样本获取，本体传感器样本可以通过仿真得到，可以包括关节编码器和惯性测量单元等。此处，关节位置可以包括人形双足机器人样本的每个关节的位置，例如双腿的膝关节位置以及双臂的肩关节位置等；关节速度可以包括人形双足机器人样本的每个关节的角速度，例如膝关节角速度、踝关节角速度。机身姿态可以包括人形双足机器人样本的整体姿态，均可以在物理仿真环境中记录得到。前一时刻的期望动作是由策略网络得到。样本速度命令是指当前时刻用户期望的机身速度信息，可以包括线速度指令和角速度指令，线速度命令是指相对于机身坐标系中各坐标轴的线速度，角速度命令是指相对于机身坐标系的转向速度。除此之外，样本信息还包括步态相位和额外观测值，该步态相位可以用于表征人形双足机器人样本的行走步态的相位，例如t时刻的步态相位可以表示为2π/Tt，T为步态周期。额外观测值可以包括双脚的受力，例如可以包括地面反作用力的大小，还可以包括地形高度等。然后执行步骤S12，利用样本信息，应用价值网络，对策略网络进行深度强化学习，得到目标策略网络。该深度强化学习的目标是学习按照速度命令进行稳定行走的性能。此处，深度强化学习将深度学习和强化学习相结合，既具有解决高维复杂问题的能力，也具有决策能力。采用演员-评论家的神经网络结构，该神经网络结构可以包括价值网络和策略网络。价值网络和策略网络中的各层均以全连接层的形式连接，隐藏层的激活函数可以均为指数线性函数，隐藏层的层数可以相同，例如可以均为3。由于价值网络的存在，可以解决策略网络收敛速度慢的问题。而且，在深度强化学习中，可以应用近端策略优化算法，探索连续的动作空间，使深度强化学习应用于双足机器人的步态控制问题。策略网络的输入可以包括样本状态观测值以及样本速度命令，除此之外，还可以包括步态相位。策略网络的输出可以包括当前时刻的期望动作。该期望动作可以是人形双足机器人样本的各关节的动作集合，用于人形双足机器人样本与环境交互。期望动作可以被定义为相对于参考关节位置/＞的相对变化量。其中，参考关节位置用于描述人形双足机器人样本默认站立时所对应的关节位置。因此，期望动作/＞和与环境交互后人形双足机器人样本的目标关节位置/＞的关系如下式所示：价值网络的输入可以与策略网络的输入相同，而且由于价值网络不需要部署在人形双足机器人，因此价值网络的输入也可以在策略网络的输入的基础上，增加额外观测值作为输入。价值网络的输出可以包括用于评价策略网络提供的期望动作的价值结果。此处，策略网络的输出维数与人形双足机器人样本的关节数目相等，价值网络的输出维数为1。深度强化学习的过程是通过智能体与环境交互学习的过程，采用的深度强化学习架构如图2所示。智能体选择一个动作后，会从环境中得到相应的状态和奖励。通过持续交互过程的学习，最终可以获得最优控制策略。智能体与环境之间的交互被建模为马尔可夫决策过程，MDP通常由一个五元组表示，其中S表示状态集合，A表示可执行的动作集合，P表示状态转移函数，表示智能体在状态/＞下采取行动/＞后转移到下一个状态/＞的概率。R表示奖励函数，表示智能体在状态/＞下采取行动/＞后转移到下一个状态/＞的立即奖赏。/＞是奖励的折扣系数。深度强化学习的目标是学习一个策略网络可以最大化有限时间T内的期望累积折扣奖励。本发明实施例中，在深度强化学习中采用的奖励函数可以包括步态奖励，步态奖励包括周期步态奖励、膝关节位置跟踪奖励和上肢关节位置跟踪奖励中的至少一个。由于人体在行走的过程中，其重心不断地周期性移动和改变，在任何时刻至少有一只脚与地面接触，而其中一段是两只脚同时着地。单脚支撑和双脚支撑交替进行，但只有单脚支撑和双脚支撑在行走周期中所占比例合理，才能保持身体平衡。因此，基于人体行走的特点，引入周期步态奖励，用于鼓励策略网络学习对称的平稳行走步态。如图3所示，一个完整的步态周期T可以包括2个双脚支撑阶段和2个单脚支撑阶段，即，为单个单脚支撑阶段的时长，/＞为单个双脚支撑阶段的时长。步态周期T按时间顺序可以依次包括左脚支撑阶段、第一双脚支撑阶段、右脚支撑阶段以及第二双脚支撑阶段。对应于DS1阶段，/＞对应于DD1阶段，/＞对应于DS2阶段，对应于DD2阶段。其中，DS1阶段和DD1阶段可以构成左脚着地阶段，DS2阶段和DD2可以构成右脚着地阶段，左脚着地阶段和右脚着地阶段均站步态周期T的50%。两个双脚支撑阶段共占步态周期的20％，两个单脚支撑阶段共占步态周期的80％。对于单腿来说，整个步态周期中只有一个摆动周期，占步态周期的40％。周期步态奖励可以在双脚支撑阶段，惩罚双脚的速度，而激励双脚的地面反作用力，在右脚支撑阶段，激励右脚的地面反作用力和左脚的速度，惩罚左脚的地面反作用力和右脚速度，在左脚支撑阶段，激励左脚的地面反作用力和右脚的速度，惩罚右脚的地面反作用力和左脚的速度，以帮助策略网络学习人形双足机器人样本在行走过程中的重心往复、双脚交替支撑运动。膝关节位置跟踪奖励可以鼓励策略网络学习人形双足机器人样本的双腿交替运动。此处，本发明实施例中可以利用周期正弦曲线拟合行走时双腿的膝关节位置变化。由于人体在行走过程中，双臂会配合双腿运动进行交替摆动。在双脚支撑阶段，双臂会收于身体两侧。在右脚支撑阶段，左腿往前摆动，右臂配合左腿运动先向前摆动然后在左脚最高抬脚时开始向后摆动，左臂配合左腿运动先向后摆动然后在左脚最高抬脚时开始向前摆动。在左脚支撑阶段，右腿往前摆动，左臂配合右腿运动先向前摆动然后在右脚最高抬脚时开始向后摆动，右臂配合右脚运动先向后摆动然后在右脚最高抬脚时开始向前摆动。为了实现人形双足机器人与人体相似的自然摆臂协调运动，可以引入上肢关节位置跟踪奖励，可以实现配合双腿运动的双臂交替摆动行为。通过深度强化学习得到的目标策略网络，可以用于对行走过程中的人形双足机器人进行步态控制。本发明实施例中提供的一种策略网络训练方法，该训练方法首先获取人形双足机器人样本在当前时刻的样本信息，样本信息包括样本状态观测值以及样本速度命令；然后基于样本信息，应用价值网络，对策略网络进行深度强化学习，得到目标策略网络。该深度强化学习中采用的奖励函数包括步态奖励，所述步态奖励包括周期步态奖励、膝关节位置跟踪奖励和上肢关节位置跟踪奖励中的至少一个。该训练方法采用深度强化学习方法得到目标策略网络，无需构建复杂的步态库，不需要增加额外的数据收集成本和计算代价，在不借助任何步态先验知识引导的情况下，通过引入包含有周期步态奖励、膝关节位置跟踪奖励和上肢关节位置跟踪奖励中的至少一个的步态奖励，可以使目标策略网络具有控制人形双足机器人实现自然、平稳、对称、协调的行走步态的功能，提升了目标策略网络的鲁棒性和抗干扰性。在上述实施例的基础上，所述样本信息还包括步态相位和额外观测值，所述额外观测值包括双脚的受力；所述样本状态观测值包括所述双脚的速度、双腿的膝关节位置以及双臂的肩关节位置；所述周期步态奖励基于步态周期内所述双脚的受力、速度以及所述步态相位确定；所述膝关节位置跟踪奖励基于所述双腿的膝关节位置以及膝关节参考位置确定；所述上肢关节位置跟踪奖励基于所述双臂的肩关节位置以及肩关节参考位置确定。具体地，样本信息还包括步态相位和额外观测值，额外观测值包括双脚的受力。样本状态观测值包括双脚的速度、双腿的膝关节位置以及双臂的肩关节位置。在此基础上，周期步态奖励可以借助于步态周期内双脚的受力、速度以及步态相位确定，通过步态相位调节双脚的受力和速度使其按人体行走时相同得到变化趋势进行周期性变化。同样地，膝关节位置跟踪奖励可以借助于双腿的膝关节位置以及对应的膝关节参考位置确定，以使双腿的膝关节位置以对应的膝关节参考位置为中心进行周期性运动。此处，双腿的膝关节位置可以通过基于步态相位表示的周期性正弦曲线进行拟合，双腿的膝关节参考位置可以通过步态相位进行调节。膝关节位置跟踪奖励可以通过如下公式进行表示：其中，表示膝关节位置跟踪奖励，/＞表示左腿的膝关节位置，/＞表示右腿的膝关节位置，/＞表示左腿的膝关节参考位置，/＞表示右腿的膝关节参考位置。上肢关节位置跟踪奖励可以借助于双臂的肩关节位置以及肩关节参考位置确定，以使双臂的肩关节位置以对应的肩关节参考位置为中心进行周期性运动。此处，双臂的肩关节位置可以通过基于步态相位表示的周期性正弦曲线进行拟合，双臂的肩关节参考位置可以通过步态相位进行调节。上肢关节位置跟踪奖励可以通过如下公式进行表示：其中，表示上肢关节位置跟踪奖励，/＞表示左臂的肩关节位置，表示右臂的肩关节位置，/＞表示左臂的肩关节参考位置，/＞表示右臂的肩关节参考位置。本发明实施例中，给出了周期步态奖励、膝关节位置跟踪奖励和上肢关节位置跟踪奖励的确定依据，可以保证各奖励的有效性。在上述实施例的基础上，所述周期步态奖励的确定步骤包括：基于所述步态周期内各时刻的步态相位，分别计算所述双脚的受力对应的第一相位指示器函数的取值，以及所述双脚的速度对应的第二相位指示器函数的取值；基于所述第一相位指示器函数的取值，对所述双脚的受力进行调节，得到所述周期步态奖励中的受力奖励；基于所述第二相位指示器函数的取值，对所述双脚的速度进行调节，得到所述周期步态奖励中的速度奖励。具体地，周期步态奖励可以包括受力奖励和速度奖励。在确定周期步态奖励时，可以先利用步态周期内各时刻的步态相位，分别计算双脚的受力对应的第一相位指示器函数的取值，以及双脚的速度对应的第二相位指示器函数的取值。第一相位指示器函数以及第二相位指示器函数均可以通过随步态周期内不同阶段的步态相位变化的分段函数进行表示。同时，为鼓励稳定学习，可以将分段函数在步态周期内相邻阶段之间的边界进行平滑过度处理。此处，分段函数的取值范围为。第一相位指示器函数随步态相位的变化曲线如图4所示，图4的横坐标为步态相位，可以用表示，纵坐标为双脚的地面反作用力对应的第一相位指示器函数，即/＞。其中，/＞，/＞可以为left和right，分别对应于左脚和右脚，即左脚对应的第一相位指示器函数为/＞，右脚对应的第一相位指示器函数为/＞。第二相位指示器函数随步态相位的变化曲线如图5所示，图5的横坐标为步态相位，纵坐标为双脚的速度对应的第二相位指示器函数/＞，即。其中，/＞，/＞可以为left和right，分别对应于左脚和右脚，即左脚对应的第二相位指示器函数为/＞，右脚对应的第二相位指示器函数为/＞。此后，利用第一相位指示器函数的取值，对双脚的受力进行调节，得到周期步态奖励中的受力奖励。受力奖励可以表示为：其中，为左脚的受力或左脚的受力归一化值，/＞为右脚的受力或右脚的受力归一化值。利用第二相位指示器函数的取值，对双脚的速度进行调节，得到周期步态奖励中的速度奖励。速度奖励可以表示为：其中，为左脚的速度或左脚的速度归一化值，/＞为右脚的速度或右脚的速度归一化值。本发明实施例中，从受力奖励和速度奖励两个方面对周期步态奖励进行表征，通过引入周期步态奖励更能体现人体行走过程中的周期性。在上述实施例的基础上，所述步态周期包括两个双脚支撑阶段和两个单脚支撑阶段；所述双腿的膝关节参考位置基于如下步骤确定：基于所述步态周期内各时刻的步态相位、单个所述双脚支撑阶段的时长在所述步态周期内的相位占比，以及支撑状态时的膝关节位置，计算所述双腿的膝关节参考位置。具体地，如图3所示，步态周期包括两个双脚支撑阶段和两个单脚支撑阶段。因此，在可以通过如下公式计算双腿的膝关节参考位置：其中，为右腿的膝关节参考位置，/＞为左腿的膝关节参考位置，为单个双脚支撑阶段的时长，/＞为单个双脚支撑阶段的时长在步态周期T内的相位占比，/＞为支撑状态时的膝关节位置。/＞为超参数，是可以根据需要进行设置的常数。由和/＞的公式可以看出，人形双足机器人样本在行走状态中膝关节最大活动位置为/＞，对应于摆动相中脚抬最高那一时刻的膝关节位置。膝关节最小活动位置为/＞，对应于支撑状态中的膝关节位置。/＞通过设置支撑状态时的膝关节位置，可以实现对人形双足机器人样本身高的精确控制。在上述实施例的基础上，所述步态周期包括两个双脚支撑阶段和两个单脚支撑阶段；所述双臂的肩关节参考位置基于如下步骤确定：基于所述步态周期内各时刻的步态相位，以及单个所述双脚支撑阶段的时长在所述步态周期内的相位占比，计算所述双臂的肩关节参考位置。具体地，由于步态周期包括SS1、SS2、DS1和DS2，而且双臂收于身体两侧时，肩关节位置对应0，肩关节往前运动时肩关节位置为负，往后摆动时肩关节位置为正。因此，人形双足机器人样本在步行前半周期，即时，左臂的肩关节参考位置/＞和右臂的肩关节参考位置/＞可以表示为：人形双足机器人样本在步行后半周期，左臂的肩关节参考位置和右臂的肩关节参考位置/＞可以表示为：在上述实施例的基础上，除周期步态奖励、膝关节位置跟踪奖励和上肢关节位置跟踪奖励外，深度强化学习中采用的步态奖励还可以包括朝向一致性约束奖励、间距约束奖励、终止奖励、惯性姿态奖励、机身运动奖励，这些奖励均用于帮助学习平稳对称协调的步态。朝向一致性约束奖励是指双足双膝双髋与机身朝向的一致性约束奖励，可以表示为：其中，为朝向一致性约束奖励，/＞、/＞、/＞、/＞分别表示人形双足机器人样本的机身坐标系、双足坐标系、双膝坐标系、双髋坐标系的四元数中的z取值。间距约束奖励是指双足双膝间距约束奖励，可以表示为：其中，和/＞分别表示预先设置的合适的双足间距值和双膝间距值，/＞和/＞可以相等，也可以不等。/＞和/＞分别表示双足间距观测值和双膝间距观测值。双足间距观测值和双膝间距观测值均可以通过样本状态观测值中的关节位置确定。惯性姿态奖励是指惯性测量单元对应的奖励，可以表示为：其中，表示投影重力矢量在x、y方向的分量。机身运动奖励是指人形双足机器人样本的机身的横滚和俯仰运动的奖励，可以表示为：其中，表示机身的滚转俯仰角速度。终止奖励用于惩罚立即终止一个轮次的任务的碰撞，可以表示为：此处，当人形双足机器人样本的上肢、躯干、骨盆或大腿与地面发生碰撞时，一个轮次的任务结束，终止奖励取值为-1。最大轮次可以根据需要进行设定，例如可以设定为不超过1000。在上述实施例的基础上，深度强化学习中采用的步态奖励还可以包括用于帮助学习低能耗步态的能耗奖励，可以表示为：其中，表示关节角速度，/＞表示关节力矩。在上述实施例的基础上，为了在部署使用时，策略网络生成的动作对硬件不造成破坏，深度强化学习中采用的步态奖励还可以包括用于帮助学习对硬件友好的硬件友好奖励，该硬件友好奖励可以包括关节角加速度奖励、关节力矩奖励、关节极限位置奖励、关节极限力矩奖励以及动作平滑奖励，以约束策略网络学习的动作尽量平滑且不超过极限物理运动范围。最终，步态奖励可以帮助学习平稳对称协调、低能耗、硬件友好的步态。关节角加速度奖励可以表示为：其中，表示关节角加速度。/＞关节力矩奖励可以表示为：其中，表示关节力矩，它由策略网络输出的期望动作与环境交互后的关节位置经PD控制器转化而来。关节极限位置奖励可以表示为：其中，表示关节位置，/＞和/＞表示关节位置的极限上下值。关节极限力矩奖励可以表示为：其中，表示关节力矩，/＞和/＞表示关节力矩的极限上下值。动作平滑奖励可以表示为：其中，、/＞、/＞分别表示t时刻、t-1时刻、t-2时刻策略网络输出的期望动作。为学习到按样本速度命令行走的性能，深度强化学习中采用的奖励函数除包括步态奖励外，还可以包括任务奖励，该任务奖励包括线速度跟踪奖励和角速度跟踪奖励。其中，样本速度命令可以包括线速度命令和角速度命令。线速度跟踪奖励可以表示为：其中，表示机身坐标系下人形双足机器人样本的机身在x、y方向的线速度观测值构成的向量，/＞表示机身坐标系下人形双足机器人样本的机身在x、y方向的线速度命令构成的向量。角速度跟踪奖励可以表示为：其中，表示机身坐标系下人形双足机器人样本的机身的角速度，/＞表示机身坐标系下人形双足机器人样本的机身的角速度命令。在确定上述各类奖励之后，可以将各类奖励进行加权求和，构成奖励函数，通过该奖励函数可以计算得到综合奖励。在上述实施例的基础上，所述样本状态观测值具体包括所述人形双足机器人样本的关节位置、关节速度、机身姿态以及前一时刻的期望动作；所述基于所述样本信息，应用价值网络，对策略网络进行深度强化学习，得到目标策略网络，包括：将所述样本状态观测值以及所述样本速度命令作为输入，基于所述策略网络得到所述当前时刻的期望动作，并基于所述当前时刻的期望动作与环境交互，得到新的样本状态观测值以及综合奖励；将与环境交互前后的样本状态观测值作为输入，基于所述价值网络得到所述策略网络的价值；基于所述综合奖励以及所述价值，计算损失，并基于所述损失，对所述价值网络和所述策略网络的结构参数进行更新，得到所述目标策略网络。具体地，在进行深度强化学习的过程中，可以先将样本状态观测值以及样本速度命令输入至策略网络，由策略网络得到当前时刻的期望动作。然后，利用当前时刻的期望动作与环境交互，得到新的样本状态观测值以及综合奖励。此后，将与环境交互前后的样本状态观测值输入至价值网络，由价值网络得到策略网络的价值。最后，应用综合奖励以及价值，计算损失。该损失可以包括价值网络对应的第一损失以及策略网络对应的第二损失。利用第一损失，可以对价值网络的结构参数进行更新，得到目标价值网络。利用第二损失，可以对策略网络的结构参数进行更新，得到目标策略网络。此后，将目标策略网络迁移到人形双足机器人上，即可控制人形双足机器人实现自然、平稳、对称、协调的行走步态。在上述实施例的基础上，所述样本信息还包括额外观测值，所述额外观测值包括双脚的受力以及地形高度等；所述将与环境交互前后的样本状态观测值作为输入，基于所述价值网络得到所述策略网络的价值，包括：将与环境交互前后的样本状态观测值以及所述额外观测值作为输入，基于所述价值网络得到所述策略网络的价值。具体地，由于价值网络不需要部署在人形双足机器人，因此，为了在不影响网络部署的前提下进一步提升价值网络的评价性能，可以引入额外观测值作为价值网络的输入。目前，将仿真环境中训练的策略移植到真机上，并使真机展现出同仿真环境中同样的优异性能，是基于深度强化学习的机器人运动控制面临的一大挑战。物理仿真器精度有限，无法对真实世界的物理特性精准建模。仿真环境中的机器人建模，如URDF模型，其动力学模型一般都是3D模型的简化，没有很高的精度，控制器的特性也很难在仿真中体现，由此就会导致仿真中训练的策略不能直接部署到真机上使用，导致仿真和真机间存在sim2real问题。在上述实施例的基础上，针对仿真和真机间存在的sim2real问题，本发明实施例中提供的策略网络训练方法还包括：在所述深度强化学习中执行如下至少一个步骤：在所述将所述样本状态观测值以及所述样本速度命令作为输入，基于所述策略网络得到所述当前时刻的期望动作之前，对所述人形双足机器人样本的状态空间进行加噪，并对所述人形双足机器人样本的动力学参数进行随机化处理；在所述基于所述当前时刻的期望动作与环境交互，得到新的样本状态观测值以及综合奖励之前，将所述环境进行随机化处理；对所述人形双足机器人样本的任意位置随机施加预设时长的推力。具体地，在应用样本状态观测值以及样本速度命令之前，为了减少实际部署使用时本体传感器带来的噪声影响，可以对人形双足机器人样本的状态空间进行加噪，即对样本状态观测值进行加噪。加噪可以通过加入随机噪声实现。同时，为了减少仿真中人形双足机器人样本的建模误差，可以对人形双足机器人样本的动力学参数进行随机化处理。该动力学参数可以包括人形双足机器人样本的连杆质量、连杆质心位置、关节摩擦系数和电机PD增益等参数。在与环境交互之前，为了训练出能应对各种不同复杂路面的鲁棒性行走步态，可以对环境进行随机化处理，例如可以随机化地面摩擦系数和恢复系数。为了进一步提高策略网络生成的行走步态的鲁棒性、平稳性和抗外力干扰性，还可以在深度强化学习中，可以每隔预设时间段对人形双足机器人样本的任意位置随机施加预设时长的推力。该预设时间段的长度以及预设时长均可以根据需要进行设定。该预设时长的量级可以是数秒。如图6所示，在上述实施例的基础上，本发明实施例中还提供了一种人形双足机器人步态控制方法，包括：S21，获取目标人形双足机器人在当前时刻的目标信息，所述目标信息包括目标状态观测值以及目标速度命令；S22，将所述目标状态观测值以及所述目标速度命令输入至目标策略网络，得到所述目标策略网络输出的所述当前时刻的预测动作；S23，基于所述当前时刻的预测动作，对所述目标人形双足机器人进行步态控制；其中，所述目标策略网络基于上述各实施例中提供的策略网络训练方法得到。具体地，本发明实施例中提供的人形双足机器人步态控制方法，其执行主体为人形双足机器人步态控制装置，该装置可以配置于人形双足机器人内。首先执行步骤S21，获取目标人形双足机器人在当前时刻的目标信息，目标信息包括目标状态观测值以及目标速度命令。该目标信息可以通过搭载在目标人形双足机器人上的本体传感器确定。目标信息的类型与上述样本信息的类型可以完全一致，区别仅在于信息来源不同，目标信息的来源是真机，即目标人形双足机器人，样本信息的来源是人机双足机器人样本。详见上述实施例，此处不再赘述。然后执行步骤S22，将目标状态观测值以及目标速度命令输入至目标策略网络，得到目标策略网络输出的当前时刻的预测动作。该预测动作可以是目标人形双足机器人的各关节的动作集合。最后执行步骤S23，可以将当前时刻的预测动作转换为控制指令，通过控制指令对目标人形双足机器人进行步态控制。可以理解的是，步骤S22采用的目标策略网络是通过上述各实施例中提供的策略网络训练方法训练得到。本发明实施例中提供的人形双足机器人步态控制方法，由于采用了目标策略网络，可以使目标人形双足机器人实现自然、平稳、对称、协调的行走步态。综上所述，本发明实施例中提供的策略网络训练方法及应用通过该策略网络训练得到的目标策略网络实现的人形双足机器人步态控制方法，针对人形双足机器人的行走步态稳定控制问题，利用基于演员-评论家的深度强化学习方法训练得到的目标策略网络，可以实现对人形双足机器人在各种复杂路面上行走步态的自主稳定控制。具体来说，训练方法中结合人体周期行走步态的特点，引入步态奖励，能帮助策略网络学习到双腿交替支撑、重心往复移动、双手协调双腿交替摆臂的自然平稳的行走步态。同时，针对仿真和真机存在的sim2real问题，且为了提高生成步态的鲁棒性和抗外力干扰性，在仿真训练时随机化环境和人形双足机器人样本的动力学参数、对策略网络输入的状态空间进行加噪、对人形双足机器人样本任意位置随机施加一个推力并持续作用数秒。该训练方法并未引入任何步态参考先验，利用纯学习的方案，通过设计步态奖励，并引入各种sim2real解决策略，可以使目标策略网络实现对协调平稳的行走步态的控制，且行走步态具有一定的鲁棒性和抗外力干扰性。如图7所示，在上述实施例的基础上，本发明实施例中提供了一种策略网络训练装置，包括：样本信息获取模块71，用于获取人形双足机器人样本在当前时刻的样本信息，所述样本信息包括样本状态观测值以及样本速度命令；强化学习模块72，用于基于所述样本信息，应用价值网络，对策略网络进行深度强化学习，得到目标策略网络；其中，所述深度强化学习中采用的奖励函数包括步态奖励，所述步态奖励包括周期步态奖励、膝关节位置跟踪奖励和上肢关节位置跟踪奖励中的至少一个。在上述实施例的基础上，本发明实施例中提供的策略网络训练装置，所述样本信息还包括步态相位和额外观测值，所述额外观测值包括双脚的受力；所述样本状态观测值包括所述双脚的速度、双腿的膝关节位置以及双臂的肩关节位置；所述周期步态奖励基于步态周期内所述双脚的受力、速度以及所述步态相位确定；所述膝关节位置跟踪奖励基于所述双腿的膝关节位置以及膝关节参考位置确定；所述上肢关节位置跟踪奖励基于所述双臂的肩关节位置以及肩关节参考位置确定。在上述实施例的基础上，本发明实施例中提供的策略网络训练装置，所述周期步态奖励的确定步骤包括：基于所述步态周期内各时刻的步态相位，分别计算所述双脚的受力对应的第一相位指示器函数的取值，以及所述双脚的速度对应的第二相位指示器函数的取值；基于所述第一相位指示器函数的取值，对所述双脚的受力进行调节，得到所述周期步态奖励中的受力奖励；基于所述第二相位指示器函数的取值，对所述双脚的速度进行调节，得到所述周期步态奖励中的速度奖励。在上述实施例的基础上，本发明实施例中提供的策略网络训练装置，所述步态周期包括两个双脚支撑阶段和两个单脚支撑阶段；所述双腿的膝关节参考位置基于如下步骤确定：基于所述步态周期内各时刻的步态相位、单个所述双脚支撑阶段的时长在所述步态周期内的相位占比，以及支撑状态时的膝关节位置，计算所述双腿的膝关节参考位置。在上述实施例的基础上，本发明实施例中提供的策略网络训练装置，所述步态周期包括两个双脚支撑阶段和两个单脚支撑阶段；所述双臂的肩关节参考位置基于如下步骤确定：基于所述步态周期内各时刻的步态相位，以及单个所述双脚支撑阶段的时长在所述步态周期内的相位占比，计算所述双臂的肩关节参考位置。在上述实施例的基础上，本发明实施例中提供的策略网络训练装置，所述样本状态观测值具体包括所述人形双足机器人样本的关节位置、关节速度、机身姿态以及前一时刻的期望动作；所述强化学习模块具体用于：将所述样本状态观测值以及所述样本速度命令作为输入，基于所述策略网络得到所述当前时刻的期望动作，并基于所述当前时刻的期望动作与环境交互，得到新的样本状态观测值以及综合奖励；将与环境交互前后的样本状态观测值作为输入，基于所述价值网络得到所述策略网络的价值；基于所述综合奖励以及所述价值，计算损失，并基于所述损失，对所述价值网络和所述策略网络的结构参数进行更新，得到所述目标策略网络。在上述实施例的基础上，本发明实施例中提供的策略网络训练装置，所述样本信息还包括额外观测值，所述额外观测值包括双脚的受力以及地形高度；所述强化学习模块还具体用于：将与环境交互前后的样本状态观测值以及所述额外观测值作为输入，基于所述价值网络得到所述策略网络的价值。在上述实施例的基础上，本发明实施例中提供的策略网络训练装置，所述强化学习模块还具体用于：在所述深度强化学习中执行如下至少一个步骤：在所述将所述样本状态观测值以及所述样本速度命令作为输入，基于所述策略网络得到所述当前时刻的期望动作之前，对所述人形双足机器人样本的状态空间进行加噪，并对所述人形双足机器人样本的动力学参数进行随机化处理；在所述基于所述当前时刻的期望动作与环境交互，得到新的样本状态观测值以及综合奖励之前，将所述环境进行随机化处理；对所述人形双足机器人样本的任意位置随机施加预设时长的推力。具体地，本发明实施例中提供的策略网络训练装置中各模块的作用与上述方法类实施例中各步骤的操作流程是一一对应的，实现的效果也是一致的，具体参见上述实施例，本发明实施例中对此不再赘述。如图8所示，在上述实施例的基础上，本发明实施例中提供了一种人形双足机器人步态控制装置，包括：目标信息获取模块81，用于获取目标人形双足机器人在当前时刻的目标信息，所述目标信息包括目标状态观测值以及目标速度命令；策略输出模块82，用于将所述目标状态观测值以及所述目标速度命令输入至目标策略网络，得到所述目标策略网络输出的所述当前时刻的预测动作；步态控制模块83，用于基于所述当前时刻的预测动作，对所述目标人形双足机器人进行步态控制；其中，所述目标策略网络基于上述各实施例中提供的策略网络训练方法得到。具体地，本发明实施例中提供的人形双足机器人步态控制装置中各模块的作用与上述方法类实施例中各步骤的操作流程是一一对应的，实现的效果也是一致的，具体参见上述实施例，本发明实施例中对此不再赘述。图9示例了一种电子设备的实体结构示意图，如图9所示，该电子设备可以包括：处理器910、通信接口920、存储器930和通信总线940，其中，处理器910，通信接口920，存储器930通过通信总线940完成相互间的通信。处理器910可以调用存储器930中的逻辑指令，以执行上述各实施例中提供的策略网络训练方法，或人形双足机器人步态控制方法。此外，上述的存储器930中的逻辑指令可以通过软件功能单元的形式实现并作为独立的产品销售或使用时，可以存储在一个计算机可读取存储介质中。基于这样的理解，本发明的技术方案本质上或者说对现有技术做出贡献的部分或者该技术方案的部分可以以软件产品的形式体现出来，该计算机软件产品存储在一个存储介质中，包括若干指令用以使得一台计算机设备执行本发明各个实施例所述方法的全部或部分步骤。而前述的存储介质包括：U盘、移动硬盘、只读存储器、随机存取存储器、磁碟或者光盘等各种可以存储程序代码的介质。另一方面，本发明还提供一种计算机程序产品，所述计算机程序产品包括计算机程序，计算机程序可存储在非暂态计算机可读存储介质上，所述计算机程序被处理器执行时，计算机能够执行上述各实施例中提供的策略网络训练方法，或人形双足机器人步态控制方法。又一方面，本发明还提供一种非暂态计算机可读存储介质，其上存储有计算机程序，该计算机程序被处理器执行时实现以执行上述各实施例中提供的策略网络训练方法，或人形双足机器人步态控制方法。以上所描述的装置实施例仅仅是示意性的，其中所述作为分离部件说明的单元可以是或者也可以不是物理上分开的，作为单元显示的部件可以是或者也可以不是物理单元，即可以位于一个地方，或者也可以分布到多个网络单元上。可以根据实际的需要选择其中的部分或者全部模块来实现本实施例方案的目的。本领域普通技术人员在不付出创造性的劳动的情况下，即可以理解并实施。通过以上的实施方式的描述，本领域的技术人员可以清楚地了解到各实施方式可借助软件加必需的通用硬件平台的方式来实现，当然也可以通过硬件。基于这样的理解，上述技术方案本质上或者说对现有技术做出贡献的部分可以以软件产品的形式体现出来，该计算机软件产品可以存储在计算机可读存储介质中，如ROM/RAM、磁碟、光盘等，包括若干指令用以使得一台计算机设备执行各个实施例或者实施例的某些部分所述的方法。最后应说明的是：以上实施例仅用以说明本发明的技术方案，而非对其限制；尽管参照前述实施例对本发明进行了详细的说明，本领域的普通技术人员应当理解：其依然可以对前述各实施例所记载的技术方案进行修改，或者对其中部分技术特征进行等同替换；而这些修改或者替换，并不使相应技术方案的本质脱离本发明各实施例技术方案的精神和范围。
