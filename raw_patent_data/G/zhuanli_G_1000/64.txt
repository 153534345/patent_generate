标题title
基于预训练语言模型的知识图谱补全方法
摘要abst
本发明公开了一种基于预训练语言模型的知识图谱补全方法，通过对现有知识图谱进行文件格式处理以及建立实体、关系的编号与名称的映射与格式化，获得适用于模型的知识三元组数据，然后使用预训练语言模型提取三元组中实体与关系的嵌入，进行训练学习。本发明设计了基于相对变换的对抗学习方法以及实体细粒度表示方法，能够显著提高模型的学习效率，进而提高知识图谱补全的效果。
权利要求书clms
1.一种基于预训练语言模型的知识图谱补全方法，其特征在于，包括：获取已知知识图谱数据，构造输入序列，构建预训练语言模型，用于提取输入序列的特征向量，根据特征向量的匹配分数进行知识图谱补全；其中，基于可学习对抗扰动因子矩阵确定对比损失函数，根据所述对比损失函数对预训练语言模型进行训练；所述对比损失函数为：；式中，N当前批次中的负样本个数、τ为温度系数、γ为固定正常数、β表示超参数，RTAMhrt表示给定正样本在对抗扰动因子矩阵中的标量值；表示给定负样本在对抗扰动因子矩阵中的标量值，/＞表示三元组评估函数。2.根据权利要求1所述的一种基于预训练语言模型的知识图谱补全方法，其特征在于，获取已知知识图谱数据后先进行预处理，包括文本标准化和关系标准化，所述文本标准化为去除知识图谱知识库中文本数据的存在的下划线字符；所述关系标准化为检查是否存在表面形式相同的关系被标准化为不同的形式。3.根据权利要求1所述的一种基于预训练语言模型的知识图谱补全方法，其特征在于，按如下格式构造输入序列：；式中，h表示头实体，r表示关系，t表示尾实体，htext，rtext以及ttext分别代表头实体的文本描述、关系的文本描述以及尾实体的文本描述；表示拼接操作，表示一个特殊的间隔符，Seqhr表示头实体-关系的输入序列，Seqt表示尾实体的输入序列。4.根据权利要求3所述的一种基于预训练语言模型的知识图谱补全方法，其特征在于，所述预训练语言模型包括BERThr、BERTt和BERTA；提取输入序列的特征向量的步骤包括：将Seqhr输入到BERThr中得到向量ehr，将Seqt输入到BERTt得到向量et，以及将Seqt输入到BERTA中得到向量Aet。5.根据权利要求1所述的一种基于预训练语言模型的知识图谱补全方法，其特征在于，对已知知识图谱数据进行负采样；以使用对比学习范式进行头实体-关系对以及尾实体的特征学习，负采样策略包括：批内负采样，使用批次内其他尾实体替换当前输入至模型的训练样本中的尾实体；批前负采样，使用上一轮训练得到的头实体-关系特征向量以及尾实体特征向量作为当前批次训练样本的负样本；自负负采样，使用头实体替换当前三元组中的尾实体。6.根据权利要求1所述的一种基于预训练语言模型的知识图谱补全方法，其特征在于，可学习对抗扰动因子矩阵的获得过程包括：计算每组查询与其他所有候选尾实体的距离，得到相对变换矩阵；将相对变换矩阵输入至多层感知机中，得到注意力权重；将相对变换矩阵与注意力权重相乘，得到对抗扰动因子矩阵。7.根据权利要求6所述的一种基于预训练语言模型的知识图谱补全方法，其特征在于，计算相对变换矩阵时，采用的距离计算公式为余弦距离，；其中，n为当前批训练的批次大小，。8.根据权利要求4所述的一种基于预训练语言模型的知识图谱补全方法，其特征在于，基于实体表征对抗学习方法，学习细粒度的实体表征特征，过程包括：构建对抗学习虚拟样本库；获取向量Aet，使其与样本库进行交互，用已更新样本库中的正负样本，更新步骤为：；式中，ki表示样本库中的负样本的更新策略，ki+表示样本库中正样本的更新策略，i+表示正样本的选取策略，η为内存库中正负样本更新的学习率，为内存库样本更新的温度系数，p表示相对于当前的Aet，当前样本库中的样本为正样本的概率，K为样本库的大小。9.根据权利要求8所述的一种基于预训练语言模型的知识图谱补全方法，其特征在于，利用基于可学习对抗扰动因子矩阵确定的损失函数与基于实体表征对抗学习方法的损失函数，共同对预训练语言模型进行训练，包括；式中，LTMT表示基于可学习对抗扰动因子矩阵确定的损失函数，LAler表示基于实体表征对抗学习方法的损失函数。10.根据权利要求9所述的一种基于预训练语言模型的知识图谱补全方法，其特征在于，基于实体表征对抗学习方法的损失函数，表达式为：。
说明书desc
技术领域本发明涉及知识图谱补全技术领域，更具体的说是涉及一种基于预训练语言模型的知识图谱补全方法。背景技术知识图谱补全是一个富有挑战性的领域，主要体现在知识图谱数据的海量性，即包含大量的实体以及复杂的网络结构。因此对知识图谱补全模型进行建模是一个复杂的工程。随着深度学习和预训练语言模型的发展，知识图谱补全出现了一个全新的方向，即基于文本的知识图谱补全模型。该模型主要利用预训练语言模型来对知识图谱补全任务进行建模，获得了较好的效果。然而，基于文本的知识图谱补全任务大多仅关注利用语言模型对三元组匹配任务进行建模，而忽略了海量实体的表征信息，使得模型的训练信息来源不够充分和丰富；同时模型在对比学习任务中存在“捷径”问题。发明内容有鉴于此，为了至少部分解决上述技术问题，本发明利用对抗学习提供了一种基于预训练语言模型的知识图谱补全方法，目的在于引入基于注意力机制的相对变换方法，缓解了对比学习中模型出现的“捷径”问题，使模型的训练更具有稳定性。为了实现上述目的，本发明采用如下技术方案：一种基于预训练语言模型的知识图谱补全方法，包括：获取已知知识图谱数据，构造输入序列，构建预训练语言模型，用于提取输入序列的特征向量，根据特征向量的匹配分数进行知识图谱补全；其中，基于可学习对抗扰动因子矩阵确定对比损失函数，根据所述对比损失函数对预训练语言模型进行训练；所述对比损失函数为：式中，N当前批次中的负样本个数、τ为温度系数、γ为固定正常数、β表示超参数，RTAMhrt表示给定正样本在对抗扰动因子矩阵中的标量值；表示给定负样本在对抗扰动因子矩阵中的标量值，/＞表示三元组评估函数。优选的，三元组评估函数采用余弦相似度；即。作为优选，获取已知知识图谱数据后先进行预处理，包括文本标准化和关系标准化，所述文本标准化为去除知识图谱知识库中文本数据的存在的下划线字符；所述关系标准化为检查是否存在表面形式相同的关系被标准化为不同的形式。作为优选，按如下格式构造输入序列：式中，h表示头实体，r表示关系，t表示尾实体，htext，rtext以及ttext分别代表头实体的文本描述、关系的文本描述以及尾实体的文本描述；表示拼接操作，表示一个特殊的间隔符，Seqhr表示头实体-关系的输入序列，Seqt表示尾实体的输入序列。作为优选，所述预训练语言模型包括BERThr、BERTt和BERTA；提取输入序列的特征向量的步骤包括：将Seqhr输入到BERThr中得到向量ehr，将Seqt输入到BERTt得到向量et，以及将Seqt输入到BERTA中得到向量Aet。优选的，构建预训练BERT模型，用于对BERThr、BERTt和BERTA使用相同的权重进行初始化。优选的，各向量的表达式为：其中，mean_pool代表着平均池化操作，normalize代表着标准化操作。作为优选，对已知知识图谱数据进行负采样；以使用对比学习范式进行头实体-关系对以及尾实体的特征学习，所述负采样策略包括：批内负采样，使用批次内其他尾实体替换当前输入至模型的训练样本中的尾实体；批前负采样，使用上一轮训练得到的头实体-关系特征向量以及尾实体特征向量作为当前批次训练样本的负样本；自负负采样，使用头实体替换当前三元组中的尾实体。作为优选，可学习对抗扰动因子矩阵的获得过程包括：计算每组查询与其他所有候选尾实体的距离，得到相对变换矩阵；将相对变换矩阵输入至多层感知机中，得到注意力权重；将相对变换矩阵与注意力权重相乘，得到对抗扰动因子矩阵。作为优选，计算相对变换矩阵时，采用的距离计算公式为余弦距离，其中，n为当前批训练的批次大小，。作为优选，基于实体表征对抗学习方法，学习细粒度的实体表征特征，过程包括：构建对抗学习虚拟样本库；获取向量Aet，使其与样本库进行交互，用已更新样本库中的正负样本，更新步骤为：式中，ki表示样本库中的负样本的更新策略，ki+表示样本库中正样本的更新策略，i+表示正样本的选取策略，η为内存库中正负样本更新的学习率，为内存库样本更新的温度系数，p表示相对于当前的Aet，当前样本库中的样本为正样本的概率，K为样本库的大小。作为优选，利用基于可学习对抗扰动因子矩阵确定的损失函数与基于实体表征对抗学习方法的损失函数，共同对预训练语言模型进行训练，包括：作为优选，基于实体表征对抗学习方法的损失函数，表达式为：。经由上述的技术方案可知，与现有技术相比，本发明公开提供了一种基于预训练语言模型的知识图谱补全方法，通过引入基于注意力机制的相对变换方法利用样本之间的关系，得到可学习的对抗扰动因子矩阵，以缓解原始InfoNCE训练模型过程中出现的“捷径学习”问题，进而提升知识图谱补全模型训练的效率和准确性。本发明的另一有益效果在于，引入了细粒度的实体表征对抗学习模块，进一步利用了知识图谱中丰富的实体个体的表征信息，学习了细粒度的实体表示特征，从而优化了知识图谱中实体的表示空间，进而提升了模型训练过程的学习效率以及知识图谱补全的准确性和效率。本发明提供的方法具有准确率高，消耗资源小的优点。本发明的再一有益效果在于，基于注意力机制的相对变换方法以及Aler模块的组合使用，经实验表明可以使得原有的知识图谱补全模型在更少的计算资源的情况下达到与多计算资源的情况下的性能，提升了计算设备的计算效率，具有减碳节能的实际效益。附图说明为了更清楚地说明本发明实施例或现有技术中的技术方案，下面将对实施例或现有技术描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本发明的实施例，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据提供的附图获得其他的附图。图1为本发明提供的基于预训练语言模型的知识图谱补全方法流程图；图2为本发明提供的预训练语言模型的训练流程示例图。具体实施方式下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例仅仅是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有做出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。实施例一本发明实施例基于对抗学习与预训练语言模型公开了一种基于预训练语言模型的知识图谱补全方法，需要说明的是，本申请公开的基于预训练语言模型的知识图谱补全方法，可应用于针灸知识图谱的补全，也可应用于其他领域中知识图谱的补全。其中，补全方法步骤如图1所示，包括：获取已知知识图谱数据，构造输入序列，构建预训练语言模型，用于提取输入序列的特征向量，根据特征向量的匹配分数进行知识图谱补全；其中，基于可学习对抗扰动因子矩阵确定对比损失函数，根据所述对比损失函数对预训练语言模型进行训练；所述对比损失函数为：式中，N当前批次中的负样本个数、τ为温度系数、γ为固定正常数、β表示超参数，RTAMhrt表示给定正样本在对抗扰动因子矩阵中的标量值；表示给定负样本在对抗扰动因子矩阵中的标量值，/＞表示三元组评估函数。本实施例中，三元组评估函数采用余弦相似度；即。下面对各步执行过程进行详细说明。第一步、获取已知知识图谱数据，构造输入序列；本实施例中，获取已知知识图谱数据后先进行预处理，预处理步骤包括文本标准化和关系标准化；其中，文本标准化指去除知识图谱知识库中文本数据的存在的下划线字符；关系标准化指检查是否存在表面形式相同的关系被标准化为不同的形式。然后构造输入序列，本申请中，序列构造是指将知识图谱中对应实体、关系的文本描述与当前实体以及关系进行拼接，组成期望的输入序列；本申请中，知识图谱由三元组构成，以表示，其中h表示头实体，r表示关系，t表示尾实体。知识图谱中对于这三个元素还会有对应的文本描述，分别是htext，rtext以及ttext，分别代表头实体的文本描述、关系的文本描述以及尾实体的文本描述。假设当前知识图谱三元组是关于，那么对应的h就是穴位名，htext就是穴位名的文本描述，r就是功效，t就是穴位的功效，一般为较简短文字，ttext就是穴位具体功效的文本描述，一般为较长文字。作为示例性的实施方式，输入序列的构造格式有两种，分别为：其中,表示拼接操作，表示一个特殊的间隔符，Seqhr表示头实体-关系的输入序列，Seqt表示尾实体的输入序列。第二步、构建预训练语言模型，用于提取输入序列的特征向量，本发明中，使用预训练语言模型构建知识图谱三元组的编码器模块，包括头实体-关系编码器BERThr、尾实体编码器BERTt和辅助实体编码器BERTA；作为优选，构建预训练BERT模型，对BERThr、BERTt和BERTA使用相同的权重进行初始化；其中BERThr不与BERTt、BERTA共享后续训练更新得到的权重。本实施例中，利用构建的预训练语言模型分别对头实体-关系对以及尾实体进行编码，得到各自的特征向量，具体步骤为：将得到的Seqhr输入到BERThr中经由池化层并标准化得到头实体-关系编码向量ehr，将得到的Seqt输入到BERTt以及BERTA中经由池化层并标准化分别得到尾实体编码向量et以及辅助实体编码向量Aet，作为优选，对编码器模块中得到的编码向量进行平均池化；即其中，mean_pool代表着平均池化操作，normalize代表着标准化操作。一种实施例中，采用对比学习范式对预训练语言模型进行训练；如图2所示，包括：1）对已知知识图谱数据进行负采样；具体地，采用三种负采样策略即批内负采样、批前负采样以及自负负采样进行负样本采样，用于基于对比学习范式，进行头实体-关系对以及尾实体的特征学习；其中，批内负采样，即当前输入至模型的训练样本中，以作为正样本，使用批次内其他尾实体替换掉中的t，得到负样本/＞，即可能的一个批内负采样的样例为：；批前负采样，即使用上一轮训练得到的头实体-关系特征向量以及尾实体特征向量作为当前批次训练样本的负样本，即可能的一个批前负采样的样例为：，实际上这是一个正确的三元组，只不过在我们的训练阶段中，组成的序列编码得到的ehr和组成的序列编码得到的et处于不同的训练时序中，所以也视作为负样本；自负负采样，即对于当前的正样本，使用头实体来替换当前三元组中的尾实体，得到负样本。即可能的一个自负负采样的样例为：。2）传统的对比文件范式采用InfoNCE损失函数作为训练的损失函数，具体为：现有研究表明，传统InfoNCE损失函数会对模型的训练出现“捷径学习”的问题，因此，本发明使用基于注意力的相对变换方法缓解InfoNCE损失函数中的“捷径学习”。具体步骤包括：计算每组查询与其他所有候选尾实体的距离，得到相对变换矩阵；将输入至多层感知机中，得到注意力权重/＞；将与/＞相乘，得到对抗扰动因子矩阵/＞，即基于注意力机制的相对变换矩阵，具体计算方法为：其中，CD为距离函数，MLP是多层感知机，n为当前批训练的批次大小；作为优选方案，计算相对变换矩阵时，采用的距离函数为余弦距离，即其中，；最终，融入基于注意力机制的相对变换方法得到的可学习对抗扰动因子矩阵确定的对比损失函数为：式中，N当前批次中的负样本个数、τ为温度系数、γ为固定正常数、β表示超参数，RTAMhrt表示给定正样本在对抗扰动因子矩阵中的标量值；表示给定负样本在对抗扰动因子矩阵中的标量值，/＞表示三元组评估函数。本实施例中，三元组评估函数采用余弦相似度；即/＞。本实施例中，设定n=128，γ=0.2，β=0.5。3）为辅助学习任务提高三元组补全的效果，本申请进一步提出使用使用Aler方法基于对抗学习的实体表征细粒度学习模块进一步优化实体的表示空间，用于得到实体表征细粒度学习的损失；具体地，基于实体表征对抗学习方法学习细粒度的实体表征特征的过程包括：构建对抗学习虚拟样本库；作为基于对抗学习的实体表征细粒度学习模块的样本库，使用向量Aet，使其与样本库进行交互，得到反馈值用以更新样本库中的正负样本，在训练过程中同步迭代更新以及内存库的样本的表征信息，更新步骤为：式中，ki表示样本库中的负样本的更新策略，ki+表示样本库中正样本的更新策略，i+表示正样本的选取策略，η为内存库中正负样本更新的学习率，为内存库样本更新的温度系数，p表示相对于当前的Aet，当前样本库中的样本为正样本的概率，K为样本库的大小。一种实施例中，K=65536，=0.04。本申请最终所得实体表征细粒度学习的损失函数为：进一步，为了使基于对抗学习的实体表征细粒度学习模块能够应用到其他模块训练时得到的三元组结构化信息，将BERTt中的部分参数分享给BERTA，使得BERTA具有三元组结构化信息感知的能力。由于Aler中使用到的BERTA需要获得BERTt中分享的部分参数，根据链式法则在最终梯度反向传播的时候会相应的根据Aler模块的损失函数隐式更新其他相关的模块；具体地，BERThr仅从三元组匹配训练模块中得到更新，BERTt从两个损失函数项中得到更新，BERTA从BERTt和Aler的损失中得到更新；在每一轮损失回传之前，进行BERTt-＞BERTA的参数共享，回传之后进行L_Aler的反向传播参数更新；且所有参数以一个百分比的超参数进行共享，即param_BERTA = gamma * param_BERTA +  * param_BERTt。本申请引入Aler模块，进一步利用了知识图谱中丰富的实体个体的表征信息，学习了细粒度的实体表示特征，从而优化了知识图谱中实体的表示空间，进而提升了知识图谱补全的准确性和效率。一种优选的实施方式中，将上述涉及到的两种损失值进行加权相加，得到模型训练的最终损失函数，并利用该损失函数对预训练语言模型进行训练。其中，θ表示权重系数，具体地，选用θ=1.4作为最佳实施例。第三步、根据特征向量的匹配分数进行知识图谱补全；经标准化操作得到的特征向量的匹配分数计算公式可得到简化。简化后的匹配分数计算公式则为向量间的点积：其中，ex和ey分别代表两个嵌入向量，和/＞分别代表这两个嵌入向量的模长。由于从编码器中得到的向量经由normalize处理，所以，计算匹配分数的计算公式可以简化为：即给定查询时，模型预测的答案可以表示为：其中，ɛ为候选实体全集。实施例二本实施例提供一种计算机可读存储介质，存储介质可以是ROM、RAM、磁盘、光盘等储存介质，该存储介质存储有一个或多个程序，所述程序被处理器执行时，实现实施例一公开的基于预训练语言模型的知识图谱补全方法。实施例三本实施例提供一种计算设备，所述的计算设备可以是台式电脑、笔记本电脑、智能手机、PDA手持终端、平板电脑或其他具有显示功能的终端设备，该计算设备包括该计算设备包括处理器和存储器，存储器存储有一个或多个程序，处理器执行存储器存储的程序时，实现实施例一中的基于预训练语言模型的知识图谱补全方法。本说明书中各个实施例采用递进的方式描述，每个实施例重点说明的都是与其他实施例的不同之处，各个实施例之间相同相似部分互相参见即可。对于实施例公开的装置而言，由于其与实施例公开的方法相对应，所以描述的比较简单，相关之处参见方法部分说明即可。对所公开的实施例的上述说明，使本领域专业技术人员能够实现或使用本发明。对这些实施例的多种修改对本领域的专业技术人员来说将是显而易见的，本文中所定义的一般原理可以在不脱离本发明的精神或范围的情况下，在其它实施例中实现。因此，本发明将不会被限制于本文所示的这些实施例，而是要符合与本文所公开的原理和新颖特点相一致的最宽的范围。
