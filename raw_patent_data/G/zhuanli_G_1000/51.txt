标题title
一种基于知识保护及选择的主题文本生成方法
摘要abst
本发明涉及自然语言主题文本生成技术领域，提供一种基于知识保护及选择的主题文本生成方法。包括：采集待生成的主题文本数据；引入预训练的语言模型，冻结语言模型参数并构造可训练的动态前缀向量；通过语言模型编码器编码训练集，获得隐藏中间状态，作为解码器的初始状态进行解码，获得训练集对应的词表概率分布；通过包含知识选择的拷贝机制，计算获得解码器的解码状态与主题文本数据的相似度及未表达主题对应的拷贝概率分布；计算预测结果分布，计算获得负对数似然损失；更新动态前缀向量后，生成主题文本。本发明通过带有知识选择的拷贝机制同时考虑输入的主题信息与已生成文本中未表达的主题语义，保持了较高的主题一致性。
权利要求书clms
1.一种基于知识保护及选择的主题文本生成方法，其特征在于，包括：S1：采集待生成的主题文本数据，将所述主题文本数据划分为训练集与测试集；S2：引入预训练的第一语言模型，冻结所述第一语言模型参数并构造可训练的动态前缀向量，获得第二语言模型；S3：通过所述第二语言模型编码所述训练集，获得隐藏中间状态，将所述隐藏中间状态记为所述第二语言模型的解码器的初始状态，所述解码器根据所述初始状态解码，获得训练集对应的词表概率分布；S4：通过包含知识选择的拷贝机制，计算获得所述解码器的解码状态与所述主题文本数据的相似度及未表达主题对应的拷贝概率分布；S5：根据所述词表概率分布及所述拷贝概率分布计算获得预测结果分布，根据所述预测结果分布计算获得负对数似然损失；S6：以所述负对数似然损失更新所述第二语言模型中的所述动态前缀向量，获得第三语言模型；S7：基于束搜索策略，通过所述第三语言模型对所述测试集进行预测，生成主题文本。2.根据权利要求1所述的一种基于知识保护及选择的主题文本生成方法，其特征在于，步骤S2中的所述动态前缀向量的长度与输入所述第二语言模型的主题词的数量离散线性正相关。3.根据权利要求1所述的一种基于知识保护及选择的主题文本生成方法，其特征在于，步骤S4中，所述相似度的表达式为：其中，为解码器的解码状态与所述主题文本数据的相似度，/＞为当前采样时间步，为余弦相似度计算，/＞为编码器状态，/＞为解码器状态，/＞为选择辅助函数，/＞为选择辅助函数阈值，/＞为内容待过滤及选择的张量。4.根据权利要求3所述的一种基于知识保护及选择的主题文本生成方法，其特征在于，步骤S4中，所述拷贝机制中包含的知识选择过程的表达式为：其中，为知识选择后的第一状态信息，/＞为知识选择后的第二状态信息，/＞为归一化函数。5.根据权利要求4所述的一种基于知识保护及选择的主题文本生成方法，其特征在于，步骤S4中，所述拷贝概率分布的表达式为：其中，为拷贝概率分布中产生的当前时间步令牌概率，/＞为激活函数，/＞为全连接层权重，/＞为全连接层偏置项，/＞为计算获得的上下文向量，/＞为累积索引值，/＞为目标序列对源序列的注意力分数。6.根据权利要求5所述的一种基于知识保护及选择的主题文本生成方法，其特征在于，步骤S5中的所述预测结果分布的表达式为：其中，为计算获得的预测结果分布，/＞为当前时间步产生的令牌，/＞为从词表中产生令牌的概率分布，/＞为以目标序列对源序列的注意力分数作为的拷贝概率分布。7.根据权利要求6所述的一种基于知识保护及选择的主题文本生成方法，其特征在于，步骤S5中的所述负对数似然损失的表达式为：其中，为负对数似然损失，/＞为采样时间步总数，/＞为用于模型预测的标签分布，/＞为时间步中生成的随机变量。
说明书desc
技术领域本发明涉及自然语言主题文本生成技术领域，尤其涉及一种基于知识保护及选择的主题文本生成方法。背景技术主题文本生成技术是一种特殊的自然语言生成技术，通过设计合适的模型结构，从给定的若干主题词中提取信息，进而生成连贯的段落级文本。这一技术可以用于自动广告生成、特定主题的邮件生成等领域，还可以作为可控文本生成良好的测试平台，有着广泛的应用范围。主题文本生成技术与文本摘要、机器翻译等其他文本生成技术不同，对于文本摘要而言，输入源信息是大段落的文章，其所蕴含的信息量远大于需要生成的摘要中蕴含的信息量；机器翻译的输入、输出文本中包含信息量几乎相等；而主题文本生成技术以少量的若干主题词语作为输入，输出为段落级文本，其输入中所包含的信息量远远小于输出中所包含的信息量，这一现象大大增加了主题文本生成技术的难度。受该现象的制约，现有技术大多引入第三方外部知识库中的知识来弥补输入输出信息量的不足，但外部知识库的知识存储形式与模型中知识存储形式不同，无法充分利用外部知识，导致生成的主题文本仍然存在表达不通顺、语义逻辑性差以及多样性较低的问题。发明内容本发明旨在至少解决相关技术中存在的技术问题之一。为此，本发明提供一种基于知识保护及选择的主题文本生成方法。本发明提供一种基于知识保护及选择的主题文本生成方法，包括：S1：采集待生成的主题文本数据，将所述主题文本数据划分为训练集与测试集；S2：引入预训练的第一语言模型，冻结所述第一语言模型参数并构造可训练的动态前缀向量，获得第二语言模型；S3：通过所述第二语言模型编码所述训练集，获得隐藏中间状态，将所述隐藏中间状态记为所述第二语言模型的解码器的初始状态，所述解码器根据所述初始状态解码，获得训练集对应的词表概率分布；S4：通过包含知识选择的拷贝机制，计算获得所述解码器的解码状态与所述主题文本数据的相似度及未表达主题对应的拷贝概率分布；S5：根据所述词表概率分布及所述拷贝概率分布计算获得预测结果分布，根据所述预测结果分布计算获得负对数似然损失；S6：以所述负对数似然损失更新所述第二语言模型中的所述动态前缀向量，获得第三语言模型；S7：基于束搜索策略，通过所述第三语言模型对所述测试集进行预测，生成主题文本。根据本发明提供的一种基于知识保护及选择的主题文本生成方法，步骤S2中的所述动态前缀向量的长度与输入所述第二语言模型的主题词的数量离散线性正相关。根据本发明提供的一种基于知识保护及选择的主题文本生成方法，步骤S4中，所述相似度的表达式为：其中，为解码器的解码状态与所述主题文本数据的相似度，/＞为当前采样时间步，为余弦相似度计算，/＞为编码器状态，/＞为解码器状态，/＞为选择辅助函数，/＞为选择辅助函数阈值，/＞为内容待过滤及选择的张量。根据本发明提供的一种基于知识保护及选择的主题文本生成方法，步骤S4中，所述拷贝机制中包含的知识选择过程的表达式为：其中，为知识选择后的第一状态信息，/＞为知识选择后的第二状态信息，/＞为归一化函数。根据本发明提供的一种基于知识保护及选择的主题文本生成方法，步骤S4中，所述拷贝概率分布的表达式为：其中，为拷贝概率分布中产生的当前时间步令牌概率，/＞为激活函数，/＞为全连接层权重，/＞为全连接层偏置项，/＞为计算获得的上下文向量，/＞为累积索引值，/＞为目标序列对源序列的注意力分数。根据本发明提供的一种基于知识保护及选择的主题文本生成方法，步骤S5中的所述预测结果分布的表达式为：其中，为计算获得的预测结果分布，/＞为当前时间步产生的令牌，/＞为从词表中产生令牌的概率分布，/＞为以目标序列对源序列的注意力分数作为的拷贝概率分布。根据本发明提供的一种基于知识保护及选择的主题文本生成方法，步骤S5中的所述负对数似然损失的表达式为：其中，为负对数似然损失，/＞为采样时间步总数，/＞为用于模型预测的标签分布，/＞为时间步中生成的随机变量。本发明提供的一种基于知识保护及选择的主题文本生成方法，目的是针对当前主题文本生成模型无法充分利用外部知识、生成文本多样性缺乏的问题，在保护预训练语言模型丰富语义知识的基础上，提高现有生成方法的生成文本多样性。本发明提供的一种基于知识保护及选择的主题文本生成方法GCS-IPT，通过冻结预训练语言模型GENIUS的权重、只训练动态的前缀向量，能够更好的使得GCS-IPT保护预训练的先验知识，并自适应不同数量主题词的输入；另外通过带有知识选择的拷贝机制模块，GCS-IPT可以在生成时同时考虑输入的主题信息与当前已生成文本中未表达的主题语义，进而生成文本保持了较高的主题一致性，本发明充分利用了预训练知识，明显提高了主题文本生成任务上的性能表现。本发明的附加方面和优点将在下面的描述中部分给出，部分将从下面的描述中变得明显，或通过本发明的实践了解到。附图说明为了更清楚地说明本发明或现有技术中的技术方案，下面将对实施例或现有技术描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图是本发明的一些实施例，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。图1是本发明实施例提供的一种基于知识保护及选择的主题文本生成方法流程图。具体实施方式为使本发明的目的、技术方案和优点更加清楚，下面将结合本发明中的附图，对本发明中的技术方案进行清楚、完整地描述，显然，所描述的实施例是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有作出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。以下实施例用于说明本发明，但不能用来限制本发明的范围。在本发明实施例的描述中，需要说明的是，术语“中心”、“纵向”、“横向”、“上”、“下”、“前”、“后”、“左”、“右”、“竖直”、“水平”、“顶”、“底”、“内”、“外”等指示的方位或位置关系为基于附图所示的方位或位置关系，仅是为了便于描述本发明实施例和简化描述，而不是指示或暗示所指的装置或元件必须具有特定的方位、以特定的方位构造和操作，因此不能理解为对本发明实施例的限制。此外，术语“第一”、“第二”、“第三”仅用于描述目的，而不能理解为指示或暗示相对重要性。在本发明实施例的描述中，需要说明的是，除非另有明确的规定和限定，术语“相连”、“连接”应做广义理解，例如，可以是固定连接，也可以是可拆卸连接，或一体连接；可以是机械连接，也可以是电连接；可以是直接相连，也可以通过中间媒介间接相连。对于本领域的普通技术人员而言，可以具体情况理解上述术语在本发明实施例中的具体含义。在本发明实施例中，除非另有明确的规定和限定，第一特征在第二特征“上”或“下”可以是第一和第二特征直接接触，或第一和第二特征通过中间媒介间接接触。而且，第一特征在第二特征“之上”、“上方”和“上面”可是第一特征在第二特征正上方或斜上方，或仅仅表示第一特征水平高度高于第二特征。第一特征在第二特征“之下”、“下方”和“下面”可以是第一特征在第二特征正下方或斜下方，或仅仅表示第一特征水平高度小于第二特征。在本说明书的描述中，参考术语“一个实施例”、“一些实施例”、“示例”、“具体示例”、或“一些示例”等的描述意指结合该实施例或示例描述的具体特征、结构、材料或者特点包含于本发明实施例的至少一个实施例或示例中。在本说明书中，对上述术语的示意性表述不必须针对的是相同的实施例或示例。而且，描述的具体特征、结构、材料或者特点可以在任一个或多个实施例或示例中以合适的方式结合。此外，在不相互矛盾的情况下，本领域的技术人员可以将本说明书中描述的不同实施例或示例以及不同实施例或示例的特征进行结合和组合。下面结合图1描述本发明的实施例。本发明提供一种基于知识保护及选择的主题文本生成方法，包括：S1：采集待生成的主题文本数据，将所述主题文本数据划分为训练集与测试集；进一步的，本阶段目的是对主题文本生成数据预处理。对主题文本生成数据进行主题出现频率统计，统计不同主题词语在整个语料库中出现的频率，并以此为基准筛选出现频率较高的主题词语，防止频率过低的训练语料对模型可能产生的潜在负面影响，筛选并过滤出现频率较低的主题词语后，对于剩余部分，随机采样25000条作为训练集、2000条作为测试集。S2：引入预训练的第一语言模型，冻结所述第一语言模型参数并构造可训练的动态前缀向量，获得第二语言模型；进一步的，本阶段的目的是对语言模型进行知识保护，对预训练语言模型的全部参数进行冻结，以保护其丰富的语义知识不被破坏；同时，构造动态前缀向量，以自适应不同数量主题词的输入情况。其中，步骤S2中的所述动态前缀向量的长度与输入所述第二语言模型的主题词的数量离散线性正相关。进一步的，为了自适应不同数量主题词的输入情况而构造动态前缀向量，该向量参与训练与推理的实际长度与输入主题词的数量相关，实际长度随着主题词的数量的增加而离散的线性增长。S3：通过所述第二语言模型编码所述训练集，获得隐藏中间状态，将所述隐藏中间状态记为所述第二语言模型的解码器的初始状态，所述解码器根据所述初始状态解码，获得训练集对应的词表概率分布；进一步的，对于训练数据中的每个数据样本，送入预训练语言模型的编码器中，产生隐藏中间状态，表明输入主题词的语义信息，并以此作为解码器的初始状态。S4：通过包含知识选择的拷贝机制，计算获得所述解码器的解码状态与所述主题文本数据的相似度及未表达主题对应的拷贝概率分布；进一步的，对于每一时刻的解码器状态，通过带有知识选择的拷贝机制模块计算其与初始主题分布的相似程度，并获得还未表达主题的拷贝概率分布。上述的步骤S3及步骤S4阶段，从语言模型丰富语义知识中选择与输入主题相关联的知识，同时考虑当前已生成内容中输入主题语义的表达情况，以此指导未来时间步骤的生成过程。其中，步骤S4中，所述相似度的表达式为：其中，为解码器的解码状态与所述主题文本数据的相似度，/＞为当前采样时间步，为余弦相似度计算，/＞为编码器状态，/＞为解码器状态，/＞为选择辅助函数，/＞为选择辅助函数阈值，/＞为内容待过滤及选择的张量。进一步的，首先采用余弦相似度计算该时刻解码器状态与编码器状态/＞的相似度，并使用辅助函数/＞与特定阈值/＞辅助进行知识选择，其中的辅助函数/＞可以通过相似度/＞的不同取值与特定阈值/＞对张量/＞中的特定内容进行过滤与选择。其中，步骤S4中，所述拷贝机制中包含的知识选择过程的表达式为：其中，为知识选择后的第一状态信息，/＞为知识选择后的第二状态信息，/＞为归一化函数。进一步的，上述的归一化函数，用于将张量的最大值缩放为1、最小值缩放为0，所述拷贝机制中包含的知识选择过程的表达式表示通过当前时间步编码器与解码器状态的相似度，对知识进行过滤选择，得到知识选择后的状态信息与/＞。其中的中的非零元素来自于编码器中与当前时间步主题最相关的信息，/＞以相似度与1的差为基准过滤信息，确保解码过程中考虑到尚未表达的主题语义信息。其中，步骤S4中，所述拷贝概率分布的表达式为：其中，为拷贝概率分布中产生的当前时间步令牌概率，/＞为激活函数，/＞为全连接层权重，/＞为全连接层偏置项，/＞为计算获得的上下文向量，/＞为累积索引值，/＞为目标序列对源序列的注意力分数，/＞表示张量拼接。S5：根据所述词表概率分布及所述拷贝概率分布计算获得预测结果分布，根据所述预测结果分布计算获得负对数似然损失；其中，步骤S5中的所述预测结果分布的表达式为：其中，为计算获得的预测结果分布，/＞为当前时间步产生的令牌，/＞为从词表中产生令牌的概率分布，/＞为以目标序列对源序列的注意力分数作为的拷贝概率分布。其中，步骤S5中的所述负对数似然损失的表达式为：其中，为负对数似然损失，/＞为采样时间步总数，/＞为用于模型预测的标签分布，/＞为时间步中生成的随机变量。S6：以所述负对数似然损失更新所述第二语言模型中的所述动态前缀向量，获得第三语言模型；进一步的，本阶段的目的为更新前缀向量，首先生成当前时间步的文本令牌，并与标签计算负对数似然损失，以此获得梯度并更新动态前缀向量，具体描述，为对进行反向传播，获得对动态前缀向量的梯度信息，以此作为依据更新动态前缀向量值。S7：基于束搜索策略，通过所述第三语言模型对所述测试集进行预测，生成主题文本。进一步的，重复上述步骤，能够完成前缀向量的训练，获得步骤S6和S7中的第三语言模型，然后通过束搜索策略采样，能够生成完整的主题文本。下面对本发明的基于知识保护及选择的主题文本生成方法GCS-IPT进行有效性验证，本发明在两个广泛使用的公开主题文本生成数据集ESSAY与ZHIHU上进行实验。表1 本发明的基于知识保护及选择的主题文本生成方法与其它现有方法对比实验结果表1列出了GCS-IPT与以往方法的对比试验。其中以往的方法包括MTA：最早的主题文本生成方法，采用主题覆盖向量确保解码器生成的内容紧紧围绕输入主题词语的语义；CTEG：首次将图结构的常识库中的信息作为额外外部信息引入主题文本生成模型；SCTKG：同样引入了常识库的信息，同时从情感角度控制主题文本语义的方法；GENIUS：采用草图重构预训练任务对BART语言模型重新预训练后的模型。实验采用本领域通用的自动评价指标BLEU、DIST-2、Consistency、Novelty，每组实验均重复5次。实验结果显示，本发明提出的方法GCS-IPT相比于以往效果最好的方法SCTKG，在多样性、主题一致性及新颖性上均有提升，特别是多样性平均提升40%以上。以上对比结果充分说明了本发明提出的方法在主题文本生成任务上取得了优秀的效果。本发明的目的是针对当前主题文本生成模型无法充分利用外部知识、生成文本多样性缺乏的问题，在保护预训练语言模型丰富语义知识的基础上，提出了一种带有知识选择的主题文本生成方法，提高现有生成方法的生成文本多样性。本发明提出了一种基于知识保护和选择的主题文本生成方法，包括：对主题文本生成训练集中的主题数目分布分析，筛选出现频率较低的主题词语，构造训练、测试、验证数据集；对预训练语言模型的全部参数进行冻结，以保护其丰富的语义知识不被破坏，同时构造动态前缀向量，以自适应不同数量主题词的输入情况；对于训练数据中的每个数据样本，送入预训练语言模型的编码器中，产生隐藏中间状态，表明输入主题词的语义信息，并以此作为解码器的初始状态；对于每一时刻的解码器状态，通过带有知识选择的拷贝机制模块计算其与初始主题分布的相似程度，并获得还未表达主题的拷贝概率分布；利用拷贝概率分布与词表概率分布，计算当前时间步的预测结果分布，并与训练标签计算负对数似然损失，以此更新动态前缀向量；重复至步，完成前缀向量的训练以及主题文本的输出。本发明提供的一种基于知识保护及选择的主题文本生成方法GCS-IPT，通过冻结预训练语言模型GENIUS的权重、只训练动态的前缀向量，能够更好的使得GCS-IPT保护预训练的先验知识，并自适应不同数量主题词的输入；另外通过带有知识选择的拷贝机制模块，GCS-IPT可以在生成时同时考虑输入的主题信息与当前已生成文本中未表达的主题语义，进而生成文本保持了较高的主题一致性，本发明充分利用了预训练知识，明显提高了主题文本生成任务上的性能表现。最后应说明的是：以上实施例仅用以说明本发明的技术方案，而非对其限制；尽管参照前述实施例对本发明进行了详细的说明，本领域的普通技术人员应当理解：其依然可以对前述各实施例所记载的技术方案进行修改，或者对其中部分技术特征进行等同替换；而这些修改或者替换，并不使相应技术方案的本质脱离本发明各实施例技术方案的精神和范围。
