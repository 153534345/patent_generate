标题title
一种基于深度强化学习的智能交通灯控制方法及装置
摘要abst
本发明公开了一种基于深度强化学习的智能交通灯控制方法及装置，包括：每个路口设置一个智能体，用于该路口的交通灯控制；智能体获取当前交通状态；其中所述当前交通状态包括当前路口的数据信息和相邻路口的数据信息；将当前交通状态输入训练好的Q‑神经网络；根据Q‑神经网络的输出，确定这个智能体对对应路口交通灯的控制动作，其中所述Q‑神经网络的构建训练方法包括：基于实际交通状况在仿真平台构建了交通路网和时变交通流与智能体江西交互生成经验元组，对Q‑神经网络进行训练。采用深度强化学习算法，算法包含多个智能体，每个智能体控制一个交叉路口，通过智能体之间的通信与协调达到全局调控。
权利要求书clms
1.一种基于深度强化学习的智能交通灯控制方法，其特征在于，所述方法包括：每个路口设置一个智能体，用于该路口的交通灯控制；智能体获取当前交通状态；其中所述当前交通状态包括当前路口的数据信息和相邻路口的数据信息；将当前交通状态输入训练好的Q-神经网络；根据Q-神经网络的输出，确定这个智能体对对应路口交通灯的控制动作；其中所述Q-神经网络的构建训练方法包括：步骤1、通过仿真平台构建现实交通路网环境；步骤2、基于仿真平台随机生成时变交通流量；步骤3、智能体通过仿真平台获取当前阶段本地交通信息和邻接智能体的交通信息，结合生成当前交通状态s；步骤4、智能体以当前交通状态s作为Q-神经网络的输入，以∈概率随机选取下一阶段动作a，以1-∈的概率选取Q-神经网络输出序列的最大值作为下一阶段动作a；步骤5、仿真平台执行智能体提供的动作a，使交通状态进入下一阶段交通状态s_，并返回即时奖励值r；步骤6、基于当前交通状态s、动作a、下一阶段交通状态s_和即时奖励值r，生成经验元组，并将所述经验元组存入经验池中；每隔一段时间，利用优先经验回放机制从经验池中选取若干条经验元组；步骤7、基于已选择的经验元组，利用平滑化绝对值损失函数和经验元组的TD-error值加权和作为损失函数，使用最先进的正交初始化器和RMSprop作为梯度优化器对Q-神经网络进行训练，得到训练好的Q-神经网络。2.根据权利要求1所述的基于深度强化学习的智能交通灯控制方法，其特征在于，所述步骤3中，智能体根据本地观测到的交通信息和邻接智能体所观测到的具有折扣的交通信息加权和计算出当前交通状态s；其中交通信息包括路口车辆等待队列长度、路口通过车辆数和更新后的车辆等待时间。3.根据权利要求1所述的基于深度强化学习的智能交通灯控制方法，其特征在于，所述步骤4，包括：每个交通路口包含四个相位，交通灯将基于四个相位进行转变，智能体的动作也对应的包含四个值1，2，3，4，每次根据所选择的动作进入对应相位；智能体选取动作受∈-贪婪策略控制，智能体以∈的概率进行环境探索，即随机选取一个动作a；智能体以1-∈的概率进行已有经验的利用，即选择Q-神经网络的输出中使得Q值达到最大的动作a；并基于动作a执行一个时间步。4.根据权利要求3所述的基于深度强化学习的智能交通灯控制方法，其特征在于：将∈从固定值更改为随着时间变化线性递减的动态值，确保智能体在前期缺少经验的情况下更加侧重于探索环境，后期则更侧重于根据现有的经验训练Q-神经网络。5.根据权利要求1所述的基于深度强化学习的智能交通灯控制方法，其特征在于：设Δt作为智能体与交通环境交互的时间步，每次在环境模拟了Δt时长后，智能体将改变当前策略；设计黄灯时间ty在每次交通灯转换时都要强制执行，且保证ty＜Δt。6.根据权利要求1所述的基于深度强化学习的智能交通灯控制方法，其特征在于，奖励值的定义为以下因素的加权和：所有临近道路的队列长度L；所有临近道路的延迟总和D；所有临近道路车辆更新后的等待时间总和；最后一个动作后，时间间隔t内通过交通路口的车辆总数N。7.根据权利要求1所述的基于深度强化学习的智能交通灯控制方法，其特征在于，经验元组以叶子节点的形式存入树状经验池中，结点值为该经验元组包含的时间差分误差TD-error值，每次训练时从树状经验池中基于优先经验回放机制，选取时间差分误差TD-error值最高的前n条经验元组进行Q-神经网络训练；其中，经验元组包含的时间差分误差TD-error值δt的计算方法，包括：δt＝rt+1+γmaxaQw-Qw其中rt+1为采取动作a后一个时间步内智能体累计的奖励值，γ是未来影响因子，γ越大说明未来的经验对当前影响越重，γ越小说明智能体更关注经验的即时影响；maxaQw是智能体以当前交通状态s为Q-神经网络输入，通过Q-神经网络返回具有最大动作价值的Q值，Qw是智能体在当前交通状态s下采取动作a，利用神经网络返回的Q值。8.根据权利要求1所述的基于深度强化学习的智能交通灯控制方法，其特征在于，所述损失函数L为：其中r为奖励值，w为网络参数，B为通过经验优先回放机制选取的经验元组集合，|B|表示经验元组的个数，maxaQw是以s-为输入，利用评估网络选取可使动作价值达到最大的动作以及其Q值；Q是以s为输入，利用目标网络选取动作a返回的Q值。9.一种基于深度强化学习的智能交通灯控制装置，其特征在于，包括处理器及存储介质；所述存储介质用于存储指令；所述处理器用于根据所述指令进行操作以执行根据权利要求1至8任一项所述方法的步骤。10.一种存储介质，其上存储有计算机程序，其特征在于，所述计算机程序被处理器执行时实现权利要求1至8任一项所述方法的步骤。
说明书desc
技术领域本发明涉及交通仿真技术领域，涉及一种基于深度强化学习的智能交通灯控制方法及装置，具体涉及一种面向大规模交通路网的智能交通灯控制方法。背景技术：随着社会的快速发展，城市化所带来的各种问题越来越明显，其中以交通拥堵、交通事故、汽车引起的环境恶化等问题尤为严重。在这种情况下，越来越多的学者投入到改善交通环境的调研当中。缓解交通拥堵的办法之一就是实现对交通信号灯的智能控制。当前社会中，大多数的交通信号灯仍然采取预先设定好的固定时间进行交通控制。定时交通控制缺点十分明显。由于信号相位和时间设计方案的参数都是固定不变的，无法根据实时交通量的变化而随之改变，交通路口经常出现绿灯时无车同行，红灯时等待车辆较多的情况，使得交通路口的通行效率很低。最近的一些研究根据真实的交通数据提出了手动指定的规则，但这些规则仍旧是通过预定义方式控制交通信号，不能根据实时流量动态调整。为了实现对交通信号灯实时的控制，学者们将强化学习技术应用在智能交通控制上。与传统的模型驱动方法不同，强化学习不依赖启发式假设和方程。相反，它根据与复杂交通系统交互的经验，直接使用参数化模型来学习最优控制。然而传统的强化学习面临两个困难的挑战：如何表示环境、如何实现环境和策略之间关系的建模。近年来，研究者们将深度强化学习应用到了智能交通灯控制当中，并取得了更好的效果。但是当前很多基于深度强化学习的算法其优化目标和现实交通灯控制的最终目标并不相符。智能交通灯控制的最终目标应该是从全局角度统计达到最优效果的控制，然而很多基于深度强化学习的控制算法都单独进行交通信号控制，无法实现协同合作。综上所述，需要一种能够进行协同合作，实现对多个交通路口全局调控，从而缩短车辆在路口的平均等待时间的基于深度强化学习的智能交通灯控制方式。发明内容本发明的目的在于提供一种基于深度强化学习的智能交通灯控制方法，能够解决传统交通控制实时性差、延迟率高、拥堵车辆多等问题。技术方案：为解决上述技术问题，本发明采用的技术方案为：第一方面，提供一种基于深度强化学习的智能交通灯控制方法，包括：每个路口设置一个智能体，用于该路口的交通灯控制；智能体获取当前交通状态；其中所述当前交通状态包括当前路口的数据信息和相邻路口的数据信息；将当前交通状态输入训练好的Q-神经网络；根据Q-神经网络的输出，确定这个智能体对对应路口交通灯的控制动作；其中所述Q-神经网络的构建训练方法包括：步骤1、通过仿真平台构建现实交通路网环境；步骤2、基于仿真平台随机生成时变交通流量；步骤3、智能体通过仿真平台获取当前阶段本地交通信息和邻接智能体的交通信息，结合生成当前交通状态s；步骤4、智能体以当前交通状态s作为Q-神经网络的输入，以∈概率随机选取下一阶段动作a，以1-∈的概率选取Q-神经网络输出序列的最大值作为下一阶段动作a；步骤5、仿真平台执行智能体提供的动作a，使交通状态进入下一阶段交通状态s_，并返回即时奖励值r；步骤6、基于当前交通状态s、动作a、下一阶段交通状态s_和即时奖励值r，生成经验元组，并将所述经验元组存入经验池中；每隔一段时间，利用优先经验回放机制从经验池中选取若干条经验元组；步骤7、基于已选择的经验元组，利用平滑化绝对值损失函数和经验元组的TD-error值加权和作为损失函数，使用最先进的正交初始化器和RMSprop作为梯度优化器对Q-神经网络进行训练，得到训练好的Q-神经网络。在一些实施例中，所述步骤3中，智能体根据本地观测到的交通信息和邻接智能体所观测到的具有折扣的交通信息加权和计算出当前交通状态s；其中交通信息包括路口车辆等待队列长度、路口通过车辆数和更新后的车辆等待时间。在一些实施例中，所述步骤4，包括：每个交通路口包含四个相位，交通灯将基于四个相位进行转变，智能体的动作也对应的包含四个值1，2，3，4，每次根据所选择的动作进入对应相位；智能体选取动作受∈-贪婪策略控制，智能体以∈的概率进行环境探索，即随机选取一个动作a；智能体以1-∈的概率进行已有经验的利用，即选择Q-神经网络的输出中使得Q值达到最大的动作a；并基于动作a执行一个时间步。进一步地，将∈从固定值更改为随着时间变化线性递减的动态值，确保智能体在前期缺少经验的情况下更加侧重于探索环境，后期则更侧重于根据现有的经验训练Q-神经网络。在一些实施例中，设Δt作为智能体与交通环境交互的时间步，每次在环境模拟了Δt时长后，智能体将改变当前策略；设计黄灯时间ty在每次交通灯转换时都要强制执行，且保证ty＜Δt。在一些实施例中，奖励值的定义为以下因素的加权和：所有临近道路的队列长度L；所有临近道路的延迟总和D；所有临近道路车辆更新后的等待时间总和；最后一个动作后，时间间隔t内通过交通路口的车辆总数N。在一些实施例中，经验元组以叶子节点的形式存入树状经验池中，结点值为该经验元组包含的时间差分误差TD-error值，每次训练时从树状经验池中基于优先经验回放机制，选取时间差分误差TD-error值最高的前n条经验元组进行Q-神经网络训练；其中，经验元组包含的时间差分误差TD-error值δt的计算方法，包括：δt＝rt+1+γmaxaQw-Qw其中rt+1为采取动作a后一个时间步内智能体累计的奖励值，γ是未来影响因子，γ越大说明未来的经验对当前影响越重，γ越小说明智能体更关注经验的即时影响；maxaQw是智能体以当前交通状态s为Q-神经网络输入，通过Q-神经网络返回具有最大动作价值的Q值，Qw是智能体在当前交通状态s下采取动作a，利用神经网络返回的Q值。在一些实施例中，所述损失函数L为：其中r为奖励值，w为网络参数，B为通过经验优先回放机制选取的经验元组集合，|B|表示经验元组的个数，maxaQw是以s-为输入，利用评估网络选取可使动作价值达到最大的动作以及其Q值；Q是以s为输入，利用目标网络选取动作a返回的Q值。第二方面，本发明提供了一种基于深度强化学习的智能交通灯控制装置，包括处理器及存储介质；所述存储介质用于存储指令；所述处理器用于根据所述指令进行操作以执行根据第一方面所述方法的步骤。第三方面，本发明提供了一种存储介质，其上存储有计算机程序，所述计算机程序被处理器执行时实现第一方面所述方法的步骤。本发明的有益效果：1、本发明设计了一种基于深度强化学习的智能交通灯控制方法。首先基于实际交通状况在仿真平台构建了交通路网和时变交通流，之后提出了一种深度强化学习算法，该算法包含多个智能体，每个智能体控制一个交叉路口，通过智能体之间的通信与协调达到全局调控。2、本发明提出的带有权重的双估计器和优先经验回放机制，针对Double-DQN低估智能体最大动作奖励值的问题，以权重的方式结合深度Q-神经网络和深度双Q-神经网络，从而减小对目标网络估计的误差，得到更加精准的Q值估计，在仿真平台的测试中，表现高于基于传统深度强化学习的控制方式；3、本发明在解决多智能体之间协同的问题中提出了三个改进方法，每个智能体的在t时刻的状态S不仅包含自身状态，同时包含具有折扣的所有邻接路口的智能体状态；所有智能体在时刻t共享全局奖励值R；为了保证随着时间的迁移，每个智能体能更注重利用现有的经验进行训练，从而逐渐达到平稳状态，本发明对智能体中用于探索新经验的∈-贪婪策略进行修改。以往智能体每次与环境的交互时，会以固定∈概率随机选择一个动作，并将这条经验收入经验池中，这种做法导致智能体无法随着时间的变化去动态调整探索与利用的占比。本发明将∈从固定值更改为随着时间变化线性递减的动态值，确保智能体在前期缺少经验的情况下更加侧重于探索环境，后期则更侧重于根据现有的经验训练Q-神经网络。4、使用本发明提出的智能体设计方式，在仿真平台的测试中，显示出，在车辆拥堵长度、交叉口等待时间、车辆平均速度等性能上比传统的深度强化学习算法表现有所提升。附图说明图1为本发明一个实施例的智能交通灯控制方法流程示意图；图2为本发明实施例中Q-神经网络结构示意图；图3为本发明实施例中智能交通灯控制方法智能体与环境交互示意图。具体实施方式为使本发明实现的技术手段、创作特征、达成目的与功效易于明白了解，下面结合具体实施方式进一步阐述本发明。在本发明的描述中，若干的含义是一个以上，多个的含义是两个以上，大于、小于、超过等理解为不包括本数，以上、以下、以内等理解为包括本数。如果有描述到第一、第二只是用于区分技术特征为目的，而不能理解为指示或暗示相对重要性或者隐含指明所指示的技术特征的数量或者隐含指明所指示的技术特征的先后关系。本发明的描述中，参考术语“一个实施例”、“一些实施例”、“示意性实施例”、“示例”、“具体示例”、或“一些示例”等的描述意指结合该实施例或示例描述的具体特征、结构、材料或者特点包含于本发明的至少一个实施例或示例中。在本说明书中，对上述术语的示意性表述不一定指的是相同的实施例或示例。而且，描述的具体特征、结构、材料或者特点可以在任何的一个或多个实施例或示例中以合适的方式结合。实施例1一种基于深度强化学习的智能交通灯控制方法，包括：每个路口设置一个智能体，用于该路口的交通灯控制；智能体获取当前交通状态；其中所述当前交通状态包括当前路口的数据信息和相邻路口的数据信息；将当前交通状态输入训练好的Q-神经网络；根据Q-神经网络的输出，确定这个智能体对对应路口交通灯的控制动作；其中所述Q-神经网络的构建训练方法包括：步骤1、通过仿真平台构建现实交通路网环境；步骤2、基于仿真平台随机生成时变交通流量；步骤3、智能体通过仿真平台获取当前阶段本地交通信息和邻接智能体的交通信息，结合生成当前交通状态s；步骤4、智能体以当前交通状态s作为Q-神经网络的输入，以∈概率随机选取下一阶段动作a，以1-∈的概率选取Q-神经网络输出序列的最大值作为下一阶段动作a；步骤5、仿真平台执行智能体提供的动作a，使交通状态进入下一阶段交通状态s_，并返回即时奖励值r；步骤6、基于当前交通状态s、动作a、下一阶段交通状态s_和即时奖励值r，生成经验元组，并将所述经验元组存入经验池中；每隔一段时间，利用优先经验回放机制从经验池中选取若干条经验元组；步骤7、基于已选择的经验元组，利用平滑化绝对值损失函数和经验元组的TD-error值加权和作为损失函数，使用最先进的正交初始化器和RMSprop作为梯度优化器对Q-神经网络进行训练，得到训练好的Q-神经网络。本发明实施例利用仿真技术实现交通路网的仿真；在仿真平台中智能体根据当前环境的状态，选取具有最大动作奖励值的动作，执行该动作后获取元组信息，其中s为当前状态，a为智能体采取的行动，s_为智能体采取动作a后环境的新状态，r为智能体采取动作a后的即使奖励值。将该元组信息存储到经验池中，每隔一段时间，基于优先经验回放机制从经验池中采样n条元组信息对智能体进行训练。在一些实施例中，如图1、图3所示，本发明提供一种用于基于深度强化学习的智能交通灯控制方法，包括如下步骤：1、以仿真平台SUMO为实验环境，在该平台上实现现实道路的仿真。以一个含有七个交叉口的交通路网为例。在仿真平台上进行交通环境的设计，其中除交叉4之外，每个交叉口与四个150米长的路段相连，每条道路有两条引入车道和两条引出车道。交叉口4的交通灯包括两个阶段：东西向绿灯、东西向红灯。其余交叉口的交通灯设计依照现实情况包括五个阶段：E-W直行阶段、E-W左转阶段以及E、W和N-S的三个直行和左转阶段。当一个方向上有绿灯时，另一个方向上有红灯。此外，绿灯后接3秒黄灯，然后变为红灯。2、在仿真平台上设计实现时变交通流。设计了随机和时变的交通流来模拟高峰期的交通。具体而言，在每小时引入1000辆汽车的前提下，设计了三组额外交通流，F1由三组起点-终点对,组成,F2由三组起点-终点对,,组成，F3由两组起点-终点对,组成。在实验过程中，每隔十五分钟，F1与F2交替成为主交通流，其余两组作为支交通流。主交通流在指定区域内将生成400辆/小时“流量”的整数倍，支交通流则在指定区域内将生成200辆/小时“流量”的整数倍。3、智能体通过仿真平台获取当前阶段本地交通状态和邻接智能体的交通状态,结合生成当前交通状态s。获取相邻路口的数据信息，所述相邻路口的数据信息包括路口车辆等待队列长度，路口通过车辆数，更新后的车辆等待时间。4、智能体以当前交通状态s作为Q-神经网络的输入，以∈概率随机选取下一阶段动作a，以1-∈的概率选取神经网络输出序列的最大值作为下一阶段动作a。利用FC层和LSTM层构建Q-神经网络，用于预测智能体动作。图2为本发明实施例中Q-神经网络结构示意图，Q-神经网路网络根据当前路口的数据信息以及相邻路口的数据信息预测出t秒后当前路口的拥堵情况，拥堵情况是由当前车辆队列等待长度，已通过车辆数和更新后的车辆等待时间共同决定。并根据预测的各个车道拥堵情况，以∈概率随机选取随机动作a，以1-∈的概率选取可使当前拥堵情况得到最大缓解的动作。5、基于所有智能体返回的动作数组，仿真环境执行一个时间步，记录时间步内所有临近道路的队列长度L、所有临近道路的延迟总和D、所有临近道路车辆更新后的等待时间总和、最后一个动作后，时间间隔t内通过交通路口的车辆总数N，将上述数据的权重和定义为即时奖励值r。6、生成经验元组，计算出该经验元组包含的TD-error值，公式如下：δt＝rt+1+γmaxaQw-Qw其中rt+1为采取动作a后一个时间步内智能体累计的奖励值，γ是未来影响因子，γ越大说明未来的经验对当前影响越重，γ越小说明智能体更关注经验的即时影响。maxaQw是智能体以s_为网络输入，通过Q-神经网络返回具有最大动作价值的Q值，Qw是智能体在s状态下采取a动作，利用神经网络返回的Q值。计算出经验元组的TD-error值后，以该值为权重将智能体存入到树状经验池中。7、智能体训练。从经验池中读取权重最大的前N条经验元组，利用这些元组对智能体进行训练。其中损失函数如下式所示：其中maxaQw是以s-为输入，利用评估网络选取可使动作价值达到最大的动作以及其Q值。Qw是以s为输入，利用目标网络选取动作a返回的Q值。其中，使用最先进的正交初始化器和RMSprop作为梯度优化器对Q-神经网络进行训练。实施例2第二方面，本实施例提供了一种基于深度强化学习的智能交通灯控制装置，包括处理器及存储介质；所述存储介质用于存储指令；所述处理器用于根据所述指令进行操作以执行根据实施例1所述方法的步骤。实施例3第三方面，本实施例提供了一种存储介质，其上存储有计算机程序，所述计算机程序被处理器执行时实现实施例1所述方法的步骤。本领域内的技术人员应明白，本申请的实施例可提供为方法、系统、或计算机程序产品。因此，本申请可采用完全硬件实施例、完全软件实施例、或结合软件和硬件方面的实施例的形式。而且，本申请可采用在一个或多个其中包含有计算机可用程序代码的计算机可用存储介质上实施的计算机程序产品的形式。本申请是参照根据本申请实施例的方法、设备、和计算机程序产品的流程图和/或方框图来描述的。应理解可由计算机程序指令实现流程图和/或方框图中的每一流程和/或方框、以及流程图和/或方框图中的流程和/或方框的结合。可提供这些计算机程序指令到通用计算机、专用计算机、嵌入式处理机或其他可编程数据处理设备的处理器以产生一个机器，使得通过计算机或其他可编程数据处理设备的处理器执行的指令产生用于实现在流程图一个流程或多个流程和/或方框图一个方框或多个方框中指定的功能的装置。这些计算机程序指令也可存储在能引导计算机或其他可编程数据处理设备以特定方式工作的计算机可读存储器中，使得存储在该计算机可读存储器中的指令产生包括指令装置的制造品，该指令装置实现在流程图一个流程或多个流程和/或方框图一个方框或多个方框中指定的功能。这些计算机程序指令也可装载到计算机或其他可编程数据处理设备上，使得在计算机或其他可编程设备上执行一系列操作步骤以产生计算机实现的处理，从而在计算机或其他可编程设备上执行的指令提供用于实现在流程图一个流程或多个流程和/或方框图一个方框或多个方框中指定的功能的步骤。由技术常识可知，本发明可以通过其它的不脱离其精神实质或必要特征的实施方案来实现。因此，上述公开的实施方案，就各方面而言，都只是举例说明，并不是仅有的。所有在本发明范围内或在等同于本发明的范围内的改变均被本发明包含。
