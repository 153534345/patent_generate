标题title
电力领域模型预训练方法、精调方法、装置及设备
摘要abst
本发明公开了一种电力领域模型预训练方法、精调方法、装置及设备，其中，所述预训练方法包括：获取原始电力语料数据；对所述原始电力语料数据进行处理，所述处理至少包括分词处理；对处理后得到的电力语料数据，采用全词遮蔽的方法，构建电力领域模型的预训练语料；构建电力领域模型，所述电力领域模型包括注意力矩阵，所述注意力矩阵引入了词与词之间的相对位置编码；利用所述预训练语料，对所述电力领域模型进行预训练。本发明提供的技术方案，能够提升预训练模型的迁移能力。
权利要求书clms
1.一种电力领域模型预训练方法，其特征在于，所述方法包括：获取原始电力语料数据；对所述原始电力语料数据进行处理，所述处理至少包括分词处理；对处理后得到的电力语料数据，采用全词遮蔽的方法，构建电力领域模型的预训练语料；构建电力领域模型，所述电力领域模型包括注意力矩阵，所述注意力矩阵引入了词与词之间的相对位置编码；利用所述预训练语料，对所述电力领域模型进行预训练。2.根据权利要求1所述的方法，其特征在于，引入了词与词之间的相对位置编码的所述注意力矩阵的算法公式为：Attention_rel= Attention +rel其中，Attention 为未引入所述相对位置编码的注意力矩阵的算法公式，V是输入特征的向量，Q、K是计算Attention权重的特征向量，rel是词与词之间的相对位置有关的参数。3.根据权利要求1所述的方法，其特征在于，所述对所述原始电力语料数据进行处理，包括：采用BERT-CRF模型和电力领域词典对所述原始电力语料数据进行分词处理，所述BERT-CRF模型是利用电力分词语料进行训练得到。4.根据权利要求1所述的方法，其特征在于，所述对处理后得到的电力语料数据，采用全词遮蔽的方法，构建电力领域模型的预训练语料，包括：对所述处理后得到的电力语料数据采用预设概率进行随机全词遮蔽，将所有需要遮蔽的词语对应的字符中的一部分替换为随机字符、另一部分替换为遮蔽符号、剩余部分保留原来的字符不变。5.一种电力领域模型的精调方法，其特征在于，包括：针对下游任务构建训练用数据集；将电力领域预训练模型中除输出层以外的其他网络结构作为底层编码器，并根据所述下游任务构建输出层网络结构，将所述输出层网络结构连接至所述底层编码器之后，得到针对下游任务的电力领域模型，所述电力领域预训练模型的预训练语料是通过对原始电力语料数据进行分词处理之后采用全词遮蔽得到的，且所述电力领域预训练模型包括注意力矩阵，所述注意力矩阵引入了词与词之间的相对位置编码；利用所述训练用数据集对所述针对下游任务的电力领域模型进行训练。6.根据权利要求5所述的方法，其特征在于，所述下游任务为分类任务，所述输出层网络结构为全连接网络；且所述底层编码器与所述全连接网络之间还包括第一网络结构；所述第一网络结构用于抽取所述底层编码器中的第一层和最后一层的编码向量并求平均，得到第一编码向量，再对各个词的所述第一编码向量取平均得到所述底层编码器的编码向量；所述全连接网络用于基于所述底层编码器的编码向量输出每个类别对应的置信度。7.根据权利要求5所述的方法，其特征在于，所述下游任务为序列标注任务，所述输出层网络结构为条件随机场，且所述底层编码器与条件随机场层之间还包括Dropout层和映射层；所述底层编码器的输出为batch_size，time_steps，hidden_size形状的张量，其中，batch_size为批大小、time_steps为序列长度、hidden_size为所述底层编码器的隐层单元大小；所述底层编码器的输出经过所述Dropout层和所述映射层转换为batch_size, time_steps, num_classes形状的张量，其中，num_classes为目标类的数量；所述条件随机场层用于基于所述batch_size, time_steps, num_classes形状的张量得到整个序列中每个元素的标签。8.一种电力领域模型预训练装置，其特征在于，包括：获取模块，用于获取原始电力语料数据；处理模块，用于对所述原始电力语料数据进行处理，所述处理至少包括分词处理；第一构建模块，用于对处理后得到的电力语料数据，采用全词遮蔽的方法，构建电力领域模型的预训练语料；第二构建模块，用于构建电力领域模型，所述电力领域模型包括注意力矩阵，所述注意力矩阵引入了词与词之间的相对位置编码；预训练模块，用于利用所述预训练语料，对所述电力领域模型进行预训练。9.一种电力领域模型的精调装置，其特征在于，包括：第三构建模块，用于针对下游任务构建训练用数据集；第四构建模块，用于将电力领域预训练模型中除输出层以外的其他网络结构作为底层编码器，并根据所述下游任务构建输出层网络结构，将所述输出层网络结构连接至所述底层编码器之后，得到针对下游任务的电力领域模型，所述电力领域预训练模型的预训练语料是通过对原始电力语料数据进行分词处理之后采用全词遮蔽得到的，且所述电力领域预训练模型包括注意力矩阵，所述注意力矩阵引入了词与词之间的相对位置编码；训练模块，用于利用所述训练用数据集对所述针对下游任务的电力领域模型进行训练。10.一种电子设备，其特征在于，包括：存储器和处理器，所述存储器和所述处理器之间互相通信连接，所述存储器用于存储计算机程序，所述计算机程序被所述处理器执行时，实现权利要求1至4中任一项所述的电力领域模型预训练方法、或实现权利要求5至7中任一项所述的电力领域模型的精调方法。11.一种计算机可读存储介质，其特征在于，所述计算机可读存储介质用于存储计算机程序，所述计算机程序被处理器执行时，实现权利要求1至4中任一项所述的电力领域模型预训练方法、或实现权利要求5至7中任一项所述的电力领域模型的精调方法。
说明书desc
技术领域本发明涉及人工智能技术领域，具体涉及一种电力领域模型预训练方法、精调方法、装置及设备。背景技术现有的自然语言处理 模型包含的参数可以达到上百万。因此，训练出具有良好性能的NLP模型需要大量的训练样本和标签数据。通常，采用人工对训练样本进行标注。因此，获取大量的标签数据，需要较高的人工成本。在此背景下，预训练加精调的模式广泛应用于NLP模型训练。首先利用成本较低且容易获取的训练数据训练一个预训练模型。通过这种方式，预训练模型可以学习到语言学的通用知识。因此，针对不同的下游任务，可以利用其相关的标签数据对其相关的参数进行精调，使得训练的NLP模型具有良好性能。但是，在自然语言处理模型的预训练阶段，由于并非是针对下游任务进行训练的，而是针对预训练阶段的任务进行训练的，因此会导致预训练出的模型的迁移能力弱，即在对预训练模型进行精调得到针对下游任务的模型时，模型的适应性差，预测精度低。发明内容有鉴于此，本发明实施例提供了一种电力领域模型预训练方法、精调方法、装置及设备，以解决现有自然语言处理的预训练模型的迁移能力弱的问题。根据第一方面，本发明实施例提供了一种电力领域模型预训练方法，所述方法包括：获取原始电力语料数据；对所述原始电力语料数据进行处理，所述处理至少包括分词处理；对处理后得到的电力语料数据，采用全词遮蔽的方法，构建电力领域模型的预训练语料；构建电力领域模型，所述电力领域模型包括注意力矩阵，所述注意力矩阵引入了词与词之间的相对位置编码；利用所述预训练语料，对所述电力领域模型进行预训练。可选的，引入了词与词之间的相对位置编码的所述注意力矩阵的算法公式为：Attention_rel= Attention +rel其中，Attention 为未引入所述相对位置编码的注意力矩阵的算法公式，rel是与词与词之间的相对位置有关的参数。可选的，所述对所述原始电力语料数据进行处理，包括：采用BERT-CRF模型和电力领域词典对所述原始电力语料数据进行分词处理，所述BERT-CRF模型是利用电力分词语料进行训练得到。可选的，所述对处理后得到的电力语料数据，采用全词遮蔽的方法，构建电力领域模型的预训练语料，包括：对所述处理后得到的电力语料数据采用预设概率进行随机全词遮蔽，将所有需要遮蔽的词语对应的字符中的一部分替换为随机字符、另一部分替换为遮蔽符号、剩余部分保留原来的字符不变。根据第二方面，本发明实施例提供了一种电力领域模型的精调方法，包括：针对下游任务构建训练用数据集；将电力领域预训练模型中除输出层以外的其他网络结构作为底层编码器，并根据所述下游任务构建输出层网络结构，将所述输出层网络结构连接至所述底层编码器之后，得到针对下游任务的电力领域模型，所述电力领域预训练模型的预训练语料是通过对原始电力语料数据进行分词处理之后采用全词遮蔽得到的，且所述电力领域预训练模型包括注意力矩阵，所述注意力矩阵引入了词与词之间的相对位置编码；利用所述训练用数据集对所述针对下游任务的电力领域模型进行训练。可选的，所述下游任务为分类任务，所述输出层网络结构为全连接网络；且所述底层编码器与所述全连接网络之间还包括第一网络结构；所述第一网络结构用于抽取所述底层编码器中的第一层和最后一层的编码向量并求平均，得到第一编码向量，再对各个词的所述第一编码向量取平均得到所述底层编码器的编码向量；所述全连接网络用于基于所述底层编码器的编码向量输出每个类别对应的置信度。可选的，所述下游任务为序列标注任务，所述输出层网络结构为条件随机场，且所述底层编码器与条件随机场层之间还包括Dropout层和映射层；所述底层编码器的输出为形状的张量，其中，batch_size为批大小、time_steps为序列长度、hidden_size为所述底层编码器的隐层单元大小；所述底层编码器的输出经过所述Dropout层和所述映射层转换为形状的张量，其中，num_classes为目标类的数量；所述条件随机场层用于基于所述形状的张量得到整个序列中每个元素的标签。根据第三方面，本发明实施例提供了一种电力领域模型预训练装置，包括：获取模块，用于获取原始电力语料数据；处理模块，用于对所述原始电力语料数据进行处理，所述处理至少包括分词处理；第一构建模块，用于对处理后得到的电力语料数据，采用全词遮蔽的方法，构建电力领域模型的预训练语料；第二构建模块，用于构建电力领域模型，所述电力领域模型包括注意力矩阵，所述注意力矩阵引入了词与词之间的相对位置编码；预训练模块，用于利用所述预训练语料，对所述电力领域模型进行预训练。根据第四方面，本发明实施例提供了一种电力领域模型的精调装置，包括：第三构建模块，用于针对下游任务构建训练用数据集；第四构建模块，用于将电力领域预训练模型中除输出层以外的其他网络结构作为底层编码器，并根据所述下游任务构建输出层网络结构，将所述输出层网络结构连接至所述底层编码器之后，得到针对下游任务的电力领域模型，所述电力领域预训练模型的预训练语料是通过对原始电力语料数据进行分词处理之后采用全词遮蔽得到的，且所述电力领域预训练模型包括注意力矩阵，所述注意力矩阵引入了词与词之间的相对位置编码；训练模块，用于利用所述训练用数据集对所述针对下游任务的电力领域模型进行训练。根据第五方面，本发明实施例提供了一种电子设备，包括：存储器和处理器，所述存储器和所述处理器之间互相通信连接，所述存储器用于存储计算机程序，所述计算机程序被所述处理器执行时，实现上述第一方面所述的任一种电力领域模型预训练方法、或实现上述第二方面所述的任一种电力领域模型的精调方法。根据第六方面，本发明实施例提供了一种计算机可读存储介质，所述计算机可读存储介质用于存储计算机程序，所述计算机程序被处理器执行时，实现上述第一方面所述的任一种电力领域模型预训练方法、或实现上述第二方面所述的任一种电力领域模型的精调方法。本发明实施例中，通过全词遮蔽的方式构建电力领域模型的预训练语料，避免了使用字符遮蔽方式构建电力领域模型的预训练语料时，模型能轻易猜出遮蔽的词语，而忽略了词语和整个句子之间的语义信息的问题，可以提升预训练模型的迁移能力。另外，本发明实施例还在构建的预训练模型，即电力领域模型中引入了词与词之间的相对位置建模，具体来说是增加了引入词与词之间的相对位置编码的注意力矩阵，从而可以使得模型更加关注词与词之间的相对位置，进而对词与词之间的相对位置更加敏感，从而使得预训练的电力领域模型不仅适用于预训练阶段的遮蔽词语预测任务，而且更容易迁移至下游任务。附图说明通过参考附图会更加清楚的理解本发明的特征和优点，附图是示意性的而不应理解为对本发明进行任何限制，在附图中：图1为本发明实施例提供的一种电力领域模型预训练方法的流程示意图；图2为本发明实施例中的对所述原始电力语料数据进行处理的过程示意图；图3本发明实施例提供的一种电力领域模型的精调方法的流程示意图；图4为本发明实施例提供的一种电力领域模型预训练装置的结构示意图；图5为本发明实施例提供的一种电力领域模型的精调装置的结构示意图；图6为本发明实施例提供的一种电子设备的结构示意图。具体实施方式为使本发明实施例的目的、技术方案和优点更加清楚，下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域技术人员在没有作出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。需要说明的是，术语“包括”、“包含”或者其任何其他变体意在涵盖非排他性的包含，从而使得包括一系列要素的过程、方法、商品或者设备不仅包括那些要素，而且还包括没有明确列出的其他要素，或者是还包括为这种过程、方法、商品或者设备所固有的要素。在没有更多限制的情况下，由语句“包括一个……”限定的要素，并不排除在包括所述要素的过程、方法、商品或者设备中还存在另外的相同要素。此外，术语“第一”、“第二”等仅用于描述目的，而不能理解为指示或暗示相对重要性或者隐含指明所指示的技术特征的数量。在以下各实施例的描述中，“多个”的含义是两个以上，除非另有明确具体的限定。请参阅图1，本发明实施例提供一种电力领域模型预训练方法，所述方法包括：S101：获取原始电力语料数据；S102：对所述原始电力语料数据进行处理，所述处理至少包括分词处理；S103：对处理后得到的电力语料数据，采用全词遮蔽的方法，构建电力领域模型的预训练语料；S104：构建电力领域模型，所述电力领域模型包括注意力矩阵，所述注意力矩阵引入了词与词之间的相对位置编码；S105：利用所述预训练语料，对所述电力领域模型进行预训练。具体的，电力领域模型可以是电力领域大模型，即电力领域大规模模型。原始电力语料数据可以是大量的电力数据，所述处理还可以包括清洗，清洗处理可以在分词操作之前，具体可以采用正则匹配、BeautifulSoup等工具包实现，清洗处理用于过滤掉原始电力语料数据中的一些特殊符号，包括乱码、html符号等，得到较为干净的语料数据。在对所述电力领域模型进行训练时，利用所述电力领域模型对采用全词遮蔽方法构建的预训练语料中被遮蔽的词语进行预测，并将预测结果与被遮蔽前的词语进行比较，根据比较结果调整电力领域模型的参数。本发明实施例中，通过全词遮蔽的方式构建电力领域模型的预训练语料，避免了使用字符遮蔽方式构建电力领域模型的预训练语料时，模型能轻易猜出遮蔽的词语，而忽略了词语和整个句子之间的语义信息的问题，可以提升预训练模型的迁移能力。另外，本发明实施例还在构建的预训练模型，即电力领域模型中引入了词与词之间的相对位置建模，具体来说是增加了引入词与词之间的相对位置编码的注意力矩阵，从而可以使得模型更加关注词与词之间的相对位置，进而对词与词之间的相对位置更加敏感，从而使得预训练的电力领域模型不仅适用于预训练阶段的遮蔽词语预测任务，而且更容易迁移至下游任务。一些具体的实施方式中，引入了词与词之间的相对位置编码的所述注意力矩阵的算法公式为：Attention_rel= Attention +rel其中，Attention 为未引入所述相对位置编码的注意力矩阵的算法公式，该式计算的是针对一个注意力头的Attention矩阵。rel是与词与词之间的相对位置有关的参数，rel对每一个输入样本为一个对应于一个注意力头的标量。具体的，，Q、K和V分别代表Query、Key和Value，V是表示输入特征的向量，Q、K是计算Attention权重的特征向量。它们都是由输入特征得到的。Attention是根据关注程度对V乘以相应权重。Attention机制中的Q,K,V，即是，对当前的Query和所有的Key计算相似度，将这个相似度值通过Softmax层得到一组权重，根据这组权重与对应Value的乘积求和得到Attention下的Value值。Q、K和V是将输入向量X通过矩阵WQ、WK、WV变换得到，WQ、WK、WV是三个可训练的参数矩阵。dk为K的维度大小。本发明实施例中，相对位置编码采用T5的编码方式，将位置偏置引入到注意力矩阵中。即在注意力矩阵的基础上加入一项相对位置偏置rel。一些具体的实施方式中，所述对所述原始电力语料数据进行处理，包括：采用BERT-CRF模型和电力领域词典对所述原始电力语料数据进行分词处理，所述BERT-CRF模型是利用电力分词语料进行训练得到。其中，利用电力分词语料进行训练得到的所述BERT-CRF模型是一种电力领域分词工具。BERT模型是自然语言处理领域一个常用的预训练语言模型，BERT的全称是Bidirectional Encoder Representation from Transformers，CRF：条件随机场，是一种传统的机器学习方法。BERT-CRF模型采用“BMES”的编码模式，其中，“B”代表当前字符为多字词语的开头字符，“M”代表当前字符为多字词语的中间字符，“E”代表当前字符为多字词语的结尾字符，“S”代表当前字符为一个单字词。例如“变压器的检修规范”经过标注后得到“B，M，E，S，B，E，B，E”，对应的分词结果为：“变压器/ 的/ 检修/ 规范”。电力领域词典也即电力词典。本发明实施例中，首先利用BERT-CRF模型对原始电力语料数据进行分词处理，然后利用电力词典，将被切分开的电力词语进行合并，得到最终的分词结果。这里分词处理所针对的原始电力语料数据可以是已经经过清洗处理的电力语料数据。请参阅图2，分词处理后得到的是一系列的词语组成的词语序列。本发明实施例中，采用利用电力分词语料进行训练得到的BERT-CRF模型和电力领域词典对所述原始电力语料数据进行分词处理，能够将电力领域的实体作为一个整体进行切分，最大限度地保证电力专有名词不被分开。在其他的可选具体实施方式中，还可以采用其他的电力领域分词工具并结合所述电力领域词典对所述原始电力语料数据进行分词处理。传统的模型预训练阶段，采用非全词遮蔽的字符遮蔽方法，可能会导致词语在处理过程中出现部分遮蔽的问题。例如“变压器的检修规范”经过字符遮蔽，可能变成：“变”“”“器”“的”“检”“修”“规”“范”。其中，“变压器”的“压”字被单独遮蔽了。这种情况可能会使模型更关注局部的词语信息。上例中，模型从“变”和“器”字就能猜出“压”字，进而忽略了词语和整个句子之间的语义信息。而全词遮蔽则会对整个电力名词进行遮蔽，上例经过全词遮蔽后，变为：“”“”“”“的”“检”“修”“规”“范”。模型为了预测被遮蔽的电力名词“变压器”，需要从整个句子中挖掘被遮蔽词的语义信息，进而使模型建立起电力名词和整个句子之间的语义联系。本发明的一些具体实施方式中，所述对处理后得到的电力语料数据，采用全词遮蔽的方法，构建电力领域模型的预训练语料，包括：对所述处理后得到的电力语料数据采用预设概率进行随机全词遮蔽，将所有需要遮蔽的词语对应的字符中的一部分替换为随机字符、另一部分替换为遮蔽符号、剩余部分保留原来的字符不变。举例来说，可以对分词处理后得到的词语序列采用0.15的概率进行随机全词遮蔽，对所有需要遮蔽的词语对应的字符按照下述方法处理：按照10%替换为随机字符、80%替换为遮蔽符号，10%保留原来的字符不变的方法进行处理。另外，本发明实施例中，所述电力领域模型可以是基于BERT模型构建的，因此，为保持模型训练的一致性，在采用全词遮蔽的方法构建电力领域模型的预训练语料时，对每个已进行全词遮蔽处理的句子，在句首加入特殊符号、在句末加入特殊符号。请参阅图3，本发明实施例还提供一种电力领域模型的精调方法，包括：S301：针对下游任务构建训练用数据集；S302：将电力领域预训练模型中除输出层以外的其他网络结构作为底层编码器，并根据所述下游任务构建输出层网络结构，将所述输出层网络结构连接至所述底层编码器之后，得到针对下游任务的电力领域模型，所述电力领域预训练模型的预训练语料是通过对原始电力语料数据进行分词处理之后采用全词遮蔽得到的，且所述电力领域预训练模型包括注意力矩阵，所述注意力矩阵引入了词与词之间的相对位置编码；S303：利用所述训练用数据集对所述针对下游任务的电力领域模型进行训练。具体的，所述电力领域预训练模型可以是利用上述实施例所述的任一种电力领域模型预训练方法预训练得到。本发明实施例中，通过全词遮蔽的方式构建电力领域模型的预训练语料，避免了使用字符遮蔽方式构建电力领域模型的预训练语料时，模型能轻易猜出遮蔽的词语，而忽略了词语和整个句子之间的语义信息的问题，可以提升预训练模型的迁移能力。另外，本发明实施例还在构建的预训练模型，即电力领域模型中引入了词与词之间的相对位置建模，具体来说是增加了引入词与词之间的相对位置编码的注意力矩阵，从而可以使得模型更加关注词与词之间的相对位置，进而对词与词之间的相对位置更加敏感，从而使得预训练的电力领域模型不仅适用于预训练阶段的遮蔽词语预测任务，而且更容易迁移至下游任务。本发明实施例中，在电力领域模型的精调阶段，需要根据不同的下游任务设计不同的输出层网络结构。下面针对自然语言处理任务中的常见任务进行举例说明。一些具体的实施方式中，所述下游任务为分类任务，所述输出层网络结构为全连接网络；且所述底层编码器与所述全连接网络之间还包括第一网络结构；所述第一网络结构用于抽取所述底层编码器中的第一层和最后一层的编码向量并求平均，得到第一编码向量，再对各个词的所述第一编码向量取平均得到所述底层编码器的编码向量；所述全连接网络用于基于所述底层编码器的编码向量输出每个类别对应的置信度。另一些具体的实施方式中，所述下游任务为序列标注任务，所述输出层网络结构为条件随机场，且所述底层编码器与条件随机场层之间还包括Dropout层和映射层；所述底层编码器的输出为形状的张量，其中，batch_size为批大小、time_steps为序列长度、hidden_size为所述底层编码器的隐层单元大小；所述底层编码器的输出经过所述Dropout层和所述映射层转换为形状的张量，其中，num_classes为目标类的数量；所述条件随机场层用于基于所述形状的张量得到整个序列中每个元素的标签。该整个序列是指输入至针对序列标注任务的电力领域模型、待进行标注的序列。其中，条件随机场作为序列标注任务的标注结构。所述Dropout层用于对所述底层编码器输出的形状的张量中的元素以一定概率置零，可以增加模型的鲁棒性。经过Dropout的张量通过所述映射层转换为形状的张量。相应地，请参考图4，本发明实施例提供一种电力领域模型预训练装置，该装置包括：获取模块401，用于获取原始电力语料数据；处理模块402，用于对所述原始电力语料数据进行处理，所述处理至少包括分词处理；第一构建模块403，用于对处理后得到的电力语料数据，采用全词遮蔽的方法，构建电力领域模型的预训练语料；第二构建模块404，用于构建电力领域模型，所述电力领域模型包括注意力矩阵，所述注意力矩阵引入了词与词之间的相对位置编码；预训练模块405，用于利用所述预训练语料，对所述电力领域模型进行预训练。本发明实施例中，通过全词遮蔽的方式构建电力领域模型的预训练语料，避免了使用字符遮蔽方式构建电力领域模型的预训练语料时，模型能轻易猜出遮蔽的词语，而忽略了词语和整个句子之间的语义信息的问题，可以提升预训练模型的迁移能力。另外，本发明实施例还在构建的预训练模型，即电力领域模型中引入了词与词之间的相对位置建模，具体来说是增加了引入词与词之间的相对位置编码的注意力矩阵，从而可以使得模型更加关注词与词之间的相对位置，进而对词与词之间的相对位置更加敏感，从而使得预训练的电力领域模型不仅适用于预训练阶段的遮蔽词语预测任务，而且更容易迁移至下游任务。一些具体的实施方式中，引入了词与词之间的相对位置编码的所述注意力矩阵的算法公式为：Attention_rel= Attention +rel其中，Attention 为未引入所述相对位置编码的注意力矩阵的算法公式，rel是与词与词之间的相对位置有关的参数。一些具体的实施方式中，所述处理模块402用于采用BERT-CRF模型和电力领域词典对所述原始电力语料数据进行分词处理，所述BERT-CRF模型是利用电力分词语料进行训练得到。一些具体的实施方式中，所述第一构建模块403包括：遮蔽单元，用于对所述处理后得到的电力语料数据采用预设概率进行随机全词遮蔽，将所有需要遮蔽的词语对应的字符中的一部分替换为随机字符、另一部分替换为遮蔽符号、剩余部分保留原来的字符不变。本发明实施例是与上述电力领域模型预训练方法实施例基于相同的发明构思的装置实施例，因此具体的技术细节和对应的技术效果请参阅上述电力领域模型预训练方法实施例，此处不再赘述。相应地，请参考图5，本发明实施例提供一种电力领域模型的精调装置，该装置包括：第三构建模块501，用于针对下游任务构建训练用数据集；第四构建模块502，用于将电力领域预训练模型中除输出层以外的其他网络结构作为底层编码器，并根据所述下游任务构建输出层网络结构，将所述输出层网络结构连接至所述底层编码器之后，得到针对下游任务的电力领域模型，所述电力领域预训练模型的预训练语料是通过对原始电力语料数据进行分词处理之后采用全词遮蔽得到的，且所述电力领域预训练模型包括注意力矩阵，所述注意力矩阵引入了词与词之间的相对位置编码；训练模块503，用于利用所述训练用数据集对所述针对下游任务的电力领域模型进行训练。本发明实施例中，通过全词遮蔽的方式构建电力领域模型的预训练语料，避免了使用字符遮蔽方式构建电力领域模型的预训练语料时，模型能轻易猜出遮蔽的词语，而忽略了词语和整个句子之间的语义信息的问题，可以提升预训练模型的迁移能力。另外，本发明实施例还在构建的预训练模型，即电力领域模型中引入了词与词之间的相对位置建模，具体来说是增加了引入词与词之间的相对位置编码的注意力矩阵，从而可以使得模型更加关注词与词之间的相对位置，进而对词与词之间的相对位置更加敏感，从而使得预训练的电力领域模型不仅适用于预训练阶段的遮蔽词语预测任务，而且更容易迁移至下游任务。一些具体的实施方式中，所述下游任务为分类任务，所述输出层网络结构为全连接网络；且所述底层编码器与所述全连接网络之间还包括第一网络结构；所述第一网络结构用于抽取所述底层编码器中的第一层和最后一层的编码向量并求平均，得到第一编码向量，再对各个词的所述第一编码向量取平均得到所述底层编码器的编码向量；所述全连接网络用于基于所述底层编码器的编码向量输出每个类别对应的置信度。一些具体的实施方式中，所述下游任务为序列标注任务，所述输出层网络结构为条件随机场，且所述底层编码器与条件随机场层之间还包括Dropout层和映射层；所述底层编码器的输出为形状的张量，其中，batch_size为批大小、time_steps为序列长度、hidden_size为所述底层编码器的隐层单元大小；所述底层编码器的输出经过所述Dropout层和所述映射层转换为形状的张量，其中，num_classes为目标类的数量；所述条件随机场层用于基于所述形状的张量得到整个序列中每个元素的标签。本发明实施例是与上述电力领域模型的精调方法实施例基于相同的发明构思的装置实施例，因此具体的技术细节和对应的技术效果请参阅上述电力领域模型的精调方法实施例，此处不再赘述。本发明实施例还提供了一种电子设备，如图6所示，该电子设备可以包括处理器61和存储器62，其中处理器61和存储器62可以通过总线或者其他方式互相通信连接，图6中以通过总线连接为例。处理器61可以为中央处理器。处理器61还可以为其他通用处理器、数字信号处理器、专用集成电路、现场可编程门阵列或者其他可编程逻辑器件、分立门或者晶体管逻辑器件、分立硬件组件等芯片，或者上述各类芯片的组合。存储器62作为一种非暂态计算机可读存储介质，可用于存储非暂态软件程序、非暂态计算机可执行程序以及模块，如本发明实施例中的电力领域模型预训练方法对应的程序指令/模块或本发明实施例中的电力领域模型的精调方法对应的程序指令/模块。处理器61通过运行存储在存储器62中的非暂态软件程序、指令以及模块，从而执行处理器的各种功能应用以及数据处理，即实现上述方法实施例中的电力领域模型预训练方法或电力领域模型的精调方法。存储器62可以包括存储程序区和存储数据区，其中，存储程序区可存储操作系统、至少一个功能所需要的应用程序；存储数据区可存储处理器61所创建的数据等。此外，存储器62可以包括高速随机存取存储器，还可以包括非暂态存储器，例如至少一个磁盘存储器件、闪存器件、或其他非暂态固态存储器件。在一些实施例中，存储器62可选包括相对于处理器61远程设置的存储器，这些远程存储器可以通过网络连接至处理器61。上述网络的实例包括但不限于互联网、企业内部网、局域网、移动通信网及其组合。所述一个或者多个模块存储在所述存储器62中，当被所述处理器61执行时，执行上述方法实施例中的电力领域模型预训练方法或电力领域模型的精调方法。上述电子设备具体细节可以对应参阅上文中的方法实施例中对应的相关描述和效果进行理解，此处不再赘述。相应地，本发明实施例还提供一种计算机可读存储介质，所述计算机可读存储介质用于存储计算机程序，所述计算机程序被处理器执行时，实现上述电力领域模型预训练方法实施例的各个过程或者实现上述电力领域模型的精调方法实施例的各个过程，且能达到相同的技术效果，为避免重复，这里不再赘述。计算机可读介质包括永久性和非永久性、可移动和非可移动媒体可以由任何方法或技术来实现信息存储。信息可以是计算机可读指令、数据结构、程序的模块或其他数据。计算机的存储介质的例子包括，但不限于相变内存、静态随机存取存储器、动态随机存取存储器、其他类型的随机存取存储器、只读存储器、电可擦除可编程只读存储器、快闪记忆体或其他内存技术、只读光盘只读存储器、数字多功能光盘或其他光学存储、磁盒式磁带，磁带磁盘存储或其他磁性存储设备或任何其他非传输介质，可用于存储可以被计算设备访问的信息。按照本文中的界定，计算机可读介质不包括暂存电脑可读媒体，如调制的数据信号和载波。以上所述仅为本申请的实施例而已，并不用于限制本申请。对于本领域技术人员来说，本申请可以有各种更改和变化。凡在本申请的精神和原理之内所作的任何修改、等同替换、改进等，均应包含在本申请的权利要求范围之内。
