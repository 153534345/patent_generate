标题title
媒体资源推荐方法、装置、电子设备以及存储介质
摘要abst
本公开关于一种媒体资源推荐方法、装置、电子设备以及存储介质，属于互联网技术领域，该方法包括：针对目标对象的资源请求，通过获取在目标推荐阶段该目标对象对应的状态信息，将该状态信息输入融合参数分布预测模型，得到该目标对象对应的融合参数分布信息，从而根据该融合参数分布信息对候选媒体资源的多个预测互动结果进行融合，得到候选媒体资源的推荐参考信息以实现媒体资源推荐。在这一过程中，由于融合参数分布预测模型是以目标对象的资源请求为粒度来预测融合参数分布信息的，因此预测得到的融合参数分布信息具有个性化，能够有效改善媒体资源的推荐效果，提升对象体验。
权利要求书clms
1.一种媒体资源推荐方法，其特征在于，所述方法包括：响应于目标对象的资源请求，获取在目标推荐阶段所述目标对象对应的状态信息，所述状态信息指示所述目标推荐阶段对应的候选媒体资源的资源信息、所述目标对象的对象信息以及所述目标对象的历史互动行为；将所述目标对象对应的状态信息输入融合参数分布预测模型，得到所述目标对象对应的融合参数分布信息，所述融合参数分布信息指示融合参数的分布情况，所述融合参数用于在所述目标推荐阶段中对所述候选媒体资源的预测互动结果进行融合；基于所述目标对象对应的融合参数分布信息，对所述目标对象针对所述候选媒体资源的多个预测互动结果进行融合，得到所述候选媒体资源的推荐参考信息，基于所述推荐参考信息，向所述目标对象推荐所述候选媒体资源。2.根据权利要求1所述的媒体资源推荐方法，其特征在于，所述方法还包括：基于至少一个长期互动行为预测模型，对所述目标对象针对所述候选媒体资源的长期互动行为进行预测，得到所述候选媒体资源的至少一个长期预测互动结果；基于至少一个短期互动行为预测模型，对所述目标对象针对所述候选媒体资源的短期互动行为进行预测，得到所述候选媒体资源的至少一个短期预测互动结果。3.根据权利要求1所述的媒体资源推荐方法，其特征在于，所述基于所述目标对象对应的融合参数分布信息，对所述目标对象针对所述候选媒体资源的多个预测互动结果进行融合，得到所述候选媒体资源的推荐参考信息，包括：基于所述融合参数分布信息进行高斯采样，得到所述目标对象对应的融合参数向量，所述融合参数向量指示在所述目标推荐阶段对所述多个预测互动结果进行融合的多个融合参数；基于所述融合参数向量，对所述多个预测互动结果进行融合，得到所述推荐参考信息。4.根据权利要求1所述的媒体资源推荐方法，其特征在于，所述方法还包括：基于样本数据集，对基于媒体资源的多个互动行为对应的评估模型进行训练，得到训练后的多个评估模型，所述样本数据集包括样本对象在目标时间段内的多个样本资源请求所对应的样本数据，所述样本数据包括在样本推荐阶段所述样本对象对应的样本状态信息、样本融合参数分布信息以及样本对象反馈信息，所述评估模型用于评估所述样本融合参数分布信息对互动行为的影响程度，所述样本对象反馈信息指示对象基于媒体资源的互动行为；基于所述样本数据集和训练后的多个评估模型，将训练后的多个评估模型对所述融合参数分布预测模型的评估结果作为奖励值，对所述融合参数分布预测模型进行训练，得到训练后的所述融合参数分布预测模型。5.根据权利要求4所述的媒体资源推荐方法，其特征在于，所述基于样本数据集，对基于媒体资源的多个互动行为对应的评估模型进行训练，得到训练后的多个评估模型，包括：基于第i个样本数据中的样本状态信息和第一评估模型，获取第一奖励值，所述第一评估模型为基于媒体资源的多个互动行为中任一个互动行为对应的评估模型，所述第i个样本数据为所述样本对象在目标时间段内第i次样本资源请求对应的样本数据，i为正整数；基于第i+1个样本数据中的样本状态信息和所述第一评估模型，获取第二奖励值，所述第i+1个样本数据为所述样本对象在目标时间段内第i+1次样本资源请求对应的样本数据；基于所述第i个样本数据中的样本对象反馈信息、所述第一奖励值、所述第二奖励值以及折扣系数，计算第一损失值；基于所述第一损失值，对所述第一评估模型的模型参数进行更新，直至满足训练结束条件，得到训练后的所述第一评估模型。6.根据权利要求4所述的媒体资源推荐方法，其特征在于，所述基于所述样本数据集和训练后的多个评估模型，将训练后的多个评估模型对所述融合参数分布预测模型的评估结果作为奖励值，对所述融合参数分布预测模型进行训练，得到训练后的所述融合参数分布预测模型，包括：基于所述样本数据集和训练后的至少一个短期互动行为对应的评估模型，对所述融合参数分布预测模型的模型参数进行更新，直至满足第一训练条件，得到中间融合参数分布预测模型；基于所述样本数据集和训练后的至少一个长期互动行为对应的评估模型，对所述中间融合参数分布预测模型的模型参数进行更新，直至满足第二训练条件，得到训练后的所述融合参数分布预测模型。7.一种媒体资源推荐装置，其特征在于，所述装置包括：获取单元，被配置为执行响应于目标对象的资源请求，获取在目标推荐阶段所述目标对象对应的状态信息，所述状态信息指示所述目标推荐阶段对应的候选媒体资源的资源信息、所述目标对象的对象信息以及所述目标对象的历史互动行为；输入单元，被配置为执行将所述目标对象对应的状态信息输入融合参数分布预测模型，得到所述目标对象对应的融合参数分布信息，所述融合参数分布信息指示融合参数的分布情况，所述融合参数用于在所述目标推荐阶段中对所述候选媒体资源的预测互动结果进行融合；融合单元，被配置为执行基于所述目标对象对应的融合参数分布信息，对所述目标对象针对所述候选媒体资源的多个预测互动结果进行融合，得到所述候选媒体资源的推荐参考信息，基于所述推荐参考信息，向所述目标对象推荐所述候选媒体资源。8.一种电子设备，其特征在于，所述电子设备包括：一个或多个处理器；用于存储所述处理器可执行程序代码的存储器；其中，所述处理器被配置为执行所述程序代码，以实现如权利要求1至6中任一项所述的媒体资源推荐方法。9.一种计算机可读存储介质，其特征在于，当所述计算机可读存储介质中的程序代码由电子设备的处理器执行时，使得电子设备能够执行如权利要求1至6中任一项所述的媒体资源推荐方法。10.一种计算机程序产品，包括计算机程序，其特征在于，所述计算机程序被处理器执行时实现权利要求1至6中任一项所述的媒体资源推荐方法。
说明书desc
技术领域本公开涉及互联网技术领域，尤其涉及一种媒体资源推荐方法、装置、电子设备以及存储介质。背景技术随着互联网的快速发展和广泛普及，在一些媒体资源的推荐业务场景中，通过对召回的候选媒体资源经过粗排、精排、重排以及混排等多个媒体资源推荐阶段实现多目标排序，将最终排序后的候选媒体资源推荐给目标对象，以满足目标对象的需求。相关技术中，通过交叉熵算法或贝叶斯优化算法等自动调参的方式，对媒体资源推荐阶段中用于融合候选媒体资源的多个预测互动结果的融合参数进行及时调整，以确保媒体资源推荐的准确率。然而，采用上述方式得到的融合参数是非个性化的，即针对不同对象推荐媒体资源时所涉及的融合参数均相同，从而导致媒体资源推荐的效果不佳，影响对象体验。发明内容本公开提供一种媒体资源推荐方法、装置、电子设备以及存储介质，能够有效改善媒体资源的推荐效果，提升对象体验。本公开的技术方案如下：根据本公开实施例的第一方面，提供一种媒体资源推荐方法，该方法包括：响应于目标对象的资源请求，获取在目标推荐阶段该目标对象对应的状态信息，该状态信息指示该目标推荐阶段对应的候选媒体资源的资源信息、该目标对象的对象信息以及该目标对象的历史互动行为；将该目标对象对应的状态信息输入融合参数分布预测模型，得到该目标对象对应的融合参数分布信息，该融合参数分布信息指示融合参数的分布情况，该融合参数用于在该目标推荐阶段中对该候选媒体资源的预测互动结果进行融合；基于该目标对象对应的融合参数分布信息，对该目标对象针对该候选媒体资源的多个预测互动结果进行融合，得到该候选媒体资源的推荐参考信息，基于该推荐参考信息，向该目标对象推荐该候选媒体资源。在该方法中，针对目标对象的资源请求，通过获取在目标推荐阶段该目标对象对应的状态信息，将该状态信息输入融合参数分布预测模型，得到该目标对象对应的融合参数分布信息，从而根据该融合参数分布信息对候选媒体资源的多个预测互动结果进行融合，得到候选媒体资源的推荐参考信息以实现媒体资源推荐。在这一过程中，由于融合参数分布预测模型是以目标对象的资源请求为粒度来预测融合参数分布信息的，因此预测得到的融合参数分布信息具有个性化，能够有效改善媒体资源的推荐效果，提升对象体验。在一些实施例中，该方法还包括：基于至少一个长期互动行为预测模型，对该目标对象针对该候选媒体资源的长期互动行为进行预测，得到该候选媒体资源的至少一个长期预测互动结果；基于至少一个短期互动行为预测模型，对该目标对象针对该候选媒体资源的短期互动行为进行预测，得到该候选媒体资源的至少一个短期预测互动结果。通过这种方式，本公开实施例提供的融合参数分布预测模型所输出的融合参数分布信息能够融合媒体资源的长期预测互动结果和短期预测互动结果，从而既优化了媒体资源推荐的短期互动指标又优化了长期互动指标，能够有效改善媒体资源推荐效果，提升对象体验。在一些实施例中，基于该融合参数分布信息，对该目标对象针对该候选媒体资源的多个预测互动结果进行融合，得到该候选媒体资源的推荐参考信息，包括：基于该融合参数分布信息进行高斯采样，得到该目标对象对应的融合参数向量，该融合参数向量指示在该目标推荐阶段对该多个预测互动结果进行融合的多个融合参数；基于该融合参数向量，对该多个预测互动结果进行融合，得到该推荐参考信息。通过这种融合参数向量的形式，来对多个预测互动结果进行融合，简化了推荐参考信息的获取方式，提高了媒体资源推荐效率。在一些实施例中，该方法还包括：基于样本数据集，对基于媒体资源的多个互动行为对应的评估模型进行训练，得到训练后的多个评估模型，该样本数据集包括样本对象在目标时间段内的多个样本资源请求所对应的样本数据，该样本数据包括在样本推荐阶段该样本对象对应的样本状态信息、样本融合参数分布信息以及样本对象反馈信息，该评估模型用于评估该样本融合参数分布信息对互动行为的影响程度，该样本对象反馈信息指示对象基于媒体资源的互动行为；基于该样本数据集和训练后的多个评估模型，将训练后的多个评估模型对该融合参数分布预测模型的评估结果作为奖励值，对该融合参数分布预测模型进行训练，得到训练后的该融合参数分布预测模型。通过这种方式，采用强化学习，利用评估模型对融合参数分布预测模型学习到的策略进行评估，以便提高融合参数分布预测模型的准确率。其中，该目标时间段为目标对象每次打开终端上运行的应用程序至退出该应用程序的时间段，通过以目标时间段为优化窗口来训练融合参数分布预测模型，能够最大化目标时间段内的互动指标，有效改善媒体资源推荐效果，提升对象体验。在一些实施例中，该基于样本数据集，对基于媒体资源的多个互动行为对应的评估模型进行训练，得到训练后的多个评估模型，包括：基于第i个样本数据中的样本状态信息和第一评估模型，获取第一奖励值，该第一评估模型为基于媒体资源的多个互动行为中任一个互动行为对应的评估模型，该第i个样本数据为该样本对象在目标时间段内第i次样本资源请求对应的样本数据，i为正整数；基于第i+1个样本数据中的样本状态信息和该第一评估模型，获取第二奖励值，该第i+1个样本数据为该样本对象在目标时间段内第i+1次样本资源请求对应的样本数据；基于该第i个样本数据中的样本对象反馈信息、该第一奖励值、该第二奖励值以及折扣系数，计算第一损失值；基于该第一损失值，对该第一评估模型的模型参数进行更新，直至满足训练结束条件，得到训练后的该第一评估模型。通过这种方式，对每个互动行为对应的评估模型均进行了训练，为融合参数分布预测模型的参数更新提供了基础，从而既优化了媒体资源推荐的短期互动指标又优化了长期互动指标，能够有效改善媒体资源推荐效果，提升对象体验。在一些实施例中，该基于该样本数据集和训练后的多个评估模型，将训练后的多个评估模型对该融合参数分布预测模型的评估结果作为奖励值，对该融合参数分布预测模型进行训练，得到训练后的该融合参数分布预测模型，包括：基于该样本数据集和训练后的至少一个短期互动行为对应的评估模型，对该融合参数分布预测模型的模型参数进行更新，直至满足第一训练条件，得到中间融合参数分布预测模型；基于该样本数据集和训练后的至少一个长期互动行为对应的评估模型，对该中间融合参数分布预测模型的模型参数进行更新，直至满足第二训练条件，得到训练后的该融合参数分布预测模型。通过这种两阶段训练的方式，能够在确保媒体资源的长期互动指标满足需求的条件下，进一步优化媒体资源的短期互动指标，从而有效改善媒体资源推荐效果，提升对象体验。在一些实施例中，该基于该样本数据集和训练后的至少一个短期互动行为对应的评估模型，对该融合参数分布预测模型进行训练，直至满足第一训练条件，得到中间融合参数分布预测模型，包括：基于第i个样本数据和第二评估模型，获取该融合参数分布预测模型基于该第二评估模型所评估的短期互动行为的第一优势函数值，该第二评估模型为任一个短期互动行为对应的评估模型；基于该第i个样本数据和该融合参数分布预测模型，获取第一概率密度，该第一概率密度指示该融合参数分布预测模型基于该第i个样本数据中的样本状态信息输出该第i个样本数据中的样本融合参数分布信息的概率密度；基于该第一优势函数值和该第一概率密度，计算第二损失值；基于该第二损失值，对该融合参数分布预测模型的模型参数进行更新，直至满足该第一训练条件，得到该中间融合参数分布预测模型。通过这种方式，实现了融合参数分布预测模型的第一训练阶段，即基于每个短期互动行为对应的评估模型，指导融合参数分布预测模型的参数更新。这一过程也可以理解为将每个短期互动行为作为融合参数分布预测模型的辅助目标，利用强化学习的方式，通过和环境交互学习来优化每个辅助目标对应的优势函数，或者说对于每一个辅助目标学习一个策略来优化该目标，直至满足训练条件，得到中间融合参数分布预测模型。在一些实施例中，该基于该样本训练集和训练后的至少一个长期互动行为对应的评估模型，对该中间融合参数分布预测模型进行训练，直至满足第二训练条件，得到训练后的该融合参数分布预测模型，包括：基于第i个样本数据、该多个互动行为的约束参数以及该中间融合参数分布预测模型，获取目标权重，该目标权重指示该至少一个短期互动行为对应的融合参数分布信息与该第i个样本数据中的样本融合参数分布信息之间的偏离程度；基于该第i个样本数据和第三评估模型，获取该中间融合参数分布预测模型基于该第三评估模型所评估的长期互动行为的第二优势函数值，该第三评估模型为任一个长期互动行为对应的评估模型；基于该第i个样本数据和该中间融合参数分布预测模型，获取第二概率密度，该第二概率密度指示该中间融合参数分布预测模型基于该第i个样本数据中的样本状态信息输出该第i个样本数据中的样本融合参数分布信息的概率密度；基于该多个互动行为的约束参数、该目标权重、该第二优势函数值以及该第二概率密度，计算第三损失值；基于该第三损失值，对该中间融合参数分布预测模型的模型参数进行更新，直至满足该第二训练条件，得到训练后的该融合参数分布预测模型。通过这种方式，实现了融合参数分布预测模型的第二训练阶段，即基于每个长期互动行为对应的评估模型，指导中间融合参数分布预测模型的参数更新。这一过程也可以理解为将长期互动行为作为融合参数分布预测模型的主目标，利用强化学习的方式，通过和环境交互学习来优化主目标对应的优势函数，在这一过程中，通过多个约束参数，来约束长期互动行为对应的融合参数分布信息与短期互动行为对应的融合参数分布信息之间的偏离程度，以使在优化主目标对应的优势函数时保证其不会距离其他辅助目标策略太远，或者说在优化主目标的同时让策略不要偏离每一个辅助目标对应策略太远，直至满足训练条件，得到训练后的融合参数分布预测模型。根据本公开实施例的第二方面，提供一种媒体资源推荐装置，该装置包括：获取单元，被配置为执行响应于目标对象的资源请求，获取在目标推荐阶段该目标对象对应的状态信息，该状态信息指示该目标推荐阶段对应的候选媒体资源的资源信息、该目标对象的对象信息以及该目标对象的历史互动行为；输入单元，被配置为执行将该目标对象对应的状态信息输入融合参数分布预测模型，得到该目标对象对应的融合参数分布信息，该融合参数分布信息指示融合参数的分布情况，该融合参数用于在该目标推荐阶段中对该候选媒体资源的预测互动结果进行融合；融合单元，被配置为执行基于该目标对象对应的融合参数分布信息，对该目标对象针对该候选媒体资源的多个预测互动结果进行融合，得到该候选媒体资源的推荐参考信息，基于该推荐参考信息，向该目标对象推荐该候选媒体资源。在一些实施例中，该装置还包括：第一预测单元，被配置为执行基于至少一个长期互动行为预测模型，对该目标对象针对该候选媒体资源的长期互动行为进行预测，得到该候选媒体资源的至少一个长期预测互动结果；第二预测单元，被配置为执行基于至少一个短期互动行为预测模型，对该目标对象针对该候选媒体资源的短期互动行为进行预测，得到该候选媒体资源的至少一个短期预测互动结果。在一些实施例中，该融合单元，被配置为执行：基于该融合参数分布信息进行高斯采样，得到该目标对象对应的融合参数向量，该融合参数向量指示在该目标推荐阶段对该多个预测互动结果进行融合的多个融合参数；基于该融合参数向量，对该多个预测互动结果进行融合，得到该推荐参考信息。在一些实施例中，该装置还包括：第一训练单元，被配置为执行基于样本数据集，对基于媒体资源的多个互动行为对应的评估模型进行训练，得到训练后的多个评估模型，该样本数据集包括样本对象在目标时间段内的多个样本资源请求所对应的样本数据，该样本数据包括在样本推荐阶段该样本对象对应的样本状态信息、样本融合参数分布信息以及样本对象反馈信息，该评估模型用于评估该样本融合参数分布信息对互动行为的影响程度，该样本对象反馈信息指示对象基于媒体资源的互动行为；第二训练单元，被配置为执行基于该样本数据集和训练后的多个评估模型，将训练后的多个评估模型对该融合参数分布预测模型的评估结果作为奖励值，对该融合参数分布预测模型进行训练，得到训练后的该融合参数分布预测模型。在一些实施例中，该第一训练单元，被配置为执行：基于第i个样本数据中的样本状态信息和第一评估模型，获取第一奖励值，该第一评估模型为基于媒体资源的多个互动行为中任一个互动行为对应的评估模型，该第i个样本数据为该样本对象在目标时间段内第i次样本资源请求对应的样本数据，i为正整数；基于第i+1个样本数据中的样本状态信息和该第一评估模型，获取第二奖励值，该第i+1个样本数据为该样本对象在目标时间段内第i+1次样本资源请求对应的样本数据；基于该第i个样本数据中的样本对象反馈信息、该第一奖励值、该第二奖励值以及折扣系数，计算第一损失值；基于该第一损失值，对该第一评估模型的模型参数进行更新，直至满足训练结束条件，得到训练后的该第一评估模型。在一些实施例中，该第二训练单元包括：第一训练子单元，被配置为执行基于该样本数据集和训练后的至少一个短期互动行为对应的评估模型，对该融合参数分布预测模型的模型参数进行更新，直至满足第一训练条件，得到中间融合参数分布预测模型；第二训练子单元，被配置为执行基于该样本数据集和训练后的至少一个长期互动行为对应的评估模型，对该中间融合参数分布预测模型的模型参数进行更新，直至满足第二训练条件，得到训练后的该融合参数分布预测模型。在一些实施例中，该第一训练子单元，被配置为执行：基于第i个样本数据和第二评估模型，获取该融合参数分布预测模型基于该第二评估模型所评估的短期互动行为的第一优势函数值，该第二评估模型为任一个短期互动行为对应的评估模型；基于该第i个样本数据和该融合参数分布预测模型，获取第一概率密度，该第一概率密度指示该融合参数分布预测模型基于该第i个样本数据中的样本状态信息输出该第i个样本数据中的样本融合参数分布信息的概率密度；基于该第一优势函数值和该第一概率密度，计算第二损失值；基于该第二损失值，对该融合参数分布预测模型的模型参数进行更新，直至满足该第一训练条件，得到该中间融合参数分布预测模型。在一些实施例中，该第二训练子单元，被配置为执行：基于第i个样本数据、该多个互动行为的约束参数以及该中间融合参数分布预测模型，获取目标权重，该目标权重指示该至少一个短期互动行为对应的融合参数分布信息与该第i个样本数据中的样本融合参数分布信息之间的偏离程度；基于该第i个样本数据和第三评估模型，获取该中间融合参数分布预测模型基于该第三评估模型所评估的长期互动行为的第二优势函数值，该第三评估模型为任一个长期互动行为对应的评估模型；基于该第i个样本数据和该中间融合参数分布预测模型，获取第二概率密度，该第二概率密度指示该中间融合参数分布预测模型基于该第i个样本数据中的样本状态信息输出该第i个样本数据中的样本融合参数分布信息的概率密度；基于该多个互动行为的约束参数、该目标权重、该第二优势函数值以及该第二概率密度，计算第三损失值；基于该第三损失值，对该中间融合参数分布预测模型的模型参数进行更新，直至满足该第二训练条件，得到训练后的该融合参数分布预测模型。根据本公开实施例的第三方面，提供一种电子设备，该电子设备包括：一个或多个处理器；用于存储该处理器可执行程序代码的存储器；其中，该处理器被配置为执行该程序代码，以实现上述的媒体资源推荐方法。根据本公开实施例的第四方面，提供一种计算机可读存储介质，该计算机可读存储介质包括：当该计算机可读存储介质中的程序代码由电子设备的处理器执行时，使得电子设备能够执行上述的媒体资源推荐方法。根据本公开实施例的第五方面，提供一种计算机程序产品，包括计算机程序，该计算机程序被处理器执行时实现上述的媒体资源推荐方法。应当理解的是，以上的一般描述和后文的细节描述仅是示例性和解释性的，并不能限制本公开。附图说明此处的附图被并入说明书中并构成本说明书的一部分，示出了符合本公开的实施例，并与说明书一起用于解释本公开的原理，并不构成对本公开的不当限定。图1是根据一示例性实施例示出的一种实施环境示意图；图2是根据一示例性实施例示出的一种用于推荐阶段的融合参数优化系统；图3是根据一示例性实施例示出的一种媒体资源推荐方法的流程图；图4是根据一示例性实施例示出的一种用于推荐阶段的融合参数分布预测模型的训练方法的流程图；图5是根据一示例性实施例示出的一种媒体资源推荐装置的框图；图6是根据一示例性实施例示出的一种终端的框图；图7是根据一示例性实施例示出的一种服务器的框图。具体实施方式为了使本领域普通人员更好地理解本公开的技术方案，下面将结合附图，对本公开实施例中的技术方案进行清楚、完整地描述。需要说明的是，本公开的说明书和权利要求书及上述附图中的术语“第一”、“第二”等是用于区别类似的对象，而不必用于描述特定的顺序或先后次序。应该理解这样使用的数据在适当情况下可以互换，以便这里描述的本公开的实施例能够以除了在这里图示或描述的那些以外的顺序实施。以下示例性实施例中所描述的实施方式并不代表与本公开相一致的所有实施方式。相反，它们仅是与如所附权利要求书中所详述的、本公开的一些方面相一致的装置和方法的例子。需要说明的是，本公开所涉及的信息、数据以及信号，均为经用户授权或者经过各方充分授权的，且相关数据的收集、使用和处理需要遵守相关国家和地区的相关法律法规和标准。例如，本公开实施例中涉及到的目标对象的对象信息、目标对象针对媒体资源的互动行为等都是在充分授权的情况下获取的。在一些实施例中，本公开实施例提供有权限询问页面，该权限询问页面用于询问是否授予上述信息的获取权限，在该权限询问页面中，显示同意授权控件和拒绝授权控件，在检测到对该同意授权控件的触发操作的情况下，利用本公开实施例所提供的媒体资源推荐方法来获取上述信息，从而实现对媒体资源的推荐。图1是本公开实施例提供的一种实施环境示意图，参见图1，该实施环境包括：终端101和服务器102。终端101可以为智能手机、智能手表、台式电脑、手提电脑、虚拟现实终端、增强现实终端、无线终端和膝上型便携计算机等设备中的至少一种。终端101具有通信功能，可以接入有线网络或无线网络。终端101可以泛指多个终端中的一个，本实施例仅以终端101来举例说明。本领域技术人员可以知晓，上述终端的数量可以更多或更少。示意性地，终端101能够安装和运行有应用程序，该应用程序用于为对象提供针对媒体资源的浏览服务，如视频类应用程序、音频类应用程序等等，对此不作限定。服务器102可以是独立的物理服务器，也可以是多个物理服务器构成的服务器集群或者分布式文件系统，还可以是提供云服务、云数据库、云计算、云函数、云存储、网络服务、云通信、中间件服务、域名服务、安全服务、CDN、以及大数据和人工智能平台等基础云计算服务的云服务器。在一些实施例中，服务器102与终端101通过有线或无线通信方式进行直接或间接的连接，本公开实施例对此不作限定。可选地，上述服务器102的数量可以更多或更少，本公开实施例对此不加以限定。当然，服务器102还可以包括其他功能服务器，以便提供更全面且多样化的服务。在一些实施例中，在该媒体资源推荐过程中，服务器102承担主要计算工作，终端101承担次要计算工作；或者，服务器102承担次要计算工作，终端101承担主要计算工作；或者，服务器102或终端101分别能够单独承担计算工作，本申请实施例对此不作限定。在介绍本公开实施例提供的媒体资源推荐方法之前，为便于理解，下面先对本公开实施例的应用场景进行介绍。示意性地，在一些媒体资源的推荐业务场景中，在对象请求获取媒体资源的情况下，对召回的候选媒体资源经过粗排、精排、重排以及混排等多个媒体资源推荐阶段实现多目标排序，将最终排序后的候选媒体资源推荐给对象，以满足对象的需求。在这一过程中，各个推荐阶段在排序过程中基于多个融合参数对候选媒体资源的多个预测互动结果进行融合，得到候选媒体资源的推荐参考信息，从而按照候选媒体资源的推荐参考信息进行排序后推荐给对象，其中，候选媒体资源的预测互动结果是指对象针对候选媒体资源的预测互动行为，通过互动行为预测模型进行预测得到，该推荐参考信息指示向对象推荐候选媒体资源的顺序。基于此，本公开实施例提供了一种基于强化学习的融合参数分布预测模型，能够基于对象针对媒体资源的资源请求，预测目标推荐阶段该目标对象的融合参数分布信息，从而根据该融合参数分布信息对候选媒体资源的多个预测互动结果进行融合，得到候选媒体资源的推荐参考信息以实现媒体资源推荐，该融合参数分布信息指示融合参数的分布情况，该融合参数用于在该目标推荐阶段中对该候选媒体资源的预测互动结果进行融合。在这一过程中，由于该融合参数分布预测模型是以对象的资源请求为粒度来实现融合参数分布信息预测的，因此预测得到的融合参数分布信息具有个性化，能够有效改善媒体资源的推荐效果，提升对象体验。另外，需要说明的是，该目标推荐阶段是指媒体资源推荐过程中所涉及的任意一个推荐阶段，本公开实施例对此不作限定。例如，以媒体资源为视频为例，在对象请求获取视频的情况下，服务器召回N个候选视频以用于视频推荐的精排阶段，对于任一个候选视频，通过多个互动行为预测模型来分别预测对象针对该候选视频的互动行为，从而得到该候选视频的多个预测互动结果，如多个预测互动结果包括视频观看时长、浏览深度、点击率、点赞率、评论率、转发率以及关注率等等，对此不作限定。进一步地，服务器基于对象针对视频的资源请求，通过融合参数分布预测模型，预测精排阶段该对象的融合参数分布信息，根据该融合参数分布信息对上述多个预测互动结果进行融合，得到该候选视频的推荐参考信息，按照该候选视频的推荐参考信息对候选视频进行精排后推荐给对象。下面对媒体资源推荐过程中如何应用上述融合参数分布预测模型以及如何训练该融合参数分布预测模型的过程进行介绍。示意性地，参考图2，图2是本公开实施例提供的一种用于推荐阶段的融合参数优化系统。如图2所示，该融合参数优化系统包括下述三个部分：在线部分、数据流部分以及训练部分。通过这三个部分之间的协同作用，在媒体资源的推荐过程中实现自动寻参，从而有效改善媒体资源推荐效果，提升对象体验。下面基于图2，对这三个部分的功能分别进行介绍。在线部分以目标对象为例，在线部分用于在媒体资源的目标推荐阶段中，响应于目标对象针对媒体资源的资源请求，获取在该目标推荐阶段该目标对象对应的状态信息，将该目标对象对应的状态信息输入融合参数分布预测模型，得到该目标对象对应的融合参数分布信息，基于该融合参数分布信息，对目标对象针对候选媒体资源的多个预测互动结果进行融合，得到候选媒体资源的推荐参考信息，基于该推荐参考信息，向该目标对象推荐媒体资源。其中，该目标推荐阶段该目标对象对应的状态信息指示该目标推荐阶段对应的候选媒体资源的资源信息、该目标对象的对象信息以及该目标对象的历史互动行为。例如，以媒体资源为视频为例，候选媒体资源的资源信息包括视频内容、视频类型、视频长度以及视频上下文信息等；目标对象的对象信息包括目标对象的画像信息如年龄、性别以及所在地区等等；目标对象的历史互动行为包括该目标对象在目标时间段内基于历史资源请求观看视频的时长、点赞、点击、关注次数以及评论次数等互动行为、目标对象的移动平均特征等，对此不作限定。在一些实施例中，目标时间段为目标对象每次打开终端上运行的应用程序至退出该应用程序的时间段。在一些实施例中，该状态信息还指示该目标对象的浏览深度以及在该资源请求之前的资源请求的相关信息等，对此不作限定。示意性地，在线部分根据目标对象的对象标识，从数据库数据库)中获取该目标对象的对象信息和历史互动行为等，该数据库中的数据基于目标对象的客户端日志得到，对此不作限定。在一些实施例中，在线部分基于融合参数分布信息进行高斯采样，得到该目标对象对应的融合参数向量，该融合参数向量指示在目标推荐阶段对多个预测互动结果进行融合的多个融合参数；基于该融合参数向量，对候选媒体资源的多个预测互动结果进行融合，得到候选媒体资源的推荐参考信息。其中，融合参数分布信息为M维参数的高斯分布，μ为M维参数的期望，σ为M维参数的标准差，基于该融合参数分布信息进行高斯采样，能够得到M维参数向量，该M维参数向量即上述融合参数向量，该融合参数向量包括M个参数值，用于融合候选媒体资源的M个预测互动结果。需要说明的是，在线部分基于融合参数分布预测模型实现媒体资源推荐的具体实现方式会在后续方法实施例中进行详细介绍，在此不再赘述。数据流部分数据流部分用于收集在线部分所产生的数据，将这些收集到的数据作为样本资源请求，对融合参数分布预测模型进行训练。示意性地，以目标对象为例，将该目标对象在目标时间段内的资源请求按照时间顺序排序，形成一组用于训练融合参数分布预测模型的资源请求记录。下面以目标对象的一组资源请求记录中任一资源请求为例，对该资源请求所对应的数据进行介绍，该资源请求所对应的数据包括如下几个部分：1.在目标推荐阶段该目标对象对应的状态信息，包括目标推荐阶段对应的候选媒体资源的资源信息、该目标对象的对象信息以及该目标对象的历史互动行为等。2.在目标推荐阶段该目标对象对应的动作信息，包括基于该资源请求得到的该目标对象对应的融合参数分布信息和相应的融合参数向量。3.资源请求的对象反馈信息，用于指示目标对象针对媒体资源的互动行为。在一些实施例中，该对象反馈信息为向量形式。例如，以媒体资源为视频为例，该对象反馈信息为基于视频播放时长、点赞、评论、转发、关注以及观看视频数量等互动行为得到的拼接向量，对此不作限定。训练部分训练部分用于根据数据流部分收集到的数据对融合参数分布预测模型进行训练和优化。在训练过程中，将目标时间段作为优化窗口，以目标时间段内样本对象的资源请求作为样本资源请求，组成样本数据集，以强化学习的方式，对融合参数分布预测模型进行训练和优化。示意性地，以样本数据集中第i个样本数据为例，该第i个样本数据包括：基于第i次样本资源请求得到的样本对象对应的样本状态信息、样本动作信息、样本对象反馈信息、基于第i+1次样本资源请求得到的样本对象对应的样本状态信息以及标志位。其中，标志位用于标志第i次样本资源请求是否为样本对象在目标时间段内的最后一次资源请求。例如，标志位为0或1，在标志位为1的情况下，表明第i次样本资源请求为样本对象在目标时间段内的最后一次资源请求，在标志位为0的情况下，表明第i次样本资源请求不是样本对象在目标时间段内的最后一次资源请求。在本公开实施例中，训练部分基于强化学习的方式，将融合参数分布预测模型作为Actor网络，将多个互动行为对应的评估模型作为Critic网络来进行训练。其中，该融合参数分布预测模型和多个互动行为对应的评估模型均为基于深度神经网络构建的模型。示意性地，该融合参数分布预测模型用于根据样本对象对应的样本状态信息输出相应的融合参数分布信息，该评估模型用于根据样本对象对应的样本状态信息输出相应的奖励值，该奖励值用于指导融合参数分布预测模型的参数更新。需要说明的是，训练部分的具体训练过程会在后续方法实施例中进行详细介绍，在此不再赘述。在基于上述图2介绍了用于推荐阶段的融合参数优化系统的基础上，下面通过几个方法实施例，对媒体资源推荐阶段如何应用融合参数分布预测模型实现媒体资源推荐以及如何训练该融合参数分布预测模型的过程进行介绍。图3是本公开实施例提供的一种媒体资源推荐方法的流程图。如图3所示，以该方法由服务器执行，应用于媒体资推荐过程中涉及的任一个推荐阶段为例进行介绍，示意性地，该方法包括下述步骤301至步骤303。在步骤301中，服务器响应于目标对象的资源请求，获取在目标推荐阶段该目标对象对应的状态信息。在本公开实施例中，该资源请求是指针对媒体资源的资源获取请求。在一些实施例中，目标对象所使用的终端运行有提供媒体资源浏览服务的应用程序，终端响应于目标对象对该应用程序的操作，向服务器发送资源请求。例如，以终端运行有视频类应用程序为例，终端响应于目标对象对该应用程序的打开操作，向服务器发送资源请求，或者，终端响应于目标对象在该应用程序中的视频搜索操作，向服务器发送资源请求，又或者，终端响应于基于第h次资源请求获取到的视频已播放完毕，向服务器发送第h+1次资源请求，等等，本申请实施例对于终端向服务器发送资源请求的时机不作限定。在目标推荐阶段该目标对象对应的状态信息指示该目标推荐阶段对应的候选媒体资源的资源信息、该目标对象的对象信息以及该目标对象的历史互动行为。在一些实施例中，该状态信息还指示该目标对象的浏览深度以及在该资源请求之前的资源请求的相关信息等，对此不作限定。示意性地，对于目标对象的任一次资源请求，服务器基于该资源请求，获取多个候选媒体资源，基于该多个候选媒体资源、该目标对象的对象信息以及该目标对象的历史互动行为，获取在目标推荐阶段该目标对象对应的状态信息。需要说明的是，该目标对象对应的状态信息的具体内容与前述图2所示同理，在此不再赘述。在步骤302中，服务器将该目标对象对应的状态信息输入融合参数分布预测模型，得到该目标对象对应的融合参数分布信息。在本公开实施例中，该融合参数分布信息指示融合参数的分布情况，该融合参数用于在该目标推荐阶段中对该候选媒体资源的预测互动结果进行融合，该预测互动结果指示该目标对象针对该候选媒体资源的预测互动行为。在一些实施例中，对于任一个候选媒体资源，服务器基于多个互动行为预测模型，预测目标对象针对该候选媒体资源的互动行为，得到该候选媒体资源的多个预测互动结果。例如，以媒体资源为视频为例，该多个互动行为预测模型包括视频观看时长预测模型、点击率预测模型、点赞率预测模型以及关注预测模型等等，服务器基于这些互动行为预测模型预测得到该候选媒体资源的多个预测互动结果，包括预测视频观看时长、预测点击率、预测点赞率以及预测关注率等等，本公开实施例对于该多个预测互动结果的类型不作限定，在实际应用中能够根据需求对目标对象针对候选媒体资源的互动行为进行预测。在一些实施例中，服务器基于至少一个长期互动行为预测模型，对该目标对象针对该候选媒体资源的长期互动行为进行预测，得到该候选媒体资源的至少一个长期预测互动结果；基于至少一个短期互动行为预测模型，对该目标对象针对该候选媒体资源的短期互动行为进行预测，得到该候选媒体资源的至少一个短期预测互动结果。示意性地，长期互动行为是指目标对象与候选媒体资源之间基于持续性交互所产生的互动行为，例如，以媒体资源为视频为例，该长期互动行为包括视频观看时长和浏览深度等，又例如，以媒体资源为音频为例，该长期互动行为包括音频播放时长和浏览深度等。短期互动行为是指目标对象与候选媒体资源之间基于暂时性交互所产生的互动行为，例如，以媒体资源为视频为例，该短期互动行为包括点赞、评论、转发以及关注等等。通过这种方式，本公开实施例提供的融合参数分布预测模型所输出的融合参数分布信息能够融合媒体资源的长期预测互动结果和短期预测互动结果，从而既优化了媒体资源推荐的短期互动指标又优化了长期互动指标，能够有效改善媒体资源推荐效果，提升对象体验。例如，以媒体资源为视频为例，在视频推荐场景中，往往存在短期且稀疏的互动行为和长期且稠密的互动行为，这两类互动行为出现的频率并不相同，如目标对象在观看某一视频的情况下一定会产生视频观看时长这一互动行为，但并不一定会产生点赞这一互动行为，因此，通过本公开实施例提供的融合参数分布预测模型，能够输出将不同出现频率的预测互动结果进行融合的融合参数分布信息，提高了后续候选媒体资源的推荐参考信息的准确度，从而有效改善了视频推荐效果，提升了对象体验。在步骤303中，服务器基于该目标对象对应的融合参数分布信息，对目标对象针对候选媒体资源的多个预测互动结果进行融合，得到该候选媒体资源的推荐参考信息，基于该推荐参考信息，向该目标对象推荐该候选媒体资源。在本公开实施例中，该推荐参考信息指示向该目标对象推荐该候选媒体资源的顺序。在一些实施例中，该推荐参考信息为数值类信息，服务器按照候选媒体资源的推荐参考信息的大小进行排序，按照推荐参考信息从大到小的顺序，向目标对象推荐媒体资源。例如，以媒体资源为视频为例，该候选媒体资源的推荐参考信息为视频的排序分，服务器基于目标对象的资源请求从视频库中召回3个候选视频，基于融合参数分布信息，获取每个候选视频的排序分，分别为90、85、80，从而按照排序分从大到小的顺序将这些候选视频推荐给目标对象。当然，在实际应用中，能够根据实际需求选择不同的推荐策略，例如，以3个候选视频的排序分为90、85、80为例，服务器按照排序分80、85、90的顺序将这3个候选视频推荐给目标对象，使得目标对象所观看的视频越来越符合自身偏好，从而提升对象体验，本公开实施例对此不作限定。在一些实施例中，服务器基于该目标对象对应的融合参数分布信息，对目标对象针对候选媒体资源的多个预测互动结果进行融合，得到该媒体资源的推荐参考信息，包括：基于融合参数分布信息进行高斯采样，得到该目标对象对应的融合参数向量，该融合参数向量指示在目标推荐阶段对多个预测互动结果进行融合的多个融合参数；基于该融合参数向量，对该候选媒体资源的多个预测互动结果进行融合，得到该推荐参考信息。通过这种融合参数向量的形式，来对多个预测互动结果进行融合，简化了推荐参考信息的获取方式，提高了媒体资源推荐效率。基于前述图2所示内容可知，融合参数分布信息为M维参数的高斯分布，基于该融合参数分布信息进行高斯采样，能够得到M维参数向量，即融合参数向量，该融合参数向量包括M个参数值，用于融合候选媒体资源的M个预测互动结果。其中，该M个参数值可以理解为M个预测互动结果的权重，服务器基于该融合参数向量，对该M个预测互动结果进行加权求和，得到该候选媒体资源的推荐参考信息。例如，以任一候选媒体资源为视频A为例，融合参数向量包括3个参数值：0.8、0.5以及0.9，分别对应3个预测互动结果：视频观看时长60秒、点赞率50％以及关注率30％，服务器计算得到该视频A的推荐参考信息为48.25。需要说明的是，此处举例仅为示意性地，本公开实施例对于上述预测互动结果、融合参数向量以及推荐参考信息的具体形式不作限定。本公开实施例提供的技术方案，针对目标对象的资源请求，通过获取在目标推荐阶段该目标对象对应的状态信息，将该状态信息输入融合参数分布预测模型，得到该目标对象对应的融合参数分布信息，从而根据该融合参数分布信息对候选媒体资源的多个预测互动结果进行融合，得到候选媒体资源的推荐参考信息以实现媒体资源推荐。在这一过程中，由于融合参数分布预测模型是以目标对象的资源请求为粒度来预测融合参数分布信息的，因此预测得到的融合参数分布信息具有个性化，能够有效改善媒体资源的推荐效果，提升对象体验。基于上述图3所示实施例，对本公开实施例提供的融合参数分布预测模型的应用过程进行了介绍，结合前述图2可知，在本公开实施例提供的媒体资源推荐方法的基础上，本公开实施例还提供了一种基于强化学习训练融合参数分布预测模型的过程，下面结合图4，对融合参数分布预测模型的训练过程进行介绍。图4是本公开实施例提供的一种用于推荐阶段的融合参数分布预测模型的训练方法的流程图。如图4所示，以该方法由服务器执行为例进行介绍，示意性地，该方法包括下述步骤401至步骤404。在步骤401中，服务器获取样本数据集，该样本数据集包括样本对象在目标时间段内的多个样本资源请求所对应的样本数据，该样本数据包括在样本推荐阶段该样本对象对应的样本状态信息、样本融合参数分布信息以及样本对象反馈信息。在本公开实施例中，样本资源请求是服务器基于在线媒体资源推荐过程收集到的，具体参考前述图2所示。该样本对象反馈信息指示对象基于媒体资源的互动行为。示意性地，对于样本数据集中任一个样本数据，该样本数据包括基于第i次样本资源请求得到的样本对象对应的样本状态信息、基于第i次样本资源请求得到的样本对象对应的样本动作信息、基于第i次样本资源请求得到的样本对象反馈信息、基于第i+1次样本资源请求得到的样本对象对应的样本状态信息以及基于第i+1次样本资源请求得到的样本对象反馈信息。在一些实施例中，该目标时间段为目标对象每次打开终端上运行的应用程序至退出该应用程序的时间段，通过以目标时间段为优化窗口来训练融合参数分布预测模型，能够最大化目标时间段内的互动指标，有效改善媒体资源推荐效果，提升对象体验。在步骤402中，服务器基于该样本数据集，对基于媒体资源的多个互动行为对应的评估模型进行训练，得到训练后的多个评估模型。在本公开实施例中，对于任一互动行为对应的评估模型，该评估模型用于评估样本资源请求的样本融合参数分布信息对该互动行为的影响程度，也可以理解为评价融合参数分布预测模型输出的策略是否改善了媒体资源推荐的效果，提升了对象体验。训练后的评估模型能够用于指导融合参数分布预测模型的参数更新。对于媒体资源推荐过程中涉及的基于媒体资源的多个互动行为，每个互动行为都有对应的评估模型，服务器利用强化学习的方式，基于样本数据集，对每个互动行为对应的评估模型进行训练，得到相应的训练后的评估模型。下面以第一评估模型为例，对服务器基于样本数据集训练该评估模型的过程进行介绍，示意性地，以训练过程中第p次迭代为例，该训练过程包括下述步骤A至步骤D：步骤A、基于第i个样本数据中的样本状态信息和第一评估模型，获取第一奖励值，i为正整数。其中，该第i个样本数据为该样本对象在目标时间段内第i次样本资源请求对应的样本数据。服务器将第i个样本数据中的样本状态信息输入该第一评估模型，得到该第一奖励值，该第一奖励值为1维实数值。步骤B、基于第i+1个样本数据中的样本状态信息和该第一评估模型，获取第二奖励值。其中，该第i+1个样本数据为该样本对象在目标时间段内第i+1次样本资源请求对应的样本数据。服务器将第i+1个样本数据中的样本状态信息输入该第一评估模型，得到该第二奖励值，该第二奖励值为1维实数值。步骤C、基于该第i个样本数据中的样本对象反馈信息、该第一奖励值、该第二奖励值以及折扣系数，计算第一损失值。其中，服务器根据第一损失函数，基于该第i个样本数据中的样本对象反馈信息、该第一奖励值、该第二奖励值以及折扣系数，计算第一损失值。示意性地，以媒体资源为视频、该第一评估模型为第j个评估模型、该第j个评估模型为视频观看时长对应的评估模型为例，该第i个样本数据表示为{}，其中，si为基于第i次样本资源请求得到的样本对象对应的样本状态信息，ai为基于第i次样本资源请求得到的样本对象对应的样本动作信息，ri为基于第i次样本资源请求得到的样本对象反馈信息，s′i为基于第i+1次样本资源请求得到的样本对象对应的样本状态信息，done为标志位，用于标志第i次样本资源请求是否为目标对象在目标时间段内的最后一次资源请求。该评估模型的第一损失函数如下述公式所示：式中，为基于第i次样本资源请求得到的视频观看时长，γj为折扣系数，n为样本资源请求的个数，Vj为该第j个评估模型基于si输出的第一奖励值，Vj为该第j个评估模型基于s′i输出的第二奖励值，θj为该第j个评估模型的模型参数。其中，γj为预先设置的参数，例如，γj为0.95。步骤D、基于该第一损失值，对该第一评估模型的模型参数进行更新，直至满足训练结束条件，得到训练后的第一评估模型。其中，在第一损失值满足训练结束条件的情况下，输出训练后的第一评估模型，在不满足的情况下，调整该第一评估模型的模型参数，基于调整后的第一评估模型，进行第p+1次迭代，直至满足训练结束条件。示意性地，该训练结束条件为评估模型收敛，例如损失值小于设定阈值，或者两次迭代之间的变化小于设定阈值，又或者，迭代次数达到目标次数等等，本公开实施例对此不作限定。需要说明的是，对其他互动行为对应的评估模型进行训练时所采用的损失函数与上述步骤A至步骤D同理，在此不再赘述。另外，在一些实施例中，对于不同互动行为对应的评估模型，该评估模型所采用的损失函数中的折扣系数不同，在实际应用中能够根据需求进行设置。例如，对于短期互动行为对应的评估模型，将其损失函数中的折扣系数设置为0，对于长期互动行为对应的评估模型，将其损失函数中的折扣系数设置为0.95，对此不作限定。通过这种方式，针对不同互动类型的评估模型有针对性地设置相应的折扣系数，能够最大化提升评估模型的准确度，从而为后续融合参数分布预测模型的参数更新提供更加准确的指导，提高融合参数分布预测模型的准确率。经过上述步骤402，服务器基于强化学习，训练得到训练后的多个互动行为对应的评估模型，便于为后续融合参数模型的参数更新提供指导。通过这种方式，对每个互动行为对应的评估模型均进行了训练，从而既优化了媒体资源推荐的短期互动指标又优化了长期互动指标，能够有效改善媒体资源推荐效果，提升对象体验。例如，以媒体资源为视频为例，在视频推荐场景中，往往存在短期且稀疏的互动行为和长期且稠密的互动行为，这两类互动行为出现的频率并不相同，如目标对象在观看某一视频的情况下一定会产生视频观看时长这一互动行为，而并不一定会产生点赞这一互动行为，因此，通过训练多个评估模型，使得融合参数分布预测模型能够学习到综合性策略，输出将不同类型的预测互动结果进行融合的融合参数分布信息，提高了该融合参数分布信息的准确度。下面对服务器训练融合参数分布预测模型的过程进行介绍。示意性地，服务器基于该样本数据集和训练后的多个评估模型，将训练后的多个评估模型对该融合参数分布预测模型的评估结果作为奖励值，对该融合参数分布预测模型进行训练，得到训练后的该融合参数分布预测模型。在这一过程中，本公开实施例提供了一种两阶段策略学习法，在第一训练阶段，基于训练后的至少一个短期互动行为对应的评估模型来更新融合参数分布预测模型的模型参数，以得到中间融合参数分布预测模型，在第二训练阶段，基于训练后的至少一个长期互动行为对应的评估模型来更新融合参数分布预测模型，以得到训练后的融合参数分布预测模型。通过这种两阶段策略学习的方式，能够更好地平衡媒体资源的长期互动指标和短期互动指标对融合参数分布预测模型的影响，在确保媒体资源的长期互动指标满足需求的条件下，进一步优化媒体资源的短期互动指标，从而提高融合参数分布预测模型的准确度，改善媒体资源推荐效果，提升对象体验。下面基于步骤403和步骤404，对上述这种两阶段策略学习方式进行介绍。在步骤403中，服务器基于样本数据集和训练后的至少一个短期互动行为对应的评估模型，对该融合参数分布预测模型的模型参数进行更新，直至满足第一训练条件，得到中间融合参数分布预测模型。在本公开实施例中，对于任一个短期互动行为对应的评估模型，服务器基于样本数据集和训练后的短期互动行为对应的评估模型，对该融合参数分布预测模型的模型参数进行更新，直至满足第一训练条件，得到中间融合参数模型。下面以第二评估模型为例，对服务器基于样本数据集，训练融合参数分布预测模型的过程进行介绍。示意性地，以训练过程中第q次迭代为例，该训练过程包括下述步骤A至步骤D：步骤A、基于第i个样本数据和该第二评估模型，获取融合参数分布预测模型基于该第二评估模型所评估的短期互动行为的第一优势函数值。其中，该第二评估模型基于第i个样本数据输出的奖励值能够指导融合参数分布预测模型的参数更新，该第i个样本数据与前述步骤402中的样本数据同理，在此不再赘述。示意性地，服务器基于该第二评估模型输出的奖励值、该第i个样本数据中的样本对象反馈信息以及折扣系数，计算该第一优势函数值。例如，以媒体资源为视频、该第二评估模型为第j个评估模型、该第j个评估模型为视频关注次数对应的评估模型为例，该第i个样本数据表示为{}，该第一优势函数值通过下述公式计算得到：式中，为基于第i次样本资源请求得到的视频关注量，γj为折扣系数，done为标志位，Vj为该第j个评估模型基于si输出的第三奖励值，Vj为该第j个评估模型基于s′i输出的第四奖励值，θj为该第j个评估模型的模型参数。其中，γj为预先设置的参数，例如，γj为0。步骤B、基于该第i个样本数据和该融合参数分布预测模型，获取第一概率密度，该第一概率密度指示该融合参数分布预测模型基于该第i个样本数据中的样本状态信息输出该第i个样本数据中的样本融合参数分布信息的概率密度。步骤C、基于该第一优势函数值和该第一概率密度，计算第二损失值。其中，服务器根据第二损失函数，基于该第一优势函数值和该第一概率密度，计算第二损失值。示意性地，继续以前述步骤A中第j个评估模型为例，该第i个样本数据表示为{}，该第二损失函数如下述公式所示：式中，为策略参数，为第一优势函数值，其具体含义参考前述公式，为该融合参数分布预测模型基于样本状态信息si输出样本融合参数分布信息ai的第一概率密度。步骤D、基于该第二损失值，对该融合参数分布预测模型的模型参数进行更新，直至满足该第一训练条件，得到该中间融合参数分布预测模型。其中，在该第二损失值满足第一训练条件的情况下，输出中间融合参数分布预测模型，在不满足的情况下，调整该融合参数分布预测模型的模型参数，基于调整后的融合参数分布预测模型，进行第q+1次迭代，直至满足第一训练条件。在一些实施例中，该第一训练条件是指基于该融合参数分布预测模型进行A/B测试得到的测试结果符合要求。在另一些实施例中，该第一训练条件是指迭代次数达到目标次数或者损失值小于设定阈值等等，对此不作限定。需要说明的是，基于其他短期互动行为对应的评估模型对融合参数分布预测模型进行训练时所采用的损失函数与上述步骤A至步骤D同理，在此不再赘述。经过上述步骤403，实现了融合参数分布预测模型的第一训练阶段，即基于每个短期互动行为对应的评估模型，指导融合参数分布预测模型的参数更新。这一过程也可以理解为将每个短期互动行为作为融合参数分布预测模型的辅助目标，利用强化学习的方式，通过和环境交互学习来优化每个辅助目标对应的优势函数，或者说对于每一个辅助目标学习一个策略来优化该目标，直至满足训练条件，得到中间融合参数分布预测模型。在步骤404中，服务器基于样本数据集和训练后的至少一个长期互动行为对应的评估模型，对该中间融合参数分布预测模型的模型参数进行更新，直至满足第二训练条件，得到训练后的该融合参数分布预测模型。在本公开实施例中，对于任一个长期互动行为对应的评估模型，服务器基于强化学习和训练后的长期互动行为对应的评估模型，对该中间融合参数分布预测模型的模型参数进行更新，直至满足第一训练条件，得到训练后的融合参数分布预测模型。下面以第三评估模型为例，对服务器基于样本数据集，训练中间融合参数分布预测模型的过程进行介绍。示意性地，以训练过程中第t次迭代为例，该训练过程包括下述步骤A至步骤E：步骤A、基于第i个样本数据、该多个互动行为的约束参数以及该中间融合参数分布预测模型，获取目标权重，该目标权重指示该至少一个短期互动行为对应的融合参数分布信息与该第i个样本数据中的样本融合参数分布信息之间的偏离程度。其中，该多个互动行为的约束参数为预设参数，用于约束长期互动行为对应的融合参数分布信息与短期互动行为对应的融合参数分布信息之间的偏离程度，能够根据实际需求进行设置，对此不作限定，该第i个样本数据与前述步骤402中的样本数据同理，在此不再赘述。例如，以媒体资源为视频、该第三评估模型为第1个评估模型、该第1个评估模型为视频观看时长对应的评估模型为例，该第i个样本数据表示为{}，该目标权重通过下述公式计算得到：式中，m为评估模型个数，为策略参数，λj为约束参数，其中，λj＞0，该约束参数越大表明约束力越强。步骤B、基于该第i个样本数据和该第三评估模型，获取该中间融合参数分布预测模型基于该第三评估模型所评估的长期互动行为的第二优势函数值。其中，该第三评估模型基于第i个样本数据输出的奖励值能够指导中间融合参数分布预测模型的参数更新，该第i个样本数据与前述步骤402中的样本数据同理，在此不再赘述。示意性地，服务器基于该第三评估模型输出的奖励值、该第i个样本数据中的样本对象反馈信息以及折扣系数，计算该第二优势函数值。例如，以媒体资源为视频、该第三评估模型为第1个评估模型、该第1个评估模型为视频观看时长对应的评估模型为例，该第i个样本数据表示为{}，该第二优势函数值通过下述公式计算得到：式中，为基于第i次样本资源请求得到的视频观看时长，γ1为该第1个评估模型的折扣系数，done为标志位，V1为该第1个评估模型基于si输出的第五奖励值，V1为该第1个评估模型基于s′i输出的第六奖励值，θ1为该第1个评估模型的模型参数。其中，γ1为预先设置的参数，例如，γ1为0.95。步骤C、基于该第i个样本数据和该中间融合参数分布预测模型，获取第二概率密度，该第二概率密度指示中间融合参数分布预测模型基于该第i个样本数据中的样本状态信息输出该第i个样本数据中的样本融合参数分布信息的概率密度。步骤D、基于该多个互动行为的约束参数、该目标权重、该第二优势函数值以及该第二概率密度，计算第三损失值。其中，服务器根据第三损失函数，基于该多个互动行为的约束参数、该目标权重、该第二优势函数值以及该第二概率密度，计算第三损失值。示意性地，继续以前述步骤A和步骤B中第1个评估模型为例，该第i个样本数据表示为{}，该第三损失函数如下述公式所示：式中，为视频观看时长对应的策略参数，n为样本资源请求个数，m为评估模型个数，参考上述公式，为该中间融合参数分布预测模型基于样本状态信息si输出样本融合参数分布信息ai的第二概率密度。步骤E、基于该第三损失值，对该中间融合参数分布预测模型的模型参数进行更新，直至满足该第二训练条件，得到训练后的该融合参数分布预测模型。其中，在该第三损失值满足第二训练条件的情况下，输出训练后的融合参数分布预测模型，在不满足的情况下，调整该中间融合参数分布预测模型的模型参数，基于调整后的中间融合参数分布预测模型，进行第t+1次迭代，直至满足第二训练条件。在一些实施例中，该训练结束条件为融合参数分布预测模型收敛，例如损失值小于设定阈值，或者两次迭代之间的变化小于设定阈值，又或者，迭代次数达到目标次数等等，本公开实施例对此不作限定。经过上述步骤404，实现了融合参数分布预测模型的第二训练阶段，即基于每个长期互动行为对应的评估模型，指导中间融合参数分布预测模型的参数更新。这一过程也可以理解为将长期互动行为作为融合参数分布预测模型的主目标，利用强化学习的方式，通过和环境交互学习来优化主目标对应的优势函数，在这一过程中，通过多个互动行为的约束参数，来约束长期互动行为对应的融合参数分布信息与短期互动行为对应的融合参数分布信息之间的偏离程度，以使在优化主目标对应的优势函数时保证其不会距离其他辅助目标策略太远，或者说在优化主目标的同时让策略不要偏离每一个辅助目标对应策略太远，直至满足训练条件，得到训练后的融合参数分布预测模型。下面继续以上述步骤401至步骤404提供的融合参数分布预测模型的训练方法为例，基于本公开实施例与相关方案的实验结果，对本公开实施例带来的有益效果进行说明。为了便于描述，将本公开实施例提供的融合参数分布预测模型的训练方法命名为“两阶段策略学习算法”。示意性地，参考表1，表1为本公开实施例提供的两阶段策略学习算法与相关方案的实验结果对比表。以媒体资源为视频为例，多个预测结果包括视频观看时长、分享率、下载率以及评论率，每个测试桶5％流量，基准桶为CEM超参优化方法80％流量，实验结果如下所示。表1算法观看时长分享下载评论时长目标-A3C+0.31％-0.71％+0.15％-1.3％RCPO-A3C+0.28％-1.08％-0.52％-0.77％互动目标-A3C+0.12％+5.01％+1.95％-0.10％两阶段策略学习算法+0.34％+3.32％+1.79％-0.62％其中，时长目标-A3C算法为基于单目标的强化学习算法，用于优化主目标视频观看时长；RCPO-A3C算法为基于约束项的强化学习算法，该算法将约束项转换成对偶问题，通过调节约束项的拉格朗日乘子进行优化；互动目标-A3C是纯粹优化互动行为指标的A3C算法。通过比较发现，本公开实施例提供的两阶段策略学习算法不仅在主目标观看时长上效果最优，并且在其他约束项上效果显著优于相关方案中的单目标强化学习算法和约束强化学习算法。并且，本公开实施例提供的两阶段策略学习算法在主目标观看时长上显著优于互动目标-A3C算法，其他互动行为指标也取得了良好效果。综上，本公开实施例提供的技术方案，在训练融合参数分布预测模型的过程中，通过对多个互动行为对应的评估模型进行训练，以便指导融合参数分布预测模型的参数更新，进一步地，通过两阶段策略学习方式，能够更好地平衡媒体资源的长期互动指标和短期互动指标对融合参数分布预测模型的影响，在确保媒体资源的长期互动指标满足需求的条件下，进一步优化媒体资源的短期互动指标，从而提高融合参数分布预测模型的准确度，进而为后续融合参数分布预测模型应用于媒体资源推荐阶段提供支撑，有效改善媒体资源推荐效果，提升对象体验。图5是根据一示例性实施例示出的一种媒体资源推荐装置的框图。参照图5，该装置包括获取单元501、输入单元502和融合单元503。该获取单元501，被配置为执行响应于目标对象的资源请求，获取在目标推荐阶段该目标对象对应的状态信息，该状态信息指示该目标推荐阶段对应的候选媒体资源的资源信息、该目标对象的对象信息以及该目标对象的历史互动行为；该输入单元502，被配置为执行将该目标对象对应的状态信息输入融合参数分布预测模型，得到该目标对象对应的融合参数分布信息，该融合参数分布信息指示融合参数的分布情况，该融合参数用于在该目标推荐阶段中对该候选媒体资源的预测互动结果进行融合；该融合单元503，被配置为执行基于该目标对象对应的融合参数分布信息，对该目标对象针对该候选媒体资源的多个预测互动结果进行融合，得到该候选媒体资源的推荐参考信息，基于该推荐参考信息，向该目标对象推荐该候选媒体资源。在一些实施例中，该装置还包括：第一预测单元，被配置为执行基于至少一个长期互动行为预测模型，对该目标对象针对该候选媒体资源的长期互动行为进行预测，得到该候选媒体资源的至少一个长期预测互动结果；第二预测单元，被配置为执行基于至少一个短期互动行为预测模型，对该目标对象针对该候选媒体资源的短期互动行为进行预测，得到该候选媒体资源的至少一个短期预测互动结果。在一些实施例中，该融合单元503，被配置为执行：基于该融合参数分布信息进行高斯采样，得到该目标对象对应的融合参数向量，该融合参数向量指示在该目标推荐阶段对该多个预测互动结果进行融合的多个融合参数；基于该融合参数向量，对该多个预测互动结果进行融合，得到该推荐参考信息。在一些实施例中，该装置还包括：第一训练单元，被配置为执行基于样本数据集，对基于媒体资源的多个互动行为对应的评估模型进行训练，得到训练后的多个评估模型，该样本数据集包括样本对象在目标时间段内的多个样本资源请求所对应的样本数据，该样本数据包括在样本推荐阶段该样本对象对应的样本状态信息、样本融合参数分布信息以及样本对象反馈信息，该评估模型用于评估该样本融合参数分布信息对互动行为的影响程度，该样本对象反馈信息指示对象基于媒体资源的互动行为；第二训练单元，被配置为执行基于该样本数据集和训练后的多个评估模型，将训练后的多个评估模型对该融合参数分布预测模型的评估结果作为奖励值，对该融合参数分布预测模型进行训练，得到训练后的该融合参数分布预测模型。在一些实施例中，该第一训练单元，被配置为执行：基于第i个样本数据中的样本状态信息和第一评估模型，获取第一奖励值，该第一评估模型为基于媒体资源的多个互动行为中任一个互动行为对应的评估模型，该第i个样本数据为该样本对象在目标时间段内第i次样本资源请求对应的样本数据，i为正整数；基于第i+1个样本数据中的样本状态信息和该第一评估模型，获取第二奖励值，该第i+1个样本数据为该样本对象在目标时间段内第i+1次样本资源请求对应的样本数据；基于该第i个样本数据中的样本对象反馈信息、该第一奖励值、该第二奖励值以及折扣系数，计算第一损失值；基于该第一损失值，对该第一评估模型的模型参数进行更新，直至满足训练结束条件，得到训练后的该第一评估模型。在一些实施例中，该第二训练单元包括：第一训练子单元，被配置为执行基于该样本数据集和训练后的至少一个短期互动行为对应的评估模型，对该融合参数分布预测模型的模型参数进行更新，直至满足第一训练条件，得到中间融合参数分布预测模型；第二训练子单元，被配置为执行基于该样本数据集和训练后的至少一个长期互动行为对应的评估模型，对该中间融合参数分布预测模型的模型参数进行更新，直至满足第二训练条件，得到训练后的该融合参数分布预测模型。在一些实施例中，该第一训练子单元，被配置为执行：基于第i个样本数据和第二评估模型，获取该融合参数分布预测模型基于该第二评估模型所评估的短期互动行为的第一优势函数值，该第二评估模型为任一个短期互动行为对应的评估模型；基于该第i个样本数据和该融合参数分布预测模型，获取第一概率密度，该第一概率密度指示该融合参数分布预测模型基于该第i个样本数据中的样本状态信息输出该第i个样本数据中的样本融合参数分布信息的概率密度；基于该第一优势函数值和该第一概率密度，计算第二损失值；基于该第二损失值，对该融合参数分布预测模型的模型参数进行更新，直至满足该第一训练条件，得到该中间融合参数分布预测模型。在一些实施例中，该第二训练子单元，被配置为执行：基于第i个样本数据、该多个互动行为的约束参数以及该中间融合参数分布预测模型，获取目标权重，该目标权重指示该至少一个短期互动行为对应的融合参数分布信息与该第i个样本数据中的样本融合参数分布信息之间的偏离程度；基于该第i个样本数据和第三评估模型，获取该中间融合参数分布预测模型基于该第三评估模型所评估的长期互动行为的第二优势函数值，该第三评估模型为任一个长期互动行为对应的评估模型；基于该第i个样本数据和该中间融合参数分布预测模型，获取第二概率密度，该第二概率密度指示该中间融合参数分布预测模型基于该第i个样本数据中的样本状态信息输出该第i个样本数据中的样本融合参数分布信息的概率密度；基于该多个互动行为的约束参数、该目标权重、该第二优势函数值以及该第二概率密度，计算第三损失值；基于该第三损失值，对该中间融合参数分布预测模型的模型参数进行更新，直至满足该第二训练条件，得到训练后的该融合参数分布预测模型。基于本公开实施例提供的媒体资源推荐装置，针对目标对象的资源请求，通过获取在目标推荐阶段该目标对象对应的状态信息，将该状态信息输入融合参数分布预测模型，得到该目标对象对应的融合参数分布信息，从而根据该融合参数分布信息对候选媒体资源的多个预测互动结果进行融合，得到候选媒体资源的推荐参考信息以实现媒体资源推荐。在这一过程中，由于融合参数分布预测模型是以目标对象的资源请求为粒度来预测融合参数分布信息的，因此预测得到的融合参数分布信息具有个性化，能够有效改善媒体资源的推荐效果，提升对象体验。需要说明的是：上述实施例提供的媒体资源推荐装置在媒体资源推荐时，仅以上述各功能模块的划分进行举例说明，实际应用中，可以根据需要而将上述功能分配由不同的功能模块完成，即将设备的内部结构划分成不同的功能模块，以完成以上描述的全部或者部分功能。另外，上述实施例提供的媒体资源推荐装置与媒体资源推荐方法实施例属于同一构思，其具体实现过程详见方法实施例，这里不再赘述。图6是根据一示例性实施例示出的一种终端的框图。该终端600可以是：智能手机、平板电脑、MP3播放器、MP4播放器、笔记本电脑或台式电脑。终端600还可能被称为用户设备、便携式终端、膝上型终端、台式终端等其他名称。通常，终端600包括有：处理器601和存储器602。处理器601可以包括一个或多个处理核心，比如4核心处理器、8核心处理器等。处理器601可以采用DSP、FPGA、PLA中的至少一种硬件形式来实现。处理器601也可以包括主处理器和协处理器，主处理器是用于对在唤醒状态下的数据进行处理的处理器，也称CPU；协处理器是用于对在待机状态下的数据进行处理的低功耗处理器。在一些实施例中，处理器601可以集成有GPU，GPU用于负责显示屏所需要显示的内容的渲染和绘制。一些实施例中，处理器601还可以包括AI处理器，该AI处理器用于处理有关机器学习的计算操作。存储器602可以包括一个或多个计算机可读存储介质，该计算机可读存储介质可以是非暂态的。存储器602还可包括高速随机存取存储器，以及非易失性存储器，比如一个或多个磁盘存储设备、闪存存储设备。在一些实施例中，存储器602中的非暂态的计算机可读存储介质用于存储至少一个程序代码，该至少一个程序代码用于被处理器601所执行以实现本公开方法实施例提供的媒体资源推荐方法。在一些实施例中，终端600还可选包括有：外围设备接口603和至少一个外围设备。处理器601、存储器602和外围设备接口603之间可以通过总线或信号线相连。各个外围设备可以通过总线、信号线或电路板与外围设备接口603相连。具体地，外围设备包括：射频电路604、显示屏605、摄像头组件606、音频电路607、定位组件608和电源609中的至少一种。外围设备接口603可被用于将I/O相关的至少一个外围设备连接到处理器601和存储器602。在一些实施例中，处理器601、存储器602和外围设备接口603被集成在同一芯片或电路板上；在一些其他实施例中，处理器601、存储器602和外围设备接口603中的任意一个或两个可以在单独的芯片或电路板上实现，本实施例对此不加以限定。射频电路604用于接收和发射RF信号，也称电磁信号。射频电路604通过电磁信号与通信网络以及其他通信设备进行通信。射频电路604将电信号转换为电磁信号进行发送，或者，将接收到的电磁信号转换为电信号。可选地，射频电路604包括：天线系统、RF收发器、一个或多个放大器、调谐器、振荡器、数字信号处理器、编解码芯片组、用户身份模块卡等等。射频电路604可以通过至少一种无线通信协议来与其它终端进行通信。该无线通信协议包括但不限于：城域网、各代移动通信网络、无线局域网和/或WiFi网络。在一些实施例中，射频电路604还可以包括NFC有关的电路，本申请对此不加以限定。显示屏605用于显示UI。该UI可以包括图形、文本、图标、视频及其它们的任意组合。当显示屏605是触摸显示屏时，显示屏605还具有采集在显示屏605的表面或表面上方的触摸信号的能力。该触摸信号可以作为控制信号输入至处理器601进行处理。此时，显示屏605还可以用于提供虚拟按钮和/或虚拟键盘，也称软按钮和/或软键盘。在一些实施例中，显示屏605可以为一个，设置在终端600的前面板；在另一些实施例中，显示屏605可以为至少两个，分别设置在终端600的不同表面或呈折叠设计；在另一些实施例中，显示屏605可以是柔性显示屏，设置在终端600的弯曲表面上或折叠面上。甚至，显示屏605还可以设置成非矩形的不规则图形，也即异形屏。显示屏605可以采用LCD、OLED等材质制备。摄像头组件606用于采集图像或视频。可选地，摄像头组件606包括前置摄像头和后置摄像头。通常，前置摄像头设置在终端的前面板，后置摄像头设置在终端的背面。在一些实施例中，后置摄像头为至少两个，分别为主摄像头、景深摄像头、广角摄像头、长焦摄像头中的任意一种，以实现主摄像头和景深摄像头融合实现背景虚化功能、主摄像头和广角摄像头融合实现全景拍摄以及VR拍摄功能或者其它融合拍摄功能。在一些实施例中，摄像头组件606还可以包括闪光灯。闪光灯可以是单色温闪光灯，也可以是双色温闪光灯。双色温闪光灯是指暖光闪光灯和冷光闪光灯的组合，可以用于不同色温下的光线补偿。音频电路607可以包括麦克风和扬声器。麦克风用于采集用户及环境的声波，并将声波转换为电信号输入至处理器601进行处理，或者输入至射频电路604以实现语音通信。出于立体声采集或降噪的目的，麦克风可以为多个，分别设置在终端600的不同部位。麦克风还可以是阵列麦克风或全向采集型麦克风。扬声器则用于将来自处理器601或射频电路604的电信号转换为声波。扬声器可以是传统的薄膜扬声器，也可以是压电陶瓷扬声器。当扬声器是压电陶瓷扬声器时，不仅可以将电信号转换为人类可听见的声波，也可以将电信号转换为人类听不见的声波以进行测距等用途。在一些实施例中，音频电路607还可以包括耳机插孔。定位组件608用于定位终端600的当前地理位置，以实现导航或LBS。电源609用于为终端600中的各个组件进行供电。电源609可以是交流电、直流电、一次性电池或可充电电池。当电源609包括可充电电池时，该可充电电池可以支持有线充电或无线充电。该可充电电池还可以用于支持快充技术。在一些实施例中，终端600还包括有一个或多个传感器610。该一个或多个传感器610包括但不限于：加速度传感器611、陀螺仪传感器612、压力传感器613、指纹传感器614、光学传感器615以及接近传感器616。加速度传感器611可以检测以终端600建立的坐标系的三个坐标轴上的加速度大小。比如，加速度传感器611可以用于检测重力加速度在三个坐标轴上的分量。处理器601可以根据加速度传感器611采集的重力加速度信号，控制显示屏605以横向视图或纵向视图进行用户界面的显示。加速度传感器611还可以用于游戏或者用户的运动数据的采集。陀螺仪传感器612可以检测终端600的机体方向及转动角度，陀螺仪传感器612可以与加速度传感器611协同采集用户对终端600的3D动作。处理器601根据陀螺仪传感器612采集的数据，可以实现如下功能：动作感应、拍摄时的图像稳定、游戏控制以及惯性导航。压力传感器613可以设置在终端600的侧边框和/或显示屏605的下层。当压力传感器613设置在终端600的侧边框时，可以检测用户对终端600的握持信号，由处理器601根据压力传感器613采集的握持信号进行左右手识别或快捷操作。当压力传感器613设置在显示屏605的下层时，由处理器601根据用户对显示屏605的压力操作，实现对UI界面上的可操作性控件进行控制。可操作性控件包括按钮控件、滚动条控件、图标控件、菜单控件中的至少一种。指纹传感器614用于采集用户的指纹，由处理器601根据指纹传感器614采集到的指纹识别用户的身份，或者，由指纹传感器614根据采集到的指纹识别用户的身份。在识别出用户的身份为可信身份时，由处理器601授权该用户执行相关的敏感操作，该敏感操作包括解锁屏幕、查看加密信息、下载软件、支付及更改设置等。指纹传感器614可以被设置在终端600的正面、背面或侧面。当终端600上设置有物理按键或厂商Logo时，指纹传感器614可以与物理按键或厂商Logo集成在一起。光学传感器615用于采集环境光强度。在一个实施例中，处理器601可以根据光学传感器615采集的环境光强度，控制显示屏605的显示亮度。具体地，当环境光强度较高时，调高显示屏605的显示亮度；当环境光强度较低时，调低显示屏605的显示亮度。在另一个实施例中，处理器601还可以根据光学传感器615采集的环境光强度，动态调整摄像头组件606的拍摄参数。接近传感器616，也称距离传感器，通常设置在终端600的前面板。接近传感器616用于采集用户与终端600的正面之间的距离。在一个实施例中，当接近传感器616检测到用户与终端600的正面之间的距离逐渐变小时，由处理器601控制显示屏605从亮屏状态切换为息屏状态；当接近传感器616检测到用户与终端600的正面之间的距离逐渐变大时，由处理器601控制显示屏605从息屏状态切换为亮屏状态。本领域技术人员可以理解，图6中示出的结构并不构成对终端600的限定，可以包括比图示更多或更少的组件，或者组合某些组件，或者采用不同的组件布置。图7是根据一示例性实施例示出的一种服务器的框图。示意性地，该服务器700可因配置或性能不同而产生比较大的差异，可以包括一个或多个处理器701和一个或多个的存储器702，其中，该一个或多个存储器702中存储有至少一条程序代码，该至少一条程序代码由该一个或多个处理器701加载并执行以实现上述各个方法实施例提供的媒体资源推荐方法。当然，该服务器700还可以具有有线或无线网络接口、键盘以及输入输出接口等部件，以便进行输入输出，该服务器700还可以包括其他用于实现设备功能的部件，在此不做赘述。在示例性实施例中，还提供了一种包括程序代码的计算机可读存储介质，例如包括程序代码的存储器702，上述程序代码可由服务器700的处理器701执行以完成上述媒体资源推荐方法。可选地，计算机可读存储介质可以是只读内存、随机存取存储器、只读光盘、磁带、软盘和光数据存储设备等。在示例性实施例中，还提供了一种计算机程序产品，包括计算机程序，该计算机程序被处理器执行时实现上述的媒体资源推荐方法。本领域技术人员在考虑说明书及实践这里公开的发明后，将容易想到本公开的其它实施方案。本公开旨在涵盖本公开的任何变型、用途或者适应性变化，这些变型、用途或者适应性变化遵循本公开的一般性原理并包括本公开未公开的本技术领域中的公知常识或惯用技术手段。说明书和实施例仅被视为示例性的，本公开的真正范围和精神由下面的权利要求指出。应当理解的是，本公开并不局限于上面已经描述并在附图中示出的精确结构，并且可以在不脱离其范围进行各种修改和改变。本公开的范围仅由所附的权利要求来限制。
