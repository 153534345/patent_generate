标题title
混合工作负载引流方法及计算机集群
摘要abst
本发明提供了一种混合工作负载引流方法及计算机集群，混合工作负载引流方法包括：在安全容器中组建由第一虚拟网卡与第二虚拟网卡共同定义的第一虚拟网线、第二虚拟网线及第一引流网线，并在工作负载与安全容器之间组建执行引流的第二引流网线，第一虚拟网卡配置数据报文检测单元，以由数据报文检测单元对在引流过程中进出工作负载的数据报文执行清洗和/或过滤；下发引流策略至第二虚拟网卡、第一引流网线与第二引流网线部署于工作节点内核空间的内核空间网卡，第一引流网线与第二引流网线基于引流策略择一地或者同时对同一工作节点的工作负载执行引流。本申请实现了对同一工作节点所部署的混合工作负载的引流且避免了单点问题。
权利要求书clms
1.混合工作负载引流方法，对工作节点的工作负载执行引流，其特征在于，包括：在安全容器中组建由第一虚拟网卡与第二虚拟网卡共同定义的第一虚拟网线、第二虚拟网线及第一引流网线，并在工作负载与安全容器之间组建执行引流的第二引流网线，所述第一虚拟网卡配置数据报文检测单元，以由所述数据报文检测单元对在引流过程中进出所述工作负载的数据报文执行清洗和/或过滤；下发引流策略至第二虚拟网卡、第一引流网线与第二引流网线部署于工作节点内核空间的内核空间网卡，所述第一引流网线与第二引流网线基于所述引流策略择一地或者同时对同一工作节点的工作负载执行引流。2.根据权利要求1所述的混合工作负载引流方法，其特征在于，所述第一虚拟网线与第二虚拟网线基于选定的CNI组件在安全容器中予以组建，所述第一引流网线与第二引流网线不依赖所述选定的CNI组件在工作负载与安全容器之间予以组建。3.根据权利要求1所述的混合工作负载引流方法，其特征在于，还包括：通过所述数据报文检测单元对进出所述第二虚拟网卡的数据报文基于用户下发的用户规则对进出所述工作负载的数据报文执行清洗和/或过滤，所述用户规则包括防火墙规则。4.根据权利要求1所述的混合工作负载引流方法，其特征在于，所述引流策略由部署于安全容器中的引流策略下发单元在对工作负载执行引流之前予以下发，所述引流策略选自tc策略或者流表策略。5.根据权利要求1所述的混合工作负载引流方法，其特征在于，还包括：由所述第二虚拟网线对安全容器下发安全规则，并对工作负载在执行引流过程中所形成的流量执行监控。6.根据权利要求1所述的混合工作负载引流方法，其特征在于，所述工作节点仅部署一个安全容器及两种工作负载，数据报文进出所述安全容器，所述第一引流网线与第二引流网线基于所述引流策略择一地或者同时对同一工作节点的两种不同的工作负载独立地执行引流。7.根据权利要求6所述的混合工作负载引流方法，其特征在于，所述工作负载为业务容器，所述第一引流网线由组建所述第一虚拟网线与第二虚拟网线所依赖的CNI组件在所述业务容器中预先创建的第三虚拟网线后，由所述第三虚拟网线被迁移至所述安全容器所形成，所述第二引流网线由管理员以命令行形式予以创建。8.根据权利要求7所述的混合工作负载引流方法，其特征在于，在对业务容器执行引流过程中，当数据报文达到第二虚拟网卡时，触发将所述第三虚拟网线迁移至所述安全容器以形成第一引流网线的修改事件。9.根据权利要求7所述的混合工作负载引流方法，其特征在于，在将所述第三虚拟网线迁移至所述安全容器以形成第一引流网线后还包括：下发引流策略至所述第一引流网线，并在将所述第三虚拟网线迁移至所述安全容器以形成第一引流网线过程中，通过所述数据报文检测单元修改第三虚拟网线两端的虚拟网卡的名称，且不修改第三虚拟网线两端的虚拟网卡的MAC地址，所述第一虚拟网线与第一引流网线一端的虚拟网卡部署于安全容器所属工作节点的内核空间，所述第一虚拟网线与第一引流网线另一端的虚拟网卡部署于安全容器所属工作节点的内核空间的外部。10.根据权利要求6所述的混合工作负载引流方法，其特征在于，所述工作负载为虚拟机，所述第一引流网线由br-int及连接所述br-int与安全容器的第四虚拟网线组成，所述第二引流网线由QBR、连接所述QBR与安全容器的第五虚拟网线及连接所述虚拟机与所述QBR的第六虚拟网线组成。11.根据权利要求10所述的混合工作负载引流方法，其特征在于，所述br-int配置暴露于工作节点的物理网卡，所述QBR配置与虚拟机建立虚拟网络连接的虚拟网络设备，所述虚拟网络设备包括Tap设备或者Tun设备。12.一种计算机集群，其特征在于，包括：控制器，至少一个工作节点及纳管所述工作节点的控制管理平面；所述工作节点仅部署一个安全容器及至少一个工作负载，所述安全容器配置数据报文检测单元及用于下发引流策略的引流策略下发单元；所述工作节点运行如权利要求1至11中任一项所述的混合工作负载引流方法，以择一地或者同时对同一工作节点的工作负载执行引流。13.根据权利要求12所述的计算机集群，其特征在于，所述工作节点中同时部署一个安全容器，至少一个虚拟机与至少一个业务容器。14.根据权利要求12所述的计算机集群，其特征在于，所述数据报文检测单元部署于安全容器，或者，所述数据报文检测单元部署于独立于安全容器的数据库。15.根据权利要求12所述的计算机集群，其特征在于，所述安全容器与创建第二虚拟网线，所述控制管理平面连接所述第二虚拟网线，由所述第二虚拟网线对安全容器下发安全规则，并对工作负载在执行引流过程中所形成的流量执行监控。
说明书desc
技术领域本发明涉及计算机技术领域，尤其涉及一种混合工作负载引流方法及计算机集群。背景技术CNI是Kubernetes集群中调用网络实现的接口。CNI 旨在为容器平台提供网络的标准化，为解决容器网络连接和容器销毁时的资源释放，提供了一套框架。CNI 可以支持大量不同的网络模式，并且容易实现。不同的容器平台能够通过相同的接口调用不同的网络组件。Node作为Kubernetes集群中的工作节点用于运行应用程序，其管理的最小运行单元是Pod。Node上运行Kubelet、kube-proxy，以负责执行Pod的创建、启动、监控、重启、销毁、以及实现软件模式的负载均衡。每一个Pod中部署一个或者多个容器。CNI组件负责为容器创建虚拟网卡，以实现同一工作节点内部署的多个容器之间的通信。CNI组件的实现方式包括Overlay、路由、Underlay，由于现有技术中存在由于CNI组件实现方式存在多样化及不统一的原因，从而导致了通用性不强，因此导致了现有技术中同一工作节点内对业务容器的引流效果不佳。同时，现有的引流技术无法实现对同一个工作节点中同时部署一个或者多个虚拟机与一个或者多个业务容器所形成的混合工作负载场景中时，同时对进出不同类型的工作负载执行数据报文所形成的流量执行引流。有鉴于此，有必要对现有技术中的对同一工作节点所部署的虚拟机与容器所组成的混合工作负载进行引流的技术方案予以改进，以解决上述问题。发明内容本发明的目的在于揭示一种混合工作负载引流方法及计算机集群，用以实现同一工作节点所部署的虚拟机与业务容器所形成的混合工作负载场景中同时对不同的工作负载实现引流，解决现有技术中使用CNI组件对不同工作负载执行引流过程中所存在的通用性不强的技术问题，避免在对工作负载执行引流过程中对工作负载的虚拟网卡IP地址及MAC地址进行修改，以避免用户感知部署工作负载的工作节点的变化。为实现上述目的之一，本发明提供了混合工作负载引流方法，对工作节点的工作负载执行引流，包括：在安全容器中组建由第一虚拟网卡与第二虚拟网卡共同定义的第一虚拟网线、第二虚拟网线及第一引流网线，并在工作负载与安全容器之间组建执行引流的第二引流网线，所述第一虚拟网卡配置数据报文检测单元，以由所述数据报文检测单元对在引流过程中进出所述工作负载的数据报文执行清洗和/或过滤；下发引流策略至第二虚拟网卡、第一引流网线与第二引流网线部署于工作节点内核空间的内核空间网卡，所述第一引流网线与第二引流网线基于所述引流策略择一地或者同时对同一工作节点的工作负载执行引流。作为本发明的进一步改进，所述第一虚拟网线与第二虚拟网线基于选定的CNI组件在安全容器中予以组建，所述第一引流网线与第二引流网线不依赖所述选定的CNI组件在工作负载与安全容器之间予以组建。作为本发明的进一步改进，还包括：通过所述数据报文检测单元对进出所述第二虚拟网卡的数据报文基于用户下发的用户规则对进出所述工作负载的数据报文执行清洗和/或过滤，所述用户规则包括防火墙规则。作为本发明的进一步改进，所述引流策略由部署于安全容器中的引流策略下发单元在对工作负载执行引流之前予以下发，所述引流策略选自tc策略或者流表策略。作为本发明的进一步改进，还包括：由所述第二虚拟网线对安全容器下发安全规则，并对工作负载在执行引流过程中所形成的流量执行监控。作为本发明的进一步改进，所述工作节点仅部署一个安全容器及两种工作负载，数据报文进出所述安全容器，所述第一引流网线与第二引流网线基于所述引流策略择一地或者同时对同一工作节点的两种不同的工作负载独立地执行引流。作为本发明的进一步改进，所述工作负载为业务容器，所述第一引流网线由组建所述第一虚拟网线与第二虚拟网线所依赖的CNI组件在所述业务容器中预先创建的第三虚拟网线后，由所述第三虚拟网线被迁移至所述安全容器所形成，所述第二引流网线由管理员以命令行形式予以创建。作为本发明的进一步改进，在对业务容器执行引流过程中，当数据报文达到第二虚拟网卡时，触发将所述第三虚拟网线迁移至所述安全容器以形成第一引流网线的修改事件。作为本发明的进一步改进，在将所述第三虚拟网线迁移至所述安全容器以形成第一引流网线后还包括：下发引流策略至所述第一引流网线，并在将所述第三虚拟网线迁移至所述安全容器以形成第一引流网线过程中，通过所述数据报文检测单元修改第三虚拟网线两端的虚拟网卡的名称，且不修改第三虚拟网线两端的虚拟网卡的MAC地址，所述第一虚拟网线与第一引流网线一端的虚拟网卡部署于安全容器所属工作节点的内核空间，所述第一虚拟网线与第一引流网线另一端的虚拟网卡部署于安全容器所属工作节点的内核空间的外部。作为本发明的进一步改进，所述工作负载为虚拟机，所述第一引流网线由br-int及连接所述br-int与安全容器的第四虚拟网线组成，所述第二引流网线由QBR、连接所述QBR与安全容器的第五虚拟网线及连接所述虚拟机与所述QBR的第六虚拟网线组成。作为本发明的进一步改进，所述br-int配置暴露于工作节点的物理网卡，所述QBR配置与虚拟机建立虚拟网络连接的虚拟网络设备，所述虚拟网络设备包括Tap设备或者Tun设备。基于相同发明思想，本申请还提供了一种计算机集群，包括：控制器，至少一个工作节点及纳管所述工作节点的控制管理平面；所述工作节点仅部署一个安全容器及至少一个工作负载，所述安全容器配置数据报文检测单元及用于下发引流策略的引流策略下发单元；所述工作节点运行如上述任一项发明创造所揭示的混合工作负载引流方法，以择一地或者同时对同一工作节点的工作负载执行引流。作为本发明的进一步改进，所述工作节点中同时部署一个安全容器，至少一个虚拟机与至少一个业务容器。作为本发明的进一步改进，所述数据报文检测单元部署于安全容器，或者，所述数据报文检测单元部署于独立于安全容器的数据库。作为本发明的进一步改进，所述安全容器与创建第二虚拟网线，所述控制管理平面连接所述第二虚拟网线，由所述第二虚拟网线对安全容器下发安全规则，并对工作负载在执行引流过程中所形成的流量执行监控。与现有技术相比，本发明的有益效果是：首先，在本申请中，基于引流策略通过第一引流网线与第二引流网线对进出工作负载的数据报文执行引流，且组件第一引流网线与第二引流网线不依赖CNI组件，从而解决了现有技术中使用CNI组件对混合工作负载场景下执行引流过程中所存在的通用性不强的技术问题；同时，在本申请中，在对混合工作负载执行引流过程中通过安全容器对业务容器及虚拟机执行引流过程中对进出业务容器及虚拟机的数据报文执行清洗和/或过滤，由于不需要使用交换机，并实现了在对业务容器及虚拟机执行引流过程中不被用户所感知，并通过采用诸如tc策略或者流表策略的引流策略，且有效地避免了单点问题。附图说明图1为本发明混合工作负载引流方法的整体流程图；图2为本发明一种计算机集群的整体拓扑图，该计算机集群所部署的多个工作节点被控制器所纳管的示意图，其中，同一工作节点中创建安全容器并由安全容器所部署的引流策略下发单元将引流策略配置至位于工作节点的内核空间的各个虚拟网卡，每一个工作节点中运行如图1所揭示的混合工作负载引流方法；图3为将通过CNI组件创建的虚拟网线迁移至安全容器中并形成虚拟网线的示意图；图4为工作节点中的安全容器与业务容器之间通过管理员以命令行形式创建虚拟网线的示意图；图5为对工作节点中部署的虚拟机执行引流过程中，基于桥设备连接虚拟机与安全容器并建立安全容器与OVS的连接，且示出了在对虚拟机及业务容器执行引流过程中数据报文从虚拟机流出的示意图；图6为对工作节点中部署的虚拟机执行引流过程中，基于桥设备连接虚拟机与安全容器并建立安全容器与OVS的连接，且示出了在对虚拟机及业务容器执行引流过程中数据报文流入虚拟机的示意图；图7为在一种变形实施例中数据报文检测单元部署于独立于安全容器的数据库的示意图；图8为数据报文检测单元对数据报文执行清洗和/或过滤所生成结果的示意图；图9为一种计算机可读介质的拓扑图。具体实施方式下面结合附图所示的各实施方式对本发明进行详细说明，但应当说明的是，这些实施方式并非对本发明的限制，本领域普通技术人员根据这些实施方式所作的功能、方法、或者结构上的等效变换或替代，均属于本发明的保护范围之内。混合工作负载引流方法，对工作节点的工作负载1执行引流，工作节点参图2中的Node-1至Node-n所示。为简化阐述，在本申请中对工作节点Node-1中部署一个业务容器11与一个虚拟机40所组成的混合工作负载技术场景中对业务容器11与虚拟机40执行引流予以范例性阐述。工作节点Node-1中部署一个安全容器10、业务容器11及虚拟机40，以通过该混合工作负载引流方法对数据报文进出业务容器11与虚拟机40所形成的数据报文执行引流操作，且可对业务容器11或者虚拟机40单独执行引流。引流过程中进出工作负载1的含义可被理解为数据报文流入工作负载1及流出工作负载1的过程。示例性地，参图2所示，计算机集群100中包含工作节点Node-1~工作节点Node-n，工作节点Node-1中包含一个或者多个业务容器11，工作节点Node-n也可包含一个或者多个业务容器和/或虚拟机等工作负载1。本实施例所揭示的混合工作负载引流方法包括如下步骤S1至步骤S2。步骤S1、在安全容器10中组建由第一虚拟网卡110与第二虚拟网卡111共同定义的第一虚拟网线31、第二虚拟网线32及第一引流网线，并在工作负载1与安全容器10之间组建执行引流的第二引流网线，第一虚拟网卡110配置数据报文检测单元30，以由数据报文检测单元30对在引流过程中进出工作负载1的数据报文执行清洗和/或过滤。本申请各具体实施方式中所指的第一虚拟网卡110、第二虚拟网卡111及下文所提及的虚拟网卡及内核空间网卡的类型包括Veth虚拟网卡或者macvlan虚拟网卡。无论是Veth虚拟网卡还是macvlan虚拟网卡均可成对配置以形成数据报文的转发链路。数据报文可从成对的一个Veth虚拟网卡转发至另一个Veth虚拟网卡且转发过程对用户无感知。工作节点Node-1仅部署一个安全容器10及两种不同的工作负载1，数据报文进出安全容器10，第一引流网线与第二引流网线基于引流策略择一地或者同时对同一工作节点的两种不同的工作负载1独立地执行引流。第一虚拟网线31与第二虚拟网线32基于选定的CNI组件在安全容器10中予以组建，第一引流网线与第二引流网线不依赖选定的CNI组件在工作负载1与安全容器10之间予以组建。由此使得在对两种不同的工作负载1同时或者分别进行引流过程中，可不依赖选定的CNI组件予以组建，从而解决了现有技术中使用CNI组件对混合工作负载执行引流过程中所存在的通用性不强的技术问题。例如，由于Flannel需要依赖于第三方网络插件所导致的通用性不强的技术问题。CNI的初衷是创建一个框架，用于在配置或销毁容器时动态配置适当的网络配置和资源。但如果第一引流网线与第二引流网线也基于组建第一虚拟网线31及第二虚拟网线32的CNI组件所创建的话，则会导致由于第三方网络插件的不同而导致在执行对业务容器11或者同时包含业务容器11与虚拟机40的混合工作负载的引流场景中通用性不强的问题。因此，在本具体实施方式中，第一引流网线与第二引流网线不依赖选定的CNI组件在工作负载1与安全容器10之间予以组建，有利于打通业务容器11与安全容器10之间的边界，以及有利于打通虚拟机40与安全容器10之间的边界。步骤S2、下发引流策略至第二虚拟网卡111、第一引流网线与第二引流网线部署于工作节点内核空间的内核空间网卡，第一引流网线与第二引流网线基于引流策略择一地或者同时对同一工作节点所设置的同种或者不同种的工作负载1执行引流。优选地，本申请所揭示的混合工作负载引流方法还包括：通过数据报文检测单元30对进出第二虚拟网卡111的数据报文基于用户下发的用户规则对进出工作负载1的数据报文执行清洗和/或过滤，从而确保了东西向数据转发过程的安全性。具体地，用户规则包括防火墙规则。用户规则通过图6中的控制器70并通过第二虚拟网线32向安全容器10中予以配置。结合图2所示，在计算机实例中，数据报文检测单元30逻辑上配置于第一虚拟网卡110中，申请人为便于理解本申请而将数据报文检测单元30从第一虚拟网卡110中剥离并独立地予以示出。数据报文检测单元30用于对流经第二虚拟网卡111的数据报文执行清洗和/或过滤，数据报文检测单元30用于实现业务识别、业务控制与业务统计，从而确保在引流过程中确保对数据报文沿东西向转发过程中的安全性与可靠性。参图8所示，数据报文检测单元30用于对流经第二虚拟网卡111的数据报文执行清洗和/或过滤生成如图8所示出的页面，对于合法数据报文，“动作”栏位显示为“允许”，对于非法数据报文，“动作”栏位显示为“禁止”，“更新于”栏位表示具体对某个具体的数据报文进行清洗和/或过滤事件的时间。具体而言，业务识别包括通过IP五元组来识别数据报文并解析数据报文以确定业务具体内容和信息。业务控制包括转发流向、带宽限制、阻挡、整形、丢弃等处理。业务统计是基于数据报文检测结果对预设时间段内的流量行为进行统计，以区分流媒体播放、即时通讯工具及游戏娱乐，从而根据不同的业务类型确定业务所需带宽资源，并在某个工作负载1不满足业务所需带宽资源时，增加工作负载1所配置的带宽资源，或者，在工作负载1的带宽资源存在明显富余时，降低工作负载1的带宽资源。参图2所示，本申请所揭示的混合工作负载引流方法旨在对同一个工作节点内的工作负载1进行引流，并禁止在两个独立的工作节点之间所分别部署的工作负载之间执行引流。同时，该混合工作负载引流方法可对同一个工作节点内对同种工作负载之间对进出工作负载的数据报文执行引流。引流或者更确切而言是对同一种类型的工作负载1之间进行引流的目的在于对数据报文进出同一个工作节点的业务容器之间所形成的流量首先被汇聚到安全容器10，并在安全容器10中基于引流策略并通过第一引流网线与第二引流网线对进出指定的工作负载1执行东西向与南北向的转发操作，并使得数据报文进入虚拟机40或者流出虚拟机40，或者，当某个虚拟机不可用时，将在可用的虚拟机与安全容器10之间建立由QBR41与br-int42所组建新的第一引流网线与新的第二引流网线，以将数据报文在新的虚拟机与用户之间转发数据报文。引流策略由部署于安全容器10中的引流策略下发单元50在对工作负载1执行引流之前予以下发，引流策略选自tc策略或者流表策略。在本申请所揭示的混合工作负载引流方法中，安全容器10通过第二引流网线连接一个或者多个业务容器11及一个或者多个虚拟机40基于不同的引流策略独立地执行引流。一个或者多个业务容器11及一个或者多个虚拟机40组成混合工作负载，基于引流策略对由虚拟机40与业务容器11所组成的混合工作负载执行引流，同时在计算机集群100所包含的一个或者多个工作节点中分别部署一个安全容器10，并基于每个工作节点的安全容器10对安全容器10所属工作节点中的一个或者多个工作负载执行引流从而有效地避免了单点问题。所谓单点问题是指现有技术中采用交换机纳管多个工作节点时，如果交换机出现异常而导致由多个业务容器和/或多个虚拟机所组成的计算机集群100出现异常。虽然，诸如图5仅示出了一个虚拟机40与一个业务容器11，但本领域技术人员可以合理地预见到安全容器10还可通过多组包含第一引流网线与第二引流网线的形式同时连接多个虚拟机40与多个业务容器11。假设，一个计算机集群100中包含了工作节点Node-1与工作节点Node-n，工作节点Node-1包含业务容器A与业务容器B，工作节点Node-n包含业务容器C与业务容器D。由于工作节点Node-1与工作节点Node-n中均设置了一个安全容器10。因此，当纳管工作节点Node-1与工作节点Node- n的交换机出现异常时，会导致整个计算机集群100出现瘫痪与异常，从而导致单个工作节点不可用，并由此导致前述单点问题。然而，通过本申请所揭示的混合工作负载引流方法，由于不要需要使用交换机，并将交换机所执行的引流作业转由每个工作节点中安全容器10予以实施，即使某个工作节点不可用时，并不影响同一个计算机集群100中其他工作节点的正常工作，且将对计算机集群100的流量监控作业分散到各个工作节点中予以独立执行，并具体为通过每个工作节点中的安全容器10予以实现，从而有效地避免了现有技术中依赖交换机执行引流所存在的单点问题。同时，在对工作负载1执行引流过程中流入指定的工作负载1或者从指定的工作负载1流出的数据报文均汇聚到安全容器10，从而便于后续通过数据报文检测单元30在沿东西向执行转发数据报文过程中监测流经第二虚拟网卡111数据报文所形成的流量，以便于集中地在安全容器10中实现DPI检测，且DPI检测过程不影响业务容器11和/或虚拟机40所运行的服务，以确保为用户提供良好的用户体验。参图5所示，引流策略下发单元50沿箭头501~箭头506所示出的方向分别下发引流策略至虚拟网卡103、虚拟网卡114、虚拟网卡112、第二虚拟网卡111、虚拟网卡106及虚拟网卡105，且前述下发引流策略的动作参图2及图5中mirror配置所对应的箭头，以通过引流策略下发单元50下发引流策略。优选地，本申请所揭示的混合工作负载引流方法还包括：由第二虚拟网线32对安全容器10下发安全规则，并对工作负载1在执行引流过程中所形成的流量执行监控。安全规则通过控制器70及第二虚拟网线32暴露在安全容器10所属工作节点的内核空间的外部的虚拟网卡113传入安全容器10。安全容器10与业务容器11通过namespace予以隔离。安全容器10区别于提供业务运行环境的业务容器11及虚拟机40，且在计算机实例中，工作节点Node-1可视为一个物理节点，且该物理节点可由服务器或者超融合一体机予以部署所形成。同时，本实施例所揭示的安全容器10旨在提供为工作节点Node-1所部署的虚拟机40及业务容器11提供引流，并在引流过程中对进出工作负载1的数据报文执行清洗和/或过滤，并尤其旨在对恶意用户对工作负载1发起的非法流量攻击行为所形成的非法流量予以拦截，以确保虚拟机40及业务容器11的稳定性与安全性。同时，安全容器10基于容器技术所组建，安全容器10为容器应用及虚拟机应用提供一个完整的操作系统执行环境，并与宿主机操作系统隔离开，避免应用直接访问主机资源，从而可以在业务容器11之间提供额外的保护，并独立于业务容器11。用户只能访问业务容器11而不能访问安全容器10，且安全容器10对用户及业务容器11不可见。参图2至图6所示，可选地，工作负载1为业务容器11，第一引流网线由组建第一虚拟网线31与第二虚拟网线32所依赖的CNI组件在业务容器11中预先创建的第三虚拟网线33后，由第三虚拟网线33被迁移至安全容器10所形成，第二引流网线由管理员以命令行形式予以创建，因此所形成的对业务容器11执行引流的第一引流网线不依赖组建第一虚拟网线31与第二虚拟网线32所依赖的CNI组件而形成，从而有利于形成统一的引流方案；同时，由于不要需要依赖CNI组件，而在Kubernetes架构的计算机集群100中，对CNI组件的调用通常需要借助Kubelet并基于Yaml格式的CNI配置文件去配置具体的CNI组件。因此，在本申请中被创建的第一引流网线与第二引流网线不需要将借助CNI组件予以实现，从而降低了对计算机集群100的代码入侵性，并能够降低对业务容器11执行引流过程中所产生的计算开销。另一方面，参图3所示，在对业务容器11执行引流过程中，当数据报文达到第二虚拟网卡111时，触发将第三虚拟网线33迁移至安全容器10以形成第一引流网线的修改事件。此时，业务容器11与安全容器10之间的网络连接是断开的，并通过第二引流网线在业务容器11与安全容器10之间建立虚拟网络连接。具体地，第三虚拟网线33迁移至安全容器10以形成包含虚拟网卡114与虚拟网卡115的虚拟网线34，并将虚拟网线34作为第一引流网线，从而在对业务容器11执行引流过程中将进出业务容器11数据报文所形成的流量汇聚至安全容器10。诸如前述第一虚拟网线31、第二虚拟网线32、第三虚拟网线33等均为虚拟网线。虚拟网线用于连接两个成对设置的虚拟网卡。虚拟网线根据数据链路层的MAC地址对数据报文执行转发予以实现，其本质是反转通信数据的方向，将需要转发的数据报文转换成需要接收该数据报文，并重新送入内核网络进行处理，并最终完成数据报文的注入。同时，本申请中所涉及的各个虚拟网卡的IP地址及MAC地址均是通过namespace予以隔离的。参图3与图4所示，在本申请中，在将第三虚拟网线迁移至安全容器10以形成第一引流网线后还包括：下发引流策略至第一引流网线，并在将第三虚拟网线33迁移至安全容器10以形成第一引流网线过程中，通过数据报文检测单元30修改第三虚拟网线33两端的虚拟网卡的名称，且不修改第三虚拟网线33两端的虚拟网卡的MAC地址，第一虚拟网线31与第一引流网线一端的虚拟网卡部署于安全容器10所属工作节点的内核空间，第一虚拟网线31与第一引流网线另一端的虚拟网卡部署于安全容器10所属工作节点的内核空间的外部。具体而言，虚拟网卡115、虚拟网卡113及虚拟网卡102均暴露于工作节点Node-1的内核空间的外部。第三虚拟网线33两端分别形成位于所属工作节点的内核空间的虚拟网卡101及部署于安全容器10所属工作节点的内核空间的外部的虚拟网卡102，且虚拟网卡101及虚拟网卡102可在对业务容器11执行引流前通过组建第一虚拟网线31与第二虚拟网线32所依赖的CNI组件予以同时建立。数据报文检测单元30修改第三虚拟网线33两端的虚拟网卡的名称是指通过数据报文检测单元30修改虚拟网卡101与虚拟网卡102的名称，但在整个修改过程中不改变虚拟网卡101与虚拟网卡102的虚拟网卡IP地址及MAC地址，从而使得第三虚拟网线33从业务容器11向安全容器10执行迁移过程中不干涉业务容器11中响应用户发起的访问请求或者应用的运行，从而避免此种迁移过程被业务容器11所感知，以确保了业务容器11对用户始终能够提供可靠且稳定的服务，其中，服务运行在业务容器11中。安全容器10可通过前述网络连接方式连接一个或者多个业务容器11和/或一个或者多个虚拟机40。具体而言，参图4与图5所示，第一引流网线由成对的虚拟网卡112与虚拟网卡113组成，虚拟网卡113连接控制器70。基于用户对业务容器11发起的访问请求所生成的数据报文从双向箭头336向上的方向指向虚拟网卡115。参图4所示，第一虚拟网卡110、第二虚拟网卡111、虚拟网卡112、虚拟网卡114、虚拟网卡103及虚拟网卡104均为内核空间网卡，并仅向工作节点内核空间予以暴露，内核空间为Linux Kernel；虚拟网卡113与虚拟网卡115并不暴露于工作节点内核空间并仅用于向工作节点流入数据报文或者将数据报文流出工作节点Node-1。虚拟网卡114与虚拟网卡115之间组成一个成对设置的虚拟网线34，虚拟网卡103与虚拟网卡104之间组成一个成对设置的虚拟网线35。当数据报文通过该混合工作负载引流方法所揭示的技术方案进入业务容器11时，数据报文沿双向箭头336向上的方向流向虚拟网卡115并沿箭头34a的方向通过虚拟网线34及沿箭头35a的方向通过虚拟网线35先进入到安全容器10后再进入到业务容器11。数据报文在引流策略引导下在进入到安全容器10后首先被转发至第二虚拟网卡111，并从第二虚拟网卡111转发重新转发至虚拟网卡103，并最终通过虚拟网线35进入到业务容器11中。同理所述，数据报文从业务容器11中流出工作节点Node-1的过程为前述过程的逆向转发。即，数据报文从虚拟网卡104沿箭头35a的逆向方向通过虚拟网线35转发至虚拟网卡103以进入安全容器10，虚拟网卡103将数据报文转发至第二虚拟网卡111，然后由第二虚拟网卡111将数据报文转发至虚拟网卡114，并沿箭头34a的逆向方向通过虚拟网线34将数据报文转发至虚拟网卡115，从而将数据报文流出安全容器10，并最终通过双向箭头336向下所示出的方向流出安全容器10，并最终流出该工作节点Node-1。参图2至图6所示，可选地，工作负载1为虚拟机40，第一引流网线由br-int42及连接br-int42与安全容器10的第四虚拟网线组成，第二引流网线由QBR41、连接QBR41与安全容器10的第五虚拟网线及连接虚拟机40与QBR41的第六虚拟网线组成。br-int42配置暴露于工作节点Node-1的物理网卡414，QBR41配置与虚拟机40建立虚拟网络连接的虚拟网络设备411，虚拟网络设备411包括Tap设备或者Tun设备。具体而言，参图6所示，第六虚拟网线由部署于虚拟机40的虚拟网卡Eth0与部署于QBR41的虚拟网络设备411组成并成对设置，第五虚拟网线由部署于QBR41的虚拟网卡412与部署于安全容器10的虚拟网卡105组成并成对设置。第四虚拟网线由安全容器10的虚拟网卡106与部署于br-int42的虚拟网卡413组成并成对设置。br-int42配置物理网卡P_Eth414。如图5与图6所示，在工作负载1为虚拟机40的技术场景中，申请人对虚拟机40执行引流的具体过程予以阐述。第一引流网线可视为由br-int42及第四虚拟网线所组成的数据报文转发路径，第二引流网线可视为由第五虚拟网线、QBR41及第六虚拟网线所组成的数据报文转发路径。QBR41北向连接虚拟机40，南向连接安全容器10。当虚拟机40的数据报文流出虚拟机40并通过物理网卡P_Eth414最终流出工作节点Node-1时，数据报文沿箭头381通过第六虚拟网线从虚拟网卡Eth0转发至Tap设备，在QBR41内部沿箭头382转发至虚拟网卡412。数据报文在QBR41中执行二层转发，并转发至部署于QBR41的虚拟网卡412中。在实际环境中，虚拟网卡412可由Tap设备予以创建，且虚拟网卡412的数量可为一个或者多个。然后，数据报文沿箭头383所示出的方向通过第五虚拟网线将数据报文转发至部署于安全容器10的虚拟网卡105。数据报文在引流策略引导下首先被转发至第二虚拟网卡111，并从第二虚拟网卡111重新转发至虚拟网卡106。然后，沿箭头384所示出的方向通过第四虚拟网线将数据报文转发至br-int42的虚拟网卡413。然后，沿箭头385所示出的方向将数据报文转发至br-int42的物理网卡P_Eth414，并从最终从物理网卡P_Eth414流出工作节点Node-1。流出工作节点Node-1的数据报文沿双向箭头386向下的方向流出工作节点Node-1以离开虚拟机40，并通过公网发送至用户所在的客户端。同理所述，参图6所示，数据报文从用户所在的客户端流入工作节点Node-1并最终流入虚拟机40的过程为前述过程的逆向转发。即，数据报文沿双向箭头386向上的方向流入工作节点Node-1并最终流入虚拟机40，物理网卡P_Eth414沿箭头391所示出的方向将数据报文转发至虚拟网卡413。然后，沿箭头392所示出的方向通过第四虚拟网线将数据报文从虚拟网卡413转发至部署于安全容器10的虚拟网卡106。数据报文在引流策略引导下首先从虚拟网卡106被转发至第二虚拟网卡111，然后再从第二虚拟网卡111重新转发至虚拟网卡105。然后，沿箭头393所示出的方向通过第五虚拟网线将数据报文从虚拟网卡105转发至部署于QBR42的虚拟网卡412，并通过二层转发沿箭头394所示出的方向将数据报文转发至虚拟网络设备411。最后，数据报文沿箭头395所示出的方向通过第六虚拟网线将数据报文最终转发至虚拟机40的虚拟网卡Eth0，以实现数据报文流入虚拟机40。在对进出虚拟机40的数据报文执行引流的技术场景中，数据报文在QBR41与br-int42内部执行转发过程中，通过QBR41与br-int42所组成的OVS连接且不要执行诸如前述对业务容器11执行引流前的迁移第三虚拟网线33至安全容器10的迁移操作，因此可对流经QBR41与br-int42的数据报文所指向的虚拟机40的虚拟网卡IP地址与MAC进行修改，且此种修改也是对虚拟机40无感知的，从而确保了在对虚拟机40执行引流过程中，虚拟机40对用户始终能够提供可靠且稳定的服务，其中，服务运行在虚拟机40中。安全容器10可通过前述网络连接方式连接一个或者多个虚拟机40。在本实施例中，借助OVS连接，有利于实现组建复杂的虚拟网络，以形成多条南北向的数据报文转发链路。因此，基于前述具体技术方案可知，通过实施本申请所揭示的一种混合工作负载引流方法，可同时对一个或者多个业务容器11或者一个或者多个虚拟机40基于相同的引流策略或者不同的引流策略独立地执行引流，以满足复杂场景中对不同的工作负载1的引流需要。基于前述具体实施方式所揭示的混合工作负载方法所包含的技术方案并参图2所示，本申请还揭示了一种计算机集群100，包括：控制器70，至少一个工作节点及纳管工作节点Node-1的控制管理平面71。示例性地，工作节点Node-1仅部署一个安全容器10及至少一个工作负载1，安全容器10配置数据报文检测单元30及用于下发引流策略的引流策略下发单元50。工作节点运行如上述具体实施方式所揭示的混合工作负载引流方法，以基于混合工作负载引流方法择一地或者同时对同一工作节点的工作负载1对进出工作负载1所形成的数据报文流量执行引流。结合图5与图6所示，本申请所揭示的计算机集群100可对进出虚拟机40的数据报文单独执行引流，也可对进出业务容器11的数据报文单独执行引流，或者，同时对虚拟机40与业务容器11基于不同的引流策略同时执行引流。具体而言，工作节点Node-1中同时部署一个安全容器10，至少一个虚拟机40与至少一个业务容器11。可选地，参图2所示，数据报文检测单元30部署于安全容器10且作为最优选的实施方式，或者，可选地，参图7所示，数据报文检测单元30部署于独立于安全容器10的数据库60，从而有利于数据报文检测单元30的部署操作具有更好的简便性与灵活性，并尤其地可降低安全容器10中各个模块的耦合度。同时，数据库60中的数据报文检测单元30可通过MQ、RPC、GRPC或者HTTP协议与第一虚拟网线31建立会话。安全容器10与创建第二虚拟网线32，控制管理平面71连接第二虚拟网线32，由第二虚拟网线32对安全容器10下发安全规则，并对工作负载1在执行引流过程中所形成的流量执行监控。对业务容器11及虚拟机40执行引流过程中所转发的数据报文进出安全容器10所形成的流量也可通过控制管理平面71供管理员72予以查看。控制管理平面71沿双向箭头711连接控制器70，管理员72通过双向箭头712在控制管理平面71所形成的可视化界面中以手动键入方式或者手动导入方式定义或者创建安全规则，并通过控制器70通过第二虚拟网线32向安全容器10下发安全规则。示例性地，双向箭头711与双向箭头712可基于HTTP协议的网络连接予以实现。最后，基于前述混合工作负载引流方法及计算机集群100所包含的技术方案，参图9所示，本申请还揭示了一种计算机可读介质900的一种具体实施方式。该计算机可读介质900可整体或者部分配置于物理形态的计算机、服务器、集群服务器或者数据中心中。具体地，在本实施例中，一种计算机可读介质900，该计算机可读介质900中存储有计算机程序指令901，计算机程序指令901被一处理器902读取并运行时，执行如前述具体实施方式所揭示的一种混合工作负载引流方法中的步骤。可选地，计算机可读介质900可配置为服务器，且该服务器运行于构建私有云、混合云或者公有云的物理设备上。混合工作负载引流方法的具体实现过程参前述具体实施方式所示，在此不再赘述。在本发明各个实施例中的各功能单元可以集成在一个处理单元中，也可以是各个单元单独物理存在，也可以两个或两个以上单元集成在一个单元中。上述集成的单元既可以采用硬件的形式实现，也可以采用软件功能单元的形式实现。上文所列出的一系列的详细说明仅仅是针对本发明的可行性实施方式的具体说明，它们并非用以限制本发明的保护范围，凡未脱离本发明技艺精神所作的等效实施方式或变更均应包含在本发明的保护范围之内。上文所列出的一系列的详细说明仅仅是针对本发明的可行性实施方式的具体说明，它们并非用以限制本发明的保护范围，凡未脱离本发明技艺精神所作的等效实施方式或变更均应包含在本发明的保护范围之内。对于本领域技术人员而言，显然本发明不限于上述示范性实施例的细节，而且在不背离本发明的精神或基本特征的情况下，能够以其他的具体形式实现本发明。因此，无论从哪一点来看，均应将实施例看作是示范性的，而且是非限制性的，本发明的范围由所附权利要求而不是上述说明限定，因此旨在将落在权利要求的等同要件的含义和范围内的所有变化囊括在本发明内。不应将权利要求中的任何附图标记视为限制所涉及的权利要求。此外，应当理解，虽然本说明书按照实施方式加以描述，但并非每个实施方式仅包含一个独立的技术方案，说明书的这种叙述方式仅仅是为清楚起见，本领域技术人员应当将说明书作为一个整体，各实施例中的技术方案也可以经适当组合，形成本领域技术人员可以理解的其他实施方式。
