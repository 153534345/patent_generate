标题title
个性化公共安全突发事件检测模型训练方法、检测方法及装置
摘要abst
本发明提供一种个性化公共安全突发事件检测模型训练方法、检测方法及装置，包括：获取完成当前轮次训练的各客户端的本地检测模型及其梯度；将构建的随机图输入各客户端本地检测模型，计算得到各客户端模型的梯度状态；在各客户端部署强化学习模块，以本地检测模型准确率构建奖励值；将基于策略网络得到的所有客户端之间梯度聚合的动作归一化，得到各客户端梯度的权重，根据权重确定下一轮参与聚合的客户端以及是否进行梯度量化；重复进行多轮训练，直至得到各客户端的公共安全突发事件检测模型。本发明提供的训练方法能够减小本地数据非独立同分布的影响，减小梯度通信的消耗，平衡模型的性能与通信压力。
权利要求书clms
1.一种个性化公共安全突发事件检测模型训练方法，其特征在于，所述方法包括以下步骤：获取完成当前轮次训练的各客户端的本地检测模型以及所述本地检测模型的梯度；所述本地检测模块根据相应客户端的本地数据训练得到；所述本地检测模型包括GarphSage和分类器，所述GarphSage对输入的本地数据的节点特征进行聚合，生成节点聚合特征；所述分类器根据所述节点聚合特征，得到各节点的分类概率；构建随机图，将所述随机图输入各客户端的本地检测模型，生成所述随机图中所有节点的节点聚合特征并计算均值，得到聚合特征均值，将所述聚合特征均值作为本地检测模型状态；将本轮训练后的本地检测模型状态与上一轮训练后的本地检测模型状态作差，得到本地检测模型梯度状态；在各客户端上部署强化学习模块，所述强化学习模块包括策略网络和价值网络；所述策略网络根据各客户端的本地检测模型梯度状态构建状态空间，根据各客户端之间的梯度聚合操作构建动作空间；所述策略网络根据状态选择相应的动作，所述价值网络根据所述策略网络选择的动作生成预估q值；采用经验回放的方法对所述强化学习模块进行训练，构建所述策略网络和所述价值网络的联合损失，以最小化所述联合损失、最大化奖励值为目标，优化所述强化学习模块；其中，所述奖励值根据所述本地检测模型的分类准确率设置；将所有客户端之间梯度聚合的动作进行归一化，得到各客户端梯度的权重；按照所述权重排序，权重大的前第一预设数量个客户端参与下一轮聚合，在所述第一预设数量中，权重大的前第二预设数量个客户端的梯度不进行量化，其余进行梯度量化；基于上述步骤进行多轮训练，直至满足预设性能要求，得到各客户端的公共安全突发事件检测模型。2.根据权利要求1所述的个性化公共安全突发事件检测模型训练方法，其特征在于，所述本地检测模型的训练方法包括以下步骤：获取各客户端的本地数据，所述本地数据为图结构，包括节点、边和节点特征，将预设社交平台上关于公共安全突发事件的文本作为节点，将存在关联的文本对应的节点之间连边，为各节点添加真实分类标签；获取初始网络模型，所述初始网络模型包括GarphSage和分类器；采用所述本地数据对所述初始网络模型进行训练，并构建所述分类概率和所述真实分类标签之间的分类损失，优化得到各客户端的本地检测模型。3.根据权利要求2所述的个性化公共安全突发事件检测模型训练方法，其特征在于，采用基于图采样的mini-batch机制训练所述初始网络模型，包括以下步骤：在所述本地数据中随机选择第三预设数量个节点，作为目标节点；从每个目标节点的邻居节点中随机选择第四预设数量个节点，作为一阶邻居节点；从所述本地数据中抽取相应的边，由所述目标节点、所述一阶邻居节点和抽取得到的边生成子图，将所述子图作为训练集训练所述初始网络模型。4.根据权利要求2所述的个性化公共安全突发事件检测模型训练方法，其特征在于，所述分类损失为交叉熵损失，计算式为：其中，Lcl表示所述分类损失；pvi表示所述节点vi的分类概率；yvi表示所述节点的真实分类标签；V表示总节点数。5.根据权利要求1所述的个性化公共安全突发事件检测模型训练方法，其特征在于，在所述GarphSage之前设有一个全连接层，利用所述全连接层对输入的各节点特征进行降维操作，以去除冗余信息；降维的计算过程为：其中，表示节点vi降维后的节点特征；winput和binput是特征降维隐藏层的参数；σ表示激活函数；hvi表示节点vi降维前的节点特征。6.根据权利要求1所述的个性化公共安全突发事件检测模型训练方法，其特征在于，所述策略网络根据各客户端的本地检测模型梯度状态构建状态空间，还包括：采用各客户端本地检测模型梯度状态的差值作为状态构建所述状态空间，计算式为：其中，表示客户端i的本地检测模型梯度状态；/＞表示客户端m的本地检测模型梯度状态。7.根据权利要求1所述的个性化公共安全突发事件检测模型训练方法，其特征在于，构建所述策略网络和所述价值网络的联合损失，还包括：在所述价值网络中，构建所述预估q值和目标q值之间的损失函数，所述损失函数采用均方误差损失，计算式为：其中，s表示状态；a表示动作；r表示所述奖励值；s'表示新的状态；D表示缓冲区，用来保存经验；fc是所述价值网络，生成所述预估q值；z表示所述目标q值；在所述策略网络中，策略网络的损失函数为预期回报的负值，计算式为：其中，s表示状态；D表示缓冲区，用来保存经验；fc是所述价值网络，生成所述预估q值；fa是所述策略网络。8.根据权利要求1所述的个性化公共安全突发事件检测模型训练方法，其特征在于，所述奖励值的计算式为：其中，accpre和acc分别表示各客户端本地检测模型的分类准确率和根据所述策略网络执行的相应动作后，新的本地检测模型的分类准确率。9.一种公共安全突发事件检测方法，其特征在于，所述方法包括以下步骤：获取待检测的图结构数据，所述图结构数据将预设社交平台上关于公共安全突发事件的文本作为节点，存在关联的文本对应的节点之间连边；将所述图结构数据输入如权利要求1至8中任一项所述个性化公共安全突发事件检测模型训练方法得到的公共安全突发事件检测模型，得到所述图结构数据中各节点的分类结果；所述分类结果用于判断各节点对应的公共安全突发事件。10.一种计算机可读存储介质，其上存储有计算机程序，其特征在于，该程序被处理器执行时实现如权利要求1至9中任一项所述方法的步骤。
说明书desc
技术领域本发明涉及公共安全突发事件技术领域，尤其涉及一种基于强化联邦图神经网络的个性化公共安全突发事件检测模型训练方法、检测方法及装置。背景技术公共安全突发事件是指在一段时间内与公共安全相关的事件爆发并迅速传播、引起公众广泛关注的现象。对这类事件进行及时检测和响应具有重要的意义，可以帮助决策者更好地管理危机和做出决策。近年来，随着社交媒体的兴起，公共安全突发事件的检测和演化发现已成为社交媒体挖掘的研究热点，受到了学术界和工业界的广泛关注。相比传统的文本挖掘或社会网络挖掘，公共安全突发事件检测任务更具挑战性，因为它涉及社交网络和文本流的复杂交互。在社交媒体平台如微博、Twitter上，公共安全事件通常以短文本的形式描述，并通过时空共现、主题、发布信息、转发关系和标签信息等多个维度进行关联构建。将公共安全数据转化为图的形式，进一步进行事件检测和演化发现，已成为主流方法。图神经网络是机器学习领域的热门研究方向之一。与传统的神经网络不同，图神经网络是一种专门处理图数据的神经网络模型，可以同时利用数据的特征信息与结构信息。图神经网络已经被广泛应用于社交网络分析、交通预测、药物结构预测、推荐系统、查询检索等。随着大数据的发展，每天都会产生大量的原始公共安全数据，高质量的数据可以提高模型的有效性。但是大量数据缺少标注信息，而人工标注成本高、时间长、效率低，需要训练模型对图数据进行自动的分析。由于高质量的数据通常归属于政府、公司和组织，以及涉及隐私、法规和利益等因素，这些数据不能在各方之间自由流动，很难通过数据集中的方式学习到有效的事件检测模型。联邦学习是一种分布式机器学习方法，它可以在数据不出本地的情况下，采用多方协作的方式共同训练模型。联邦学习不需要集中数据，在一定程度上保护了数据的隐私，还可以减少数据传输和存储的成本。在实际应用中，拥有公共安全数据的各方由于关注的主题与任务不同，导致各方数据是非独立同分布的，每个客户端中的数据都只有部分的标签，与传统的数据Non-iid体现在标签分布不均匀不同，这种Non-iid会同时体现在标签与图结构上。然而，在联邦设置中训练图神经网络仍然存在联邦图神经网络在Non-iid设置中表现不佳的问题，其原因是因为错误地假设一个全局模型可以适合所有客户端。为了让各客户端可以在数据不出本地的条件下，利用各方数据学习适用于本地任务的模型，研究人员提出了个性化联邦学习，允许各客户端采用差异化聚合策略，将其他客户端的模型参数或梯度聚合到本地，构建个性化的模型完成本地任务。现有方法主要基于微调全局模型、正则化和注意力机制，用于突发事件检测的个性化联邦图神经网络的研究存在通信量大、性能与通信压力难以平衡的问题。强化学习可以根据环境与状态学习一个最优的动作策略, 已经广泛应用到机器人控制、图神经网络节点选择、自然语言处理等任务中。有研究将强化学习引入到联邦学习的节点选择任务中，但使用客户端选择的方式减小通信压力会造成联邦学习训练过程中梯度信息的损失、模型状态信息的损失。发明内容鉴于此，本发明实施例提供了一种个性化公共安全突发事件检测模型训练方法、检测方法及装置，以消除或改善现有技术中存在的一个或更多个缺陷，解决由于公共安全数据获取途径受限、非独立同分布的特性，导致难以集中训练统一检测模型的问题，以及现有个性化联邦图神经网络存在通信量大、性能和通信压力难以平衡的问题。一方面，本发明提供了一种个性化公共安全突发事件检测模型训练方法，其特征在于，所述方法包括以下步骤：获取完成当前轮次训练的各客户端的本地检测模型以及所述本地检测模型的梯度；所述本地检测模块根据相应客户端的本地数据训练得到；所述本地检测模型包括GarphSage和分类器，所述GarphSage对输入的本地数据的节点特征进行聚合，生成节点聚合特征；所述分类器根据所述节点聚合特征，得到各节点的分类概率；构建随机图，将所述随机图输入各客户端的本地检测模型，生成所述随机图中所有节点的节点聚合特征并计算均值，得到聚合特征均值，将所述聚合特征均值作为本地检测模型状态；将本轮训练后的本地检测模型状态与上一轮训练后的本地检测模型状态作差，得到本地检测模型梯度状态；在各客户端上部署强化学习模块，所述强化学习模块包括策略网络和价值网络；所述策略网络根据各客户端的本地检测模型梯度状态构建状态空间，根据各客户端之间的梯度聚合操作构建动作空间；所述策略网络根据状态选择相应的动作，所述价值网络根据所述策略网络选择的动作生成预估q值；采用经验回放的方法对所述强化学习模块进行训练，构建所述策略网络和所述价值网络的联合损失，以最小化所述联合损失、最大化奖励值为目标，优化所述强化学习模块；其中，所述奖励值根据所述本地检测模型的分类准确率设置；将所有客户端之间梯度聚合的动作进行归一化，得到各客户端梯度的权重；按照所述权重排序，权重大的前第一预设数量个客户端参与下一轮聚合，在所述第一预设数量中，权重大的前第二预设数量个客户端的梯度不进行量化，其余进行梯度量化；基于上述步骤进行多轮训练，直至满足预设性能要求，得到各客户端的公共安全突发事件检测模型。在本发明的一些实施例中，所述本地检测模型的训练方法包括以下步骤：获取各客户端的本地数据，所述本地数据为图结构，包括节点、边和节点特征，将预设社交平台上关于公共安全突发事件的文本作为节点，将存在关联的文本对应的节点之间连边，为各节点添加真实分类标签；获取初始网络模型，所述初始网络模型包括GarphSage和分类器；采用所述本地数据对所述初始网络模型进行训练，并构建所述分类概率和所述真实分类标签之间的分类损失，优化得到各客户端的本地检测模型。在本发明的一些实施例中，采用基于图采样的mini-batch机制训练所述初始网络模型，包括以下步骤：在所述本地数据中随机选择第三预设数量个节点，作为目标节点；从每个目标节点的邻居节点中随机选择第四预设数量个节点，作为一阶邻居节点；从所述本地数据中抽取相应的边，由所述目标节点、所述一阶邻居节点和抽取得到的边生成子图，将所述子图作为训练集训练所述初始网络模型。在本发明的一些实施例中，所述分类损失为交叉熵损失，计算式为：其中，Lcl表示所述分类损失；pvi表示所述节点vi的分类概率；yvi表示所述节点的真实分类标签；V表示总节点数。在本发明的一些实施例中，在所述GarphSage之前设有一个全连接层，利用所述全连接层对输入的各节点特征进行降维操作，以去除冗余信息；降维的计算过程为：其中，表示节点vi降维后的节点特征；winput和binput是特征降维隐藏层的参数；σ表示激活函数；hvi表示节点vi降维前的节点特征。在本发明的一些实施例中，所述策略网络根据各客户端的本地检测模型梯度状态构建状态空间，还包括：采用各客户端本地检测模型梯度状态的差值作为状态构建所述状态空间，计算式为：其中，表示客户端i的本地检测模型梯度状态；/＞表示客户端m的本地检测模型梯度状态。在本发明的一些实施例中，构建所述策略网络和所述价值网络的联合损失，还包括：在所述价值网络中，构建所述预估q值和目标q值之间的损失函数，所述损失函数采用均方误差损失，计算式为：其中，s表示状态；a表示动作；r表示所述奖励值；s'表示新的状态；D表示缓冲区，用来保存经验；fc是所述价值网络，生成所述预估q值；z表示所述目标q值；在所述策略网络中，策略网络的损失函数为预期回报的负值，计算式为：其中，s表示状态；D表示缓冲区，用来保存经验；fc是所述价值网络，生成所述预估q值；fa是所述策略网络。在本发明的一些实施例中，所述奖励值的计算式为：其中，accpre和acc分别表示各客户端本地检测模型的分类准确率和根据所述策略网络执行的相应动作后，新的本地检测模型的分类准确率。另一方面，本发明还提供一种公共安全突发事件检测方法，其特征在于，所述方法包括以下步骤：获取待检测的图结构数据，所述图结构数据将预设社交平台上关于公共安全突发事件的文本作为节点，存在关联的文本对应的节点之间连边；将所述图结构数据输入如上文中任一项所述个性化公共安全突发事件检测模型训练方法得到的公共安全突发事件检测模型，得到所述图结构数据中各节点的分类结果；所述分类结果用于判断各节点对应的公共安全突发事件。另一方面，本发明还提供一种计算机可读存储介质，其上存储有计算机程序，该程序被处理器执行时实现如上文中提及的任意一项所述方法的步骤。本发明的有益效果至少是：本发明提供一种个性化公共安全突发事件检测模型训练方法、检测方法及装置，基于去中心化的联邦学习和强化学习帮助各客户端训练一个个性化的公共安全突发事件检测模型，用于对本地公共安全事件进行检测。首先设计客户端本地训练模块，采用基于图采样的minibatch机制训练本地模型，减少本地数据因采集时间、主体等不同导致的非独立同分布问题对其他客户端的影响，提高模型鲁棒性。支持对各客户端本地检测模型的梯度进行量化，减轻通讯压力。其次设计基于随机图嵌入的客户端状态感知模块，在保护本地数据隐私的同时，将各客户端的本地检测模型梯度信息转换为低维向量的形式，保留模型有价值的信息，帮助强化学习模块更好地感知各客户端的梯度状态。最后设计强化学习模块，采用深度确定性的策略梯度模型DDPG，基于本地检测模型准确率，构建强化学习的奖励值，拟合一个个性化联邦学习梯度聚合加权策略，根据权重决定是否可以对梯度进行量化来减小通信压力，平衡模型的性能与通信压力。本发明的附加优点、目的，以及特征将在下面的描述中将部分地加以阐述，且将对于本领域普通技术人员在研究下文后部分地变得明显，或者可以根据本发明的实践而获知。本发明的目的和其它优点可以通过在说明书以及附图中具体指出的结构实现到并获得。本领域技术人员将会理解的是，能够用本发明实现的目的和优点不限于以上具体所述，并且根据以下详细说明将更清楚地理解本发明能够实现的上述和其他目的。附图说明此处所说明的附图用来提供对本发明的进一步理解，构成本申请的一部分，并不构成对本发明的限定。在附图中：图1为本发明一实施例中个性化公共安全突发事件检测模型训练方法的步骤示意图。图2为本发明一实施例中个性化公共安全突发事件检测模型的结构示意图。具体实施方式为使本发明的目的、技术方案和优点更加清楚明白，下面结合实施方式和附图，对本发明做进一步详细说明。在此，本发明的示意性实施方式及其说明用于解释本发明，但并不作为对本发明的限定。在此，还需要说明的是，为了避免因不必要的细节而模糊了本发明，在附图中仅仅示出了与根据本发明的方案密切相关的结构和/或处理步骤，而省略了与本发明关系不大的其他细节。应该强调，术语“包括/包含”在本文使用时指特征、要素、步骤或组件的存在，但并不排除一个或更多个其它特征、要素、步骤或组件的存在或附加。在此，还需要说明的是，如果没有特殊说明，术语“连接”在本文不仅可以指直接连接，也可以表示存在中间物的间接连接。在下文中，将参考附图描述本发明的实施例。在附图中，相同的附图标记代表相同或类似的部件，或者相同或类似的步骤。为了解决由于公共安全数据获取途径受限、非独立同分布的特性，导致难以集中训练统一检测模型的问题，以及现有个性化联邦图神经网络存在通信量大、性能和通信压力难以平衡的问题，本发明提供一种个性化公共安全突发事件检测模型训练方法，如图1所示，该方法包括以下步骤S101~S105：步骤S101：获取完成当前轮次训练的各客户端的本地检测模型以及本地检测模型的梯度。其中，本地检测模块根据相应客户端的本地数据训练得到；本地检测模型包括GarphSage和分类器，GarphSage对输入的本地数据的节点特征进行聚合，生成节点聚合特征；分类器根据节点聚合特征，得到各节点的分类概率。步骤S102：构建随机图，将随机图输入各客户端的本地检测模型，生成随机图中所有节点的节点聚合特征并计算均值，得到聚合特征均值，将聚合特征均值作为本地检测模型状态；将本轮训练后的本地检测模型状态与上一轮训练后的本地检测模型状态作差，得到本地检测模型梯度状态。步骤S103：在各客户端上部署强化学习模块，其中，强化学习模块包括策略网络和价值网络；策略网络根据各客户端的本地检测模型梯度状态构建状态空间，根据各客户端之间的梯度聚合操作构建动作空间；策略网络根据状态选择相应的动作，价值网络根据所述策略网络选择的动作生成预估q值。采用经验回放的方法对强化学习模块进行训练，构建策略网络和价值网络的联合损失，以最小化联合损失、最大化奖励值为目标，优化强化学习模块。其中，奖励值根据本地检测模型的分类准确率设置。步骤S104：将所有客户端之间梯度聚合的动作进行归一化，得到各客户端梯度的权重；按照权重排序，权重大的前第一预设数量个客户端参与下一轮聚合，在第一预设数量中，权重大的前第二预设数量个客户端的梯度不进行量化，其余进行梯度量化。步骤S105：基于上述步骤进行多轮训练，直至满足预设性能要求，得到各客户端的公共安全突发事件检测模型。如图2所示，为训练各客户端公共安全突发事件检测模型的结构图。步骤S101对应于图2中的联邦公共安全突发事件检测模型的本地训练与梯度量化部分，即，在各客户端本地，利用本地数据训练得到本地检测模型，并对本地检测模型的梯度进行量化。在一些实施例中，本地数据为图结构，包括节点、边和节点特征，即图2中的，G=，将预设社交平台上关于公共安全突发事件的文本作为节点，将存在关联的文本对应的节点之间连边。其中，预设社交平台包括微博、Twitter等，文本之间存在关联指任意两个文本中包含相同事件名称、拥有相关或相同的主题、拥有相同的标签等。本地检测模型包括GarphSage和分类器，GarphSage对输入的本地数据的节点特征进行聚合，生成节点聚合特征，分类器根据节点聚合特征，得到各节点的分类概率，即得到各节点对应的文本属于哪种公共安全突发事件的概率。在一些实施例中，考虑到在本地检测模型的GraphSage中，如果节点特征的维度过大，会导致模型难以学习到有效的节点表示。因为在图结构中，一个节点的特征不仅仅取决于自身的特征，还取决于其周围邻居节点的特征，如果特征维度过大，在聚合周围邻居节点的特征时会产生噪声，因此，本发明在GarphSage之前设有一个全连接层，利用全连接层对输入的各节点特征进行降维操作，可以看作是非线性的映射，将高维的节点特征映射到一个低维的空间，在这个低维空间中，原本高维特征中的冗余信息被剔除，保留了更加关键的信息。将降维后的节点特征输入GraphSage，模型只需要处理低维的特征，提高了模型的性能。根据上述内容，降维的计算过程可以表示为公式： 其中，表示节点vi降维后的节点特征；winput和binput是特征降维隐藏层的参数；σ表示激活函数；hvi表示节点vi降维前的节点特征。在节点分类任务中，将由GraphSage生成的节点聚合特征输入分类器，通过一个全连接层和一个Softmax函数得到各节点的分类概率分布，如公式所示：其中，pvi表示节点vi的分类概率；wcl和bcl是分类器的可训练参数；表示节点聚合特征。采用本地数据对初始网络模型进行训练，以得到各客户端的本地检测模型。在一些实施例中，采用基于图采样的mini-batch机制对模型进行训练，具体包括以下步骤：在本地数据中随机选择第三预设数量个节点，作为mini-batch的目标节点；对于每个目标节点，从其邻居节点中再随机选择第四预设数量个节点，作为一阶邻居节点；由此，得到包含有目标节点和一阶邻居节点的节点集合；从本地数据中抽取节点集合中各节点相应的边，由目标节点、一阶邻居节点和抽取得到的边生成子图，该子图包含了目标节点以及多跳邻居节点的信息，适用于mini-batch训练，因此，将得到的子图作为训练集，训练各客户端的初始模型，以得到本地检测模型。在一些实施例中，在模型训练中，通过构建分类概率和真实分类标签之间的分类损失，对模型参数进行优化。在一些实施例中，分类损失选用交叉熵损失，计算式如公式所示： 其中，Lcl表示分类损失；pvi表示节点vi的分类概率；yvi表示节点的真实分类标签；V表示总节点数。在一些实施例中，在获得分类损失后，使用反向传播算法计算各客户端本地检测模型的梯度，即GraphSage和分类器的梯度。采用梯度量化的方式对各客户端本地检测模型的梯度进行压缩。其中，梯度量化需要遍历所有的梯度，找到梯度的最大值和最小值，确定量化范围，根据梯度的范围与得到的量化范围计算缩放因子和零点。缩放因子表示浮点值和量化值之间的比例关系。各客户端共享传播各自本地检测模型的梯度，当梯度传到其他客户端进行聚合计算时，再基于缩放因子和零点，将量化的梯度值恢复回浮点数。该设计能够在尽可能小地损失模型性能的情况下，大幅降低了梯度所需的存储空间和传输带宽。在步骤S102中，考虑到各客户端本地数据的隐私性，本发明设计了基于随机图嵌入的客户端状态感知，在保护隐私的同时，将客户端的模型梯度信息转换为低维向量的形式，保留客户端模型有价值的信息，并且通过计算本地检测模型的梯度状态为下文强化学习模块构建状态空间，使强化学习模块更好地感知各客户端的状态，从而更有效地进行联邦学习中梯度的加权操作。构建一个全局统一的随机图。其中，随机图的节点数量可以根据实际需求进行设置，随机图中节点之间存在边的概率是根据各客户端本地数据中节点之间存在边的概率均值决定的，随机图中的节点特征是通过对基于各客户端本地数据节点特征的分布信息构建的全局数据分布进行随机采样得到。将随机图输入各客户端的本地检测模型中，由GarphSage计算得到随机图中所有节点的节点聚合特征，求取所有节点的节点聚合特征的均值，得到聚合特征均值，作为本地检测模型的状态表示，计算过程如公式所示： 其中，表示客户端m本地检测模型状态；mean表示计算均值；Gr表示随机图；Hr表示随机图的节点特征。在联邦学习中各客户端需要聚合的是梯度，为了使强化学习能够感知梯度的状态，将本轮训练后的本地检测模型状态与上一轮训练后的本地检测模型状态作差，得到本地检测模型梯度状态，计算过程如公式所示：其中，表示第e轮训练后客户端m的本地检测模型梯度状态；/＞表示第e轮训练后客户端m的本地检测模型状态；/＞第e-1轮训练后客户端m的本地检测模型状态。随着训练的进行，各客户端的本地检测模型状态会发生变化，这种变化可以通过梯度状态得到体现。通过分析梯度状态，可以帮助强化学习模块感知本地检测模型参数更新的情况，从而对梯度的加权与量化进行决策，解决了传统降维方法可能存在的信息丢失问题，在保留了客户端信息的同时，减小状态的维度，提高模型训练的效率，同时减小了通信的压力。在步骤S103中，在各客户端上部署强化学习模块，基于DDPG去拟合一个个性化联邦学习梯度聚合加权策略，根据权重决定是否对梯度进行量化来减小通信压力。强化学习模块包括策略网络和价值网络。其中，策略网络用于选择最优的动作，而价值网络用于评估策略网络选择的动作。策略网络和价值网络都由深度神经网络构建。策略网络对于给定的状态s，输出一个确定的动作a，即a=fa，其中fa是策略网络。价值网络则根据给定的状态-动作对，预测期望的回报，即预估q值，q=fc)，其中fc是价值网络。策略网络根据各客户端的本地检测模型梯度状态构建状态空间，可以采用拼接或差值的方式构建状态空间。在一些实施例中，采用各客户端本地检测模型梯度状态的差值作为状态构建所述状态空间，可以减小DDPG参数量，增加联邦模型的稳定性，计算式如公式所示： 其中，表示客户端i的本地检测模型梯度状态；/＞表示客户端m的本地检测模型梯度状态。根据各客户端之间的梯度聚合操作构建动作空间，即策略网络根据各客户端之间的梯度状态，选择梯度聚合动作，计算式如公式所示： 其中，表示在第e轮训练后客户端m聚合到客户端j的动作；fa表示策略网络；/＞表示第e轮训练后客户端m的本地检测模型梯度状态；/＞表示第e轮训练后客户端j的本地检测模型梯度状态。然后对强化学习模块的训练进行说明。在训练过程中，强化学习模块采用经验回放和目标网络两种机制来增强模型的稳定性。其中，经验回放是指从环境中采样状态s，根据状态s使用策略网络选择动作a。执行动作a后，从反馈中得到奖励值r和新的状态s'，由此，得到经验，并存储到缓冲区，然后从缓冲区中随机采样一批经验对强化学习模块进行训练。对于策略网络和价值网络的训练可以看作是最小化两个损失函数的过程。在一些实施例中，对于价值网络，构建预估q值和目标q值之间的损失函数，其损失函数采用均方误差损失，表示了价值网络预估的q值和实际经验的目标q值之间的差距，计算式如公式所示： 其中，s表示状态；a表示动作；r表示奖励值；s'表示新的状态；D表示缓冲区，用来保存经验；fc是价值网络，生成预估q值；z表示目标q值。公式中目标q值的计算式如公式所示：其中，z表示目标q值；r表示奖励值；γ表示折扣因子；fc'是目标价值网络；fa'是目标策略网络；s'表示新的状态。对于策略网络，策略网络的损失函数定义为预期回报的负值，计算式如公式所示：其中，s表示状态；D表示缓冲区，用来保存经验；fc是价值网络，生成预估q值；fa是策略网络。此损失函数表示的是策略网络生成的策略的预期回报的负值。在优化过程中，试图最小化此损失，从而最大化预期回报。目标网络主要作用是提供稳定的目标值，以帮助稳定训练过程，因为如果目标不断变化，网络便很难收敛。在一些实施例中，强化学习模块的奖励值根据本地检测模型的分类准确率设置，计算式如公式所示： 其中，accpre和acc分别表示各客户端本地检测模型的分类准确率和根据策略网络执行的相应动作后，新的本地检测模型的分类准确率。如公式所示，如果本地检测模型的准确率提升，则奖励值就是准确率提升的数值，得到积极的反馈；相反，如果本地检测模型的准确率下降，奖励值就是准确率下降的负值，得到负面的反馈；同时，如果通信量过大，也会对奖励值产生负面影响。在步骤S104中，将基于策略网络得到的所有客户端之间梯度聚合的动作进行归一化，得到各客户端梯度的权重。按照权重排序，权重大的前第一预设数量个客户端参与下一轮聚合，在第一预设数量中，权重大的前第二预设数量个客户端的梯度不进行量化，其余进行梯度量化。具体的，为了支持选择参与聚合的客户端比例pc和被选中的客户端梯度进行量化的比例pq，将梯度聚合的权重从大到小排序，选择权重大的前n×pc个客户端参与下一轮聚合，前n×pc×个客户端的梯度不进行量化，其他被选中客户端的梯度在传输到本地前需要进行量化。由此确定了在下一轮训练后参与聚合的客户端，及其是否需要进行梯度量化。由此，在个性化联邦梯度聚合工作中，通过多方协作的方式，为每个客户端训练一个本地检测模型和强化学习模块。其中，在第一轮训练中，各客户端需对模型参数进行统一的初始化，每轮训练中，每个客户端都会接收到其他客户端本轮训练得到的本地检测模型的梯度。除了学习最优的客户端选择策略以外，还需要根据不同客户端本地检测模型的梯度对本地检测模型的重要性赋予不同的梯度权重，并根据重要性决定下一轮参与聚合的客户端以及相应客户端模型的梯度是否需要进行量化后再传输，以减小通信压力。在一些实施例中，各客户端的强化学习模块采用fedavg来聚合参数。基于上述步骤，如图2右侧客户端通信部分可知，在每轮训练后，各客户端需要分享其本地检测模型的梯度、强化学习模块的梯度、梯度量化指令和客户端梯度状态，以及在生成随机图时，各客户端需要分享本地数据节点间存在边的比例和节点特征的均值与标准差信息。在步骤S105中，基于步骤S101~S104，对各客户端的本地检测模型进行多轮训练，直至满足预设性能要求，以得到各客户端个性化的公共安全突发事件检测模型。本发明还提供一种公共安全突发事件检测方法，该方法包括以下步骤S201~S202：步骤S201：获取待检测的图结构数据，其中，图结构数据将预设社交平台上关于公共安全突发事件的文本作为节点，存在关联的文本对应的节点之间连边。步骤S202：将图结构数据输入如上文所述个性化公共安全突发事件检测模型训练方法得到的公共安全突发事件检测模型，得到图结构数据中各节点的分类结果，基于分类结果判断各节点对应的公共安全突发事件。本发明还提供一种计算机可读存储介质，其上存储有计算机程序，该程序被处理器执行时实现个性化公共安全突发事件检测模型训练方法和公共安全突发事件检测方法的步骤。与上述方法相应地，本发明还提供了一种设备，该设备包括计算机设备，所述计算机设备包括处理器和存储器，所述存储器中存储有计算机指令，所述处理器用于执行所述存储器中存储的计算机指令，当所述计算机指令被处理器执行时该设备实现如前所述方法的步骤。本发明实施例还提供一种计算机可读存储介质，其上存储有计算机程序，该计算机程序被处理器执行时以实现前述边缘计算服务器部署方法的步骤。该计算机可读存储介质可以是有形存储介质，诸如随机存储器、内存、只读存储器、电可编程ROM、电可擦除可编程ROM、寄存器、软盘、硬盘、可移动存储盘、CD-ROM、或技术领域内所公知的任意其它形式的存储介质。综上所述，本发明提供一种个性化公共安全突发事件检测模型训练方法、检测方法及装置，基于去中心化的联邦学习和强化学习帮助各客户端训练一个个性化的公共安全突发事件检测模型，用于对本地公共安全事件进行检测。首先设计客户端本地训练模块，采用基于图采样的minibatch机制训练本地模型，减少本地数据因采集时间、主体等不同导致的非独立同分布问题对其他客户端的影响，提高模型鲁棒性。支持对各客户端本地检测模型的梯度进行量化，减轻通讯压力。其次设计基于随机图嵌入的客户端状态感知模块，在保护本地数据隐私的同时，将各客户端的本地检测模型梯度信息转换为低维向量的形式，保留模型有价值的信息，帮助强化学习模块更好地感知各客户端的梯度状态。最后设计强化学习模块，采用深度确定性的策略梯度模型DDPG，基于本地检测的模型准确率，构建强化学习的奖励值，拟合一个个性化联邦学习梯度聚合加权策略，根据权重决定是否可以对梯度进行量化来减小通信压力，平衡模型的性能与通信压力。本领域普通技术人员应该可以明白，结合本文中所公开的实施方式描述的各示例性的组成部分、系统和方法，能够以硬件、软件或者二者的结合来实现。具体究竟以硬件还是软件方式来执行，取决于技术方案的特定应用和设计约束条件。专业技术人员可以对每个特定的应用来使用不同方法来实现所描述的功能，但是这种实现不应认为超出本发明的范围。当以硬件方式实现时，其可以例如是电子电路、专用集成电路、适当的固件、插件、功能卡等等。当以软件方式实现时，本发明的元素是被用于执行所需任务的程序或者代码段。程序或者代码段可以存储在机器可读介质中，或者通过载波中携带的数据信号在传输介质或者通信链路上传送。需要明确的是，本发明并不局限于上文所描述并在图中示出的特定配置和处理。为了简明起见，这里省略了对已知方法的详细描述。在上述实施例中，描述和示出了若干具体的步骤作为示例。但是，本发明的方法过程并不限于所描述和示出的具体步骤，本领域的技术人员可以在领会本发明的精神后，作出各种改变、修改和添加，或者改变步骤之间的顺序。本发明中，针对一个实施方式描述和/或例示的特征，可以在一个或更多个其它实施方式中以相同方式或以类似方式使用，和/或与其他实施方式的特征相结合或代替其他实施方式的特征。以上所述仅为本发明的优选实施例，并不用于限制本发明，对于本领域的技术人员来说，本发明实施例可以有各种更改和变化。凡在本发明的精神和原则之内，所作的任何修改、等同替换、改进等，均应包含在本发明的保护范围之内。
