标题title
基于RC模型和深度强化学习的室内热环境的控制方法
摘要abst
本发明公开了一种基于RC模型和深度强化学习的室内热环境的控制方法属于人工智能和建筑环境控制相结合的技术领域，属于建筑环境控制技术领域。本发明通过对房屋搭建热容热阻RC模型，集成到能源系统中，通过与RC模型交互得到输入参数，利用构建的DRL控制器，输出得到对应建筑的暖通空调和蓄电池的运行调控策略，在保证室内温度的同时，实现了空调能耗的下降和家庭购电成本得最小化。与现有技术相比，本发明发挥基于RC模型的深度强化学习的优点，并结合PER算法使学习成本下降、学习曲线收敛加快，并提升建筑能源系统优化效果，系统整体性能提高。
权利要求书clms
1.一种基于RC模型和深度强化学习的室内热环境的控制方法，其特征在于，该方法包括以下步骤：S1：搭建建筑RC模型，作为环境模型；S2：获取观测数据：包括获取室内外热环境和蓄电池的相关参数；S3：搭建神经网络模型，使用该模型迭代预测下一时刻的室内温度值、空调耗电量、电池荷电状态；S4：利用S2获取的观测数据在S3建立的模型中进行循环运算，并引入D3QN强化学习方法，所述D3QN强化学习方法为DQN与PER算法相结合的控制策略；基于神经网络训练控制空调系统、蓄电池的智能体，通过对当前状态值的观测以及得到的奖励，自动学习空调系统控制制热量以及电池充放电决策的优化过程，得到最优控制策略，即得到最优的控制方法。2.如权利要求1所述的室内热环境的控制方法，其特征在于，所述S1中，所述建筑RC模型由热阻与热容构成；在单位面积、单位时间内透过围护结构的导热热量，称为热能，用q表示，其值为等式： 其中，T1，T2分别为围护结构两侧的表面温度℃；d为围护结构的厚度，λ为壁体材料导热系数；加热后房间内和室外的温度分别为Tr、To，围护结构的温度为TW，当热能q流入围护结构时，通过热能守恒方程，得到等式，代表室内外环境热量转换的基本过程； 式中Rr是房间空气热阻；Rw是房间围护结构热阻；C为热容。3.如权利要求1所述的室内热环境的控制方法，其特征在于，所述S2中，室内外热环境数据包括时刻信息，室外温度，室外相对湿度，太阳辐射强度，空调系统耗电量和室内温度；蓄电池的相关参数包括蓄电池模型所需的光伏、住宅负荷、电价信息。4.如权利要求1所述的室内热环境的控制方法，其特征在于，所述S3中，搭建神经网络模型：选用四层前馈全连接神经网络：一个输入层、两个隐含层、一个输出层，以建立系统的输入输出模型；选择均方差作为神经网络建模的损失函数；优化器选择随机梯度下降法来寻找模型最优解。5.如权利要求1所述的室内热环境的控制方法，其特征在于，所述神经网络模型中，选择输入层参数，并输出下一时刻的室内温度 、空调系统耗电量/＞和电池荷电状态Soc 。6.如权利要求5所述的室内热环境的控制方法，其特征在于，选择时刻信息，室外温度，室外相对湿度/＞，太阳辐射强度/＞，空调系统耗电量/＞，室内温度，S1 = 作为空调系统模型的输入层参数；选择Pv,Load, Price, S2 = 作为蓄电池模型的输入层参数； t表示时刻，隐含层节点数为10，模型的输出层为下一时刻的室内温度、下一时刻空调系统耗电量和荷电状态；隐含层选择Relu函数作为激活函数，输出层选择Sigmoid函数。7.如权利要求6所述的室内热环境的控制方法，其特征在于，所述神经网络模型中将控制方法运行优化问题建模为马尔科夫决策过程，具体包括：对空调系统S1和蓄电池S2两个部分：在空调系统S1中，状态观测空间S1设置为S1 = ，可控制变量为空调制热量，动作空间A1设置为A1 = ；在蓄电池模型S2中，状态空间S2 = ，动作空间A2 = , μ表示在能源系统中，住宅净负荷为0时，电池动作处于闲置状态；定义奖励函数设置在奖励函数R的设置为在保证室内热舒适区间的同时降低能源成本为优化目标，如式所示，由三部分组成，第一部分，P为控制时间步长内空调系统的能耗，为实时电；第二部分/＞ ，/＞为室内温度下限，/＞为室内温度上限；/＞为惩罚系数，体现了超越温度范围相对系统能源成本的影响；冬季室内温度保持在18~24°C之间，即为18°C，/＞为24°C；在满足室内热舒适温度区间时，智能体惩罚函数值为0，否则将对智能体进行惩罚；第三部分penalty,表示当电池容量保持在合理的工作范围内时，惩罚值为0;当智能体选择错误的动作,导致电池容量低于最小容量或高于最大容量而不能保证电池正常工作时，惩罚值为更大的100；R = -.P + penalty  + penalty   运行阶段设学习率为lr，折现因子为γ，贪婪率为ε，经验样本数为k，网络参数更新频率为C，迭代次数为U，单集最大步长为T。
说明书desc
技术领域本发明属于人工智能和建筑环境控制相结合的技术领域，特别是涉及一种基于RC模型和深度强化学习的室内热环境的控制方法。背景技术随着人类的活动加剧了气候变暖的进程，全球平均气温正在以前所未有的速度上升。据调查显示，目前建筑领域碳排放量每年约20亿吨，约占全国总碳排放量的40%。而住宅作为建筑领域能耗的主体，有着巨大的节能潜力。有很多方法可以实现住宅建筑的能耗目标，其中之一就是住宅能源管理系统接入可再生能源。可再生能源具有环境友好、发展潜力和可持续利用等优点，这无疑是缓解当前能源危机的关键因素。在所有可再生能源中，太阳能是增长最快的能源之一，由于近年来光伏安装成本逐渐下降，并且部署方便，家庭屋顶光伏安装呈现快速增长态势。随着其他可再生能源设备接入建筑能源系统，能源系统日趋复杂，给建筑能源系统的优化控制带来挑战。传统的基于规则和PID的控制，完全忽略了系统环境的随机性，遵循的是一种静态的操作策略，通常与最优策略相去甚远。强化学习作为机器学习的一个分支，专门为控制问题设计，并且结合人工智能领域的深度神经网络强大的非线性拟合能力在优化建筑控制方面得到了迅速发展，实现建筑能源效率、需求灵活性和弹性。为了避免对建筑能源系统的复杂建模以及提高计算速度，大多数RL在建筑能源系统的研究集中无模型的方法，然而在优化控制上存在以下局限： 由于每个建筑的保温性能会有所差异，采用无模型的方法无法做到精确控制，导致建筑热性能预测结果不准确 ；目前室内热舒适评价主要集中在热感觉投票指标上，即推荐一个满足舒适环境的室内温度设定范围。然而，建筑的动态物理模型对室内的热舒适有显著影响。发明内容本发明的目的在于提供一种基于RC模型和深度强化学习的室内热环境的控制方法，以弥补现有技术的不足。热电阻-电容网络的优点是简单性和计算效率，基于RC模型的数据驱动模型能够用于提高建筑物的能源性能；此外，建筑物的慢热动态特性使它相当于储热设施，为家庭能源管理系统在调度HVAC系统方面提供了额外的灵活性。RC模型代表了控制建筑及其与环境之间能量相互作用的基本物理关系，保留了对建筑能源和区域环境研究中通常使用的设计参数的敏感性；能够更好地理解建筑的物理问题，并使容易评估建模和不同参数的设置成为可能。强化学习的基本原理为智能体在环境反馈奖励或惩罚的刺激下持续学习，根据反馈不断调整策略，最终达成奖励最大化或实现特定目标。其中智能体是进行不断学习和实施动作的控制器，智能体之外所有与其相互作用的部分被称为环境。智能体在状态下，根据策略选择动作 ，环境将会基于智能体所做出的动作给出响应的奖励，并转移到下一时刻新的状态 ；根据获得的奖励获得最优策略：；其中： 为折扣率。强化学习的最终目的是通过最大化奖励值来获得最优策，具有较强的决策能力，在越来越复杂的现实场景中，深度学习可以从原始大规模交互数据中提取高级特征，具有较强的感知能力。本发明的目标是满足室内热舒适性要求的前提下，降低能源成本，这一目标由两个调控对象实现：降低空调系统的能耗；通过调控蓄电池，使本地光伏消纳率最大化，实现购电成本的最小化。为了给强化学习的智能体提供一个精确的环境模型，以便在模拟中学习到更好的策略。为此，本发明特地将一个房屋进行系统的热力学建模，并将简化的房屋RC模型集成到HEMS调度中，由DRL控制器进行新风系统进行调控，实现室内热舒适在合理的区间前提下，通过住宅内蓄电池与电网实时交互，上网电价高时放电，电价低时蓄电，从而实现购电成本的最小化。深度强化学习将强化学习与深度学习结合起来，本发明将构建一个住宅RC模型，作为训练的环境模型。控制算法采用D3QN算法控制策略，该算法与DRL经典算法DQN的区别主要是D3QN引入经验优先算法，与DQN的经验重放方法相比，引入PER算法提高了模型采样的效率，加快了本发明应用实例算法的学习过程。为达到上述目的，基于上述技术原理和分析，本发明是通过以下技术方案实现的：一种基于RC模型和深度强化学习的室内热环境的控制方法，该方法包括以下步骤：S1：搭建建筑RC模型，作为环境模型；S2：获取观测数据：包括获取室内外热环境和蓄电池的相关参数；S3：搭建神经网络模型，使用该模型迭代预测下一时刻的室内温度值、空调耗电量、电池荷电状态；S4：利用S2获取的观测数据在S3建立的模型中进行循环运算，并引入D3QN强化学习方法，所述D3QN强化学习方法为DQN与PER算法相结合的控制策略；基于神经网络训练控制空调系统、蓄电池的智能体，通过对当前状态值的观测以及得到的奖励，自动学习空调系统控制制热量以及电池充放电决策的优化过程，得到最优控制策略，即得到最优的控制方法。进一步的，所述S1中，利用RC储能模型对建筑的制热区域建立简化的物理模型，建筑的RC热网格模型由热阻与热容构成，其中，热阻具有热传输的能力，将整个建筑连接在一起，热容是具有存储热的能力；在单位面积、单位时间内透过围护结构的导热热量，称为热流强度，通常用q表示，其值为等式： 等式中T1，T2分别为围护结构两侧的表面温度℃；d为围护结构的厚度，λ为壁体材料导热系数。建筑物RC热能耗模型，有加热器为房间提供恒定的热能q，来模拟室内的空调或暖气，加热后房间内和室外的温度分别为Tr、To，围护结构的温度为TW，当热能q流入围护结构时，通过热能守恒方程，得到等式，代表了房间室内外环境热量转换的基本过程； 式中Rr是房间空气热阻；Rw是房间围护结构热阻；C为热容。进一步的，所述S2中，室内外热环境数据包括时刻信息，室外温度，室外相对湿度，太阳辐射强度，空调系统耗电量和室内温度；蓄电池的相关参数包括蓄电池模型所需的光伏、住宅负荷、电价信息。进一步的，所述S3中，搭建神经网络模型：选用四层前馈全连接神经网络建立系统的输入输出模型；选择均方差作为神经网络建模的损失函数；优化器选择随机梯度下降法来寻找模型最优解。进一步的，上述神经网络模型中，选择输入层参数，并输出下一时刻的室内温度 、空调系统耗电量/＞和电池荷电状态Soc 。具体的，最终选择时刻信息，室外温度/＞，室外相对湿度/＞，太阳辐射强度/＞，空调系统耗电量/＞，室内温度/＞，S1 = 作为空调系统模型的输入层参数；选择Pv, Load, Price, S2 = 作为蓄电池模型的输入层参数； t表示时刻，隐含层节点数为10，模型的输出层为下一时刻的室内温度、下一时刻空调系统耗电量和荷电状态；隐含层选择Relu函数作为激活函数，输出层选择Sigmoid函数。神经网络参数建模的目的就是寻找合适的权值参数，使损失函数的值尽可能的小，选择均方差作为神经网络建模的损失函数；优化器选择随机梯度下降法来寻找模型最优解。进一步的，所述S3中，将控制方法运行优化问题建模为马尔科夫决策过程，具体包括：对空调系统S1和蓄电池S2两个部分：在空调系统S1中，状态观测空间S1设置为S1 = ，可控制变量为空调制热量，动作空间A1设置为A1 = ；在蓄电池模型S2中，状态空间S2 = ，动作空间A2 = , μ表示在能源系统中，住宅净负荷为0时，电池动作处于闲置状态；定义奖励函数设置在奖励函数R的设置为在保证室内热舒适区间的同时降低能源成本为优化目标，如式所示，由三部分组成，第一部分，P为控制时间步长内空调系统的能耗，为实时电；第二部分/＞ ，/＞为室内温度下限，/＞为室内温度上限；/＞为惩罚系数，体现了超越温度范围相对系统能源成本的影响；为确保空调系统运行满足舒适度约束，根据2012《民用建筑供暖通风与空气调节设计规范》，冬季室内温度应保持在18~24°C之间，即/＞为18°C，/＞为24°C；在满足室内热舒适温度区间时，智能体惩罚函数值为0，否则将对智能体进行惩罚；第三部分penalty,表示当电池容量保持在合理的工作范围内时，惩罚值为0;当智能体选择错误的动作,导致电池容量低于最小容量或高于最大容量而不能保证电池正常工作时，惩罚值为更大的100；R = -.P + penalty  + penalty   运行阶段设学习率为lr，折现因子为γ，贪婪率为ε，经验样本数为k，网络参数更新频率为C，迭代次数为U，单集最大步长为T。进一步的，所述D3QN算法流程如下所示：第一步：初始化经验重放池；第二步：初始化当前网络Q和目标值网络Q^，当前网络权重参数θ，目标网络权重参数θ^；第三步：初始化变更后权重Δ = 0；第四步：循环遍历事件episode=1,2,…,M。当episode=1,U做初始化状态S1；第五步：当t = 1时，T根据以下情况做出决策：当贪婪政策为ε时，选择随机动作at，否则选择at = argmax Q在环境中执行动作at并观察rt和st+1存储转换和概率pt ；第六步：当j = 1时，k根据等式计算采样概率P，并根据等式计算抽样权重； 其中P为经验样本的优先级;O为采样系数，当O等于0时，表示采用均匀随机抽样进行经验重放。等式中p 采用优先计算方法，它可以表示下面的等式：等式中ε为贪婪策略，该策略可以避免计算TD误差为零时样本不被重放的问题。在与房屋RC模型学习到样本后，为了消除抽样的误差，引入新的权重计算方法，该权重表示为等式；第七步：计算TD误差，并根据等式更新概率Pt；第八步：根据等式计算损失函数： 等式中r 为回报，γ为衰减因子，为目标价值网络，/＞为目标价值网络的权重参数；第九步：累积权重变化Δ←Δ +∇θL；第十步：重复步骤第四步至第九步T次，每隔固定步数C，将估计的动作值网络参数复制给目标动作值网络参数，更新目标动作值网络参数；第十一步：重复以上步骤M次，直到智能体学习出使得累计奖励值R最大的策略。与现有技术相比，本发明的优点和有益效果是：本发明设计一种基于深度强化学习的建筑能源系统优化控制方法具有重要意义，能够形成一套完整的感知决策体系，维持室内热舒适环境，并有效提高住宅建筑的经济效益。附图说明图1是本发明热容热阻模型图。图2是本发明的算法神经网络结构图。图3是本发明的D3QN算法流程图。图4是本发明实施例中的模型学习曲线图。图5是本发明实施例中的调控效果图；为室内温度调控效果图，为现场光伏消纳结果图。具体实施方式下面结合实施例和附图对本发明所述的技术方案作进一步地描述说明。实施例1本实施例优化控制策略的目标是满足室内热舒适性要求的前提下，实现经济收益的最大化，这一目标通过降低空调系统能耗和调节家庭蓄电池实现。为了给强化学习的智能体提供一个精确的环境模型，以便在模拟中学习到更好的策略。为此，本发明特地将一个房屋进行系统的热力学建模，并将简化的房屋RC模型集成到HEMS调度中，由RL控制器进行新风系统进行调控，实现室内热舒适在合理的区间前提下，通过住宅内蓄电池与电网实时交互，上网电价高时放电，电价低时蓄电，从而实现购电成本的最小化。深度强化学习将强化学习与深度学习结合起来，本发明将构建一个住宅RC模型，作为训练的环境模型。控制算法采用D3QN与经验优先算法相结合的DQN控制策略，与传统的经验重放方法相比，引入PER算法提高了模型采样的效率，加快了本发明应用实例算法的学习过程。为达到上述目的，本发明是通过以下技术方案实现的：一种基于RC模型和数据驱动住宅能源系统优化控制方法，该方法包括以下步骤：S1：搭建研究实例的房屋RC模型，作为训练用的环境模型。S2：获取观测数据：包括获取室内外热环境和蓄电池的相关参数；S3：搭建神经网络模型，使用该模型迭代预测下一时刻的室内温度值、空调耗电量、电池荷电状态；S4：利用S2获取的数据在S3建立的模型中进行循环运算，并引入D3QN强化学习方法，基于神经网络训练控制空调系统、蓄电池的智能体，通过对当前状态值的观测以及得到的奖励，自动学习空调系统控制制热量以及电池充放电决策的优化过程，得到最优控制策略，即得到最优的控制方法，实现能源成本的优化。在一实施例中，所述S1中，利用RC储能模型对建筑的制热区域建立简化的物理模型，建筑的RC热网格模型由热阻与热容构成，其中，热阻具有热传输的能力，将整个建筑连接在一起，热容是具有存储热的能力；在单位面积、单位时间内透过围护结构的导热热量，称为热流强度，通常用q表示，其值为等式： 等式中T1，T2分别为围护结构两侧的表面温度℃；d为围护结构的厚度，λ为壁体材料导热系数。建筑物RC热能耗模型,如上图1所示，有加热器为房间提供恒定的热能q，来模拟我们室内的空调或暖气，加热后房间内和室外的温度分别为Tr、To，围护结构的温度为TW，当热能q流入围护结构时,通过热能守恒方程，可以得到等式，它代表了房间室内外环境热量转换的基本过程。 式中Rr是房间空气热阻；Rw是房间围护结构热阻；C为热容。在一实施例中，所述S2中，室内外热环境数据：时刻信息，室外温度，室外相对湿度，太阳辐射强度，空调系统耗电量和室内温度，以及蓄电池模型所需的光伏、住宅负荷、电价信息。在一实施例中，所述S3中，搭建神经网络模型：选用四层前馈全连接神经网络建立系统的输入输出模型；选择均方差作为神经网络建模的损失函数；优化器选择随机梯度下降法来寻找模型最优解。与DQN中直接输出Q值的深度神经网络不同，D3QN在获得中间特征后分别输出预测状态值函数V和预测相对优势函数A，这两个函数作为动作值函数添加。通过同时训练V和A，可以避免网络训练时的过拟合问题，加快网络的训练速度。神经网络结构如图2所示。在一实施例中，上述神经网络模型中，选择输入层参数，并输出下一时刻的室内温度 、空调系统耗电量/＞和电池荷电状态Soc 。具体的，最终选择时刻信息，室外温度/＞，室外相对湿度/＞，太阳辐射强度/＞，空调系统耗电量/＞，室内温度/＞，S1 = 作为空调系统模型的输入层参数。选择Pv, Load, Price, S2 = 作为蓄电池模型的输入层参数。 t表示时刻，隐含层节点数为10，模型的输出层为下一时刻的室内温度、下一时刻空调系统耗电量和荷电状态；隐含层选择Relu函数作为激活函数，输出层选择Sigmoid函数。神经网络参数建模的目的就是寻找合适的权值参数，使损失函数的值尽可能的小，选择均方差作为神经网络建模的损失函数；优化器选择随机梯度下降法来寻找模型最优解。在一实施例中，所述S3中，将控制方法运行优化问题建模为马尔科夫决策过程，具体包括：对空调系统S1和蓄电池S2两个部分：在空调系统S1中，状态观测空间S1设置为S1 = ，可控制变量为空调制热量，动作空间A1设置为A1 = 。在蓄电池模型S2中，状态空间S2 = ,动作空间A2 = , μ表示在能源系统中，住宅净负荷为0时，电池动作处于闲置状态。定义奖励函数设置在奖励函数R的设置为在保证室内热舒适区间的同时降低能源成本为优化目标，如式所示，由三部分组成，第一部分，P为控制时间步长内空调系统的能耗，为实时电；第二部分/＞ ，/＞为室内温度下限，/＞为室内温度上限；/＞为惩罚系数，体现了超越温度范围相对系统能源成本的影响；为确保空调系统运行满足舒适度约束，根据2012《民用建筑供暖通风与空气调节设计规范》，冬季室内温度应保持在18~24°C之间，即/＞为18°C，/＞为24°C；在满足室内热舒适温度区间时，智能体惩罚函数值为0，否则将对智能体进行惩罚。第三部分penalty,表示当电池容量保持在合理的工作范围内时，惩罚值为0;当智能体选择错误的动作,导致电池容量低于最小容量或高于最大容量而不能保证电池正常工作时，惩罚值为更大的100。R = -.P + penalty  + penalty   运行阶段设学习率为lr，折现因子为γ，贪婪率为ε，经验样本数为k，网络参数更新频率为C，迭代次数为U，单集最大步长为T 。D3QN-PER算法流程如图3所示。第一步：初始化经验重放池；第二步：初始化当前网络Q和目标值网络Q^，当前网络权重参数θ，目标网络权重参数θ^；第三步：初始化变更后权重Δ = 0；第四步：循环遍历事件episode=1,2,…,M。当episode=1,U做初始化状态S1；第五步：当t = 1时，T根据以下情况做出决策： 当贪婪政策为ε时，选择随机动作at，否则选择at = argmax Q 在环境中执行动作at并观察rt和st+1 存储转换和概率pt ；第六步：当j = 1时，k根据等式计算采样概率P，并根据等式计算抽样权重； 其中P为经验样本的优先级;O为采样系数，当O等于0时，表示采用均匀随机抽样进行经验重放。等式中p 采用优先计算方法，它可以表示下面的等式： 等式中ε为贪婪策略，该策略可以避免计算TD误差为零时样本不被重放的问题。在与房屋RC模型学习到样本后，为了消除抽样的误差，引入新的权重计算方法，该权重表示为等式 ；第七步：计算TD误差，并根据等式更新概率Pt；第八步：根据等式计算损失函数： 等式中r 为回报，γ为衰减因子，为目标价值网络，/＞为目标价值网络的权重参数；第九步：累积权重变化Δ←Δ +∇θL；第十步：重复步骤第四步至第九步T次，每隔固定步数C，将估计的动作值网络参数复制给目标动作值网络参数，更新目标动作值网络参数；第十一步：重复以上步骤M次，直到智能体学习出使得累计奖励值R最大的策略 。以上述实施例为基础，进行验证：为了检验所提出的强化学习算法的可行性和有效性，选择了现实两层的零能耗住宅作为测试对象，其中总建筑面积105m2，选用的零能耗住宅围护结构填充玻璃棉保温效果好，保温性能高，围护结构平均整体热损失率Ua为0.58W/，所以零能耗住宅具有很高的热灵活性潜力。为此收集了该房子从 2020 年1月1日到3月30日，以 30 分钟为间隔收集的运行数据，该房子配备有4.8kWp的光伏系统和容量为5.6kW的储能电池，房间收集数据特征，鉴于可用数据，使用10周数据来作为输入端并训练 DRL 智能体，输入到S3步骤中，模型的求解过程详细见第三部分运行阶段。模型的训练曲线见图4。图4展示了本发明设计模型训练曲线，训练过程主要分为两个过程，奖励在训练过程的初始阶段迅速增加，然后奖励值越来越稳定，达到训练阶段结束时的收敛阶段，这表明所提出的D3QN智能体有效学习到了能源系统的最佳调控策略，而且训练次数大大减少便可以达到收敛的效果，节省了计算资源和时间。如图5所示，本发明提出的控制方法能实现兼顾室内热舒适的前提下实现空调电费的下降，同时蓄电池模型作为平衡能源系统的调节器，有效提升了现场光伏的消纳率，实现家庭购买能源经济成本的有效降低。在上述实施例的基础上，本发明继续对其中涉及到的技术特征及该技术特征在本发明中所起到的功能、作用进行详细的描述，以帮助本领域的技术人员充分理解本发明的技术方案并且予以重现。最后，虽然本说明书按照实施方式加以描述，但并非每个实施方式仅包含一个独立的技术方案，说明书的这种叙述方式仅仅是为清楚起见，本领域技术人员应当将说明书作为一个整体，各实施例中的技术方案也可以经适当组合，形成本领域技术人员可以理解的其他实施方式。
