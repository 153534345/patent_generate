标题title
一种基于单帧图像的3D动态视频生成方法
摘要abst
本发明属于计算机视觉技术领域，具体涉及一种基于单帧图像的3D动态视频生成方法，包括：确定单帧输入图像的深度图，将输入图像转换为分层深度图像，对分层深度图像进行特征提取，经投射得到3D特征点云；基于每帧待生成RGB图像相对所述输入图像的时间差，对输入图像中的流体进行双向的2D光流估计，并基于流体所在区域的深度信息，将估计得到的每一个方向的光流投射为3D场景流，得到两个方向的3D场景流；基于新的相机视角，采用双向3D场景流对3D特征点云进行双向移动并渲染，得到特征图；将两帧特征图进行加权融合并解码得到上述时间差对应的一帧RGB图像；对各帧RGB图像进行合成，得到3D动态视频。本发明同时解决了场景运动和新视图合成两个难题。
权利要求书clms
1.一种基于单帧图像的3D动态视频生成方法，其特征在于，包括：确定单帧输入图像的深度图，根据所述深度图将输入图像转换为分层深度图像，对所述分层深度图像进行特征提取，经过投射得到3D特征点云；基于每帧待生成RGB图像相对所述输入图像的时间差，对所述输入图像中的流体进行双向的2D光流估计，并基于所述流体所在区域的深度信息，将估计得到的每一个方向的光流投射为3D场景流，得到两个方向的3D场景流；基于相对于所述输入图像的新的相机视角，采用双向3D场景流对所述3D特征点云进行相应方向的移动并渲染，得到特征图、深度图和alpha图；将两帧所述特征图进行加权融合并解码得到所述时间差对应的一帧RGB图像；所述加权融合采用的加权矩阵是通过基于所述时间差、所述深度图和所述alpha图计算得到；对各帧RGB图像进行合成，得到3D动态视频。2.根据权利要求1所述的3D动态视频生成方法，其特征在于，采用已训练的单目深度估计网络，估计得到单帧所述输入图像的深度图。3.根据权利要求1所述的3D动态视频生成方法，其特征在于，采用已训练的2D特征提取网络，对所述分层深度图像中的每层图像进行两维特征提取。4.根据权利要求3所述的3D动态视频生成方法，其特征在于，采用已训练的2D光流估计网络，对所述输入图像中的流体进行一个方向的2D光流估计，并基于运动对称原理，得到另一个方向的2D光流。5.根据权利要求4所述的3D动态视频生成方法，其特征在于，所述2D光流估计网络采用欧拉场来近似场景流体的运动，基于欧拉积分，递归地获得任意时刻的各像素位移场。6.根据权利要求4所述的3D动态视频生成方法，其特征在于，所述2D特征提取网络、所述2D光流估计网络和用于所述解码的解码器网络采用如下两阶段式进行训练：第一个阶段训练所述2D光流估计网络，第二个阶段冻结所述2D光流估计网络，训练所述2D特征提取网络和所述解码器网络。7.根据权利要求1所述的3D动态视频生成方法，其特征在于，采用权重矩阵将两帧所述特征图进行融合，表示为：Ft＝Wt·Ff+·Fb；Dt＝Wt·Df+·Db；式中，Ff和Fb表示两帧所述特征图，Df和Db表示两帧所述深度图，αf和αb表示两帧所述alpha图，t表示所述时间差，N表示用于生成3D动态视频的总帧数，Wt表示权重矩阵。8.一种基于单帧图像的3D动态视频生成系统，其特征在于，用于执行如权利要求1至7任一项所述的一种基于单帧图像的3D动态视频生成方法，包括：预处理模块，用于确定单帧输入图像的深度图；三维场景表达模块，用于根据所述深度图将输入图像转换为分层深度图像，对所述分层深度图像进行特征提取，经过投射得到3D特征点云；运动估计模块，用于基于每帧待生成RGB图像相对所述输入图像的时间差，对所述输入图像中的流体进行双向的2D光流估计，并基于所述流体所在区域的深度信息，将估计得到的每一个方向的光流投射为3D场景流，得到两个方向的3D场景流；点云运动与渲染模块，用于基于相对于所述输入图像的新的相机视角，采用双向3D场景流对所述3D特征点云进行相应方向的移动并渲染，得到特征图、深度图和alpha图；将两帧所述特征图进行加权融合并解码得到所述时间差对应的一帧RGB图像；所述加权融合采用的加权矩阵是通过基于所述时间差、所述深度图和所述alpha图计算得到；视频合成模块，用于对各帧RGB图像进行合成，得到3D动态视频。9.一种计算机可读存储介质，其特征在于，所述计算机可读存储介质包括存储的计算机程序，其中，在所述计算机程序被处理器运行时控制所述存储介质所在设备执行如权利要求1至7任一项所述的一种基于单帧图像的3D动态视频生成方法。
说明书desc
技术领域本发明属于计算机视觉技术领域，更具体地，涉及一种基于单帧图像的3D动态视频生成方法。背景技术如今，由于人们可以轻易地使用智能手机摄像头拍摄照片，因此在线照片的数量急剧增加。然而，随着在线视频分享平台的兴起，人们不再满足于静态图像，而是已经习惯于观看视频。因此，如果能够使这些静止图像动起来，合成视频，将会带来高质量的体验。事实上，早在2014年，就有学者提出动态图像，这些动态图像被称为Cinemagraphs，一经创作出来就在网络上迅速走红。Cinemagraphs是一种创造活动图像的技术，它是一种介于照片和视频之间的媒介形式。Cinemagraphs是由一系列静态图像组成的，其中只有一小部分区域是以循环形式播放的动态内容，其余部分仍然保持静态。这种技术可以通过使用专业软件和相机生成，也可以通过移动设备上的应用程序进行生成。Cinemagraphs已经在各种数字平台上广泛使用，包括社交媒体、广告和数字艺术。这种形式的媒介能够吸引人们的注意力并提供更加生动的视觉体验，比传统的静态图像更加吸引人们的注意力。虽然Cinemagraphs比传统的静态图像更加吸引人们的注意力，但它们通常无法为观众提供沉浸式的3D感受。这是因为Cinemagraphs通常基于静态摄像机，无法产生视差效果。由此可见，现有技术存在难以提供3D沉浸感的问题。发明内容针对现有技术的缺陷和改进需求，本发明提供了一种基于单帧图像的3D动态视频生成方法，其目的在于旨在解决现有技术难以提供3D沉浸感的问题。为实现上述目的，按照本发明的一个方面，提供了一种基于单帧图像的3D动态视频生成方法，包括：确定单帧输入图像的深度图，根据所述深度图将输入图像转换为分层深度图像，对所述分层深度图像进行特征提取，经过投射得到3D特征点云；基于每帧待生成RGB图像相对所述输入图像的时间差，对所述输入图像中的流体进行双向的2D光流估计，并基于所述流体所在区域的深度信息，将估计得到的每一个方向的光流投射为3D场景流，得到两个方向的3D场景流；基于相对于所述输入图像的新的相机视角，采用双向3D场景流对所述3D特征点云进行相应方向的移动并渲染，得到特征图、深度图和alpha图；将两帧所述特征图进行加权融合并解码得到所述时间差对应的一帧RGB图像；所述加权融合采用的加权矩阵是通过基于所述时间差、所述深度图和所述alpha图计算得到；对各帧RGB图像进行合成，得到3D动态视频。进一步，采用已训练的单目深度估计网络，估计得到单帧所述输入图像的深度图。进一步，采用已训练的2D特征提取网络，对所述分层深度图像中的每层图像进行两维特征提取。进一步，采用已训练的2D光流估计网络，对所述输入图像中的流体进行一个方向的2D光流估计，并基于运动对称原理，得到另一个方向的2D光流。进一步，所述2D光流估计网络采用欧拉场来近似场景流体的运动，基于欧拉积分，递归地获得任意时刻的各像素位移场。进一步，所述2D特征提取网络、所述2D光流估计网络和用于所述解码的解码器网络采用如下两阶段式进行训练：第一个阶段训练所述2D光流估计网络，第二个阶段冻结所述2D光流估计网络，训练所述2D特征提取网络和所述解码器网络。进一步，采用权重矩阵将两帧所述特征图进行融合，表示为：Ft＝Wt·Ff+·Fb；Dt＝Wt·Df+·Db；式中，Ff和Fb表示两帧所述特征图，Df和Db表示两帧所述深度图，αf和αb表示两帧所述alpha图，t表示所述时间差，N表示用于生成3D动态视频的总帧数，Wt表示权重矩阵。本发明还提供一种基于单帧图像的3D动态视频生成系统，用于执行如上所述的一种基于单帧图像的3D动态视频生成方法，包括：预处理模块，用于确定单帧输入图像的深度图；三维场景表达模块，用于根据所述深度图将输入图像转换为分层深度图像，对所述分层深度图像进行特征提取，经过投射得到3D特征点云；运动估计模块，用于基于每帧待生成RGB图像相对所述输入图像的时间差，对所述输入图像中的流体进行双向的2D光流估计，并基于所述流体所在区域的深度信息，将估计得到的每一个方向的光流投射为3D场景流，得到两个方向的3D场景流；点云运动与渲染模块，用于基于相对于所述输入图像的新的相机视角，采用双向3D场景流对所述3D特征点云进行相应方向的移动并渲染，得到特征图、深度图和alpha图；将两帧所述特征图进行加权融合并解码得到所述时间差对应的一帧RGB图像；所述加权融合采用的加权矩阵是通过基于所述时间差、所述深度图和所述alpha图计算得到；视频合成模块，用于对各帧RGB图像进行合成，得到3D动态视频。本发明还提供一种计算机可读存储介质，所述计算机可读存储介质包括存储的计算机程序，其中，在所述计算机程序被处理器运行时控制所述存储介质所在设备执行如上所述的一种基于单帧图像的3D动态视频生成方法。总体而言，通过本发明所构思的以上技术方案，能够取得以下有益效果：本发明提出的基于单张图像的3D动态视频生成方法，提出了从单张图像生成3D动态视频的新范式。本发明提出了一个新的框架，其可同时解决场景运动和新视图合成两个难题，具体地，将3D特征点云进行表达，同时估计双向的2D光流并将每个方向的2D光流投射成3D场景流，在3D空间中进行操作，具体基于相对于输入图像的新的相机视角，采用每个方向的3D场景流对3D特征点云进行相应方向的移动并渲染，得到特征图；将两帧特征图进行加权融合并解码得到一个时间差对应的一帧RGB图像，最终实现基于单张图像的3D动态视频生成。也就是，输入一张静态图像，能够获得具有视差效果的3D动态视频，为用户提供了3D沉浸感。本发明通过设计了一个3D对称运动技术来解决点向前移动时的产生空洞问题。附图说明图1为本发明实施例提供的一种基于单帧图像的3D动态视频生成方法的流程框图；图2为本发明实施例提供的3D场景表达与运动估计的工作流程图；图3是本发明实施例提供的点云运动与渲染操作的工作流程图；图4是本发明实施例提供的2D光流估计网络的结构示意图；图5是本发明实施例提供的特征提取网络的结构示意图；图6是本发明实施例提供的解码网络的结构示意图；图7是本发明实施例提供的3D对称运动技术的示意图；图8是本发明实施例提供的通过输入掩码和运动方向实现可控运动的示意图。具体实施方式为了使本发明的目的、技术方案及优点更加清楚明白，以下结合附图及实施例，对本发明进行进一步详细说明。应当理解，此处所描述的具体实施例仅仅用以解释本发明，并不用于限定本发明。此外，下面所描述的本发明各个实施方式中所涉及到的技术特征只要彼此之间未构成冲突就可以相互组合。实施例一一种基于单帧图像的3D动态视频生成方法，如图1所示，包括：确定单帧输入图像的深度图，根据深度图将输入图像转换为分层深度图像，对分层深度图像进行特征提取，经过投射得到3D特征点云；基于每帧待生成RGB图像相对输入图像的时间差，对上述输入图像中的流体进行双向的2D光流估计，并基于上述流体所在区域的深度信息，将估计得到的每一个方向的光流投射为3D场景流，得到两个方向的3D场景流；基于相对于上述输入图像的新的相机视角，采用双向3D场景流对上述3D特征点云进行相应方向的移动并渲染，得到特征图、深度图和alpha图；将两帧特征图进行融合并解码得到上述时间差对应的一帧RGB图像；执行上述加权融合所采用的加权矩阵是通过基于上述时间差、上述深度图和上述alpha图计算得到；对各帧RGB图像进行合成，得到3D动态视频。可作为优选的实施方式，采用已训练的单目深度估计网络，估计得到单帧输入图像的深度图。根据本发明的实施例，单目深度估计网络已在许多数据集上预训练，可以合理地估计出任意输入的深度图。需要说明的是，深度估计网络可以替换为任意具有单目深度估计能力的算法及网络。如图2所示，将输入图像与相对应的深度图同时输入3D场景表达与运动估计中，分别输出特征点云和场景流。首先，生成特征点云的方式为：利用深度图将输入图像转化为分层深度图像，并可作为优选的实施方式，使用2D特征提取网络对上述分层深度图像中的每层图像进行两维特征提取，投射成3D特征点云。具体的，可分为以下几个步骤：将深度范围进行分层，根据深度分层将输入图像转化为分层深度图像，具体地，使用分层聚类算法将深度图分成若干个深度区间，再根据深度区间将原RGB图进行分层，即在本实施例中，使用一个固定的距离阈值，超过该阈值的簇将不会合并，最后生成的LDIs将会有2到5个图层。对LDIs的每一层分别进行上下文感知的图像修复。在本实施例中，使用的是预训练的图像修复网络，可根据上下文的内容修复出合理的内容。使用2D特征提取网络对LDIs进行特征提取，得到特征LDIs，即如图3所示，2D特征提取网络采取的是从第三层往后截断的ResNet34网络，随后紧接两个额外的上采样层，以提取每个RGB层的特征图，除此之外，还使用二进制掩码来增强特征图，以指示该层中哪些像素可见。根据对应的深度值，将特征LDIs投射成3D特征点云，即其次，生成场景流的方式为：可作为优选的实施方式，使用已训练的2D光流估计网络从输入图像中估计场景运动，将其投射成3D场景流。具体的，可分为以下几个步骤：使用2D光流估计网络从输入图像中估计场景的2D光流Ft→t+1，特别地，本实施例采用欧拉场M来近似场景的运动，具体地，Ft→t+1＝M。如图4所示，本实施例中，2D光流估计网络是用U-Net实现的，具有16个卷积层，并使用SPADE替换批正则化。使用欧拉积分，可递归地获得任意时刻的某个像素位移场，具体地，F0→t＝F0→t-1+M)。根据深度值将2D位移场投射成3D场景流。如图5所示，在得到3D场景流和3D特征点云之后，本实施例方法还需要对3D特征点云双向地进行移动，并渲染获得特征图；通过解码器网络将融合的特征图输出为最终RGB图像。具体的，可分为以下几个步骤：使用前向3D场景流和后向3D场景流，将3D特征点云分别前向和后向移动，获得和/＞如图6所示，为了解决点云向前移动而产生的空洞问题，本实施例提出3D对称运动技术，从运动方向相反的点云中借用纹理信息，并将两个运动点云集成起来，以填补缺失的区域。将前向运动特征点云和后向运动特征点云分别渲染，获得特征图Ff和Fb，深度图Df和Db，alpha图αf和αb。将前向和后向特征图进行融合，具体地，Ft＝Wt·Ff+·Fb；Dt＝Wt·Df+·Db；其中，式中，Ff和Fb表示两帧上述特征图，Df和Db表示两帧上述深度图，αf和αb表示两帧上述alpha图，t表示上述时间差，N表示用于生成3D动态视频的总帧数，Wt表示权重矩阵。通过解码器网络将融合的特征图输出为最终RGB图像。如图7所示，本实施例中，解码器网络是2D U-Net结构。需要说明的是，如图8所示，在本实施例中，还可以额外输入对应的光流掩码和运动方向，实现运动的可控。需要说明的是，本实施例提供的一种基于单张图像的3D动态视频生成方法中，仅需要训练2D光流估计网络、特征提取网络和解码器网络，神经网络的训练包括：使用从互联网搜集的流体运动数据集作为训练数据集。该数据集包括从较长的运动视频中提取的流体运动的短视频。其中，使用每个视频剪辑的第一帧和由预训练的光流估计网络估计的相应真实运动场作为运动估计对来训练2D光流估计网络。为了使得网络具有使场景运动的能力，随机从流体运动视频中抽样训练数据。对于新视角合成训练，需要同一场景的多视角监督，但这在训练集中不可用。因此，使用预训练的单张图新视图合成算法生成伪新视角真实值用于训练。第一个阶段，首先使用运动估计对来训练2D光流估计网络。为了训练运动估计网络，按如下方式最小化GAN损失、GAN特征匹配损失和端点误差：其中，表示GAN损失，/＞表示GAN特征匹配损失，而/＞表示端点误差。第二个阶段，冻结2D光流估计网络，训练特征提取网络和解码器网络。这一阶段需要模型同时学习渲染新视角和使场景运动两种能力。对于新视角合成，设置t＝0，并使用伪造的新视角真实值来监督模型。随机采样场景的目标视角，并要求模型将其合成。对于场景运动，从流体运动视频中随机抽样训练三元组。特别地，使用F0→t和l0→t-N在不改变相机姿态和内参的情况下从两个方向渲染中间帧。除了GAN损失和GAN特征匹配损失之外，还采用VGG感知损失和合成图像与真实图像之间的l1损失进行约束，总体损失如下：总的来说，本实施例方法对于3D场景表达分支，利用深度图将输入图像转化为分层深度图像，并使用2D特征提取网络对LDIs进行特征提取；投射成3D特征点云；对于运动估计分支，使用2D光流估计网络从输入图像中估计场景运动；将其投射成3D场景流；对于点云运动及渲染模块，对3D特征点云双向地进行移动，并渲染获得特征图；通过解码器网络将融合的特征图输出为最终RGB图像。本发明不仅能根据输入的单张图像生成具有视差效果的动态视频，同时还能够根据输入的光流掩膜和运动方向，实现可控的运动和用户交互，具有很高的应用前景。实施例二一种基于单帧图像的3D动态视频生成系统，用于执行如上所述的一种基于单帧图像的3D动态视频生成方法，包括：预处理模块，用于确定单帧输入图像的深度图；三维场景表达模块，用于根据深度图将输入图像转换为分层深度图像，对分层深度图像进行特征提取，经过投射得到3D特征点云；运动估计模块，用于基于每帧待生成RGB图像相对所述输入图像的时间差，对所述输入图像中的流体进行双向的2D光流估计，并基于所述流体所在区域的深度信息，将估计得到的每一个方向的光流投射为3D场景流，得到两个方向的3D场景流；点云运动与渲染模块，用于基于相对于所述输入图像的新的相机视角，采用双向3D场景流对所述3D特征点云进行相应方向的移动并渲染，得到特征图、深度图和alpha图；将两帧所述特征图进行融合并解码得到所述时间差对应的一帧RGB图像；执行上述加权融合所采用的加权矩阵是通过基于时间差、深度图和alpha图计算得到；视频合成模块，用于对各帧RGB图像进行合成，得到3D动态视频。相关技术方案同实施例一，在此不再赘述。实施例三一种计算机可读存储介质，所述计算机可读存储介质包括存储的计算机程序，其中，在所述计算机程序被处理器运行时控制所述存储介质所在设备执行如上所述的一种基于单帧图像的3D动态视频生成方法。相关技术方案同实施例一，在此不再赘述。本领域的技术人员容易理解，以上所述仅为本发明的较佳实施例而已，并不用以限制本发明，凡在本发明的精神和原则之内所作的任何修改、等同替换和改进等，均应包含在本发明的保护范围之内。
