标题title
一种全息互动远程会议方法及系统
摘要abst
本发明涉及一种全息互动远程会议方法及系统。步骤包括：利用分类模型对预处理后的实时数据进行分类得到分类结果，再对分类结果进行解码找到概率最高的类别，将其映射为初始交互反馈参数，并结合当前会议阶段，通过策略网络对初始交互反馈参数进行优化调整以生成最优的交互反馈参数以满足与会者的期望，再基于最优的交互反馈参数，在虚拟环境中生成相应的交互内容进行实时互动，以营造出更为沉浸式的会议体验。本发明的策略网络会根据当前的实际会议情境对初始交互反馈参数进行进一步优化调整，使得生成的交互反馈更加个性化及贴合会议当前阶段的实际需求，提高交互体验，可以很好的提高与会者的参与感，调动与会者的互动积极性。
权利要求书clms
1.一种全息互动远程会议方法，其特征在于，步骤包括：实时监测并采集会议室中与会者的实时数据并进行预处理，实时数据包括实时的手势图像和语音信号；通过训练后的分类模型对预处理后的实时数据进行分类分析以得到分类结果，其中，所述分类结果为一个概率向量，概率向量中每个元素表示不同类别的概率，手势分类任务中不同类别表示不同的手势，语音指令分类任务中不同类别表示不同的语音指令；对分类结果进行解码并找到概率最高的类别，将解码后的类别映射为相应的交互操作或虚拟环境行为下的参数，设为初始交互反馈参数；根据历史数据训练策略网络，结合当前会议阶段通过训练好的策略网络对初始交互反馈参数进行优化调整以生成最优的交互反馈参数，历史数据包括历史的各种不同的手势、语音指令及会议阶段信息；根据最优的交互反馈参数在虚拟环境中生成相应的交互内容以进行实时互动，其中，交互内容包括渲染全息图像、播放动画或显示文字提示。2.根据权利要求1所述的全息互动远程会议方法，其特征在于，所述通过训练后的分类模型对预处理后的实时数据进行分类分析以得到分类结果的步骤包括：将预处理后的实时数据输入已经训练好的分类模型中，并在分类模型中进行前向传播，以将实时数据通过网络层逐层处理得到输出，输出表示为各个类别的得分；将输出归一化为一个概率分布，即为分类结果，Y= ，其中，i为当前类别的索引，i=1、2、…、K，K为所有类别的总数，zi为分类模型输出的类别i的得分，Y表示为类别i的概率，所有类别的概率之和为 1。3.根据权利要求1所述的全息互动远程会议方法，其特征在于，所述对分类结果进行解码并找到概率最高的类别，将解码后的类别映射为相应的交互操作或虚拟环境行为下的参数，设为初始交互反馈参数的步骤包括：找到分类结果的概率向量中的最大值对应的索引，并根据对应的索引确定类别以作为解码后的类别；创建一个包含所有类别标签的字典，其中，字典的键是各类别的索引，值是与各类别的索引一一关联的类别标签，为用来标识不同类别的唯一标识符；根据解码后的类别的索引从字典中找到对应的类别标签并设为目标类别标签，与目标类别标签对应的类别设为目标类别；将目标类别标签代入映射关系以将目标类别映射到交互参数，并设为初始交互反馈参数，其中，映射关系是预先设置的规则、公式或数据表格。4.根据权利要求1所述的全息互动远程会议方法，其特征在于，所述根据历史数据训练策略网络的步骤包括：将交互反馈的参数作为动作，历史数据作为状态，以定义状态空间和动作空间，其中，定义的动作空间涵盖各种可能的交互反馈参数，历史数据包括但不限于历史的各种不同的手势、语音指令及会议阶段信息；根据动作与状态创建一个Q值表并初始化，其中，所述Q值表的每个条目表示状态-动作对应的Q值，Q值为用于记录每个状态下采取每个动作的估计回报值，每个状态对应至少一个动作；从初始状态开始，对于每个状态，随机选择一个动作；执行选择的动作与环境互动以获取下一个状态和奖励；根据奖励及下一个状态的最大Q值更新Q值表中的Q值，以逐步调整每个状态-动作对应的Q值，使其逼近最优策略，更新公式为：Q新=Q+α*，其中，Q新为状态s下采取动作a更新后的Q值，Q是状态s下采取动作a的Q值，α是学习率，reward是获得的奖励，γ是折扣因子，s'是下一个状态，a'是下一个状态下选择的最优动作，Q为下一个状态s'下采取动作a'的Q值；不断与环境互动并更新所述Q值表中的Q值，直至达到收敛条件，以得到更新后的Q值表；将更新后的Q值表中每个状态的每个动作对应的Q值转化为概率，以为每个状态的动作创建一个概率分布，设为预期的目标分布；对策略网络进行训练并使策略网络输出的动作概率分布与预期的目标分布相匹配，以得到训练后的策略网络。5.根据权利要求4所述的全息互动远程会议方法，其特征在于，所述对策略网络进行训练并使策略网络输出的动作概率分布与预期的目标分布相匹配，以得到训练后的策略网络的步骤包括：将Q值表中的每个状态输入网络策略，对于每个状态，通过损失函数计算策略网络输出的动作概率分布与预期的目标分布之间的差异；根据差异值并通过反向传播算法计算相对于策略网络权重的梯度；根据梯度更新策略网络的权重，以使策略网络输出的动作概率分布与预期的目标分布之间的差异减小；通过重复迭代不断调整策略网络的权重，以使策略网络输出的动作概率分布与预期的目标分布之间的差异逐渐减小，逐渐与目标分布相匹配，便得到训练后的策略网络。6.根据权利要求1所述的全息互动远程会议方法，其特征在于，所述结合当前会议阶段通过训练好的策略网络对初始交互反馈参数进行优化调整以生成最优的交互反馈参数的步骤包括：将初始交互反馈参数与当前会议阶段信息组合成一个状态向量以定义状态空间；将状态向量作为输入，以通过训练好的策略网络生成当前状态下的动作概率分布；从当前状态下的动作概率分布中选择概率最大的动作作为最优动作，并将最优动作作为最优的交互反馈参数。7.一种全息互动远程会议系统，其特征在于，包括：数据采集模块：用于实时监测并采集会议室中与会者的实时数据并进行预处理，实时数据包括实时的手势图像和语音信号；分类模块：用于通过训练后的分类模型对预处理后的实时数据进行分类分析以得到分类结果，其中，所述分类结果为一个概率向量，概率向量中每个元素表示不同类别的概率，手势分类任务中不同类别表示不同的手势，语音指令分类任务中不同类别表示不同的语音指令；初始参数生成模块：用于对分类结果进行解码并找到概率最高的类别，将解码后的类别映射为相应的交互操作或虚拟环境行为下的参数，设为初始交互反馈参数；最优参数生成模块：用于根据历史数据训练策略网络，结合当前会议阶段通过训练好的策略网络对初始交互反馈参数进行优化调整以生成最优的交互反馈参数，历史数据包括历史的各种不同的手势、语音指令及会议阶段信息；交互内容生成模块：用于根据最优的交互反馈参数在虚拟环境中生成相应的交互内容以进行实时互动，其中，交互内容包括渲染全息图像、播放动画或显示文字提示。
说明书desc
技术领域本发明属于虚拟现实技术领域，尤其涉及一种全息互动远程会议方法及系统。背景技术全息远程会议通常涉及使用全息显示技术，该技术可以在远程会议参与者之间传输视觉信息，以创造出一种沉浸式的虚拟会议体验。传统的全息显示技术在执行全息远程会议时通常存在以下问题和不足：传统的全息远程会议系统，虽然可以传输全息图像和模型，但与会者的实时互动性受到限制，与会者无法在虚拟环境中进行实时的交互操作，也难以从系统获得实时的反馈信息，容易影响与会者的会议谈论积极性，全息远程会议系统也无法快速调整互动反馈参数以满足与会者的期望。发明内容本发明提供一种全息互动远程会议方法及系统，旨在解决上述背景技术提到的问题。本发明是这样实现的，提供一种全息互动远程会议方法，步骤包括：实时监测并采集会议室中与会者的实时数据并进行预处理，实时数据包括但不限于实时的手势图像和语音信号；通过训练后的分类模型对预处理后的实时数据进行分类分析以得到分类结果，其中，所述分类结果为一个概率向量，概率向量中每个元素表示不同类别的概率，手势分类任务中不同类别表示不同的手势，语音指令分类任务中不同类别表示不同的语音指令；对分类结果进行解码并找到概率最高的类别，将解码后的类别映射为相应的交互操作或虚拟环境行为下的参数，设为初始交互反馈参数；根据历史数据训练策略网络，结合当前会议阶段通过训练好的策略网络对初始交互反馈参数进行优化调整以生成最优的交互反馈参数，历史数据包括但不限于历史的各种不同的手势、语音指令及会议阶段信息；根据最优的交互反馈参数在虚拟环境中生成相应的交互内容以进行实时互动，其中，交互内容包括但不限于渲染全息图像、播放动画或显示文字提示。更进一步的，所述通过训练后的分类模型对预处理后的实时数据进行分类分析以得到分类结果的步骤包括：将预处理后的实时数据输入已经训练好的分类模型中，并在分类模型中进行前向传播，以将实时数据通过网络层逐步处理得到输出，输出表示为各个类别的得分；将输出归一化为一个概率分布，即为分类结果，Y=，其中，i为当前类别的索引，i=1、2、…、K，K为所有类别的总数，zi为分类模型输出的类别i的得分，Y表示为类别i的概率，所有类别的概率之和为 1。更进一步的，所述对分类结果进行解码并找到概率最高的类别，将解码后的类别映射为相应的交互操作或虚拟环境行为下的参数，设为初始交互反馈参数的步骤包括：找到分类结果的概率向量中的最大值对应的索引，并根据对应的索引确定类别以作为解码后的类别；创建一个包含所有类别标签的字典，其中，字典的键是各类别的索引，值是与各类别的索引一一关联的类别标签，为用来标识不同类别的唯一标识符；根据解码后的类别的索引从字典中找到对应的类别标签并设为目标类别标签，与目标类别标签对应的类别设为目标类别；将目标类别标签代入映射关系以将目标类别映射到交互参数，并设为初始交互反馈参数，其中，映射关系是预先设置的规则、公式或数据表格。更进一步的，所述根据历史数据训练策略网络的步骤包括：将历史的交互反馈参数作为动作，历史数据作为状态，以定义状态空间和动作空间，其中，定义的动作空间涵盖各种可能的交互反馈参数，历史数据包括但不限于历史的各种不同的手势、语音指令及会议阶段信息；根据动作与状态创建一个Q值表并初始化，其中，所述Q值表的每个条目表示状态-动作对应的Q值，Q值为用于记录每个状态下采取每个动作的估计回报值，每个状态对应至少一个动作；从初始状态开始，对于每个状态，随机选择一个动作；执行选择的动作与环境互动以获取下一个状态和奖励；根据奖励及下一个状态的最大Q值更新Q值表中的Q值，以逐步调整每个状态-动作对应的Q值，使其逼近最优策略，更新公式为：Q新=Q+α*，其中，Q新为状态s下采取动作a更新后的Q值，Q是状态s下采取动作a的Q值，α是学习率，reward是获得的奖励，γ是折扣因子，s'是下一个状态，a'是下一个状态下选择的最优动作，Q为下一个状态s'下采取动作a'的Q值；不断与环境互动并更新所述Q值表中的Q值，直至达到收敛条件，以得到更新后的Q值表；将更新后的Q值表中每个状态的每个动作对应的Q值转化为概率，以为每个状态的动作创建一个概率分布，设为预期的目标分布；对策略网络进行训练并使策略网络输出的动作概率分布与预期的目标分布相匹配，以得到训练后的策略网络。更进一步的，所述对策略网络进行训练并使策略网络输出的动作概率分布与预期的目标分布相匹配，以得到训练后的策略网络的步骤包括：将Q值表中的每个状态输入网络策略，对于每个状态，通过损失函数计算策略网络输出的动作概率分布与预期的目标分布之间的差异；根据差异值并通过反向传播算法计算相对于策略网络权重的梯度；根据梯度更新策略网络的权重，以使策略网络输出的动作概率分布与预期的目标分布之间的差异减小；通过重复迭代不断调整策略网络的权重，以使策略网络输出的动作概率分布与预期的目标分布之间的差异逐渐减小，逐渐与目标分布相匹配，便得到训练后的策略网络。更进一步的，所述结合当前会议阶段通过训练好的策略网络对初始交互反馈参数进行优化调整以生成最优的交互反馈参数的步骤包括：将初始交互反馈参数与当前会议阶段信息组合成一个状态向量以定义状态空间；将状态向量作为输入，以通过训练好的策略网络生成当前状态下的动作概率分布；从当前状态下的动作概率分布中选择概率最大的动作作为最优动作，并将最优动作作为最优的交互反馈参数。本发明还提供一种全息互动远程会议系统，用于执行全息互动远程会议方法，包括：数据采集模块：用于实时监测并采集会议室中与会者的实时数据并进行预处理，实时数据包括但不限于实时的手势图像和语音信号；分类模块：用于通过训练后的分类模型对预处理后的实时数据进行分类分析以得到分类结果，其中，所述分类结果为一个概率向量，概率向量中每个元素表示不同类别的概率，手势分类任务中不同类别表示不同的手势，语音指令分类任务中不同类别表示不同的语音指令；初始参数生成模块：用于对分类结果进行解码并找到概率最高的类别，将解码后的类别映射为相应的交互操作或虚拟环境行为下的参数，设为初始交互反馈参数；最优参数生成模块：用于根据历史数据训练策略网络，结合当前会议阶段通过训练好的策略网络对初始交互反馈参数进行优化调整以生成最优的交互反馈参数，历史数据包括但不限于历史的各种不同的手势、语音指令及会议阶段信息；交互内容生成模块：用于根据最优的交互反馈参数在虚拟环境中生成相应的交互内容以进行实时互动，其中，交互内容包括但不限于渲染全息图像、播放动画或显示文字提示。本发明的有益效果在于，与现有技术相比，本发明的全息互动远程会议方法及系统，可以根据实时数据生成交互反馈参数，由于会议过程中可能会出现实时的变化，例如讨论的重点转移、议程调整等，而根据实时数据生成的交互反馈参数能够随着会议进展进行实时调整，以适应会议的变化。且在分类、解码、参数映射等各步骤下生成了初始交互反馈参数后，再结合当前会议阶段信息，策略网络会根据当前的实际会议情境对初始交互反馈参数进行进一步优化调整，使得生成的交互反馈更加个性化及贴合会议当前阶段的实际需求，使与会者能够获得更符合其期望和需求的交互体验，这种定制化的智能交互反馈可以很好的提高与会者的参与感，调动与会者的互动积极性。进一步优化调整交互反馈参数还可以减少不必要的信息传递或干扰，使交互内容更加精准和高效，有助于会议的顺利流畅进行。通过训练好的策略网络，本系统可以在不同的情况下学习和适应最佳的交互反馈参数，从而具备更强的自适应性和智能性。附图说明图1是本发明提供的全息互动远程会议方法的流程示意图；图2是本发明提供的全息互动远程会议系统的系统框图。具体实施方式为了使本发明的目的、技术方案及优点更加清楚明白，以下结合附图及实施例，对本发明进行进一步详细说明。应当理解，此处所描述的具体实施例仅仅用以解释本发明，并不用于限定本发明。实施例一参考图1，实施例一提供一种全息互动远程会议方法，包括步骤S101~ S105：S101，实时监测并采集会议室中与会者的实时数据并进行预处理，实时数据包括但不限于实时的手势图像和语音信号。需要说明的是，在会议室内部署合适的传感器和设备，以监测与会者的活动，这些设备可以包括摄像头、麦克风等，用于捕捉与会者的手势图像和语音信号。通过摄像头捕捉与会者的实时手势图像，可以获取他们的手势动作、姿势等信息，同时，可以通过麦克风录制与会者的语音信号，包括他们的语音指令、讨论等。将采集到的实时数据传输到中央处理单元或服务器，以便进行后续的处理和分析，传输可以通过网络连接实现，确保数据的实时性和可靠性。对传输过来的实时数据进行预处理，以提取有用的特征和信息，对于手势图像，需要进行图像处理技术，如边缘检测、特征提取等，以获取手势的关键信息，对于语音信号，需要进行语音识别和特征提取，以转化为可分析的数据。还需要进行数据清洗，如剔除噪声、无效数据或错误信息，确保数据质量。再将预处理后的数据格式化为适合输入到分类模型和策略网络的形式，可以包括将图像转化为矩阵、将语音信号转化为特征向量等。将格式化后的数据存储在适当的数据库或数据结构中，以备后续使用。同时，为每个与会者标识唯一的身份信息，以便进行后续的个性化分析和交互。实时监测并采集会议室中与会者的实时数据并进行预处理，是为了从多种感知来源收集关于与会者的行为信息，为后续的交互分析和生成最优交互反馈参数提供必要的输入。S102，通过训练后的分类模型对预处理后的实时数据进行分类分析以得到分类结果，其中，所述分类结果为一个概率向量，概率向量中每个元素表示不同类别的概率，手势分类任务中不同类别表示不同的手势，语音指令分类任务中不同类别表示不同的语音指令。需要说明的是，通过训练后的分类模型对预处理后的实时数据进行分类分析以得到分类结果，是全息互动远程会议方法中的关键一步，这一步骤的目标是将采集到的实时数据与预先训练好的模型进行比较，从而确定输入数据属于哪一类别，并得到每个类别的概率分布。以下是详细的步骤阐述：预先训练一个分类模型，用于对不同类型的手势或语音指令进行分类。这个模型可以是各种机器学习算法，如深度神经网络、支持向量机、随机森林等，根据任务的复杂性选择合适的模型结构。将经过预处理的实时数据编码为适合模型输入的形式，对于图像数据，可以将图像转换为矩阵或张量，对于语音信号，可以提取音频特征并转换为特征向量。使用训练后的分类模型对编码后的数据进行推理，将编码后的数据输入到模型中，模型会计算每个类别的概率，并由所有类别的概率构成一个概率向量，因此输出的结果是一个概率向量，概率向量的每个元素表示输入数据属于对应类别的概率，因此根据任务的需要，可以得到手势分类或语音指令分类的概率分布。后续可以从概率向量中找到具有最高概率的类别，就是输入数据最可能属于的类别。假设模型的输出是一个概率向量，表示三个手势类别的概率，这里的第二个类别，为概率最高的手势类别，也是输入数据最可能属于的类别。通过训练后的分类模型对预处理后的实时数据进行分类分析，可以快速而准确地将输入数据分类为不同的类别，并得到每个类别的概率分布，可以为后续的交互操作和生成最优交互反馈参数提供基础。具体的，所述通过训练后的分类模型对预处理后的实时数据进行分类分析以得到分类结果的步骤包括：将预处理后的实时数据输入已经训练好的分类模型中，并在分类模型中进行前向传播，以将实时数据通过网络层逐层处理得到输出，输出表示为各个类别的得分；将输出归一化为一个概率分布，即为分类结果，Y=，其中，i为当前类别的索引，i=1、2、…、K，K为所有类别的总数，zi为分类模型输出的类别i的得分，Y表示为类别i的概率，所有类别的概率之和为 1。S103，对分类结果进行解码并找到概率最高的类别，将解码后的类别映射为相应的交互操作或虚拟环境行为下的参数，设为初始交互反馈参数。需要说明的是，在全息互动远程会议方法中，对分类结果进行解码并将解码后的类别映射为相应的交互操作或虚拟环境行为下的参数，是为了根据与会者的手势和语音指令，确定在虚拟环境中如何引导互动以及提供什么样的信息。以下是对这一步骤的详细阐述：针对手势数据和语音指令，分类模型会输出一个表示各个类别概率的概率向量，这些概率表示了每个类别的可能性。从手势数据的分类结果中，找到概率最高的类别。同样地，从语音指令的分类结果中，找到概率最高的类别。这些最高概率对应的类别即为解码后的类别，例如，假设手势分类的概率向量为 ，对应类别分别为 "赞成"、"反对" 和 "提问"。语音指令分类的概率向量为 ，对应类别分别为 "继续讨论" 和"结束讨论"。将解码后的类别映射为预先定义的交互操作或虚拟环境行为下的参数。这里的映射可以通过一个字典或映射关系表来实现。以手势为例，假设 "赞成" 映射为参数 1，"反对" 映射为参数 2，"提问" 映射为参数 3。同样地，"继续讨论" 映射为参数 1，"结束讨论" 映射为参数 2。再将映射后的参数组合成一个初始交互反馈参数，在这个示例中，假设手势映射后的参数是 1，语音指令映射后的参数是 1，那么生成的初始交互反馈参数可以是 。这样通过对分类结果的解码和类别映射，就可以将与会者的手势和语音指令翻译为具体的虚拟环境行为参数，为后续的互动过程提供了初步的指导和引导，不过在实际应用中，映射关系和参数的具体含义可以根据会议目标和需求进行定义，以满足不同情境下的交互需求。具体的，所述对分类结果进行解码并找到概率最高的类别，将解码后的类别映射为相应的交互操作或虚拟环境行为下的参数，设为初始交互反馈参数的步骤包括：找到分类结果的概率向量中的最大值对应的索引，并根据对应的索引确定类别以作为解码后的类别；创建一个包含所有类别标签的字典，其中，字典的键是各类别的索引，值是与各类别的索引一一关联的类别标签，为用来标识不同类别的唯一标识符；根据解码后的类别的索引从字典中找到对应的类别标签并设为目标类别标签，与目标类别标签对应的类别设为目标类别；将目标类别标签代入映射关系以将目标类别映射到交互参数，并设为初始交互反馈参数，其中，映射关系是预先设置的规则、公式或数据表格。需要说明的是，首先，从分类结果的概率向量中找到具有最大概率的索引，这个索引对应于分类结果中的类别，假设最大概率索引是max_prob_index。创建一个包含所有类别标签的字典，其中字典的键是各类别的索引，值是与各类别的索引一一关联的类别标签，这个字典用于将索引映射回类别标签。从类别标签字典中根据max_prob_index找到对应的类别标签，将其设为目标类别标签，例如，如果max_prob_index对应于 "赞成" 类别，则目标类别标签为 "赞成"。根据预先设置的映射关系，将目标类别标签映射为相应的交互参数，这个映射关系的具体形式可以根据具体需求和应用场景来定义，例如，如果 "赞成" 映射为参数 1，"反对" 映射为参数 2，那么目标类别标签 "赞成" 对应的交互参数可以是 1。将通过映射关系得到的交互参数组合成一个初始交互反馈参数。在这个示例中，如果目标类别标签为 "赞成"，并且 "赞成" 映射为参数 1，那么生成的初始交互反馈参数可以是 。通过以上步骤，分类结果的概率向量中最大概率对应的类别可以被解码，并根据预先定义的映射关系，将这个解码后的类别映射为初始交互反馈参数，为后续的虚拟环境中的交互内容生成提供了基础，需要注意的是，这里的映射关系可以根据实际情况进行灵活的定义，以满足不同应用场景的交互需求。S104，根据历史数据训练策略网络，结合当前会议阶段通过训练好的策略网络对初始交互反馈参数进行优化调整以生成最优的交互反馈参数，历史数据包括但不限于历史的各种不同的手势、语音指令及会议阶段信息。需要说明的是，收集来自过去会议的历史数据，其中包括与会者的不同手势、语音指令以及会议阶段信息，这些数据可以帮助训练策略网络了解不同情境下的最佳交互反馈参数，还可以收集情境信息、情感和情绪信息、互动历史个性化信息、环境信息、时间信息和反馈信息，这样可以构建更丰富和全面的历史数据集，帮助策略网络更准确地生成适应情境和与会者个性的最优交互反馈参数。但数据太多也会大大提高数据处理的复杂性，影响与会者的体验感，所以可以根据一些主要需求来进行数据的合理收集。策略网络的目标是学习如何根据不同的输入情境生成最优的交互反馈参数。在实时会议中，将当前会议阶段信息与初始交互反馈参数组合成一个状态向量，作为输入提供给已训练好的策略网络。策略网络将状态向量作为输入，经过前向传播，生成一个动作概率分布，从这个分布中，可以采样得到一个动作或选择具有最高概率的动作作为最优的交互反馈参数。最终目标是使生成的交互反馈参数能够在当前会议情境下引导最佳的交互。且根据策略网络生成的最优交互反馈参数，系统可以实时调整虚拟环境中的交互内容，以更好地满足与会者的需求和情境，这种实时调整能够在会议过程中持续进行，以保持交互的有效性和适应性。举例来说，如果历史数据表明在类似的讨论阶段，与会者更倾向于使用特定的手势来表达特定的意见，而策略网络在当前情境下生成了类似的手势作为最优交互反馈参数，那么这个交互反馈参数就被认为是最优的，因为它结合了历史数据和当前情境的信息，以引导与会者更好地表达意见。同样，如果历史数据显示在类似情境下，与会者更倾向于使用某些语音指令来引导讨论，策略网络可以在当前情境中生成类似的语音指令作为最优交互反馈参数。通过训练好的策略网络结合历史数据和当前情境信息，系统可以实现更智能和个性化的交互反馈，提高会议的效率和参与感。具体的，所述根据历史数据训练策略网络的步骤包括：将交互反馈的参数作为动作，历史数据作为状态，以定义状态空间和动作空间，其中，定义的动作空间涵盖各种可能的交互反馈参数，历史数据包括但不限于历史的各种不同的手势、语音指令及会议阶段信息；根据动作与状态创建一个Q值表并初始化，其中，所述Q值表的每个条目表示状态-动作对应的Q值，Q值为用于记录每个状态下采取每个动作的估计回报值，每个状态对应至少一个动作；从初始状态开始，对于每个状态，随机选择一个动作；执行选择的动作与环境互动以获取下一个状态和奖励；根据奖励及下一个状态的最大Q值更新Q值表中的Q值，以逐步调整每个状态-动作对应的Q值，使其逼近最优策略，更新公式为：Q新=Q+α*，其中，Q新为状态s下采取动作a更新后的Q值，Q是状态s下采取动作a的Q值，α是学习率，reward是获得的奖励，γ是折扣因子，s'是下一个状态，a'是下一个状态下选择的最优动作，Q为下一个状态s'下采取动作a'的Q值；不断与环境互动并更新所述Q值表中的Q值，直至达到收敛条件，以得到更新后的Q值表；将更新后的Q值表中每个状态的每个动作对应的Q值转化为概率，以为每个状态的动作创建一个概率分布，设为预期的目标分布；对策略网络进行训练并使策略网络输出的动作概率分布与预期的目标分布相匹配，以得到训练后的策略网络。需要说明的是，该步骤的目的是通过强化学习优化交互反馈参数，使系统在不同状态下做出更有利的交互决策，通过迭代优化Q值表和训练策略网络，系统可以逐步学习并优化交互反馈参数，以实现更优的交互体验和效果，使得系统在实时环境中能够适应不同的状态和用户需求，提供更个性化和有效的全息互动。更新后的Q值表，对于其每个状态，在Q值表中的每个动作对应一个Q值，通过将这些Q值进行归一化处理，可以将Q值转化为概率分布，一种常见的方式是使用softmax函数，它可以将Q值映射为概率分布，使得所有动作的概率之和等于1。对于每个状态，根据Q值转化后的概率分布，创建一个目标分布，这个目标分布表示了在每个状态下不同动作的期望概率。利用这个目标分布，训练策略网络以使其输出的动作概率分布尽可能接近预期的目标分布，这是通过监督学习的方式实现的，策略网络的输出将被调整，以使其在每个状态下输出的动作概率与目标分布更加一致。使得训练后的策略网络能够输出与目标分布一致的动作概率分布，从而在虚拟环境中生成更符合预期的交互内容。策略网络经过训练，能够更好地理解每个状态下应该采取哪些动作，从而更智能地引导与会者的互动行为，使得虚拟全息会议的交互体验更加自然、智能、准确，并能够根据会议阶段和与会者的需求灵活调整互动内容，从而实现更高效和有针对性的会议互动。进一步，所述对策略网络进行训练并使策略网络输出的动作概率分布与预期的目标分布相匹配，以得到训练后的策略网络的步骤包括：将Q值表中的每个状态输入网络策略，对于每个状态，通过损失函数计算策略网络输出的动作概率分布与预期的目标分布之间的差异；根据差异值并通过反向传播算法计算相对于策略网络权重的梯度；根据梯度更新策略网络的权重，以使策略网络输出的动作概率分布与预期的目标分布之间的差异减小；通过重复迭代不断调整策略网络的权重，以使策略网络输出的动作概率分布与预期的目标分布之间的差异逐渐减小，逐渐与目标分布相匹配，便得到训练后的策略网络。需要说明的是，通过这一步骤，策略网络逐渐学习如何输出更适合当前状态的动作概率分布，从而实现更智能、更适应性的互动反馈。策略网络通过与环境的互动和梯度下降的优化，能够逐渐调整其输出，以更好地指导虚拟环境中的交互内容生成，从而提供更优质的全息互动体验。使得系统可以适应不同的会议情境和与会者需求，实现更自然、智能和有针对性的交互指导。进一步，所述结合当前会议阶段通过训练好的策略网络对初始交互反馈参数进行优化调整以生成最优的交互反馈参数的步骤包括：将初始交互反馈参数与当前会议阶段信息组合成一个状态向量以定义状态空间；将状态向量作为输入，以通过训练好的策略网络生成当前状态下的动作概率分布；从当前状态下的动作概率分布中选择概率最大的动作作为最优动作，并将最优动作作为最优的交互反馈参数。需要说明的是，将初始交互反馈参数和当前会议阶段信息组合成状态向量，有效地将交互反馈参数与会议上下文相结合，这有助于保持灵活性，因为交互反馈的最优性可能会随着会议进展阶段而变化。使用训练好的策略网络，将状态向量作为输入，生成当前状态下的动作概率分布。并从动作概率分布中选择概率最大的动作作为最优动作。这意味着策略网络在给定当前状态的情况下，选择出最有可能获得良好交互效果的动作，这样的选择能够在保持与会者参与度的同时，最大限度地提升会议的效率和成果。S105，根据最优的交互反馈参数在虚拟环境中生成相应的交互内容以进行实时互动，其中，交互内容包括但不限于渲染全息图像、播放动画或显示文字提示。需要说明的是，渲染全息图像，是利用全息投影技术，将虚拟图像呈现在现实世界中，通过投影装置，将生成的图像投影在空气中形成立体的全息图像。例如：假设最优交互反馈参数表示用户选择了"赞成"手势和"继续讨论"语音指令，在全息投影中，系统可以呈现一个立体的图标，如一个拇指朝上的手势图标，同时在图像下方以透明的字体显示"继续讨论"的文字提示。同时，也可以播放一个小型的立体动画，展示与会者发表意见，以此鼓励更多的参与。在虚拟环境中，还可以播放动画来引导与会者的互动，动画可以是一系列的动态图像，或者是通过全息投影技术实现的立体动画。例如，如果与会者选择了"提问"手势，系统可以播放一个动画，展示一个虚拟人物举手提问的场景，以此鼓励其他与会者也参与提问。显示文字提示，是在虚拟环境中显示清晰易读的文字提示，以引导与会者的操作和互动，文字提示可以出现在虚拟屏幕、头部显示设备或其他适当位置。例如，假设最优交互反馈参数表明与会者选择了"结束讨论"语音指令，系统可以在视野中心显示一行文字，提示"讨论结束，谢谢参与！"，以通知与会者讨论即将结束。交互内容还可以包括虚拟角色互动：在虚拟环境中创建虚拟人物或角色，与与会者进行互动，虚拟角色可以回答问题、提供信息，甚至模拟真实的对话交流；虚拟物体交互：在虚拟环境中生成虚拟物体，与会者可以通过手势或语音指令与这些物体进行交互，比如拖动、旋转、放大缩小等操作；虚拟白板：在虚拟环境中创建一个虚拟白板，与会者可以在上面书写、绘制图形、写下想法等，实现实时的协作和共享；多维数据可视化：将复杂的数据转化为可视化的图表、图形或图像，以便与会者更直观地理解和讨论信息；虚拟场景切换：在不同的会议阶段切换虚拟场景，以适应不同的讨论内容，比如从会议室切换到虚拟展厅，展示新产品的模型和特点；实时共享：在虚拟环境中共享文件、文档、图片等信息，与会者可以通过手势或语音指令进行浏览、讨论和编辑；虚拟实验：在虚拟环境中模拟实验场景，让与会者可以参与实验、观察结果，并进行讨论和分析；虚拟演示：在虚拟环境中展示产品演示、流程演示等内容，与会者可以与虚拟内容互动并提出问题；身临其境体验：利用音效、视觉效果等技术，营造身临其境的体验，让与会者感觉好像真的在一个虚拟环境中参与会议。这些交互内容可以根据会议的性质、目标和与会者的需求进行定制和创新，以提供丰富多样的互动体验和更有效的会议参与方式，通过这些交互内容，系统有效地将与会者的选择和意图传达给其他与会者，可以促进更多有意义的互动和讨论，也能够更好的渲染会议氛围，提高与会者的参会乐趣感，调动与会者的积极性。以下是本实施例的一个具体实施方式：假设有一场虚拟全息远程会议，会议的目标是讨论新产品的发布计划，若会议正处于讨论阶段。在会议室中的设备捕捉与会者的实时数据，其中，一个与会者正在使用手势来表达他们的意见，另一个与会者正在说出他们的观点，这些数据被捕捉并传输到系统中进行预处理；训练后的分类模型分析预处理后的数据，模型将手势数据分类为不同的手势，如"赞成"、"反对"、"提问"等，同时将语音指令分类为"继续"、"结束讨论"等，假设模型分析后的分类结果如下：手势数据的分类结果为：概率向量 ，数值分别表示"赞成"、"反对"和"提问"的概率，语音指令分类结果为：概率向量 ，数值分别表示"继续讨论"和"结束讨论"的概率。对手势数据的分类结果进行解码，找到概率最高的类别，假设最高概率对应"赞成"，且对语音指令的分类结果进行解码，找到概率最高的类别，假设最高概率对应"继续讨论"，将这两个类别映射为一组初始交互反馈参数，如若"赞成"、"反对"和"提问"的映射值分别为1、2、3，"继续讨论"和"结束讨论"的映射值分别为1、2，则将"赞成"映射为参数1，"继续讨论"映射为参数2，编码后的初始交互反馈参数可以表示为 。对当前会议阶段信息也进行编码，例如，"讨论"阶段编码为3，"总结" 阶段编码为4，将编码后的初始交互反馈参数和当前会议阶段信息组合在一起，形成一个状态向量，在本实施例中，状态向量可以是 。将初始交互反馈参数与当前会议阶段信息组合生成的状态向量输入训练好的策略网络，生成一个动作概率分布，例如："赞成"手势类别的概率：0.3，"反对"手势类别的概率：0.3，"提问"手势类别的概率：0.4，其他手势类别的概率：0.1，"继续讨论"语音指令类别的概率：0.6，"结束讨论"语音指令类别的概率：0.4。在这种情况下，最大概率分别对应于 "提问" 手势和“继续讨论”的语音指令，最后生成最优的交互反馈参数可以是 ，其中，"提问"手势类别的参数为3，在讨论阶段，可以鼓励与会者提出问题或疑虑，"继续讨论"语音指令类别的参数为1，这样的指令可以鼓励与会者对话，进一步深入讨论话题。根据手势参数 3，系统知道与会者想要提出问题，在虚拟环境中，可以显示一个问题提示，引导与会者提出他们关于讨论话题的问题。例如，虚拟屏幕上可能会出现一个提示，如"请问有没有人对当前讨论话题有疑问或问题？"。根据语音指令参数 4，系统知道与会者想要进行更深入的探讨，在虚拟环境中，可以展示一个虚拟角色或信息面板，提供更详细的讨论信息，与会者可以通过语音或手势与虚拟角色互动，询问关于讨论话题的更深入问题，或者请求进一步的解释和见解。这样，与会者可以根据自己的需求选择提出问题或进行深入探讨，而虚拟环境根据最优的交互反馈参数为他们提供相应的指导和互动内容。整个全息会议阶段可能包括开场阶段：会议开始的阶段，通常包括欢迎致辞和议程介绍；问题提出阶段：在这个阶段，与会者可能会提出讨论的问题或议题；讨论阶段：与会者就问题进行讨论和辩论的阶段，总结阶段： 对讨论结果进行总结和归纳的阶段；决策阶段： 在这个阶段，与会者可能会做出决策或达成共识；结尾阶段： 会议的结束阶段，可能包括感谢和未来计划的说明。本发明的全息互动远程会议方法，可以根据实时数据生成交互反馈参数，由于会议过程中可能会出现实时的变化，例如讨论的重点转移、议程调整等，而根据实时数据生成的交互反馈参数能够随着会议进展进行实时调整，以适应会议的变化。且在分类、解码、参数映射等各步骤下生成了初始交互反馈参数后，再结合当前会议阶段信息，策略网络会根据当前的实际会议情境对初始交互反馈参数进行进一步优化调整，使得生成的交互反馈更加个性化及贴合会议当前阶段的实际需求，使与会者能够获得更符合其期望和需求的交互体验，这种定制化的智能交互反馈可以很好的提高与会者的参与感，调动与会者的互动积极性。进一步优化调整交互反馈参数还可以减少不必要的信息传递或干扰，使交互内容更加精准和高效，有助于会议的顺利流畅进行。通过训练好的策略网络，本系统可以在不同的情况下学习和适应最佳的交互反馈参数，从而具备更强的自适应性和智能性。实施例二参考图2，实施例二提供一种全息互动远程会议系统，包括：数据采集模块：用于实时监测并采集会议室中与会者的实时数据并进行预处理，实时数据包括但不限于实时的手势图像和语音信号。分类模块：用于通过训练后的分类模型对预处理后的实时数据进行分类分析以得到分类结果，其中，所述分类结果为一个概率向量，概率向量中每个元素表示不同类别的概率，手势分类任务中不同类别表示不同的手势，语音指令分类任务中不同类别表示不同的语音指令。分类模块还用于：将预处理后的实时数据输入已经训练好的分类模型中，并在分类模型中进行前向传播，以将实时数据通过网络层逐层处理得到输出，输出表示为各个类别的得分；将输出归一化为一个概率分布，即为分类结果，Y=，其中，i为当前类别的索引，i=1、2、…、K，K为所有类别的总数，zi为分类模型输出的类别i的得分，Y表示为类别i的概率，所有类别的概率之和为 1。初始参数生成模块：用于对分类结果进行解码并找到概率最高的类别，将解码后的类别映射为相应的交互操作或虚拟环境行为下的参数，设为初始交互反馈参数。初始参数生成模块还用于：找到分类结果的概率向量中的最大值对应的索引，并根据对应的索引确定类别以作为解码后的类别；创建一个包含所有类别标签的字典，其中，字典的键是各类别的索引，值是与各类别的索引一一关联的类别标签，为用来标识不同类别的唯一标识符；根据解码后的类别的索引从字典中找到对应的类别标签并设为目标类别标签，与目标类别标签对应的类别设为目标类别；将目标类别标签代入映射关系以将目标类别映射到交互参数，并设为初始交互反馈参数，其中，映射关系是预先设置的规则、公式或数据表格。最优参数生成模块：用于根据历史数据训练策略网络，结合当前会议阶段通过训练好的策略网络对初始交互反馈参数进行优化调整以生成最优的交互反馈参数，历史数据包括但不限于历史的各种不同的手势、语音指令及会议阶段信息。最优参数生成模块还用于：将交互反馈的参数作为动作，历史数据作为状态，以定义状态空间和动作空间，其中，定义的动作空间涵盖各种可能的交互反馈参数，历史数据包括但不限于历史的各种不同的手势、语音指令及会议阶段信息；根据动作与状态创建一个Q值表并初始化，其中，所述Q值表的每个条目表示状态-动作对应的Q值，Q值为用于记录每个状态下采取每个动作的估计回报值，每个状态对应至少一个动作；从初始状态开始，对于每个状态，随机选择一个动作；执行选择的动作与环境互动以获取下一个状态和奖励；根据奖励及下一个状态的最大Q值更新Q值表中的Q值，以逐步调整每个状态-动作对应的Q值，使其逼近最优策略，更新公式为：Q新=Q+α*，其中，Q新为状态s下采取动作a更新后的Q值，Q是状态s下采取动作a的Q值，α是学习率，reward是获得的奖励，γ是折扣因子，s'是下一个状态，a'是下一个状态下选择的最优动作，Q为下一个状态s'下采取动作a'的Q值；不断与环境互动并更新所述Q值表中的Q值，直至达到收敛条件，以得到更新后的Q值表；将更新后的Q值表中每个状态的每个动作对应的Q值转化为概率，以为每个状态的动作创建一个概率分布，设为预期的目标分布；对策略网络进行训练并使策略网络输出的动作概率分布与预期的目标分布相匹配，以得到训练后的策略网络。最优参数生成模块还用于：将Q值表中的每个状态输入网络策略，对于每个状态，通过损失函数计算策略网络输出的动作概率分布与预期的目标分布之间的差异；根据差异值并通过反向传播算法计算相对于策略网络权重的梯度；根据梯度更新策略网络的权重，以使策略网络输出的动作概率分布与预期的目标分布之间的差异减小；通过重复迭代不断调整策略网络的权重，以使策略网络输出的动作概率分布与预期的目标分布之间的差异逐渐减小，逐渐与目标分布相匹配，便得到训练后的策略网络。最优参数生成模块还用于：将初始交互反馈参数与当前会议阶段信息组合成一个状态向量以定义状态空间；将状态向量作为输入，以通过训练好的策略网络生成当前状态下的动作概率分布；从当前状态下的动作概率分布中选择概率最大的动作作为最优动作，并将最优动作作为最优的交互反馈参数。交互内容生成模块：用于根据最优的交互反馈参数在虚拟环境中生成相应的交互内容以进行实时互动，其中，交互内容包括但不限于渲染全息图像、播放动画或显示文字提示。以上所述仅为本发明的较佳实施例而已，并不用以限制本发明，凡在本发明的精神和原则之内所作的任何修改、等同替换和改进等，均应包含在本发明的保护范围之内。
