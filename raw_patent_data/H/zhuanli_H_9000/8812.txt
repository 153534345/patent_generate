标题title
强化学习智能体训练方法、模态带宽资源调度方法及装置
摘要abst
本发明公开了强化学习智能体训练方法、模态带宽资源调度方法及装置，其中强化学习智能体训练方法在多模态网络下，利用强化学习智能体与网络环境不断交互，获取最新全局网络特征并输出更新后的动作。通过调节模态所占用的带宽，设定奖励值为智能体确定优化目标，实现模态的调度，保障多模态网络资源合理使用。训练后的强化学习智能体应用于模态带宽资源调度方法中，能自适应于不同特征的网络中，可用于多模态网络的智慧管控，具有良好的适应性及调度性能。
权利要求书clms
1.一种多模态网络中的强化学习智能体训练方法，其特征在于，应用于强化学习智能体，包括：S11：构建全局网络特征状态、动作及训练所述强化学习智能体所需的深度神经网络模型，其中所述深度神经网络模型包括执行新网络、执行旧网络及动作评价网络：S12：设置一轮训练的最大步数；S13：在每一步中，获取全局网络特征状态，将所述全局网络特征状态输入所述执行新网络，控制SDN交换机执行所述执行新网络输出的动作，获取所述SDN交换机执行所述动作后网络的状态和奖励值，将所述动作、奖励值和执行所述动作前后的两个时间段内分别的状态存入经验池；S14：根据所述经验池中所有的奖励值和执行动作前的状态，更新所述动作评价网络的网络参数；S15：将所述执行新网络的网络参数赋值给所述执行旧网络，并根据所述经验池中所有的动作和执行动作前的状态，更新所述执行新网络的网络参数；S16：重复步骤S13-S15，直至多模态网络中各个模态占用的带宽均在保证通信传输质量的同时不让网络出口端过载。2.根据权利要求1所述的方法，其特征在于，所述全局网络特征状态包括各个模态的报文数量、各个模态的平均报文大小、每条流的平均时延、每条流中的数据包数量、每条流的大小、每条流中的平均数据包大小。3.根据权利要求1所述的方法，其特征在于，所述动作为在对应的全局网络特征状态下选择的动作向量的均值与噪声的和。4.根据权利要求1所述的方法，其特征在于，根据所述经验池中所有的奖励值和执行动作前的状态，更新所述动作评价网络的网络参数，包括：将所述经验池中所有的执行动作前的状态输入所述动作评价网络中，得到对应的期望价值；根据所述期望价值和对应的奖励值以及预先设定的衰减折扣，计算每个行动作前的状态的折扣奖励；计算所述折扣奖励与所述期望价值的差值，并根据所有差值计算均方差，将得到的均方差作为第一损失值，以更新所述动作评价网络的网络参数。5.根据权利要求4所述的方法，其特征在于，根据所述经验池中所有的动作和执行动作前的状态，更新所述执行新网络的网络参数，包括：将所述经验池中所有的执行动作前的状态分别输入所述执行旧网络和执行新网络，得到执行动作旧分布和执行动作新分布；计算所述经验池中每个动作在对应的所述执行动作旧分布和执行动作新分布中分别出现的第一概率和第二概率；计算所述第二概率与所述第一概率的比值；将所有的所述比值乘以对应的所述差值并求平均之后的值作为第二损失值，以更新所述执行新网络的网络参数。6.一种多模态网络中的强化学习智能体训练装置，其特征在于，应用于强化学习智能体，包括：构建模块，用于构建全局网络特征状态、动作及训练所述强化学习智能体所需的深度神经网络模型，其中所述深度神经网络模型包括执行新网络、执行旧网络及动作评价网络：设置模块，用于设置一轮训练的最大步数；执行模块，用于在每一步中，获取全局网络特征状态，将所述全局网络特征状态输入所述执行新网络，控制SDN交换机执行所述执行新网络输出的动作，获取所述SDN交换机执行所述动作后网络的状态和奖励值，将所述动作、奖励值和执行所述动作前后的两个时间段内分别的状态存入经验池；第一更新模块，用于根据所述经验池中所有的奖励值和执行动作前的状态，更新所述动作评价网络的网络参数；第二更新模块，用于将所述执行新网络的网络参数赋值给所述执行旧网络，并根据所述经验池中所有的动作和执行动作前的状态，更新所述执行新网络的网络参数；重复模块，用于重复执行模块到第二更新模块中的过程，直至多模态网络中各个模态占用的带宽均在保证通信传输质量的同时不让网络出口端过载。7.一种多模态网络中的模态带宽资源调度方法，其特征在于，包括：将根据权利要求1-5中任一项所述的多模态网络中的强化学习智能体训练方法训练后的强化学习智能体应用于多模态网络中；根据所述强化学习智能体输出的调度策略，调度各个模态占用的资源。8.一种多模态网络中的模态带宽资源调度装置，其特征在于，包括：应用模块，用于将根据权利要求1-5中任一项所述的多模态网络中的强化学习智能体训练方法训练后的强化学习智能体应用于多模态网络中；调度模块，用于根据所述强化学习智能体输出的调度策略，调度各个模态占用的资源。9.一种电子设备，其特征在于，包括：一个或多个处理器；存储器，用于存储一个或多个程序；当所述一个或多个程序被所述一个或多个处理器执行，使得所述一个或多个处理器实现如权利要求1-5任一项所述的多模态网络中的强化学习智能体训练方法或权利要求7所述的多模态网络中的模态带宽资源调度方法。10.一种计算机可读存储介质，其上存储有计算机指令，其特征在于，该指令被处理器执行时实现如权利要求1-5任一项所述的多模态网络中的强化学习智能体训练方法或权利要求7所述的多模态网络中的模态带宽资源调度方法的步骤。
说明书desc
技术领域本发明属于网络管控技术领域，尤其涉及强化学习智能体训练方法、模态带宽资源调度方法及装置。背景技术在多模态网络中，同时运行着多种网络技术体制，每一种技术体制即为一种网络模态。各网络模态共享网络资源，如不加以管控，则会导致各网络模态直接竞争网络资源，如带宽等，这会直接影响部分关键模态的通信传输质量。因此，对网络中的各个模态进行合理管控是保障多模态网络稳定运行的必要前提之一。对于上述需要，目前主流技术是控制交换机端口的带宽被使用的比例，限制出口流量大小以避免网络过载。在实现本发明过程中，本发明人发现现有技术至少存在如下问题：使用这类静态的策略将无法适应网络模态动态变化的情况。而实际网络中，很有可能因业务变化而导致个别模态流量变大，此时原来的静态策略则不再适用。发明内容本申请实施例的目的是提供强化学习智能体训练方法、模态带宽资源调度方法及装置，以解决相关技术中存在的多模态网络中的模态资源无法智慧管控的技术问题。根据本申请实施例的第一方面，提供一种多模态网络中的模态带宽资源调度方法，包括：S11：构建全局网络特征状态、动作及训练所述强化学习智能体所需的深度神经网络模型，其中所述深度神经网络模型包括执行新网络、执行旧网络及动作评价网络：S12：设置一轮训练的最大步数；S13：在每一步中，获取全局网络特征状态，将所述全局网络特征状态输入所述执行新网络，控制SDN交换机执行所述执行新网络输出的动作，获取所述SDN交换机执行所述动作后网络的状态和奖励值，将所述动作、奖励值和执行所述动作前后的两个时间段内分别的状态存入经验池；S14：根据所述经验池中所有的奖励值和执行动作前的状态，更新所述动作评价网络的网络参数；S15：将所述执行新网络的网络参数赋值给所述执行旧网络，并根据所述经验池中所有的动作和执行动作前的状态，更新所述执行新网络的网络参数；S16：重复步骤S13-S15，直至多模态网络中各个模态占用的带宽均在保证通信传输质量的同时不让网络出口端过载。进一步地，所述全局网络特征状态包括各个模态的报文数量、各个模态的平均报文大小、每条流的平均时延、每条流中的数据包数量、每条流的大小、每条流中的平均数据包大小。进一步地，所述动作为在对应的全局网络特征状态下选择的动作向量的均值与噪声的和。进一步地，根据所述经验池中所有的奖励值和执行动作前的状态，更新所述动作评价网络的网络参数，包括：将所述经验池中所有的执行动作前的状态输入所述动作评价网络中，得到对应的期望价值；根据所述期望价值和对应的奖励值以及预先设定的衰减折扣，计算每个行动作前的状态的折扣奖励；计算所述折扣奖励与所述期望价值的差值，并根据所有差值计算均方差，将得到的均方差作为第一损失值，以更新所述动作评价网络的网络参数。进一步地，根据所述经验池中所有的动作和执行动作前的状态，更新所述执行新网络的网络参数，包括：将所述经验池中所有的执行动作前的状态分别输入所述执行旧网络和执行新网络，得到执行动作旧分布和执行动作新分布；计算所述经验池中每个动作在对应的所述执行动作旧分布和执行动作新分布中分别出现的第一概率和第二概率；计算所述第二概率与所述第一概率的比值；将所有的所述比值乘以对应的所述差值并求平均之后的值作为第二损失值，以更新所述执行新网络的网络参数。根据本申请实施例的第二方面，提供一种多模态网络中的模态带宽资源调度装置，包括：构建模块，用于构建全局网络特征状态、动作及训练所述强化学习智能体所需的深度神经网络模型，其中所述深度神经网络模型包括执行新网络、执行旧网络及动作评价网络：设置模块，用于设置一轮训练的最大步数；执行模块，用于在每一步中，获取全局网络特征状态，将所述全局网络特征状态输入所述执行新网络，控制SDN交换机执行所述执行新网络输出的动作，获取所述SDN交换机执行所述动作后网络的状态和奖励值，将所述动作、奖励值和执行所述动作前后的两个时间段内分别的状态存入经验池；第一更新模块，用于根据所述经验池中所有的奖励值和执行动作前的状态，更新所述动作评价网络的网络参数；第二更新模块，用于将所述执行新网络的网络参数赋值给所述执行旧网络，并根据所述经验池中所有的动作和执行动作前的状态，更新所述执行新网络的网络参数；重复模块，用于重复执行模块到第二更新模块中的过程，直至多模态网络中各个模态占用的带宽均在保证通信传输质量的同时不让网络出口端过载。根据本申请实施例的第三方面，提供多模态网络中的模态带宽资源调度方法，包括：将根据第一方面所述的多模态网络中的强化学习智能体训练方法训练后的强化学习智能体应用于多模态网络中；根据所述强化学习智能体输出的调度策略，调度各个模态占用的资源。根据本申请实施例的第三方面，提供一种多模态网络中的模态带宽资源调度装置，包括：应用模块，用于将根据第一方面所述的多模态网络中的强化学习智能体训练方法训练后的强化学习智能体应用于多模态网络中；调度模块，用于根据所述强化学习智能体输出的调度策略，调度各个模态占用的资源。根据本申请实施例的第五方面，提供一种电子设备，包括：一个或多个处理器；存储器，用于存储一个或多个程序；当所述一个或多个程序被所述一个或多个处理器执行，使得所述一个或多个处理器实现如多模态网络中的强化学习智能体训练方法或多模态网络中的模态带宽资源调度方法。根据本申请实施例的第六方面，提供一种计算机可读存储介质，该指令被处理器执行时实现如多模态网络中的强化学习智能体训练方法或多模态网络中的模态带宽资源调度方法的步骤。本申请的实施例提供的技术方案可以包括以下有益效果：由上述实施例可知，本申请利用强化学习算法思想，构建适应于多模态网络的全局网络特征状态、执行动作、奖励函数，让强化学习智能体不断与网络进行交互，根据网络状态及奖励值的变化输出最优执行动作，从而让多模态网络资源的分配符合预期，保障网络运行性能，对于推动多模态网络的智慧管控具有较强的现实意义。应当理解的是，以上的一般描述和后文的细节描述仅是示例性和解释性的，并不能限制本申请。附图说明此处的附图被并入说明书中并构成本说明书的一部分，示出了符合本申请的实施例，并与说明书一起用于解释本申请的原理。图1是根据一示例性实施例示出的一种多模态网络中的强化学习智能体训练方法的流程图。图2是根据一示例性实施例示出的步骤S14的流程图。图3是根据一示例性实施例示出的“根据所述经验池中所有的动作和执行动作前的状态，更新所述执行新网络的网络参数”的流程图。图4是根据一示例性实施例示出的一种多模态网络中的强化学习智能体训练装置的框图。图5是根据一示例性实施例示出的一种多模态网络中的模态带宽资源调度方法的流程图。图6是根据一示例性实施例示出的一种多模态网络中的模态带宽资源调度装置的框图。图7是根据一示例性实施例示出的一种电子设备的示意图。具体实施方式这里将详细地对示例性实施例进行说明，其示例表示在附图中。下面的描述涉及附图时，除非另有表示，不同附图中的相同数字表示相同或相似的要素。以下示例性实施例中所描述的实施方式并不代表与本申请相一致的所有实施方式。在本申请使用的术语是仅仅出于描述特定实施例的目的，而非旨在限制本申请。在本申请和所附权利要求书中所使用的单数形式的“一种”、“所述”和“该”也旨在包括多数形式，除非上下文清楚地表示其他含义。还应当理解，本文中使用的术语“和/或”是指并包含一个或多个相关联的列出项目的任何或所有可能组合。应当理解，尽管在本申请可能采用术语第一、第二、第三等来描述各种信息，但这些信息不应限于这些术语。这些术语仅用来将同一类型的信息彼此区分开。例如，在不脱离本申请范围的情况下，第一信息也可以被称为第二信息，类似地，第二信息也可以被称为第一信息。取决于语境，如在此所使用的词语“如果”可以被解释成为“在……时”或“当……时”或“响应于确定”。实施例1：图1是根据一示例性实施例示出的一种多模态网络中的强化学习智能体训练方法的流程图，如图1所示，该方法应用于强化学习智能体，可以包括以下步骤：步骤S11：构建全局网络特征状态、动作及训练所述强化学习智能体所需的深度神经网络模型，其中所述深度神经网络模型包括执行新网络、执行旧网络及动作评价网络：步骤S12：设置一轮训练的最大步数；步骤S13：在每一步中，获取全局网络特征状态，将所述全局网络特征状态输入所述执行新网络，控制SDN交换机执行所述执行新网络输出的动作，获取所述SDN交换机执行所述动作后网络的状态和奖励值，将所述动作、奖励值和执行所述动作前后的两个时间段内分别的状态存入经验池；步骤S14：根据所述经验池中所有的奖励值和执行动作前的状态，更新所述动作评价网络的网络参数；步骤S15：将所述执行新网络的网络参数赋值给所述执行旧网络，并根据所述经验池中所有的动作和执行动作前的状态，更新所述执行新网络的网络参数；步骤S16：重复步骤S13-S15，直至多模态网络中各个模态占用的带宽均在保证通信传输质量的同时不让网络出口端过载。由上述实施例可知，本申请利用强化学习算法思想，构建适应于多模态网络的全局网络特征状态、执行动作、奖励函数，让强化学习智能体不断与网络进行交互，根据网络状态及奖励值的变化输出最优执行动作，从而让多模态网络资源的分配符合预期，保障网络运行性能，对于推动多模态网络的智慧管控具有较强的现实意义。在步骤S11的具体实施中，构建全局网络特征状态、动作及训练所述强化学习智能体所需的深度神经网络模型，其中所述深度神经网络模型包括执行新网络、执行旧网络及动作评价网络：具体地，所述全局网络特征状态包括各个模态的报文数量、各个模态的平均报文大小、每条流的平均时延、每条流中的数据包数量、每条流的大小、每条流中的平均数据包大小。这些特征构成当前时间间隔秒的全局网络状态。用表示第t个秒内全局网络特征。具体地，所述动作为在对应的全局网络特征状态下选择的动作向量的均值与噪声的和。用表示第t个秒的动作。所述动作用于调整流的带宽，进而调度各模态所占用的资源，保障网络通信质量符合预期目标。所述动作的物理含义为每个模态每条流到达出口区域的比例。用P表示网络中运行的模态数量，由于一种模态对应一种网络技术体制，因此假设网络中运行的模态数量固定不变。用表示每个模态中流数量的最大值，则输出的动作空间维度为。用表示在第t个秒内基于第p个模态的流数量，满足。因此，在第t个秒内，仅有个元素有对应流，因此取值为0.1-1，而其他元素由于没有实际流，取值为0。在具体实施中，为方便实现，可对执行新网络、执行旧网络及动作评价网络采用相同架构，例如可以采用深度神经网络、卷积神经网络、循环神经网络等架构。网络构建完成后随机初始化参数。在步骤S12的具体实施中，设置一轮训练的最大步数；具体地，设置每一轮训练的最大步数T，实际实施中T的取值与网络中的模态数量等因素相关，需要在训练过程中多次尝试选择较为优选的值。例如，假设网络中模态数量为8，经过多次尝试得到T为120时较为优选。在步骤S13的具体实施中，在每一步中，获取全局网络特征状态，将所述全局网络特征状态输入所述执行新网络，控制SDN交换机执行所述执行新网络输出的动作，获取所述SDN交换机执行所述动作后网络的状态和奖励值，将所述动作、奖励值和执行所述动作前后的两个时间段内分别的状态存入经验池；具体地，在每一步中，所述强化学习智能体通过控制器按采样间隔秒，获取秒时间段内全局网络特征。将当前网络状态输入执行新网络，输出基于当前参数的所述执行动作的均值和方差N，输出的所述执行动作表示为其中，表示的是强化学习智能体在某一个状态下，选择的动作向量的均值，表示的是执行新网络的参数，N表示的是噪声，是随着时间衰退的正态函数。SDN控制器根据所述执行动作中所设定的比例，为每条流设定带宽，转化为SDN交换机可识别的指令，下发配置，SDN交换机接收配置并按所配置的带宽转发各个模态的流，如果某条流需要占用的带宽超过了所配置的带宽，则会被随机丢弃部分以满足所分配的带宽。强化学习智能体获取因执行动作后网络的新状态和奖励值,将存入经验池当中。对于一轮训练，强化学习智能体会进行T次步骤S13的过程，在这个过程中网络参数不更新，其中奖励值为强化学习智能体计算奖励函数的值。所述奖励函数定义如下其中是第p个模态的权重系数，数值由人为根据网络运行质量目标确定。是在第t个秒内第p个模态中第i个流的流速，可从全局网络特征状态中获得。是在第t个秒内第p个模态中第i个流的到达该服务器的比例，可从所述执行动作中获得。是出口区域正常运行时能够承载的流量上限。上述奖励函数的设置可以根据网络中的不同模态的通信传输情况分配合适的带宽同时避免各模态抢占网络资源而导致网络过载。在带宽资源分配方面，我们用各模态到达服务器的流数目的比例表征该模态的传输情况。如果该模态传输发生拥塞，即便其权重系数不高或整体网络暂无拥塞，该奖励函数也将推动后续动作执行时为这个模态分配更大的带宽。如果网络中的多个模态都发生了拥塞，则权重系数更高的模态会获得更大的带宽，这也符合实际需要，即优先保障更加重要的通信业务。在避免网络过载方面，我们用惩罚值-1向上一步的动作做出负反馈，减小分配的带宽以避免网络过载。因此，上述奖励函数的设置能够保障网络正常运行，同时依据网络中各模态的传输情况动态调整带宽资源分配。在步骤S14的具体实施中，根据所述经验池中所有的奖励值和执行动作前的状态，更新所述动作评价网络的网络参数；具体地，如图2所示，此步骤中可以包括以下子步骤：步骤S21：将所述经验池中所有的执行动作前的状态输入所述动作评价网络中，得到对应的期望价值；具体地，在经验池中的样本，将样本中的输入动作评价网络得到对应的期望价值，。该期望价值表示了对t时刻的网络状态的评价，即当前状态对达到奖励函数所设目标的瞬时价值。步骤S22：根据所述期望价值和对应的奖励值以及预先设定的衰减折扣，计算每个行动作前的状态的折扣奖励；具体地，计算每个的折扣奖励，，其中为衰减折扣，由人为取值。由于每一轮的训练需要经历T步，我们需要知道当前网络状态对于后续网络状态变化从而达到奖励函数所设目标的长期价值。步骤S23：计算所述折扣奖励与所述期望价值的差值，并根据所有差值计算均方差，将得到的均方差作为第一损失值，以更新所述动作评价网络的网络参数；具体地，根据样本分布计算，，计算标准差作为第一损失值用于更新动作评价网络参数。该差值表征了瞬时价值与长期价值之间的差距。该差距用于调整后续动作评价网络的参数，优化输出的执行动作。该差距越小，则表示动作网络越靠近最优。在步骤S15的具体实施中，将所述执行新网络的网络参数赋值给所述执行旧网络，并根据所述经验池中所有的动作和执行动作前的状态，更新所述执行新网络的网络参数；具体地，我们需要不断比较新旧执行网络的参数不同，并更新执行网络的参数以不断优化输出的动作，最终让执行新网络的参数达到最优，以输出最优的动作。具体地，如图3所示，“根据所述经验池中所有的动作和执行动作前的状态，更新所述执行新网络的网络参数”可以包括以下子步骤：步骤S31：将所述经验池中所有的执行动作前的状态分别输入所述执行旧网络和执行新网络，得到执行动作旧分布和执行动作新分布；具体地，将所述经验池中存储的样本中的输入执行旧网络和执行新网络，分别得到动作正态分布执行动作旧分布和执行动作新分布。执行新旧网络也都是基于相同神经网络架构构建的网络，两者架构相同，仅有参数不同。因为我们设定这个两个神经网络的输入为网络状态样本，输出为目前最优执行动作的均值和方差N；同时我们不失一般性地假设动作地概率分布为正太分布，因此，可以基于两个执行网络的输出确定得到动作的旧概率分布和新概率分布。步骤S32：计算所述经验池中每个动作在对应的所述执行动作旧分布和执行动作新分布中分别出现的第一概率和第二概率；具体地，计算存储的每个所述动作，在对应分布中的第一概率和第二概率。这两个概率分别表征了样本池所存储的动作的新旧执行网络中被选中执行的概率。步骤S33：计算所述第二概率与所述第一概率的比值；具体地，计算，。该比值表征了新旧执行网络之间的参数差异。如果新旧网络之间参数一致说明执行网络已更新至最优。因为我们希望执行网络的参数能不断更新优化，因此，计算其比值将用于更新网络参数。步骤S34：将所有的所述比值乘以对应的所述差值并求平均之后的值作为第二损失值，以更新所述执行新网络的网络参数；具体地，对于，乘以并求均值作为第二损失值用来更新执行新网络参数。表征了动作网络的更新方向，表征了评价网络的参数更新方向。因为输出执行动作的优选，需要结合网络状态的变化，因此选择两者乘积，以更新执行新网络的参数，使其学习到最新的网络状态，以在下一步输出适合网络状态的动作。在步骤S16的具体实施中，重复步骤S13-S15，直至多模态网络中各个模态占用的带宽均在保证通信传输质量的同时不让网络出口端过载；具体地，S13-S15的过程为一轮训练的过程，继续开启下一轮训练，直到各个模态合理占用带宽，在保证通信传输质量的同时不让网络出口端过载。经过充分的训练后，所述强化学习智能体已经完全学习到了不同网络环境下的最优策略，即能达到所设定的预期目标的所述执行动作。与前述的多模态网络中的强化学习智能体训练方法的实施例相对应，本申请还提供了多模态网络中的强化学习智能体训练装置的实施例。图4是根据一示例性实施例示出的一种多模态网络中的强化学习智能体训练装置的框图。参照图4，该装置应用于强化学习智能体，可以包括：构建模块21，用于构建全局网络特征状态、动作及训练所述强化学习智能体所需的深度神经网络模型，其中所述深度神经网络模型包括执行新网络、执行旧网络及动作评价网络：设置模块22，用于设置一轮训练的最大步数；执行模块23，用于在每一步中，获取全局网络特征状态，将所述全局网络特征状态输入所述执行新网络，控制SDN交换机执行所述执行新网络输出的动作，获取所述SDN交换机执行所述动作后网络的状态和奖励值，将所述动作、奖励值和执行所述动作前后的两个时间段内分别的状态存入经验池；第一更新模块24，用于根据所述经验池中所有的奖励值和执行动作前的状态，更新所述动作评价网络的网络参数；第二更新模块25，用于将所述执行新网络的网络参数赋值给所述执行旧网络，并根据所述经验池中所有的动作和执行动作前的状态，更新所述执行新网络的网络参数；重复模块26，用于重复执行模块到第二更新模块中的过程，直至多模态网络中各个模态占用的带宽均在保证通信传输质量的同时不让网络出口端过载。实施例2：图5是根据一示例性实施例示出的一种多模态网络中的模态带宽资源调度方法的流程图，如图5所示，该方法可以包括以下步骤：步骤S41：将根据实施例1所述的多模态网络中的强化学习智能体训练方法训练后的强化学习智能体应用于多模态网络中；步骤S42：根据所述强化学习智能体输出的调度策略，调度各个模态占用的资源。由上述实施例可知，本申请将训练后的强化学习智能体应用于模态带宽资源调度方法中，能自适应于不同特征的网络中，可用于多模态网络的智慧管控，具有良好的适应性及调度性能。具体地，上述多模态网络中的强化学习智能体训练方法在实施例1中已有详细描述，而将强化学习智能体应用于多模态网络和根据强化学习智能体输出的调度策略进行调度均为本领域的常规技术手段，此处不作赘述。与前述的多模态网络中的模态带宽资源调度方法的实施例相对应，本申请还提供了多模态网络中的模态带宽资源调度装置的实施例。图6是根据一示例性实施例示出的一种多模态网络中的模态带宽资源调度装置的框图。参照图6，该装置可以包括：应用模块31，用于将根据实施例1所述的多模态网络中的强化学习智能体训练方法训练后的强化学习智能体应用于多模态网络中；调度模块32，用于根据所述强化学习智能体输出的调度策略，调度各个模态占用的资源。关于上述实施例中的装置，其中各个模块执行操作的具体方式已经在有关该方法的实施例中进行了详细描述，此处将不做详细阐述说明。对于装置实施例而言，由于其基本对应于方法实施例，所以相关之处参见方法实施例的部分说明即可。以上所描述的装置实施例仅仅是示意性的，其中所述作为分离部件说明的单元可以是或者也可以不是物理上分开的，作为单元显示的部件可以是或者也可以不是物理单元，即可以位于一个地方，或者也可以分布到多个网络单元上。可以根据实际的需要选择其中的部分或者全部模块来实现本申请方案的目的。本领域普通技术人员在不付出创造性劳动的情况下，即可以理解并实施。实施例3：相应地，本申请还提供一种电子设备，包括：一个或多个处理器；存储器，用于存储一个或多个程序；当所述一个或多个程序被所述一个或多个处理器执行，使得所述一个或多个处理器实现如上述的多模态网络中的强化学习智能体训练方法或多模态网络中的模态带宽资源调度方法。如图7所示，为本发明实施例提供的一种多模态网络中的强化学习智能体训练方法或多模态网络中的模态带宽资源调度方法所在任意具备数据处理能力的设备的一种硬件结构图，除了图7所示的处理器、内存以及网络接口之外，实施例中装置所在的任意具备数据处理能力的设备通常根据该任意具备数据处理能力的设备的实际功能，还可以包括其他硬件，对此不再赘述。实施例4：相应地，本申请还提供一种计算机可读存储介质，其上存储有计算机指令，该指令被处理器执行时实现如上述的多模态网络中的强化学习智能体训练方法或多模态网络中的模态带宽资源调度方法。所述计算机可读存储介质可以是前述任一实施例所述的任意具备数据处理能力的设备的内部存储单元，例如硬盘或内存。所述计算机可读存储介质也可以是风力发电机的外部存储设备，例如所述设备上配备的插接式硬盘、智能存储卡、SD卡、闪存卡等。进一步的，所述计算机可读存储介还可以既包括任意具备数据处理能力的设备的内部存储单元也包括外部存储设备。所述计算机可读存储介质用于存储所述计算机程序以及所述任意具备数据处理能力的设备所需的其他程序和数据，还可以用于暂时地存储已经输出或者将要输出的数据。本领域技术人员在考虑说明书及实践这里公开的内容后，将容易想到本申请的其它实施方案。本申请旨在涵盖本申请的任何变型、用途或者适应性变化，这些变型、用途或者适应性变化遵循本申请的一般性原理并包括本申请未公开的本技术领域中的公知常识或惯用技术手段。应当理解的是，本申请并不局限于上面已经描述并在附图中示出的精确结构，并且可以在不脱离其范围进行各种修改和改变。
