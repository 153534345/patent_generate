标题title
边缘计算卸载方法及装置
摘要abst
本发明提供一种边缘计算卸载方法及装置，该方法应用于需求方，包括：根据当前卸载任务发起计算卸载请求；结合卸载任务量，将接收到的每个性能参数和对应的历史服务评分编码成马尔科夫决策过程中的状态空间向量；将状态空间向量输入至DDPG决策模型进行卸载决策，输出已选参与方和卸载比例；根据卸载任务量、已选参与方和卸载比例进行任务卸载；若卸载成功，计算当前卸载任务的时延，已选参与方的成功次数加一；若卸载失败，已选参与方的失败次数加一；计算并更新决策算法的平均时延和成功率，并对基于决策算法的平均时延和成功率更新已选参与方的历史服务评分进行更新。通过上述方法，降低系统延时和增加成功率。
权利要求书clms
1.一种边缘计算卸载方法，其特征在于，应用于需求方，所述需求方为进行边缘计算时发起计算卸载请求的节点，所述方法包括：根据当前卸载任务发起计算卸载请求，并通过基站将所述计算卸载请求发送给其他参与边缘计算的节点，将响应所述计算卸载请求的节点作为参与方，所述参与方由所述基站进行匿名化，所述当前卸载任务至少包括卸载任务量；接收所述参与方发送的经加噪保护后的性能参数，以及接收所述基站发送的每个所述参与方的历史服务评分；结合所述卸载任务量，将每个所述性能参数和对应的历史服务评分编码成预先建立的马尔科夫决策过程MDP中的状态空间向量；将所述状态空间向量输入至预先训练的深度确定性策略梯度DDPG决策模型进行卸载决策，输出已选参与方和卸载比例，所述深度确定性策略梯度DDPG决策模型由当前策略Actor网络μ、目标策略Actor网络μ'、当前值Critic网络Q和目标值Critic网络Q'组成；与所述已选参与方进行点对点连接，并根据所述卸载任务量、所述已选参与方和所述卸载比例进行任务卸载；若卸载成功，计算所述当前卸载任务的时延，所述已选参与方的成功次数加一；若卸载失败，所述已选参与方的失败次数加一；计算决策算法的平均时延和成功率，对所述决策算法的平均时延和成功率进行更新，并基于所述决策算法的平均时延和成功率更新所述已选参与方的历史服务评分。2.根据权利要求1所述的方法，其特征在于，所述根据所述卸载任务量、所述已选参与方和所述卸载比例进行任务卸载，包括：计算所述卸载任务量和所述卸载比例的乘积，得到第一数据量，所述第一数据量为需要在所述已选参与方上进行任务卸载的数据量；将所述第一数据量的数据传输至所述已选参与方，使所述已选参与方对所述第一数据量的数据进行任务卸载；计算所述需求方的卸载比例，并计算所述需求方的卸载比例和所述卸载任务量的乘积，得到第二数据量，所述第二数据量为需要在所述需求方上进行任务卸载的数据量；对所述第二数据量的数据进行任务卸载。3.根据权利要求2所述的方法，其特征在于，还包括：若所述卸载任务量超过所述已选参与方的可用缓冲区大小，确定卸载失败；或者，若所述已选参与方为恶意节点，确定卸载失败；或者，若未成功将所述第一数据量的数据传输至所述已选参与方，确定卸载失败；将所述当前卸载任务标记为失败，并利用R＝-10，得到所述当前卸载任务对应的奖励值。4.根据权利要求1所述的方法，其特征在于，所述计算所述当前卸载任务的时延，包括：获取传输速率、所述第一数据量、所述第二数据量、第三数据量、所述已选参与方的cpu计算频率fre、以及单位比特数据需要的时钟周期数cycle，其中，所述传输速率为将所述第一数据量的数据传输至所述已选参与方的速率，由香农公式计算得到，所述第三数据量为所述已选参与方在接收所述第一数据量的数据时，缓冲区队列中等待处理的数据量；根据所述第二数据量、所述已选参与方的cpu计算频率fre以及所述单位比特数据需要的时钟周期数cycle，得到需求方本地处理时延；根据所述第一数据量、所述已选参与方的cpu计算频率fre以及所述单位比特数据需要的时钟周期数cycle，得到所述已选参与方的处理时延；根据所述第三数据量、所述已选参与方的cpu计算频率fre以及所述单位比特数据需要的时钟周期数cycle，得到排队时延；根据所述第二数据量和所述传输速率，得到传输时延；根据所述需求方本地处理时延、所述已选参与方的处理时延、所述排队时延以及所述传输时延，计算所述当前卸载任务的时延，其中，tlocal为所述需求方本地处理时延，tpro为所述已选参与方的处理时延，ttrans为所述传输时延，tque为所述排队时延。5.根据权利要求1或4所述的方法，其特征在于，在所述计算所述当前卸载任务的时延之后，还包括：根据所述当前卸载任务的时延和所述已选参与方的历史服务评分，利用R＝-delay-eτ*，得到所述当前卸载任务对应的奖励值；其中，delay为所述当前卸载任务的时延，sc为所述已选参与方的历史服务评分，τ为超参数，是大于0的实数，用于调节所述已选参与方的历史服务评分对需求方选择参与方的影响程度。6.根据权利要求1所述的方法，其特征在于，所述计算决策算法的平均时延和成功率，对所述决策算法的平均时延和成功率进行更新，并基于所述决策算法的平均时延和成功率更新所述已选参与方的历史服务评分，包括：获取每次卸载任务的时延、所述已选参与方的成功次数和所述已选参与方的总次数；根据所述每次卸载任务的时延，计算所有卸载任务的总时延；根据所述所有卸载任务的总时延和所述已选参与方的成功次数，得到决策算法的平均时延，并对所述决策算法的平均时延进行更新；根据所述已选参与方的成功次数和所述已选参与方的总次数，得到所述决策算法的成功率，并对所述决策算法的成功率进行更新；基于所述决策算法的平均时延和成功率更新所述已选参与方的历史服务评分。7.根据权利要求1所述的方法，其特征在于，所述预先训练深度确定性策略梯度DDPG决策模型的过程，包括：获取第i步的状态si和当前策略Actor网络μ基于所述状态si得到的动作ai，所述动作ai由所述动作μ掺杂高斯噪声ni形成；执行所述动作ai，得到新状态si+1和奖励ri；将由所述状态si、所述动作ai、所述奖励ri和所述新状态si+1形成的序列存放至经验回放池；从所述经验回放池中随机采样N个序列作为训练数据，其中，N为正整数；基于采样得到的批序列，预测对应状态、动作的Q值，并基于所述Q值的偏差，计算值网络CriticQ的损失函数关于参数θQ的梯度；基于所述梯度，使用优化器Adam Optimizer更新所述当前值Critic网络的参数θQ；基于所述当前值Critic网络Q的关于动作a的梯度，计算所述当前策略Actor网络μ的策略梯度；利用优化器AdamOptimizer更新所述当前策略Actor网络μ的参数θμ；基于软拷贝方式，每隔预设时间利用所述当前策略Actor网络μ的参数θμ更新目标策略Actor网络μ'的参数θμ'，以及每隔预设时间利用所述当前值Critic网络Q的参数θQ更新目标值Critic网络Q'的参数θQ'。8.根据权利要求7所述的方法，其特征在于，还包括：将由当前状态、动作、奖励值和下一状态形成的序列存放至所述经验回放池，随机抽样预设批大小的序列训练所述深度确定性策略梯度DDPG决策模型，并利用所述决策算法的平均时延和成功率调节所述奖励值、以及优化所述深度确定性策略梯度DDPG决策模型。9.一种边缘计算卸载装置，其特征在于，应用于需求方，所述需求方为进行边缘计算时发起计算卸载请求的节点，所述方法包括：发起模块，用于根据当前卸载任务发起计算卸载请求，并通过基站将所述计算卸载请求发送给其他参与边缘计算的节点，将响应所述计算卸载请求的节点作为参与方，所述参与方由所述基站进行匿名化，所述当前卸载任务至少包括卸载任务量；接收模块，用于接收所述参与方发送的经加噪保护后的性能参数，以及接收所述基站发送的每个所述参与方的历史服务评分；编码模块，用于结合所述卸载任务量，将每个所述性能参数和对应的历史服务评分编码成预先建立的马尔科夫决策过程MDP中的状态空间向量；卸载决策模块，用于将所述状态空间向量输入至预先训练的深度确定性策略梯度DDPG决策模型进行卸载决策，输出已选参与方和卸载比例，所述深度确定性策略梯度DDPG决策模型由当前策略Actor网络μ、目标策略Actor网络μ'、当前值Critic网络Q和目标值Critic网络Q'组成；任务卸载模块，用于与所述已选参与方进行点对点连接，并根据所述卸载任务量、所述已选参与方和所述卸载比例进行任务卸载，若卸载成功，执行第一处理模块，若卸载失败，执行第二处理模块；第一处理模块，用于计算所述当前卸载任务的时延，所述已选参与方的成功次数加一；第二处理模块，用于所述已选参与方的失败次数加一；计算与更新模块，用于计算决策算法的平均时延和成功率，对所述决策算法的平均时延和成功率进行更新，并基于所述决策算法的平均时延和成功率更新所述已选参与方的历史服务评分。10.根据权利要求9所述的装置，其特征在于，所述任务卸载模块，包括：第一计算单元，用于计算所述卸载任务量和所述卸载比例的乘积，得到第一数据量，所述第一数据量为需要在所述已选参与方上进行任务卸载的数据量；传输单元，用于将所述第一数据量的数据传输至所述已选参与方，使所述已选参与方对所述第一数据量的数据进行任务卸载；第二计算单元，用于计算所述需求方的卸载比例，并计算所述需求方的卸载比例和所述卸载任务量的乘积，得到第二数据量，所述第二数据量为需要在所述需求方上进行任务卸载的数据量；任务卸载单元，用于对所述第二数据量的数据进行任务卸载。
说明书desc
技术领域本发明涉及隐私保护、计算卸载与机器学习技术领域，尤其涉及一种边缘计算卸载方法及装置。背景技术随着边缘计算技术的高速发展，许多新兴技术与程序被加载在各类便捷式移动设备上，这些便捷式移动设备要求更低的时延和能耗，同时，还需要满足一定的隐私保护标准，其中，边缘计算是物联网的核心技术之一，通过将某个终端设备中计算任务或数据迁移到资源更丰富的网络边缘服务器，从而实现增强移动设备综合性能的目的。在这种架构下，用户需要在诸多参与方设备之间进行选择和卸载，通过参与方提供的网络、硬件等参数选择最优的任务调度方案。然而，由于可信的要求，参与方没有提供部分关键参数，或者，参与方将参数通过差分隐私或k-匿名技术进行泛化加噪；同时，隐私保护也导致了恶意结点、故障结点不易被发现。由此可知，亟需一种高效的边缘计算卸载方式。发明内容有鉴于此，本发明实施例提供一种边缘计算卸载方法及装置，以实现降低系统延时和增加成功率的目的。为实现上述目的，本发明实施例提供如下技术方案：本发明实施例第一方面公开了一种边缘计算卸载方法，应用于需求方，所述需求方为进行边缘计算时发起计算卸载请求的节点，所述方法包括：根据当前卸载任务发起计算卸载请求，并通过基站将所述计算卸载请求发送给其他参与边缘计算的节点，将响应所述计算卸载请求的节点作为参与方，所述参与方由所述基站进行匿名化，所述当前卸载任务至少包括卸载任务量；接收所述参与方发送的经加噪保护后的性能参数，以及接收所述基站发送的每个所述参与方的历史服务评分；结合所述卸载任务量，将每个所述性能参数和对应的历史服务评分编码成预先建立的马尔科夫决策过程MDP中的状态空间向量；将所述状态空间向量输入至预先训练的深度确定性策略梯度DDPG决策模型进行卸载决策，输出已选参与方和卸载比例，所述深度确定性策略梯度DDPG决策模型由当前策略Actor网络μ、目标策略Actor网络μ'、当前值Critic网络Q和目标值Critic网络Q'组成；与所述已选参与方进行点对点连接，并根据所述卸载任务量、所述已选参与方和所述卸载比例进行任务卸载；若卸载成功，计算所述当前卸载任务的时延，所述已选参与方的成功次数加一；若卸载失败，所述已选参与方的失败次数加一；计算决策算法的平均时延和成功率，对所述决策算法的平均时延和成功率进行更新，并基于所述决策算法的平均时延和成功率更新所述已选参与方的历史服务评分。可选的，所述根据所述卸载任务量、所述已选参与方和所述卸载比例进行任务卸载，包括：计算所述卸载任务量和所述卸载比例的乘积，得到第一数据量，所述第一数据量为需要在所述已选参与方上进行任务卸载的数据量；将所述第一数据量的数据传输至所述已选参与方，使所述已选参与方对所述第一数据量的数据进行任务卸载；计算所述需求方的卸载比例，并计算所述需求方的卸载比例和所述卸载任务量的乘积，得到第二数据量，所述第二数据量为需要在所述需求方上进行任务卸载的数据量；对所述第二数据量的数据进行任务卸载。可选的，还包括：若所述卸载任务量超过所述已选参与方的可用缓冲区大小，确定卸载失败；或者，若所述已选参与方为恶意节点，确定卸载失败；或者，若未成功将所述第一数据量的数据传输至所述已选参与方，确定卸载失败；将所述当前卸载任务标记为失败，并利用R＝-10，得到所述当前卸载任务对应的奖励值。可选的，所述计算所述当前卸载任务的时延，包括：获取传输速率、所述第一数据量、所述第二数据量、第三数据量、所述已选参与方的cpu计算频率fre、以及单位比特数据需要的时钟周期数cycle，其中，所述传输速率为将所述第一数据量的数据传输至所述已选参与方的速率，由香农公式计算得到，所述第三数据量为所述已选参与方在接收所述第一数据量的数据时，缓冲区队列中等待处理的数据量；根据所述第二数据量、所述已选参与方的cpu计算频率fre以及所述单位比特数据需要的时钟周期数cycle，得到需求方本地处理时延；根据所述第一数据量、所述已选参与方的cpu计算频率fre以及所述单位比特数据需要的时钟周期数cycle，得到所述已选参与方的处理时延；根据所述第三数据量、所述已选参与方的cpu计算频率fre以及所述单位比特数据需要的时钟周期数cycle，得到排队时延；根据所述第二数据量和所述传输速率，得到传输时延；根据所述需求方本地处理时延、所述已选参与方的处理时延、所述排队时延以及所述传输时延，计算所述当前卸载任务的时延，其中，tlocal为所述需求方本地处理时延，tpro为所述已选参与方的处理时延，ttrans为所述传输时延，tque为所述排队时延。可选的，在所述计算所述当前卸载任务的时延之后，还包括：根据所述当前卸载任务的时延和所述已选参与方的历史服务评分，利用R＝-delay-eτ*，得到所述当前卸载任务对应的奖励值；其中，delay为所述当前卸载任务的时延，sc为所述已选参与方的历史服务评分，τ为超参数，是大于0的实数，用于调节所述已选参与方的历史服务评分对需求方选择参与方的影响程度。可选的，所述计算决策算法的平均时延和成功率，对所述决策算法的平均时延和成功率进行更新，并基于所述决策算法的平均时延和成功率更新所述已选参与方的历史服务评分，包括：获取每次卸载任务的时延、所述已选参与方的成功次数和所述已选参与方的总次数；根据所述每次卸载任务的时延，计算所有卸载任务的总时延；根据所述所有卸载任务的总时延和所述已选参与方的成功次数，得到决策算法的平均时延，并对所述决策算法的平均时延进行更新；根据所述已选参与方的成功次数和所述已选参与方的总次数，得到所述决策算法的成功率，并对所述决策算法的成功率进行更新；基于所述决策算法的平均时延和成功率更新所述已选参与方的历史服务评分。可选的，所述预先训练深度确定性策略梯度DDPG决策模型的过程，包括：获取第i步的状态si和当前策略Actor网络μ基于所述状态si得到的动作ai，所述动作ai由所述动作μ掺杂高斯噪声ni形成；执行所述动作ai，得到新状态si+1和奖励ri；将由所述状态si、所述动作ai、所述奖励ri和所述新状态si+1形成的序列存放至经验回放池；从所述经验回放池中随机采样N个序列作为训练数据，其中，N为正整数；基于采样得到的批序列，预测对应状态、动作的Q值，并基于所述Q值的偏差，计算值网络CriticQ的损失函数关于参数θQ的梯度；基于所述梯度，使用优化器Adam Optimizer更新所述当前值Critic网络的参数θQ；基于所述当前值Critic网络Q的关于动作a的梯度，计算所述当前策略Actor网络μ的策略梯度；利用优化器AdamOptimizer更新所述当前策略Actor网络μ的参数θμ；基于软拷贝方式，每隔预设时间利用所述当前策略Actor网络μ的参数θμ更新目标策略Actor网络μ'的参数θμ'，以及每隔预设时间利用所述当前值Critic网络Q的参数θQ更新目标值Critic网络Q'的参数θQ'。可选的，还包括：将由当前状态、动作、奖励值和下一状态形成的序列存放至所述经验回放池，随机抽样预设批大小的序列训练所述深度确定性策略梯度DDPG决策模型，并利用所述决策算法的平均时延和成功率调节所述奖励值、以及优化所述深度确定性策略梯度DDPG决策模型。本发明实施例第二方面公开了一种边缘计算卸载装置，应用于需求方，所述需求方为进行边缘计算时发起计算卸载请求的节点，所述方法包括：发起模块，用于根据当前卸载任务发起计算卸载请求，并通过基站将所述计算卸载请求发送给其他参与边缘计算的节点，将响应所述计算卸载请求的节点作为参与方，所述参与方由所述基站进行匿名化，所述当前卸载任务至少包括卸载任务量；接收模块，用于接收所述参与方发送的经加噪保护后的性能参数，以及接收所述基站发送的每个所述参与方的历史服务评分；编码模块，用于结合所述卸载任务量，将每个所述性能参数和对应的历史服务评分编码成预先建立的马尔科夫决策过程MDP中的状态空间向量；卸载决策模块，用于将所述状态空间向量输入至预先训练的深度确定性策略梯度DDPG决策模型进行卸载决策，输出已选参与方和卸载比例，所述深度确定性策略梯度DDPG决策模型由当前策略Actor网络μ、目标策略Actor网络μ'、当前值Critic网络Q和目标值Critic网络Q'组成；任务卸载模块，用于与所述已选参与方进行点对点连接，并根据所述卸载任务量、所述已选参与方和所述卸载比例进行任务卸载，若卸载成功，执行第一处理模块，若卸载失败，执行第二处理模块；第一处理模块，用于计算所述当前卸载任务的时延，所述已选参与方的成功次数加一；第二处理模块，用于所述已选参与方的失败次数加一；计算与更新模块，用于计算决策算法的平均时延和成功率，对所述决策算法的平均时延和成功率进行更新，并基于所述决策算法的平均时延和成功率更新所述已选参与方的历史服务评分。可选的，所述任务卸载模块，包括：第一计算单元，用于计算所述卸载任务量和所述卸载比例的乘积，得到第一数据量，所述第一数据量为需要在所述已选参与方上进行任务卸载的数据量；传输单元，用于将所述第一数据量的数据传输至所述已选参与方，使所述已选参与方对所述第一数据量的数据进行任务卸载；第二计算单元，用于计算所述需求方的卸载比例，并计算所述需求方的卸载比例和所述卸载任务量的乘积，得到第二数据量，所述第二数据量为需要在所述需求方上进行任务卸载的数据量；任务卸载单元，用于对所述第二数据量的数据进行任务卸载。基于上述本发明实施例提供的一种边缘计算卸载方法及装置，应用于需求方，所述需求方为进行边缘计算时发起计算卸载请求的节点，所述方法包括：根据当前卸载任务发起计算卸载请求，并通过基站将所述计算卸载请求发送给其他参与边缘计算的节点，将响应所述计算卸载请求的节点作为参与方，所述参与方由所述基站进行匿名化，所述当前卸载任务至少包括卸载任务量；接收所述参与方发送的经加噪保护后的性能参数，以及接收所述基站发送的每个所述参与方的历史服务评分；结合所述卸载任务量，将每个所述性能参数和对应的历史服务评分编码成预先建立的马尔科夫决策过程MDP中的状态空间向量；将所述状态空间向量输入至预先训练的深度确定性策略梯度DDPG决策模型进行卸载决策，输出已选参与方和卸载比例，所述深度确定性策略梯度DDPG决策模型由当前策略Actor网络μ、目标策略Actor网络μ'、当前值Critic网络Q和目标值Critic网络Q'组成；与所述已选参与方进行点对点连接，并根据所述卸载任务量、所述已选参与方和所述卸载比例进行任务卸载；若卸载成功，计算所述当前卸载任务的时延，所述已选参与方的成功次数加一；若卸载失败，所述已选参与方的失败次数加一；计算决策算法的平均时延和成功率，对所述决策算法的平均时延和成功率进行更新，并基于所述决策算法的平均时延和成功率更新所述已选参与方的历史服务评分。在本方案中，当边缘计算的需求方发起计算卸载请求时，通过将参与方发送的经加噪保护后的性能参数和基站发送的历史服务评分编码成状态空间向量并输入至决策模型进行卸载决策，利用输出的已选参与方和卸载比例进行任务卸载，从而降低系统延时和增加成功率。附图说明为了更清楚地说明本发明实施例或现有技术中的技术方案，下面将对实施例或现有技术描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本发明的实施例，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据提供的附图获得其他的附图。图1为本发明实施例提供的一种边缘计算卸载系统的架构示意图；图2为本发明实施例提供的一种边缘计算卸载方法的流程示意图；图3为本发明实施例提供的隐马尔可夫模型的原理图；图4为本发明实施例提供的一种深度确定性策略梯度DDPG决策模型的原理图；图5为本发明实施例提供的一种任务卸载的流程示意图；图6为本发明实施例提供的一种计算当前卸载任务的时延的流程示意图；图7为本发明实施例提供的一种计算并更新决策算法的平均时延和成功率的流程示意图；图8为本发明实施例提供的一种边缘计算卸载装置的结构示意图。具体实施方式下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例仅仅是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有做出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。在本申请中，术语“包括”、“包含”或者其任何其他变体意在涵盖非排他性的包含，从而使得包括一系列要素的过程、方法、物品或者设备不仅包括那些要素，而且还包括没有明确列出的其他要素，或者是还包括为这种过程、方法、物品或者设备所固有的要素。在没有更多限制的情况下，由语句“包括一个……”限定的要素，并不排除在包括所述要素的过程、方法、物品或者设备中还存在另外的相同要素。本申请的说明书和权利要求书及上述附图中的术语“第一”、“第二”、“第三”、“第四”等是用于区别类似的对象，而不必用于描述特定的顺序或先后次序。应该理解这样使用的数据在适当情况下可以互换，以便这里描述的实施例能够以除了在这里图示或描述的内容以外的顺序实施。由背景技术可知，现有的计算卸载方式要么由于可信的要求，参与方没有提供部分关键参数，要么参与方将参数通过差分隐私或k-匿名技术进行泛化加噪；而且，隐私保护也导致了恶意结点、故障结点不易被发现。因此，本发明实施例提供一种边缘计算卸载方法及装置，在本方案中，当边缘计算的需求方发起计算卸载请求时，通过将参与方发送的经加噪保护后的性能参数和基站发送的历史服务评分编码成状态空间向量并输入至决策模型进行卸载决策，利用输出的已选参与方和卸载比例进行任务卸载，从而降低系统延时和增加成功率。首先，如图1所示，为本发明实施例提供一种边缘计算卸载系统的架构示意图，该边缘计算卸载系统包括：需求方1、基站2和参与方3。其中，需求方1为进行边缘计算中发起计算卸载请求的节点，而参与边缘计算的其他节点都可以作为参与方3。需要说明的是，边缘计算是物联网的核心技术之一，通过将某个终端设备中计算任务或数据迁移到资源更丰富的网络边缘服务器，从而实现增强移动设备综合性能的目的。在本发明实施例中，需求方1包括但不限于个人移动设备。基站2为管理控制平台。参与方3可以为服务器，也可以为个人设备，还可以为恶意攻击者，本发明不作限定。需求方1和参与方3通过无线网络的方式与基站2实现无线通信。需求方1与参与方3进行点对点连接。需求方1用于根据当前卸载任务向基站2发起卸载请求，接收基站2和参与方3发送的实现计算卸载的信息，并根据相关计算卸载的信息进行任务卸载。基站2用于为每个参与方3提供匿名化服务，并向需求方1提供每个参与方3的历史服务评分。参与方3用于在接收到需求方1通过基站2发送的卸载请求时响应卸载请求，接收需求方1发送的实现计算卸载的信息，并根据相关计算卸载的信息进行任务卸载。基于上述公开的边缘计算卸载系统实现边缘计算卸载的过程为：当边缘计算的需求方1需要进行计算卸载时，根据当前卸载任务向基站2发起计算卸载请求，并通过基站2将计算卸载请求发送给其他参与边缘计算的节点，将响应计算卸载请求的节点作为参与方3。其中，参与方3由基站进行匿名化。计算卸载请求至少包括卸载任务量。各个参与方3通过差分隐私技术对自身的性能参数进行加噪保护，得到经加噪保护后的性能参数，并将经加噪保护后的性能参数发送至需求方1。其中，性能参数包括但不限于cpu计算频率、缓冲区大小、带宽和GPS位置。基站2在接收到多个节点响应计算卸载请求作为当前卸载任务的参与方3的信号之后，向需求方1发送每个参与方3对应的历史服务评分。需求方1接收参与方3发送的经加噪保护后的性能参数，以及接收基站2发送的每个参与方3的历史服务评分，并结合卸载任务量，将每个性能参数和对应的历史服务评分编码成预先建立的马尔科夫决策过程中的状态空间向量。需求方1将状态空间向量输入至预先训练的深度确定性策略梯度决策模型进行卸载决策，输出已选参与方和卸载比例。需要说明的是，已选参与方即为参与方3。其中，深度确定性策略梯度DDPG决策模型由当前策略Actor网络μ、目标策略Actor网络μ'、当前值Critic网络Q和目标值Critic网络Q'组成。需求方1与参与方3进行点对点连接，并根据卸载任务量、参与方3和卸载比例进行任务卸载。需要说明的是，点对点连接即为D2D直连。若卸载成功，需求方1计算当前卸载任务的时延，参与方3的成功次数加一。若卸载失败，参与方3的失败次数加一。需求方1计算决策算法的平均时延和成功率，对决策算法的平均时延和成功率进行更新，并基于决策算法的平均时延和成功率更新参与方3的历史服务评分。基于本发明实施例提供的一种边缘计算卸载系统，通过根据当前卸载任务发起计算卸载请求，并通过基站将计算卸载请求发送给其他参与边缘计算的节点，将响应计算卸载请求的节点作为参与方，参与方由基站进行匿名化，当前卸载任务至少包括卸载任务量；接收参与方发送的经加噪保护后的性能参数，以及接收基站发送的每个参与方的历史服务评分；结合卸载任务量，将每个性能参数和对应的历史服务评分编码成预先建立的马尔科夫决策过程MDP中的状态空间向量；将状态空间向量输入至预先训练的深度确定性策略梯度DDPG决策模型进行卸载决策，输出已选参与方和卸载比例，深度确定性策略梯度DDPG决策模型由当前策略Actor网络μ、目标策略Actor网络μ'、当前值Critic网络Q和目标值Critic网络Q'组成；与已选参与方进行点对点连接，并根据卸载任务量、已选参与方和卸载比例进行任务卸载；若卸载成功，计算当前卸载任务的时延，已选参与方的成功次数加一；若卸载失败，已选参与方的失败次数加一；计算决策算法的平均时延和成功率，对决策算法的平均时延和成功率进行更新，并基于决策算法的平均时延和成功率更新已选参与方的历史服务评分。在本方案中，当边缘计算的需求方发起计算卸载请求时，通过将参与方发送的经加噪保护后的性能参数和基站发送的历史服务评分编码成状态空间向量并输入至决策模型进行卸载决策，利用输出的已选参与方和卸载比例进行任务卸载，从而降低系统延时和增加成功率。基于上述本发明实施例示出的边缘计算卸载系统，如图2所示，为本发明实施例提供的一种边缘计算卸载方法的流程示意图，该边缘计算卸载方法应用于需求方，该需求方为进行边缘计算时发起计算卸载请求的节点，该需求方可以为上述边缘计算卸载系统示出的需求方。该方法主要包括以下步骤：步骤S201：根据当前卸载任务发起计算卸载请求，并通过基站将计算卸载请求发送给其他参与边缘计算的节点，将响应计算卸载请求的节点作为参与方。需要说明的是，基站可以为上述边缘计算卸载系统示出的基站。参与方可以为上述边缘计算卸载系统示出的参与方。在步骤S201中，参与方由基站进行匿名化。当前卸载任务至少包括卸载任务量。在本发明实施例中，需求方为进行边缘计算中发起计算卸载请求的节点，而参与边缘计算的其他节点都可以作为参与方。在具体实现步骤S201的过程中，当边缘计算的需求方需要进行计算卸载时，根据当前卸载任务发起计算卸载请求，并通过基站将计算卸载请求发送给其他参与边缘计算的节点，将响应计算卸载请求的节点作为参与方。可以理解的是，单一节点作为当前卸载任务的需求方向基站发起计算卸载请求，多个节点响应计算卸载请求作为当前卸载任务的参与方。步骤S202：接收参与方发送的经加噪保护后的性能参数，以及接收基站发送的每个参与方的历史服务评分。在步骤S202中，性能参数包括但不限于cpu计算频率、缓冲区大小、带宽和GPS位置。在具体实现步骤S202的过程中，在多个节点响应计算卸载请求作为当前卸载任务的参与方之后，参与方通过差分隐私技术对自身的性能参数进行加噪保护，得到经加噪保护后的性能参数，并将经加噪保护后的性能参数发送至需求方，则需求方接收参与方发送的经加噪保护后的性能参数，而且，基站在接收到多个节点响应计算卸载请求作为当前卸载任务的参与方的信号之后，向需求方发送每个参与方的历史服务评分，则需求方接收基站发送的每个参与方的历史服务评分。需要说明的是，在进行边缘计算卸载时，如果参与方将性能参数的实际数值发送给需求方，很容易暴露参与方自己的身份，因此，需要在传递这些性能参数时进行∈-差分隐私，从而保证需求方无法反推出参与方的身份。∈-差分隐私的要求如公式所示：其中，D和D'是相邻的数据集，ο是输出结果，Pr代表在事件B发生时，事件A发生的条件概率，即算法要保证当接收方接收到数据ο时，发送方可能来自D或者D'，且对应的条件概率之比小于等于e∈。差分隐私加噪算法如公式所示：发送方利用差分隐私加噪算法在发送给接收方的参数上，增加了拉普拉斯噪声具体数值是通过在拉普拉斯分布上进行采样得到，此时参与方提供的cpu计算频率、缓冲区大小、带宽等参数均符合∈-差分隐私，对于接收方而言，无法从这些参数反推出对方身份，当选中某一参与方进行卸载作业时，参与方会使用实际参数进行传输和计算。需要说明的是，由于隐私保护的要求存在，参与方在提供自身的性能参数时可以失真，间接造成了恶意节点、故障节点难以被发现，其中，恶意节点表现出时延、丢包都远大于正常值的特性；故障节点时延和丢包轻度偏离正常值，有一定概率成功发送。部分节点可能会伪造实际参数，将偏差很大的参数发送给需求方，为此，本发明引入评分机制，由基站提供每个参与方一个匿名身份，且根据参与方节点的历史服务情况提供一个公开的历史服务评分。需求方通过基站查询每个匿名身份的历史服务评分。本发明提供一种历史服务评分的模拟生成方法：构建一个映射f，该映射f反映节点i的服务性能与历史服务评分sci之间的关系如公式所示：sci＝f，，其中，参数p＝是影响用户评分的性能参数。在计算卸载中，用户的评分主要受额外时延以及传输是否成功的影响，因此，本发明将参与方节点的时延偏差值bt、故障率mr作为性能参数。将参与方节点的评分取值离散化至{0,1,...,10}，使用以下公式近似逼近f，模拟生成单个节点的历史服务评分sci：历史服务评分：偏差：故障率：其中，上式中E为期望运算符，tr是一次计算卸载中的真实时延，tt是理论计算得到的时延，failure是该节点作为参与方卸载失败的次数，而total是总次数。步骤S203：结合卸载任务量，将每个性能参数和对应的历史服务评分编码成预先建立的马尔科夫决策过程MDP中的状态空间向量。步骤S204：将状态空间向量输入至预先训练的深度确定性策略梯度DDPG决策模型进行卸载决策，输出已选参与方和卸载比例。在步骤S204中，深度确定性策略梯度DDPG决策模型由当前策略Actor网络μ、目标策略Actor网络μ'、当前值Critic网络Q和目标值Critic网络Q'组成。可选的，在一具体实施例中，决策模型可以为深度强化学习决策模型。其中，深度确定性策略梯度DDPG是深度强化学习算法中，一种被广泛认可、成熟的网络模型。下面对步骤S203和步骤S204进行解释说明。当需求方接收到各个参与方的性能参数和对应的历史服务评分时，结合卸载任务量，根据各个参与方的性能参数和对应的历史服务评分进行计算分析，选择某一参与方与卸载比例，该问题实际可以抽象成以下针对需求方的约束条件的最优规划问题：whereα,β∈andα+β＝1，s.t.rs∈tlocal∝1-rs,tpro∝rsdelay＝max，其中，用户体验质量QoE是指用户对节点质量和性能的主观感受，是最优化的目标参数。它通过平均时延ad和成功率sr进行加权计算得来。表示需求方节点在状态s下对参与方节点n的选择情况，1为选中，0为未选中。rs为卸载至参与方的数据比例。同一个卸载任务中只能选中一个参与方，且卸载比例介于0至1之间。tlocal是需求方本地处理时延，tpro是选中的参与方处理时延，ttrans是传输时延，tque是排队时延。delay是一次卸载任务的总时延。success是执行成功的次数。基于上述分析，将计算卸载中的卸载决策过程建模成隐马尔可夫模型。其中，HMM是马尔科夫决策过程MDP中的一种模型，MDP使用经典的四元组＜S,A,P,R＞来描述整个决策系统，进而形式化的使用强化学习算法进行决策。如图3所示，为本发明实施例提供的隐马尔可夫模型的原理图。在图3中，表明了隐状态bt、mr和对应的可观测状态sc之间的关联关系，即sc的状态转移概率p由隐状态bt与mr的转移概率和历史服务评分映射f决定。本发明结合上述提到的历史服务评分，构建了以下MDP模型，并设计了基于历史服务评分sci的奖励函数，具体描述如下：状态空间：S＝{f1,size1,pos1,sc1,...,fn,sizen,posn,scn}，其中，上述参数分别代表：n个参与方的cpu频率、可用缓冲区大小、当前位置与历史服务评分。当实际参与方数量大于n时，仅选择距离最近的n个节点，当小于n时补充值全为0的节点。因为状态空间是强化学习模型的输入向量，需要保证输入维度的一致性。动作空间：A＝{id,ratio}，其中，id表示选择的参与方标识符，ratio表示卸载比例。转移概率：p由状态空间中的可观测变量决定。奖励函数：其中，奖励函数是训练阶段节点每次执行完动作时，获得的即时回报。τ是超参数，用于调节选择的参与方的历史服务评分对需求方节点选择参与方节点的影响程度，它是一个正值。当需求方节点选中参与方进行传输却传输失败时，将本次卸载失败标记为failed，需求方节点获得-10的奖励值，否则卸载成功，需求方节点根据本次卸载任务的时延和参与方的历史服务评分sc，通过上式计算并获得对应的奖励值。通过上述内容，可以理解的是，需求方结合卸载任务量，将每个性能参数和对应的历史服务评分编码成预先建立的马尔科夫决策过程MDP中的状态空间向量，并将状态空间向量输入至预先训练的深度确定性策略梯度DDPG决策模型进行卸载决策，输出已选参与方和卸载比例。需要说明的是，已选参与方对应的标识符和卸载比例构成动作空间向量。由上述内容可知，输出的动作空间向量中的卸载比例为一个连续值，必须使用基于策略梯度的算法，因此，本发明设计了深度确定性策略梯度DDPG决策模型，具体而言，该模型架构如下：深度确定性策略梯度DDPG算法是无模型、异策略且基于策略Actor-值Critic网络结构。Actor-Critic网络结构中由四个独立的神经网络组成，分别为：当前策略Actor网络μ、目标策略Actor网络μ'、当前值Critic网络Q和目标值Critic网络Q'。在具体实现中，采用神经网络中损失函数的思想，当前值Critic网络Q的更新方式如下：L＝Eμ'，yi＝ri+γQ|θQ)。其中，si，ai，ri分别对应训练阶段中，智能体agent第i步的状态、动作与获得的奖励，θx是网络x的参数，γ∈是折扣因子。利用策略梯度法对当前策略Actor网络θμ进行更新：综上所述，预先训练决策模型的过程，即预先训练深度确定性策略梯度DDPG决策模型的过程，如图4本发明提供的深度确定性策略梯度DDPG决策模型的原理图所示：该训练过程主要包括以下步骤：1.获取第i步的状态si和当前策略Actor网络μ基于状态si得到的动作ai。其中，动作ai由动作μ掺杂高斯噪声ni形成。具体如下式所示：ai＝μ+ni，其中，在具体实现的过程中，需求方获取第i步的状态si，当前策略Actor网络μ根据状态si输出动作μ。为了平衡“探索”与“利用”，基于异策略的方式，在训练DDPG阶段进行探索时加入了高斯噪声ni，则当前策略Actor网络μ基于状态si得到由动作μ和高斯噪声ni形成的动作ai，并将动作ai下发至环境Environment。需要说明的是，在本发明实施例中，需求方训练一个代理进行决策。2.执行动作ai，得到新状态si+1和奖励ri。在具体实现的过程中，需求方在执行动作ai后，得到新状态si+1和奖励ri，也就是说，需求方执行动作ai，从环境Environment中获得新状态si+1和奖励ri。3.将由状态si、动作ai、奖励ri和新状态si+1形成的序列存放至经验回放池。4.从经验回放池中随机采样N个序列作为训练数据。换而言之，随机抽样经验回放池中若干个序列作为小批量样本，并将该序列作为值Critic-策略Actor网络的输入。其中，N为正整数。需要说明的是，为了减少序列间的相关性，每一轮迭代更新中选择的序列集并不按照放入时的顺序选择。5.基于采样得到的批序列，预测对应状态、动作的Q值，并基于Q值的偏差，计算值网络CriticQ的损失函数关于参数θQ的梯度。在具体实现的过程中，在小批量迭代更新中，目标策略Actor网络μ’输出动作μ'至目标值Critic网络Q'。目标值Critic网络Q'基于动作μ'和当前值Critic网络Q的更新方式计算预测的真实Q值yi，并计算Q值的偏差，目标值Critic网络Q'将真实Q值yi输入至当前值Critic网络Q，当前值Critic网络Q基于Q值的偏差，计算值网络CriticQ的损失函数关于参数θQ的梯度。换而言之，基于采样得到的批序列，预测对应状态、动作的Q值，并基于Q值的偏差，计算值网络CriticQ的损失函数关于参数θQ的梯度。其中，当前值Critic网络Q的更新方式为：L＝Eμ'，其中，yi＝ri+γQ|θQ)。6.基于所述梯度，使用优化器Adam Optimizer更新当前值Critic网络的参数θQ。在具体实现的过程中，当前值Critic网络Q基于该梯度，利用优化器AdamOptimizer更新当前值Critic网络Q的参数θQ。7.基于当前值Critic网络Q的关于动作a的梯度，计算当前策略Actor网络μ的策略梯度。在具体实现的过程中，当前策略Actor网络μ给出小批量样本对应动作a＝μ至当前值Critic网络Q，当前值Critic网络Q基于动作a＝μ，计算关于动作a的梯度，并将关于动作a的梯度输入至当前策略Actor网络μ，当前策略Actor网络μ基于关于动作a的梯度，计算当前策略Actor网络μ的策略梯度。其中，关于动作a的梯度为：当前策略Actor网络μ的策略梯度为：8.利用优化器Adam Optimizer更新当前策略Actor网络μ的参数θμ。在具体实现的过程中，当前策略Actor网络μ利用优化器Adam Optimizer更新当前策略Actor网络μ的参数θμ。其中，当前策略Actor网络μ的更新方式为：9.基于软拷贝方式，每隔预设时间利用当前策略Actor网络μ的参数θμ更新目标策略Actor网络μ'的参数θμ'，以及每隔预设时间利用当前值Critic网络Q的参数θQ更新目标值Critic网络Q'的参数θQ'。即：θQ'＝θQ+θQ'，θμ'＝θμ+θμ'。步骤S205：与已选参与方进行点对点连接，并根据卸载任务量、已选参与方和卸载比例进行任务卸载。在步骤S205中，点对点连接即为D2D直连。在具体实现步骤S205的过程中，需求方通过深度确定性策略梯度DDPG决策模型完成卸载决策后，根据输出的已选参与方和卸载比例可以确定需要进行任务卸载的参与方和需要卸载至该参与方上的卸载比例，则需求方与已选参与方进行点对点连接，并根据卸载任务量、已选参与方和卸载比例进行任务卸载。步骤S206：判断任务卸载是否成功，若是，执行步骤S207，若否，执行步骤S208。需要说明的是，当卸载任务量超过已选参与方的可用缓冲区大小时，确定卸载失败，执行步骤S208。或者，当已选参与方为恶意节点时，确定卸载失败，执行步骤S208。或者，当未成功将第一数据量的数据传输至已选参与方时，换而言之，若丢包，确定卸载失败，执行步骤S208。需要说明的是，在确定卸载失败后，将当前卸载任务标记为failed，并利用R＝-10，得到当前卸载任务对应的奖励值。步骤S207：计算当前卸载任务的时延，已选参与方的成功次数加一。在具体实现步骤S207的过程中，需求方确定卸载成功之后，计算当前卸载任务的时延，并将已选参与方的成功次数进行加一，得到已选参与方的成功次数的最新数据。步骤S208：已选参与方的失败次数加一。在具体实现步骤S208的过程中，需求方确定卸载失败之后，将已选参与方的失败次数进行加一，得到已选参与方的失败次数的最新数据。步骤S209：计算决策算法的平均时延和成功率，对决策算法的平均时延和成功率进行更新，并基于决策算法的平均时延和成功率更新已选参与方的历史服务评分。在本发明实施例中，决策算法为深度强化学习DRL算法。在具体实现步骤S209的过程中，需求方利用计算得到的当前卸载任务的时延，计算决策算法的平均时延和成功率，对决策算法的平均时延和成功率进行更新，并基于决策算法的平均时延和成功率更新已选参与方的历史服务评分。也就是说，需求方利用计算得到的当前卸载任务的时延，计算DRL算法的平均时延和成功率，对DRL算法的平均时延和成功率进行更新，并基于DRL算法的平均时延和成功率更新已选参与方的历史服务评分。可选的，在基于DRL算法的平均时延和成功率更新已选参与方的历史服务评分之后，还包括：将由当前状态、动作、奖励值和下一状态形成的序列存放至经验回放池，随机抽样预设批大小的序列训练深度确定性策略梯度DDPG决策模型，并利用决策算法的平均时延和成功率调节奖励值、以及训练优化深度确定性策略梯度DDPG决策模型。也就是说，将由当前状态、动作、奖励值和下一状态形成的序列存放至经验回放池，随机抽样预设批大小的序列训练深度确定性策略梯度DDPG决策模型，并利用DRL算法的平均时延和成功率调节奖励值、以及训练优化深度确定性策略梯度DDPG决策模型。基于本发明实施例提供的一种边缘计算卸载方法，通过根据当前卸载任务发起计算卸载请求，并通过基站将计算卸载请求发送给其他参与边缘计算的节点，将响应计算卸载请求的节点作为参与方，参与方由基站进行匿名化，当前卸载任务至少包括卸载任务量；接收参与方发送的经加噪保护后的性能参数，以及接收基站发送的每个参与方的历史服务评分；结合卸载任务量，将每个性能参数和对应的历史服务评分编码成预先建立的马尔科夫决策过程MDP中的状态空间向量；将状态空间向量输入至预先训练的深度确定性策略梯度DDPG决策模型进行卸载决策，输出已选参与方和卸载比例，深度确定性策略梯度DDPG决策模型由当前策略Actor网络μ、目标策略Actor网络μ'、当前值Critic网络Q和目标值Critic网络Q'组成；与已选参与方进行点对点连接，并根据卸载任务量、已选参与方和卸载比例进行任务卸载；若卸载成功，计算当前卸载任务的时延，已选参与方的成功次数加一；若卸载失败，已选参与方的失败次数加一；计算决策算法的平均时延和成功率，对决策算法的平均时延和成功率进行更新，并基于决策算法的平均时延和成功率更新已选参与方的历史服务评分。在本方案中，当边缘计算的需求方发起计算卸载请求时，通过将参与方发送的经加噪保护后的性能参数和基站发送的历史服务评分编码成状态空间向量并输入至决策模型进行卸载决策，利用输出的已选参与方和卸载比例进行任务卸载，从而降低系统延时和增加成功率。基于上述本发明实施例提供的一种边缘计算卸载方法，执行步骤S205根据卸载任务量、已选参与方和卸载比例进行任务卸载的过程，如图5所示，为本发明实施例提供的一种任务卸载的流程示意图，主要包括以下步骤：步骤S501：计算卸载任务量和卸载比例的乘积，得到第一数据量。在步骤S501中，第一数据量为需要在已选参与方上进行任务卸载的数据量。在具体实现步骤S501的过程中，需求方根据卸载任务量和输出的卸载比例进行乘法计算，得到第一数据量。步骤S502：将第一数据量的数据传输至已选参与方，使已选参与方对第一数据量的数据进行任务卸载。在具体实现步骤S502的过程中，需求方将得到的第一数据量对应的数据传输至已选参与方，使已选参与方利用实际参数对第一数据量的数据进行任务卸载，从而减轻需求方的任务量。步骤S503：计算需求方的卸载比例，并计算需求方的卸载比例和卸载任务量的乘积，得到第二数据量。需要说明的是，上述输出的卸载比例为已选参与方的卸载比例，其中，已选参与方的卸载比例和需求方的卸载比例的和值为1。在步骤S503中，第二数据量为需要在需求方上进行任务卸载的数据量。在具体实现步骤S503的过程中，需求方先根据输出的卸载比例计算需求方的卸载比例，再根据得到的需求方的卸载比例和卸载任务量进行乘法计算，得到第二数据量。步骤S504：对第二数据量的数据进行任务卸载。举例说明步骤S501至步骤S504，为了让每次任务卸载的平均时延不受任务量大小影响，固定每次卸载的任务数据大小均为M比特，即每次卸载的卸载任务量为M比特。假设输出决策的已选参与方的节点编号为i，卸载比例为α，则需求方根据卸载任务量和卸载比例计算第一数据量，即：第一数据量dpro＝M*α，并将第一数据量的数据传输至已选参与方i，使已选参与方i对第一数据量的数据进行任务卸载。需求方根据参与方的卸载比例计算需求方的卸载比例，即：需求方的卸载比例＝1-α，并根据需求方的卸载比例和卸载任务量计算第二数据量，即：第二数据量dlocal＝M*，最后，对第二数据量的数据进行任务卸载。可选的，执行步骤S205根据卸载任务量、已选参与方和卸载比例进行任务卸载的过程，还包括以下步骤：步骤S11：判断卸载任务量是否超过已选参与方的可用缓冲区大小，若是，执行步骤S14，若否，执行步骤S15。步骤S12：判断已选参与方是否为恶意节点，若是，执行步骤S14，若否，执行步骤S15。步骤S13：判断是否成功将第一数据量的数据传输至已选参与方，若是，执行步骤S15，若否，执行步骤S14。步骤S14：确定卸载失败。需要说明的是，在确定卸载失败之后，需求方将当前卸载任务标记为失败，并获得预设数值的奖励值。可选的，在本发明实施例中，预设数值为-10。可以理解的是，在确定卸载失败之后，需求方将当前卸载任务标记为失败，并利用R＝-10，得到当前卸载任务对应的奖励值。步骤S15：确定卸载成功。基于本发明实施例提供的一种边缘计算卸载方法，结合卸载任务量，并利用输出的已选参与方和卸载比例进行任务卸载，从而降低系统延时和增加成功率。基于上述本发明实施例提供的一种边缘计算卸载方法，在执行步骤S207计算当前卸载任务的时延的过程，如图6所示，为本发明实施例提供的一种计算当前卸载任务的时延的流程示意图，主要包括以下步骤：步骤S601：获取传输速率、第一数据量、第二数据量、第三数据量、已选参与方的cpu计算频率fre、以及单位比特数据需要的时钟周期数cycle。在步骤S601中，传输速率为将第一数据量的数据传输至已选参与方的速率，由香农公式计算得到。第三数据量为已选参与方在接收第一数据量的数据时，缓冲区队列中等待处理的数据量。在具体实现步骤S601的过程中，需求方利用输出的已选参与方和卸载比例完成任务卸载并卸载成功之后，需要计算当前卸载任务的时延，则需获取计算当前卸载任务的时延所需的数据，即获取传输速率、第三数据量、已选参与方的cpu计算频率fre、以及单位比特数据需要的时钟周期数cycle。步骤S602：根据第二数据量、已选参与方的cpu计算频率fre以及单位比特数据需要的时钟周期数cycle，得到需求方本地处理时延。在具体实现步骤S602的过程中，需求方利用进行计算，得到需求方本地处理时延。其中，dlocal为第二数据量，fre为已选参与方的cpu计算频率，cycle为单位比特数据需要的时钟周期数，M为卸载任务量，α为卸载比例。步骤S603：根据第一数据量、已选参与方的cpu计算频率fre以及单位比特数据需要的时钟周期数cycle，得到已选参与方的处理时延。在具体实现步骤S603的过程中，需求方利用进行计算，得到已选参与方的处理时延。其中，dpro为第一数据量。步骤S604：根据第三数据量、已选参与方的cpu计算频率fre以及单位比特数据需要的时钟周期数cycle，得到排队时延。在具体实现步骤S604的过程中，需求方利用进行计算，得到已选参与方的处理时延。其中，dque为第三数据量，即Mq为第三数据量。步骤S605：根据第二数据量和传输速率，得到传输时延。在具体实现步骤S605的过程中，需求方利用进行计算，得到传输时延。其中，B为通信信道带宽，S为信号功率，N为噪声功率，dis为传输距离。在本发明实施例中，计算传输速率rtrans的过程中，不同参与方除了信号功率S与传输距离dis成反比不一样之外，其余均视为相同的常量。步骤S606：根据需求方本地处理时延、已选参与方的处理时延、排队时延以及传输时延，计算当前卸载任务的时延。在具体实现步骤S606的过程中，需求方利用delay＝max进行计算，得到当前卸载任务的时延。其中，tlocal为需求方本地处理时延，tpro为已选参与方的处理时延，ttrans为传输时延，tque为排队时延。可选的，在执行步骤S207或者步骤S606计算当前卸载任务的时延的过程之后，还包括：根据当前卸载任务的时延和已选参与方的历史服务评分，利用R＝-delay-eτ*，得到当前卸载任务对应的奖励值。其中，delay为当前卸载任务的时延，sc为已选参与方的历史服务评分，τ为超参数，是大于0的实数，用于调节已选参与方的历史服务评分对需求方选择参与方的影响程度。基于本发明实施例提供的一种边缘计算卸载方法，在利用输出的已选参与方和卸载比例完成任务卸载并卸载成功之后，计算当前卸载任务的时延，从而保证决策算法的准确性，进而降低系统延时和增加成功率。基于上述本发明实施例提供的一种边缘计算卸载方法，执行步骤S209计算决策算法的平均时延和成功率，对决策算法的平均时延和成功率进行更新，并基于决策算法的平均时延和成功率更新已选参与方的历史服务评分的过程，如图7所示，为本发明实施例提供的一种计算并更新决策算法的平均时延和成功率的流程示意图，主要包括以下步骤：步骤S701：获取每次卸载任务的时延、已选参与方的成功次数和已选参与方的总次数。在具体实现步骤S701的过程中，需求方在完成任务卸载之后，需要根据当前卸载任务的情况对决策算法进行更新，则先获取每次卸载任务的时延、已选参与方的成功次数和已选参与方的总次数。步骤S702：根据每次卸载任务的时延，计算所有卸载任务的总时延。在具体实现步骤S702的过程中，由上述内容可知，一次卸载任务的时延为：delay＝max，则需求方根据每次卸载任务的时延delay，计算所有卸载任务的总时延。步骤S703：根据所有卸载任务的总时延和已选参与方的成功次数，得到决策算法的平均时延，并对决策算法的平均时延进行更新。在具体实现步骤S703的过程中，需求方利用进行计算，得到决策算法的平均时延，并对决策算法的平均时延进行更新。其中，success为已选参与方的成功次数。步骤S704：根据已选参与方的成功次数和已选参与方的总次数，得到决策算法的成功率，并对决策算法的成功率进行更新。在具体实现步骤S704的过程中，需求方利用进行计算，得到决策算法的成功率，并对决策算法的成功率进行更新。其中，total为已选参与方的总次数。步骤S705：基于决策算法的平均时延和成功率更新已选参与方的历史服务评分。在具体实现步骤S705的过程中，基于决策算法的平均时延和成功率更新已选参与方的历史服务评分，更新方式如公式、公式以及公式所示。基于本发明实施例提供的一种边缘计算卸载方法，在利用输出的已选参与方和卸载比例完成任务卸载之后，计算并更新决策算法的平均时延和成功率，从而保证决策算法的准确性，进而降低系统延时和增加成功率。与上述本发明实施例图2示出的一种边缘计算卸载方法相对应，本发明实施例还对应提供了一种边缘计算卸载装置，该边缘计算卸载装置应用于需求方，其中，需求方为进行边缘计算时发起计算卸载请求的节点，如图8所示，该边缘计算卸载装置包括：发起模块81、接收模块82、编码模块83、卸载决策模块84、任务卸载模块85、第一处理模块86、第二处理模块87和计算与更新模块88。发起模块81，用于根据当前卸载任务发起计算卸载请求，并通过基站将计算卸载请求发送给其他参与边缘计算的节点，将响应计算卸载请求的节点作为参与方。其中，参与方由基站进行匿名化，当前卸载任务至少包括卸载任务量。接收模块82，用于接收参与方发送的经加噪保护后的性能参数，以及接收基站发送的每个参与方的历史服务评分。编码模块83，用于结合卸载任务量，将每个性能参数和对应的历史服务评分编码成预先建立的马尔科夫决策过程MDP中的状态空间向量。卸载决策模块84，用于将状态空间向量输入至预先训练的深度确定性策略梯度DDPG决策模型进行卸载决策，输出已选参与方和卸载比例。其中，深度确定性策略梯度DDPG决策模型由当前策略Actor网络μ、目标策略Actor网络μ'、当前值Critic网络Q和目标值Critic网络Q'组成。任务卸载模块85，用于与已选参与方进行点对点连接，并根据卸载任务量、已选参与方和所述卸载比例进行任务卸载，若卸载成功，执行第一处理模块，若卸载失败，执行第二处理模块。第一处理模块86，用于计算当前卸载任务的时延，已选参与方的成功次数加一。第二处理模块87，用于已选参与方的失败次数加一。计算与更新模块88，用于计算决策算法的平均时延和成功率，对决策算法的平均时延和成功率进行更新，并基于决策算法的平均时延和成功率更新已选参与方的历史服务评分。需要说明的是，上述本发明实施例公开的边缘计算卸载装置中的各个模块具体的原理和执行过程，与上述本发明实施边缘计算卸载方法相同，可参见上述本发明实施例公开的边缘计算卸载方法中相应的部分，这里不再进行赘述。基于本发明实施例提供的一种边缘计算卸载装置，应用于需求方，通过根据当前卸载任务发起计算卸载请求，并通过基站将计算卸载请求发送给其他参与边缘计算的节点，将响应计算卸载请求的节点作为参与方，参与方由基站进行匿名化，当前卸载任务至少包括卸载任务量；接收参与方发送的经加噪保护后的性能参数，以及接收基站发送的每个参与方的历史服务评分；结合卸载任务量，将每个性能参数和对应的历史服务评分编码成预先建立的马尔科夫决策过程MDP中的状态空间向量；将状态空间向量输入至预先训练的深度确定性策略梯度DDPG决策模型进行卸载决策，输出已选参与方和卸载比例，深度确定性策略梯度DDPG决策模型由当前策略Actor网络μ、目标策略Actor网络μ'、当前值Critic网络Q和目标值Critic网络Q'组成；与已选参与方进行点对点连接，并根据卸载任务量、已选参与方和卸载比例进行任务卸载；若卸载成功，计算当前卸载任务的时延，已选参与方的成功次数加一；若卸载失败，已选参与方的失败次数加一；计算决策算法的平均时延和成功率，对决策算法的平均时延和成功率进行更新，并基于决策算法的平均时延和成功率更新已选参与方的历史服务评分。在本方案中，当边缘计算的需求方发起计算卸载请求时，通过将参与方发送的经加噪保护后的性能参数和基站发送的历史服务评分编码成状态空间向量并输入至决策模型进行卸载决策，利用输出的已选参与方和卸载比例进行任务卸载，从而降低系统延时和增加成功率。可选的，基于上述图8示出的边缘计算卸载装置，所述任务卸载模块85包括：第一计算单元，用于计算卸载任务量和所述卸载比例的乘积，得到第一数据量。其中，第一数据量为需要在已选参与方上进行任务卸载的数据量。传输单元，用于将第一数据量的数据传输至已选参与方，使已选参与方对第一数据量的数据进行任务卸载。第二计算单元，用于计算需求方的卸载比例，并计算需求方的卸载比例和卸载任务量的乘积，得到第二数据量。其中，第二数据量为需要在需求方上进行任务卸载的数据量。任务卸载单元，用于对第二数据量的数据进行任务卸载。可选的，基于上述图8示出的边缘计算卸载装置，所述任务卸载模块85还用于：若卸载任务量超过已选参与方的可用缓冲区大小，确定卸载失败；或者，若已选参与方为恶意节点，确定卸载失败；或者，若未成功将第一数据量的数据传输至已选参与方，确定卸载失败；将当前卸载任务标记为失败，并利用R＝-10，得到当前卸载任务对应的奖励值。基于本发明实施例提供的一种边缘计算卸载装置，结合卸载任务量，并利用输出的已选参与方和卸载比例进行任务卸载，从而降低系统延时和增加成功率。可选的，基于上述图8示出的边缘计算卸载装置，所述用于计算当前卸载任务的时延的第一处理模块86包括：获取单元，用于获取传输速率、第一数据量、第二数据量、第三数据量、已选参与方的cpu计算频率fre、以及单位比特数据需要的时钟周期数cycle。其中，传输速率为将第一数据量的数据传输至已选参与方的速率，由香农公式计算得到，第三数据量为已选参与方在接收第一数据量的数据时，缓冲区队列中等待处理的数据量。第一处理单元，用于根据第二数据量、已选参与方的cpu计算频率fre以及单位比特数据需要的时钟周期数cycle，得到需求方本地处理时延。第二处理单元，用于根据第一数据量、已选参与方的cpu计算频率fre以及单位比特数据需要的时钟周期数cycle，得到已选参与方的处理时延。第三处理单元，用于根据第三数据量、已选参与方的cpu计算频率fre以及单位比特数据需要的时钟周期数cycle，得到排队时延。第四处理单元，用于根据第二数据量和传输速率，得到传输时延。计算单元，用于根据需求方本地处理时延、已选参与方的处理时延、排队时延以及传输时延，计算当前卸载任务的时延。其中，tlocal为需求方本地处理时延，tpro为已选参与方的处理时延，ttrans为传输时延，tque为排队时延。可选的，基于上述图8示出的边缘计算卸载装置，所述第一处理模块86还包括：第五处理单元，用于根据当前卸载任务的时延和已选参与方的历史服务评分，利用R＝-delay-eτ*，得到当前卸载任务对应的奖励值。其中，delay为当前卸载任务的时延，sc为已选参与方的历史服务评分，τ为超参数，是大于0的实数，用于调节已选参与方的历史服务评分对需求方选择参与方的影响程度。基于本发明实施例提供的一种边缘计算卸载装置，在利用输出的已选参与方和卸载比例完成任务卸载并卸载成功之后，计算当前卸载任务的时延，从而保证决策算法的准确性，进而降低系统延时和增加成功率。可选的，基于上述图8示出的边缘计算卸载装置，所述计算与更新模块88具体用于：获取每次卸载任务的时延、已选参与方的成功次数和已选参与方的总次数；根据每次卸载任务的时延，计算所有卸载任务的总时延；根据所有卸载任务的总时延和已选参与方的成功次数，得到决策算法的平均时延，并对决策算法的平均时延进行更新；根据已选参与方的成功次数和已选参与方的总次数，得到决策算法的成功率，对决策算法的成功率进行更新；基于决策算法的平均时延和成功率更新已选参与方的历史服务评分。基于本发明实施例提供的一种边缘计算卸载装置，在利用输出的已选参与方和卸载比例完成任务卸载之后，计算并更新决策算法的平均时延和成功率，并基于决策算法的平均时延和成功率更新已选参与方的历史服务评分，从而保证决策算法的准确性，进而降低系统延时和增加成功率。可选的，基于上述图8示出的边缘计算卸载装置，结合图8，该边缘计算卸载装置还进一步设置了训练模块89，所述训练模块89，用于预先训练深度确定性策略梯度DDPG决策模型。所述训练模块89包括：获取单元，用于获取第i步的状态si和当前策略Actor网络μ基于状态si得到的动作ai。其中，动作ai由动作μ掺杂高斯噪声ni形成。执行单元，用于执行动作ai，得到新状态si+1和奖励ri。存放单元，用于将由状态si、动作ai、奖励ri和新状态si+1形成的序列存放至经验回放池。采样单元，用于从经验回放池中随机采样N个序列作为训练数据。其中，N为正整数。第一计算单元，用于基于采样得到的批序列，预测对应状态、动作的Q值，并基于Q值的偏差，计算值网络CriticQ的损失函数关于参数θQ的梯度。第一更新单元，用于基于梯度，使用优化器Adam Optimizer更新当前值Critic网络Q的参数θQ。第二计算单元，用于基于当前值Critic网络Q的关于动作a的梯度，计算当前策略Actor网络μ的策略梯度。第二更新单元，用于利用优化器Adam Optimizer更新当前策略Actor网络μ的参数θμ。第三更新单元，用于基于软拷贝方式，每隔预设时间利用当前策略Actor网络μ的参数θμ更新目标策略Actor网络μ'的参数θμ'，以及每隔预设时间利用当前值Critic网络Q的参数θQ更新目标值Critic网络Q'的参数θQ'。可选的，基于上述图8示出的边缘计算卸载装置，所述存放单元还用于：将由当前状态、动作、奖励值和下一状态形成的序列存放至经验回放池，随机抽样预设批大小的序列训练深度确定性策略梯度DDPG决策模型，并利用决策算法的平均时延和成功率调节奖励值、以及优化深度确定性策略梯度DDPG决策模型。基于本发明实施例提供的一种边缘计算卸载装置，通过训练决策模型，为卸载决策提供条件，提高卸载决策的效率，从而降低系统延时和增加成功率。本说明书中的各个实施例均采用递进的方式描述，各个实施例之间相同相似的部分互相参见即可，每个实施例重点说明的都是与其他实施例的不同之处。尤其，对于系统或系统实施例而言，由于其基本相似于方法实施例，所以描述得比较简单，相关之处参见方法实施例的部分说明即可。以上所描述的系统及系统实施例仅仅是示意性的，其中所述作为分离部件说明的单元可以是或者也可以不是物理上分开的，作为单元显示的部件可以是或者也可以不是物理单元，即可以位于一个地方，或者也可以分布到多个网络单元上。可以根据实际的需要选择其中的部分或者全部模块来实现本实施例方案的目的。本领域普通技术人员在不付出创造性劳动的情况下，即可以理解并实施。专业人员还可以进一步意识到，结合本文中所公开的实施例描述的各示例的单元及算法步骤，能够以电子硬件、计算机软件或者二者的结合来实现，为了清楚地说明硬件和软件的可互换性，在上述说明中已经按照功能一般性地描述了各示例的组成及步骤。这些功能究竟以硬件还是软件方式来执行，取决于技术方案的特定应用和设计约束条件。专业技术人员可以对每个特定的应用来使用不同方法来实现所描述的功能，但是这种实现不应认为超出本发明的范围。对所公开的实施例的上述说明，使本领域专业技术人员能够实现或使用本发明。对这些实施例的多种修改对本领域的专业技术人员来说将是显而易见的，本文中所定义的一般原理可以在不脱离本发明的精神或范围的情况下，在其它实施例中实现。因此，本发明将不会被限制于本文所示的这些实施例，而是要符合与本文所公开的原理和新颖特点相一致的最宽的范围。
