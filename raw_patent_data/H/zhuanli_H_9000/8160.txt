标题title
一种基于安全深度强化学习的微网时空感知能量管理方法
摘要abst
本公开属于电力系统运行与调控领域，公开一种基于安全深度强化学习的微网时空感知能量管理方法，所述方法包括以下步骤：将微网的能量管理问题转化为带约束的马尔可夫决策过程，其中，智能体为MG能量管理主体；利用一种安全深度强化学习方法对CMDP进行求解，所述安全深度强化学习方法包含两部分：1)构建结合边缘条件卷积网络和长短期记忆网络的特征提取网络以提取MG的时空运行状态中的空间和时间特征。该基于安全深度强化学习的微网时空感知能量管理方法增强对MG时空运行状态的感知、满足配电网约束、提高最优性，并取得更优越的能源管理策略成本效率、泛化能力和约束满足性能。
权利要求书clms
1.一种基于安全深度强化学习的微网时空感知能量管理方法，其特征在于，所述方法包括以下步骤：将微网的能量管理问题转化为带约束的马尔可夫决策过程，其中，智能体为MG能量管理主体；利用一种安全深度强化学习方法对CMDP进行求解，所述安全深度强化学习方法包含两部分：构建结合边缘条件卷积网络和长短期记忆网络的特征提取网络以提取MG的时空运行状态中的空间和时间特征；利用内点策略优化算法赋予智能体策略价值与安全性同时学习能力。2.根据权利要求1所述的一种基于安全深度强化学习的微网时空感知能量管理方法，其特征在于，所述马尔可夫决策过程包含：状态S、动作A、奖励r:S×A→R、违反约束c:S×A→RU、状态迁移函数T/:S×A×W→S和服从条件概率函数P:S×A×W×S→S，其中ω∈W表示环境中的随机性；随机策略π决定在某个状态下选择某个动作，智能体使用策略π与CMDP交互，形成状态、动作、奖励和代价的轨迹:τ＝；智能体构造一个策略，使累积折现收益最大化，策略π限制在相关可行集Πc＝{π:JCu≤ξu}中，其中T是能量管理范围的长度，γ∈是贴现因子，JCu表示策略π对辅助成本的期望折现收益Cu:CMDP可表述为如下带约束的优化：。3.根据权利要求2所述的一种基于安全深度强化学习的微网时空感知能量管理方法，其特征在于，所述状态S：t时刻的状态st反映了对MG运行状态的时空感知，设Zt表示第t步感知到的信息，定义为:其中，表示t时刻电网有功功率买卖价格，表示t时刻电网无功功率买卖价格，表示t时刻室内及室外温度，和Vn,t分别代表t时刻可再生能源发电机有功输出、有功和无功功率需求、蓄电池能量、节点电压幅值等节点特征，Sl,t代表线路的视在功率。Zt的特征分为内生特征和外生特征，所述内生特征包括RES生成和非灵活性需求所述RES生成和非灵活性需求具有内在的不确定性和可变性，不依赖于能源管理行为；所述外生特征包括特征Sl,t，作为执行的能量管理动作的反馈信号；在状态向量st中使用由过去W步组成的Zt移动窗口，推断未来趋势：st＝ 。4.根据权利要求2所述的一种基于安全深度强化学习的微网时空感知能量管理方法，其特征在于，所述动作A：第t步对环境执行的动作包括可调度发电设备、暖通空调系统、储能系统可控装置的能量管理动作，以及MG与主网之间的功率交换:其中动作调节可调度发电设备有功功率和无功功率输出的大小，动作调节HVAC系统有功功率需求的大小；动作调节储能系统的充电或放电功率的大小；动作决定了MG与主电网之间有功和无功输入或输出的大小；动作提供了光伏和风电的削减；所述策略πi可以近似为高斯分布N,σ2)，其中μ和σ2为上述作用的均值和标准差；第t步到第t+1步的状态迁移过程由st+1＝T决定，其概率函数为P，受环境当前状态st、智能体动作at和环境随机性wt的综合影响；暖通空调系统的管理方式为:其中，Ptac表示t时刻暖通空调系统的功率需求，Cac表示暖通空调系统的热容量，ηac表示暖通空调系统的效率，Rac表示暖通空调系统的热阻。在推导储能系统的充放电功率时，考虑储能系统能量的最大和最小能量限制，其管理方式为:其中，+/-＝max/min{.,0}，Ptesc，Ptesd表示t时刻蓄电池的充放电功率，表示t时刻蓄电池动作，表示t时刻蓄电池的充放电功率，Ees分别表示蓄电池的最大和最小能量限制，ηesc,ηesd表示蓄电池的充放电效率。最后根据定义计算得出机组有功、无功功率Ptdg、以及机组与主网之间有功、无功功率交换Ptgd、5.根据权利要求2所述的一种基于安全深度强化学习的微网时空感知能量管理方法，其特征在于，所述约束：规定的能量管理行为的优化需要服从如下网络约束，记为B。上式分别代表了节点n与其他节点之间的有功/无功功率交换、节点有功/无功功率平衡、对线路容量和节点电压幅值的操作约束。其中，分别代表节点n输出的所有有功和无功功率，Vn,t表示t时刻节点n的电压幅值，Bn,m,Gn,m分别表示节点n和m间线路的电纳和电导，δn,m,t表示t时刻节点n和m的相角，表示t时刻分布式发电机组的有功和无功发电量，Pl,t,Ql,t分别表示t时刻线路l上的有功、无功、视在功率。约束通常在目标中通过惩罚因子κ表示为惩罚项:目标使惩罚项最小化，同时使收益J最大化，为了达到这一目的，需要对惩罚因子κ进行适当的选择，使二者达到最佳的平衡。其中，代表实施策略π时关于辅助成本Cu的期望折现收益。6.根据权利要求2所述的一种基于安全深度强化学习的微网时空感知能量管理方法，其特征在于，所述奖励定义为MG的负总运营成本，包括与主电网的净采购成本、可调度发电机组的总生产成本和可再生能源削减的总成本:其中，rt表示奖励函数，Ptdg/表示分布式发电机组在t时刻的有功/无功发电功率,cdg,p/cdg,q表示分布式发电机组的有功/无功发电成本，cres,cu表示可再生能源削减成本，Ptres,cu表示t时刻可再生能源削减功率。7.根据权利要求1所述的一种基于安全深度强化学习的微网时空感知能量管理方法，其特征在于，所述构建结合边缘条件卷积网络网络和长短期记忆网络的特征提取网络步骤包括：MG在时间步t处的空间特征Zt构成ECC层的输入，隐空间特征提取为同时间步t的Xt；LSTM神经元将隐空间特征的前w步Xt-w-1:t作为输入，提取它们之间的时间依赖关系，对其未来趋势形成精确感知，记为Yt；Yt取代原有的状态向量st，作为智能体策略网络的输入。8.根据权利要求1所述的一种基于安全深度强化学习的微网时空感知能量管理方法，其特征在于，所述内点策略优化算法利用对数屏障函数来控制安全约束的满足；IPO的目标函数由两个部分组成:PPOLPPO的剪切智能体目标和对数势垒函数φ:rt＝πθ/πθold其中,LPPO表示剪切智能体目标、表示对数势垒函数，clip是clip函数，rt夹在之间；Ar、δr和分别表示评价智能体策略质量的优势函数、时间差误差和状态值函数；而AC、δC和表示评价智能体策略安全性的同一组函数；和分别通过构造ψ和ζ参数化的两个临界网络来估计。9.一种设备，其特征在于，所述设备包括：一个或多个处理器；存储器，用于存储一个或多个程序，当所述一个或多个程序被所述一个或多个处理器执行时，使得所述一个或多个处理器执行如权利要求1-8中任意一项所述的基于安全深度强化学习的微网时空感知能量管理方法。10.一种存储有计算机程序的计算机可读存储介质，其特征在于，该程序被处理器执行时实现如权利要求1-8中任意一项所述的基于安全深度强化学习的微网时空感知能量管理方法。
说明书desc
技术领域本公开属于电力系统运行与调控领域，具体涉及一种基于安全深度强化学习的微网时空感知能量管理方法。背景技术随着新兴电力系统发展，包括各种类型的灵活负荷、分布式发电机和储能单元在内的大量小规模分布式能源资源被集成到配电网中。因此设计考虑DER运行相关复杂特性和多源、时空不确定性以及它们对配电网约束的服从性的微网能源管理方法具有必要性。现有的针对MG能量管理问题的方法主要分为基于模型和无模型的优化方法。然而对于前者，实践中显式和精确的系统建模往往很难。后者中强化学习构成了一种无模型控制算法，智能体可以在不具备先验知识的情况下，根据与环境的反复交互获得的经验逐步学习最优控制策略。但其仍存在以下两个待解决的问题：首先，有效的能源管理策略需要对MG的时空运行特征的准确感知；其次为了保障配电网的正常运行，配电网的能源管理决策必须服从网络约束。然而，在智能体的学习过程中考虑复杂的配电网约束是一个巨大的挑战，传统的试错型RL/DRL方法是基于马尔可夫决策过程，它通常被形式化为一个无约束优化问题，为了追求约束满足，在环境中集成了一个行为矫正机制，该机制将不安全行为从当前策略投射到属于可行行为空间的最近的行为。然而，这种修正过程背后的基本原理对智能体是隐藏的，因此没有嵌入到智能体的策略改进过程中。另一种常用的方法是将约束违反表述为一个附加在奖励函数上的惩罚项。然而，这种方法需要一个繁琐的过程来调优相关的惩罚因素，当约束的数量很大时，这个过程就变得更艰难，因此，提出了一种基于安全深度强化学习的MG能量管理策略优化方法。发明内容针对现有技术的不足，本公开的目的在于提供一种基于安全深度强化学习的微网时空感知能量管理方法，该方法增强对MG时空运行状态的感知、满足配电网约束、提高最优性，并取得更优越的能源管理策略成本效率、泛化能力和约束满足性能。本公开的目的可以通过以下技术方案实现：一种基于安全深度强化学习的微网时空感知能量管理方法，所述方法包括以下步骤：首先将微网的能量管理问题转化为带约束的马尔可夫决策过程，其中，智能体为MG能量管理主体；其次利用一种安全深度强化学习方法对CMDP进行求解，该方法包含两部分：1)构建结合边缘条件卷积网络和长短期记忆网络的特征提取网络以提取MG的时空运行状态中的空间和时间特征；2)利用内点策略优化算法赋予智能体策略价值与安全性同时学习能力。优选的，所述马尔可夫决策过程包含：状态S、动作A、奖励r:S×A→R、违反约束c:S×A→RU、状态迁移函数T/:S×A×W→S和服从条件概率函数P:S×A×W×S→S，其中ω∈W表示环境中的随机性；随机策略π决定在某个状态下选择某个动作，智能体使用策略π与CMDP交互，形成状态、动作、奖励和代价的轨迹:τ＝；智能体构造一个策略，使累积折现收益最大化，策略π限制在相关可行集Πc＝{π:JCu≤ξu}中，其中T是能量管理范围的长度，γ∈是贴现因子，JCu表示策略π对辅助成本的期望折现收益Cu:CMDP可表述为如下带约束的优化：优选的，所述状态S：t时刻的状态st反映了对MG运行状态的时空感知，设Zt表示第t步感知到的信息，定义为:除价格信号和温度，Zt中还包括节点特征和Vn,t以及边缘特征Sl,t；Zt的特征分为内生特征和外生特征，所述内生特征包括RES生成和非灵活性需求所述RES生成和非灵活性需求具有内在的不确定性和可变性，不依赖于能源管理行为；所述外生特征包括特征Sl,t，作为执行的能量管理动作的反馈信号；在状态向量st中使用由过去w步组成的Zt移动窗口，推断未来趋势：st＝优选的，所述动作A：第t步对环境执行的动作包括可调度发电设备、暖通空调系统、储能系统可控装置的能量管理动作，以及MG与主网之间的功率交换:其中动作调节可调度发电设备有功功率和无功功率输出的大小，动作调节HVAC系统有功功率需求的大小；动作调节储能系统的充电或放电功率的大小；动作决定了MG与主电网之间有功和无功输入或输出的大小；动作提供了光伏和风电的削减；所述策略πi可以近似为高斯分布N,σ2)，其中μ和σ2为上述作用的均值和标准差；第t步到第t+1步的状态迁移过程由st+1＝T决定，其概率函数为P，受环境当前状态st、智能体动作at和环境随机性wt的综合影响；暖通空调系统的管理方式为:在推导储能系统的充放电功率时，考虑储能系统能量的最大和最小能量限制，其管理方式为:其中+/-＝max/min{.,0}。最后根据定义计算得出机组有功、无功功率以及机组与主网之间有功、无功功率交换优选的，所述约束：规定的能量管理行为的优化需要服从如下网络约束，记为B。约束通常在目标中通过惩罚因子κ表示为惩罚项:目标使惩罚项最小化，同时使收益J最大化。优选的，所述奖励定义为MG的负总运营成本，包括与主电网的净采购成本、可调度发电机组的总生产成本和可再生能源削减的总成本:优选的，所述构建结合边缘条件卷积网络网络和长短期记忆网络的特征提取网络步骤包括：MG在时间步t处的空间特征Zt构成ECC层的输入，隐空间特征提取为同时间步t的Xt；LSTM神经元将隐空间特征的前w步Xt-w-1:t作为输入，提取它们之间的时间依赖关系，对其未来趋势形成精确感知，记为Yt；Yt取代原有的状态向量st，作为智能体策略网络的输入。优选的，所述内点策略优化算法利用对数屏障函数来控制安全约束的满足；IPO的目标函数由两个部分组成:PPOLPPO的剪切智能体目标和对数势垒函数φ:rt＝πθ/πθold其中,clip是clip函数，rt夹在之间；Ar、δr和分别表示评价智能体策略质量的优势函数、时间差误差和状态值函数；而AC、δC和表示评价智能体策略安全性的同一组函数；和分别通过构造ψ和ζ参数化的两个临界网络来估计。根据本发明的又一方面提出了一种设备，所述设备包括：一个或多个处理器；存储器，用于存储一个或多个程序，当所述一个或多个程序被所述一个或多个处理器执行时，使得所述一个或多个处理器执行如上述中任意一项所述的基于安全深度强化学习的微网时空感知能量管理方法。根据本发明的又一方面提出了一种存储有计算机程序的计算机可读存储介质，该程序被处理器执行时实现如上述中任意一项所述的基于安全深度强化学习的微网时空感知能量管理方法。本公开的有益效果：该基于安全深度强化学习的微网时空感知能量管理方法，将微网的能量管理问题转化为带约束的马尔科夫决策过程形式，考虑了外生因素的随机性，如可再生能源发电和需求的可变性。利用ECC和LSTM网络的优势，构建提取微网运行状态的时空相关特征的特征提取网络，增强了控制策略的泛化能力采用最先进的IPO方法求解，增强了对微网运行状态的时空感知，促进了在多维、连续状态和动作空间中的学习。在改进能源管理策略质量的同时，满足配电网络相关约束。附图说明为了更清楚地说明本公开实施例或现有技术中的技术方案，下面将对实施例或现有技术描述中所需要使用的附图作简单地介绍，显而易见地，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。图1为本发明的ECC-LSTM特征提取网络结构图；图2为本发明的PS-PDDPG算法的集中式训练及分布式执行框架；图3为本发明的PS-PDDPG算法的神经网络示意图；图4为本发明的52个测试日内，MG在无约束情况和有约束情况下的能量管理情况的平均值；图5为本发明的52个测试日内室内外平均温度；图6为本发明所述方法与现有技术在热限度平均越限值方面的对比；图7为本发明所述方法与现有技术在电压幅值平均越限值方面的的对比；图8为本发明所述方法与现有技术在微网平均成本方面的对比；图9为本发明所述方法步骤关系图。具体实施方式下面将结合本公开实施例中的附图，对本公开实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例仅仅是本公开一部分实施例，而不是全部的实施例。基于本公开中的实施例，本领域普通技术人员在没有作出创造性劳动前提下所获得的所有其它实施例，都属于本公开保护的范围。1.将微网的能量管理问题转化为带约束的马尔可夫决策过程，其中包含状态空间S；动作空间A；奖励r:S×A→R；违反约束c:S×A→RU；状态迁移函数T/:S×A×W→S，服从条件概率函数P:S×A×W×S→S，其中ω∈W表示环境中的随机性。在某个状态下选择哪个动作由随机策略π决定。智能体使用策略π与CMDP交互，并形成状态、动作、奖励和代价的轨迹:τ＝。智能体的目标是构造一个策略，使累积折现收益最大化，同时将其策略π限制在相关可行集Πc＝{π:JCu≤ξu}中，其中T是能量管理范围的长度，γ∈是贴现因子，JCu表示策略π对辅助成本的期望折现收益Cu:CMDP可表述为如下带约束的优化：状态在被检验问题中，t时刻的状态st反映了对MG运行状态的时空感知，对策略学习/优化过程具有重要的指导作用。设Zt表示第t步感知到的信息，定义为:除了价格信号和温度，Zt中包含的信息还包括节点特征和Vn,t以及边缘特征Sl,t。此外，Zt的特征可分为内生特征和外生特征，内生特征包括RES生成和非灵活性需求等特征，这些特征具有内在的不确定性和可变性，不依赖于能源管理行为；后者包括特征Sl,t，作为执行的能量管理动作的反馈信号。Zt包含当前时刻t观测到的空间特征，并不能反映其未来的动态趋势。然而，后者对于制定有效的能源管理决策是至关重要的。如果智能体感知到未来负荷将急剧增加，如某些配电线路的功率流量增加，则可提前对可调度发电设备和储能系统的管理决策进行相应的调整。因此，在状态向量st中使用由过去w步组成的Zt移动窗口，以推断其未来的趋势：st＝动作和状态迁移第t步对环境执行的动作包括可调度发电设备、暖通空调系统、储能系统等可控装置的能量管理动作，以及MG与主网之间的功率交换:其中动作调节可调度发电设备有功功率和无功功率输出的大小，动作调节HVAC系统有功功率需求的大小；动作调节储能系统的充电或放电功率的大小；动作决定了MG与主电网之间有功和无功输入或输出的大小；动作提供了光伏和风电的削减。上述动作的设计满足相关的功率限制。根据上述作用定义，策略πi可以近似为高斯分布N,σ2)，其中μ和σ2为上述作用的均值和标准差。第t步到第t+1步的状态迁移过程由st+1＝T决定，其概率函数为P，受环境当前状态st、智能体动作at和环境随机性wt的综合影响。暖通空调系统的管理方式为:同样，在推导储能系统的充放电功率时，应考虑储能系统能量的最大和最小能量限制，其管理方式为:其中+/-＝max/min{.,0}。最后机组有功、无功功率以及机组与主网之间有功、无功功率交换Ptgd、可以根据定义自动计算得出。ACOPF相关的安全约束规定的能量管理行为的优化需要服从如下网络约束，记为B。节点电压幅值和相角，以及配电网的功率流向都受到所有可控分布式资源的能量管理决策影响。一旦确定了有功功率和无功功率量就可以在配电网中模拟潮流来评估所有网络约束的状态。为了考虑传统马尔科夫决策过程框架中的安全约束，约束通常在目标中通过惩罚因子κ表示为惩罚项:目标是使惩罚项最小化，同时使收益J最大化。为了达到这一目的，需要对惩罚因子κ进行适当的选择，使二者达到最佳的平衡。若ω的值很小，不能充分惩罚违反约束的行为，而κ的值很大，则会导致对违反约束的惩罚过大，导致能量管理行为的有效性降低。奖励该奖励定义为MG的负总运营成本，包括与主电网的净采购成本、可调度发电机组的总生产成本和可再生能源削减的总成本:2.构建结合边缘条件卷积网络网络和长短期记忆网络的特征提取网络，其结构如附图1所示。首先，MG在时间步t处的空间特征Zt构成ECC层的输入，隐空间特征提取为同时间步t的Xt。然后，LSTM神经元将隐空间特征的前w步Xt-w-1:t作为输入，提取它们之间的时间依赖关系，从而对其未来趋势形成精确感知，记为Yt。它取代了原有的状态向量st，作为智能体策略网络的输入，增强了时空感知能力。ECC网络和LSTM网络的工作原理：空间特征提取:电网具有典型的图结构网络，其中总线和分别被视为节点和边。根据现实世界的空间依赖性来感知和解释原始输电网运行特征比较困难。尽管卷积神经网络在提取由二维图像所代表的欧氏空间中的空间关系方面有优势，但在处理电网的拓扑结构和物理属性时，它们本质上是无效的。为此，利用图卷积网络将卷积算子扩展到非欧氏数据。进一步，ECC网络构成了原GCN的改进版本，它集成了三个主要属性:邻接矩阵、节点特征和边缘特征。设A表示节点的邻接矩阵，其中元素1和0分别表示连线连接和断开状态。具有自连接的邻接矩阵被表示为而度矩阵是一个对角矩阵，每个元素都是设FV和FE分别表示节点特征矩阵和边特征矩阵。在输入层，封装节点特征；描述边缘特征。从数学上讲，节点i上的ECC操作本质上是将每个边标签FE与动态滤波权值F相加:其中b为可训练偏差，Θij为动态边缘参数集。时间特征提取:LSTM网络在从时间序列数据中提取时间依赖性特征方面是非常有效的，在标准RNN单元的基础上，对LSTM单元的结构进行了改进，增加了遗忘门、更新门和输出门，最大限度地减小了梯度消失/爆炸的可能性。其原理公式如下：αt＝μt⊙αt-1+λt⊙αtht＝βt⊙tanhYt＝ht其中W和B是LSTM单元每个部分的权重和偏差向量。Xt,ht，αt是时间步t的输入，输出，内部状态；λ、μ和β分别为输入门、遗忘门和输出门；σ和tanh是激活函数。上式中将LSTM神经元的输出定义为第t步的时空特征Yt。3.内点策略优化算法进行问题求解。其利用对数屏障函数来控制安全约束的满足。根据TO缓解问题的设置，一个理想的屏障应该具有两个性质:1)当满足安全约束时，障碍函数的值应为零；2)当存在任何违反约束的情况时，应在原目标函数上引入一个较大的负值，但无需穷尽调优惩罚因子的值。IPO的策略更新机制继承了近端策略优化，从而保留了信任区域的属性。与二阶算法TRPO相比，PPO和IPO的导数计算是一阶算法，容易实现。IPO的目标函数由两个部分组成:PPOLPPO的剪切智能体目标和对数势垒函数φ:rt＝πθ/πθold其中,clip是clip函数，rt夹在之间；Ar、δr和分别表示评价智能体策略质量的优势函数、时间差误差和状态值函数；而AC、δC和表示评价智能体策略安全性的同一组函数；和分别通过构造ψ和ζ参数化的两个临界网络来估计。势垒函数φ构成下式中定义的理想势垒函数I的近似值。随着q值的增加，φ变得更接近于I。此外，对数势垒函数具有一阶可微在原点处不可微)的优点，与IPO的策略更新机制完全一致。在策略改进方面，IPO继承了PPO的策略梯度方法，保留了TRPO的单调性，又保留了PPO的计算效率。对于具有高维复杂状态和动作空间的MG能量管理问题，这两个性质都是理想的。此外，只有IPO方法能够在提高策略质量的同时学习约束满足，这是问题的基本要求。在训练过程中，MG的能量管理智能体使用其当前策略，与环境相互作用T时间步长，收集轨迹τ＝表示T时间步长。对于每个完整的轨迹τ，智能体分别基于批评家和的输出评估优势函数和通过上文中的目标函数的最大化来训练，通过分别使均方TD误差和的最小值来训练TD学习。在基于IEEE 15节点测试系统修正的微网上进行了案例研究，其结构如附图2所示。其中，包含1台可调度发电机组,2台光伏,2台风电机组,2台储能系统，8个刚性需求，包括工业、居民、商业需求。在此基础上，总共60个暖通空调系统随机分布到这些需求节点上。MG可通过节点N0向主网输入/输出电力。热极限设置为1.3MVA，所有节点电压的幅值在0.9p.u和1.1p.u之间。住宅需求、光伏和风力发电的时间序列数据收集自澳大利亚配电公司Ausgrid记录的真实数据集。相关室外温度数据来自澳大利亚政府的开放数据库。假设与无功功率量相关的成本和价格等于与有功功率量相关值的10％。考虑到需要探索所提方法的泛化能力，将一年的数据集分为训练集和测试集。随机选择52周中的每一天来构建测试集，剩下的313天形成训练集。为了验证所提方法的MG能源管理策略的成本效率和约束满意度，将所提方法与三种基于PPO的方法进行了对比：PPO:原始的PPO方法，在该方法下，智能体学习忽略了所有安全约束的能源管理策略；PPO-rp:PPO虽然可以求解复杂的MDPs，但不能直接用于求解CMDPs。在奖励函数rt中惩罚违反约束的行为，然后将其重新定义为PPO-ar:PPO采用了行为纠正机制，如果安全约束被违反，环境将通过解决下式优化问题修改相应的能量管理行为:s.t.PQs∈B此外，将IPO与理论最优控制器进行了比较，后者将问题形式化为一个混合整数线性规划，以最小化每日MG成本为目标，假设充分了解MG和DERs的模型和参数，并对不确定参数进行完美预测。为了评估所提出的方法和基线方法的平均性能以及相关的可变性，生成了10个不同的随机种子，每种方法为每个种子训练5000集，其中每集代表从训练集中选择的随机一天。在训练期间，在测试集上定期评估每种方法的性能。如附图6和7分别说明了所有被检查方法在52天的测试中对热极限和节点电压幅值的平均约束违反情况，分别通过实线和阴影区域说明了10个种子的平均约束违例的平均值和标准差。类似地，如附图8显示了所有方法在52天测试中的平均MG成本。从附图6和7中可以得出以下观察结果：在IPO条件下，观察到两种约束违例在1000个事件内以非常快的速度降为零，证明了对数障碍函数在帮助智能体迅速学习约束满足方面的有效性；在PPO条件下，训练过程中不考虑这两个约束条件，收敛处视潮流和节点电压均有明显违反；在PPO-rp下，惩罚方法只能在一定程度上减少约束违例，在收敛时观察到相对较大的约束违例；PPO-ar对应于零约束违例，因为它保证了训练过程中网络约束的满足。如图8所示，在完全忽略线路流和节点电压限制的情况下，PPO获得了最低的平均MG成本，PPO-rp对应的是第二低的成本和第二高的大约束违规行为，表明智能体是为了更高的经济效益而交易约束违规行为。对于IPO和ppar，虽然两种方法都保证了收敛时的约束满足，但IPO下的平均MG成本显著低于PPO-ar下的成本。证明所提出的IPO方法将安全学习机制系统地嵌入到智能体的保单改进机制中，促进安全性和质量的同步改进。在IPO和所有基线方法下测试52天的累计成本如附图3所示。可以观察到，IPO接近理论最优，表明对不可见数据集具有良好的泛化能力。在PPO下，MG运营成本被显著低估，这是完全忽视网络约束的结果。虽然PPO-ar保证了网络约束的满足，但行为修正机制背后的原理并没有被agent学习到，因此在agent的策略更新/改进过程中没有被考虑在内，这比MILP成本高14.48％。在PPO-rp下，收敛时违反约束仍然显著，因此累积成本低于IPO和MILP下获得的成本，因为智能体学会了在一些违反网络约束的情况下交易更低的成本。附图4给出了在无约束和有约束情况下，52天测试中ES的充电和放电功率、DG的发电输出、HVAC的需求输入以及MG与主电网之间的功率输入和输出平均值。室内外平均温度如附图5所示。可以观察到共同点：ES系统在非高峰时段22:00-5:00调度，以利用低，非高峰电价；通过向ES充电并提供ID和HVAC需求，光伏和风力发电被有效利用/吸收，而MG只在电价最低期间从电网进口；在16:00-20:00时段以高ID和无/低PV生产为特征的ES排放；HVAC系统主要在中午7:00-16:00时段运行，以确保室外温度较高时室内温度在舒适范围。然而，这两种方法也表现出明显的差异：在无约束的情况下，MG可以在9:00-19:00将剩余电力输出到主电网，以赚取额外收入，这是通过忽略所有电压和热限制实现的；而在有约束的情况下，由于考虑了电压和热流的限制，输出性能显著降低。基于同样的原因，在受限情况下，16:00-20:00时段ES放电较高，电网输入较低，8:00-16:00时段PV缩减较低。在本说明书的描述中，参考术语“一个实施例”、“示例”、“具体示例”等的描述意指结合该实施例或示例描述的具体特征、结构、材料或者特点包含于本公开的至少一个实施例或示例中。在本说明书中，对上述术语的示意性表述不一定指的是相同的实施例或示例。而且，描述的具体特征、结构、材料或者特点可以在任何的一个或多个实施例或示例中以合适的方式结合。以上显示和描述了本公开的基本原理、主要特征和本公开的优点。本行业的技术人员应该了解，本公开不受上述实施例的限制，上述实施例和说明书中描述的只是说明本公开的原理，在不脱离本公开精神和范围的前提下，本公开还会有各种变化和改进，这些变化和改进都落入要求保护的本公开范围内容。
