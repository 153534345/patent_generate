标题title
一种基于在线训练的超分辨率直播系统
摘要abst
本发明属于直播流媒体技术领域，具体为一种基于在线训练的超分辨率直播系统，适用于配备GPU计算能力的PC、移动终端和智能汽车的视频直播应用。本发明系统包括：在推流端上行带宽不足的情况下，直播者在推流端以RTMP协议向服务器上传低分辨率视频流，同时利用原始高清视频自适应训练空域超分辨率网络F‑EDSR并上传到远端服务器；服务器端接收低分辨率视频流和F‑EDSR网络，并用空域超分辨网络F‑EDSR和时域超分辨率网络TCSR级联实时处理视频流，合成高清超分辨率视频。结果表明，本发明可以在推流端上传带宽不足的情况下，实时对视频流做超分辨率重构，用较少的计算资源获得高质量的视频画面。
权利要求书clms
1.一种基于在线训练的超分辨率直播系统，其特征在于，包括推流端和服务器端两个部分；其中：服务器端包括空域超分F-EDSR和TCSR两个网络，并进行级联；前者用于对视频实时处理，后者用于对F-EDSR的输出进行帧挑选处理；F-EDSR空域超分F-EDSR作为一级网络，是基于图像超分网络EDSR；网络头部使用一个二维卷积层，具有更大的5*5卷积核，以及更少的通道；网络主体由深度残差模块构成，并以二维卷积层结束；每个残差模块由两个卷积层和一个激活层构成；残差模块去掉批量归一化层使得网络主体参数更少，感受动态更大；网络尾部是上采样模块和卷积模块，使得图像能以设定尺寸输出；TCSRTCSR网络作为二级网络，以多个连续的帧为作为输入，中间帧作为融合输出的目标；包括一个OFRnet运算模块、仿射变换、融合模块、重构模块；多个连续帧表示为It-k,…It-1,It,It+1,…,It+k，中间帧为It，其中k表示中间帧和相邻帧的最大间隔数；使用的视频帧输入后，每对相邻帧It+i和目标帧It经过OFRnet的运算，都能得到It+i关于It的光流图，记为Ot+i；利用Ot+i对It+i进行仿射变换得到经过运动补偿的帧It+i,t，该帧是与It高度相似的对齐帧；此时多个对齐帧{It+i,t}和目标帧It进入一个融合模块，该融合模块运用注意力机制将多帧融合，输出多通道特征图最后经由一个重构模块加上图像残差，输出超分辨率目标帧推流端推流端主要包括训练和监测两个模块；训练模块将当前的实时最佳模型传送给监测模块，监测模块负责记录当前模型随视频流变化的PSNR；一旦通过PSNR监测到场景转变，监测模块向训练模块发出训练请求；如果训练模块处于停止状态，则开始模型训练；监测模块监测算法选择使用局部加权回归算法，该算法利用某时刻的邻近值进行平滑处理；由于与时刻xi距离不同的点有不同的权重，算法公式如下所示：其中，使用Epanechnikov二次kernel：式中，λ是kernel的参数，称为窗口宽度；局部加权中只考虑附近k个值的影响，也就是：λ＝|xi-x|，其中，x是距离xi为k的时刻点；训练模块训练模块根据监测模块的结果进行训练；训练过程中，设定一个提升PSNR阈值THs以及一个预测提升PSNR阈值THp；前者依据经验作为固定参数，后者通过冷启动的训练数据实时预测最大提升值；训练模块中，通过一浅层神经网络预测数据结果；网络的输入层和输出层的数据个数都为1，其中间为隐藏层，共有3层，每个隐藏层有20个神经元，每层使用sigmoid作为激活函数；这样的网络结构能够较好地拟合在闭区间上连续的大部分函数；同时训练损失函数为MSE，使用L2正则化防止数据过拟合；最后选择提升阈值为：其中，α为小于1的比例系数；训练模型PSNR达到该阈值TH后即停止训练，以节省计算资源。2.根据权利要求1所述的基于在线训练的超分辨率直播系统，其特征在于，所述TCSR中的OFRnet模块用于进行光流图计算，是采用超分辨率估计相邻帧光流图；OFRnet模块的输入是一对目标帧It和相邻帧It+i，输出是相邻帧的超分光流图Ot+i；在OFRnet第一层，视频帧先降采样为原来的1/2得到与然后设置一个元素值全0的光流图作为初始变量；与经过仿射运算后，与进行卷积和多层残差网络处理，得到降采样大小的光流图；该光流图经过2倍升采样变为和参考帧同样大小的Ot+i；Ot+i和原始相邻帧It+i进行仿射变换，再加入It进行卷积层和残差块处理，最后得到超分辨率光流图Ot+i；Ot+i和It+i尺寸相同,直接将两者进行仿射变换,得到对齐后的相邻帧It+i,t。3.根据权利要求2所述的基于在线训练的超分辨率直播系统，其特征在于，所述融合模块模块采用注意力机制，通过计算两帧之间的相似性，决定融合时该帧所占的权重；融合模块的输入为It+i,t和It，两者分别经过3*3卷积层运算，然后将结果进行矩阵点乘运算；再经过sigmoid函数运算，得到两帧之间的距离图Dt+i,t，该距离图反映了It+i,t像素点在融合时所占有的权重；Dt+i,t和It+i,t再次进行点乘，得到对齐帧的特征图Ft+i,t；最后多个相邻的对齐特征图经过卷积运算得到多通道特征图4.根据权利要求3所述的基于在线训练的超分辨率直播系统，其特征在于，所述重构模块由2层卷积层、1层残差层组成；多通道特征图经过卷积和残差块，最终和原目标帧相加得到超分辨率帧5.根据权利要求4所述的基于在线训练的超分辨率直播系统，其特征在于，所述TCSR中，由于对光流图进行估计，其损失函数中里面中间光流图的计算精度；真实高清视频帧和超分辨率还原结果之间的损失函数表示为：光流图的精度是根据仿射变换结果It+i,t与目标帧It的误差来判断，输入时一共有2k+1帧加入计算，所以估计一层光流图的损失函数为：估计二层光流图Ot+i的损失函数为：最终的损失函数则由三者加权相加：其中，α＜β＜1。6.根据权利要求5所述的基于在线训练的超分辨率直播系统，其特征在于，由于服务器端的两级超分辨网络在处理能力上存在差异，F-EDSR对视频流进行实时处理，TCSR对视频流进行挑帧处理；输出后的视频通过HLS协议分发给播放端用户。
说明书desc
技术领域本发明属于直播流媒体技术领域，具体涉及一种基于在线训练的超分辨率直播系统。背景技术直播流媒体正在成为人们生活中越来越重要的部分。截止2020年，中国的在线直播行业用户量已经达到了将近6亿。据估计到2022年，互联网中将有13％的流量来自直播流媒体。直播流媒体，就是推流端将音视频流在较短的延时内分发到用户播放端的系统。近年来虽然流媒体直播技术不断更新迭代，新的协议和分发架构层出不穷，但是仍然遵循着一定的基本框架。直播流媒体主要有六个基础环节，即：捕捉，编码，封装，转码，分发，播放。一般对应三个终端：推流端、服务器端、用户端。每个终端之间会有相应的数据传输。推流端首先完成视频的捕捉、编码和封装，之后通过低延时传输协议将视频流推送到服务器。服务器端完成视频的转码，最后经由CDN分发到用户手中。用户则通过播放器实现直播观看。重要的直播场景非常多，在目前配备GPU硬件计算能力的PC端、移动终端和智能汽车等场景，视频直播系统的应用有了进一步拓展。由于直播流媒体有低延时的特性，传输网络协议就成了系统中重要的技术基础。如今市面上主流的直播协议有WebRTC、RTMP、HLS等。WebRTC是由Google团队收购并维护的一项视频传输协议，被视为直播流媒体的未来标准。与其他协议相比，WebRTC有着极低的延迟，通常用于需要即时互动的场景。但由于推出时间较短，在国内市场目前还没有被大规模使用。RTMP是Adobe公司为了实现播放器和服务器之间的高性能传输而研发的应用层协议，主要基于TCP长连接。由于延时低、连接可靠、使用方便等特点，该协议已经被广泛应用，成为国内主流的直播流媒体协议。HLS是由Apple公司基于HTTP实现的媒体传输协议。HLS与DASH类似，会将视频流切割成一个个连续的视频切片，用户可以在播放端通过访问列表文件，顺序下载视频流片段，以此达到观看直播的效果。HLS虽然延迟较高，但兼容性较好。适用于互动性要求不高的场景。超分辨率是指将低分辨率的图像或视频，恢复为高分辨率的图像或视频的一项技术。超分辨率领域可以细分为图像超分辨率和视频超分辨率图像超分是从指定的单幅低分辨率图像中还原出高分辨率图像的技术。传统算法中有基于插值和重建的图像超分。最近的图像超分算法大多是基于深度学习，主要学习低分辨图像和高分辨率图像之间的映射关系。Dong等人提出了超分辨率卷积神经网络。该网络有三层卷积结构，先将图像映射为小分辨率图像，再将图像放大为高分辨率图像，最后完成特征非线性映射的重建。Kim等人提出了SRGAN网络，首次将GAN用在了超分辨重建上。对于高放大倍数图片，GAN可以较好展现图像细节纹理，增加真实感。在生成网络部分，文章提出的SRResNet利用多个残差块挖掘图像特征。每个残差块包含两个卷积层，两个批量归一化层，以及一个激活层。多层残差模块的应用使得网络可以进一步学习图像的深层信息。Lim等人受到SRResNet的启发，通过去其残差网络中不必要的模块提出了EDSR。作者Lim认为SRResNet中的批量归一化层使得网络对图像特征的感受范围减少，将其删除能让网络更好地学习图像细节。同时，批量归一化层消耗的内存量与前面的卷积层相同，因此去掉之后会使GPU内存使用量大大减少，训练时间也能进一步缩短。视频超分辨率与图像超分辨率的区别在于，视频超分辨率在还原过程中使用了相邻帧信息，从而更好地补充图像细节，解决画面模糊的问题。如何将低分辨目标帧与相邻帧临时对齐是视频超分辨率的一个关键问题。大多数视频超分辨率网络主要由四个模块构成：对齐模块、融合模块、重构模块和上采样模块。前两个模块主要利用相邻帧之间的时域信息，可以被称作时域超分。后两个模块主要挖掘图像上的细节，可以被称作空域超分。视频超分网络SOFVSR可以通过计算目标帧和相邻帧之间的光流图，估计目标图像在帧间的移动信息。光流图可以将相邻帧信息还原到目标帧，多帧融合的结果可以显著提高视频帧PSNR。此外，作者创新性地将超分辨运用到光流图的估计中，极大提高了光流图估计的准确性。Wang等人提出基于可变形卷积网络的视频超分辨率网络。网络结构分为预处理模块，PCD对齐模块，TSA融合模块，重构模块。在PCD对齐模块，作者提出三级金字塔结构，基于可变形卷积，提取相邻帧之间的信息。结果表明，可变形卷积极大提高了相邻帧信息的利用率，极大程度地改善了运动模糊的问题。Chu等人将生成对抗网络应用到视频超分辨率里，提出TecoGAN网络结构。在生成器中，TecoGAN将目标帧，前一帧和前一帧的SR帧作为输入，通过低分辨率的相邻帧估计光流图。之后网络将放大后的光流图与前一帧的SR进行运算对齐，最后将运算结果与低分辨帧一起进入后续的卷积重构。文章提出的Ping-Pong损失函数，减少了较长时间内的画面细节漂移，使超分辨率结果更自然。发明内容本发明旨在提出一种基于超分辨率的在线训练直播系统。该系统主要利用PC、移动终端、智能汽车等带GPU的硬件计算性能。与传统的直播流媒体系统相比，该系统能在推流端上传带宽有限的条件下，在服务器端通过超分辨率技术重构出质量更好的视频画面。本发明提出的超分辨率直播系统，主要有推流端和服务器端两个部分。系统结构如图1所示。用户在直播推流时，系统先捕捉高分辨率视频流并转码为合适码率的低分辨率视频流，然后通过RTMP协议将低分辨率视频流上传到服务器。同时，推流端系统实时抓取高、低分辨率视频关键帧，并以此作为输入数据根据当前画面自适应训练空域超分辨率网络F-EDSR。之后系统根据训练数据预测网络提升阈值，在达到相应效果时终止训练，节约计算资源。训练好网络之后，推流端在不影响视频上传比特率的情况下，向服务器传输当前阶段训练好的超分辨率网络。在服务器端，服务器实时接收推流端传输的低分辨率视频流和F-EDSR。视频通过两层级联的超分辨率网络F-EDSR和TCSR进行超分辨率还原。F-EDSR负责提取画面细节，TCSR负责利用帧间信息消除画面模糊。由于两级神经网络的处理速度存在差异，F-EDSR能够实现实时处理，TCSR需要对F-EDSR输出的视频帧进行挑帧还原。最后服务器端通过HLS将超分辨率还原后的视频进行分发。本发明提供的基于在线训练的超分辨率直播系统，包括推流端和服务器端两个部分；其中：服务器端结构有帧间对齐的视频超分算法主要分为时域超分和空域超分两个阶段。时域超分需要多帧计算，占用内存大，处理速度慢，因此成为视频超分算法处理性能的瓶颈。本发明的服务器端架构中，系统将视频超分辨率网络的两个模块拆分为空域超分F-EDSR和TCSR两个网络，并进行级联。前者可以对视频实时处理，后者对F-EDSR的输出进行帧挑选处理。与传统有帧间对齐的视频超分辨率算法对比，该方法能实现对视频流的实时处理。F-EDSR系统提出的空域超分F-EDSR作为一级网络，该网络基于图像超分网络EDSR。网络结构如图2所示。网络头部使用的二维卷积层，有更大的5*5卷积核，以及更少的通道。这使得网络在较浅的网络结构下，训练速度有更好的表现。网络主体由深度残差模块构成，并以二维卷积层结束。每个残差模块由两个卷积层和一个激活层构成。残差模块去掉批量归一化层使得网络主体参数更少，感受动态更大。网络尾部则是上采样模块和卷积模块，这使得图像能以设定尺寸输出。TCSR系统提出的TCSR网络作为二级网络，网络结构如图3所示。网络以多个连续的帧为作为输入，中间帧作为融合输出的目标。多个连续帧表示为It-k,…It-1,It,It+1,…,It+k，中间帧为It，其中k表示中间帧和相邻帧的最大间隔数。使用的视频帧输入后，每对相邻帧It+i和目标帧It经过OFRnet的运算，都能得到It+i关于It的光流图，记为Ot+i。利用Ot+i对It+i进行仿射变换可以得到经过运动补偿的帧It+i,t，该帧是与It高度相似的对齐帧。此时多个对齐帧{It+i,t}和目标帧It进入一个融合模块，该模块运用注意力机制将多帧融合，输出多通道特征图最后经由一个重构模块加上图像残差，输出超分辨率目标帧其中：TCSR中的OFRnet模块用于进行光流图计算。根据的研究，本发明采用超分辨率估计相邻帧光流图。网络结构如图4所示。网络的输入是一对目标帧It和相邻帧It+i，输出是相邻帧的超分光流图Ot+i。在OFRnet第一层，视频帧先降采样为原来的1/2得到与这样能在减少计算量的同时更好地获取图像的整体信息。之后设置一个元素值全0的光流图作为初始变量。与经过仿射运算后，与进行卷积和多层残差网络处理，得到降采样大小的光流图。该光流图经过2倍升采样变为和参考帧同样大小的Ot+i。Ot+i和原始相邻帧It+i进行仿射变换，再加入It进行卷积层和残差块处理，最后得到超分辨率光流图Ot+i。Ot+i和It+i尺寸相同,可以直接将两者进行仿射变换,得到对齐后的相邻帧It+i,t。然后，把It+i,t与It输入融合模块进行融合。融合模块的结构如图5所示。该模块采用注意力机制，主要通过计算两帧之间的相似性，决定融和时该帧所占的权重。在相关研究中，系统参考了对图像距离的计算方式。融合模块的输入为It+i,t和It。两者分别经过3*3卷积层运算之后，再将结果进行矩阵点乘运算。这一步使得图像中相似的像素点进一步增强。之后经过sigmoid函数运算，得到两帧之间的距离图Dt+i,t，该距离图反映了It+i,t像素点在融合时所占有的权重。Dt+i,t和It+i,t再次进行点乘，得到对齐帧的特征图Ft+i,t。最后多个相邻的对齐特征图经过卷积运算得到多通道特征图多通道特征图最后经由重构模块，输出超分辨率目标帧。重构模块结构如图6所示，为2层卷积层，1层残差层组成。多通道特征图经过卷积和残差块，最终和原目标帧相加得到超分辨率帧由于TCSR需要对光流图进行估计，所以损失函数里面包含了中间光流图的计算精度。真实高清视频帧和超分辨率还原结果之间的损失函数表示为：光流图的精度是根据仿射变换结果It+i,t与目标帧It的误差来判断，输入时一共有2k+1帧加入计算，所以估计一层光流图的损失函数为：估计二层光流图Ot+i的损失函数为：最终的损失函数则由三者加权相加：其中，α＜β＜1。两级超分辨网络在处理能力上存在差异。F-EDSR可以做到对视频流的实时处理，TCSR需要对视频流进行挑帧处理。输出后的视频通过HLS协议分发给播放端用户。推流端结构超分辨率网络的还原效果和训练数据集密切相关。同一个超分辨率网络，在处理和训练集类型相似度低的视频时通常效果欠佳。由于直播流内容变化复杂，单一的预训练网络难以适应多种直播流内容。因此本系统选择在推流端实时训练超分辨率网络F-EDSR，以提高F-EDSR的网络泛化能力。与单次训练的超分辨算法相比，该策略可以保证在每个直播流上F-EDSR都有较好的还原效果。除此之外，超分辨率网络的训练会耗费大量的计算资源。高频次的网络训练会使得硬件设备过载，设备寿命减少。因此我们通过检测超分辨率网络对当前视频的还原效果，自适应开启和停止超分辨率的训练过程。与传统的训练模式相比，该训练策略可以极大降低推流设备的运算负载。推流端主要工作流程为：系统捕捉高清视频流后，会在本地进行转码压缩。低码率视频流通过RTMP协议发送到服务器端。同时，推流端将当前的高分辨率视频流和低分辨率视频流的实时关键帧作为输入数据，在线训练超分辨率网络F-EDSR，并将训练好的F-EDSR上传到服务器端。推流端中有训练和监测两个模块。训练模块将当前的实时最佳模型传送给监测模块，监测模块则负责记录当前模型随视频流变化的PSNR。一旦通过PSNR监测到场景转变，监测模块则会向训练模块发出训练请求。如果训练模块处于停止状态，则开始模型训练。监测模块检测算法上，选择使用局部加权回归算法。该算法是利用某时刻的邻近值进行平滑处理。不过考虑到了与时刻xi距离不同的点有不同的权重。算法公式如下所示：其中，使用Epanechnikov二次kernel：式中，λ是kernel的参数，称为窗口宽度。局部加权中只考虑附近k个值的影响，也就是：λ＝|xi-x|，其中，x是距离xi为k的时刻点。训练模块训练模块根据监测模块的结果进行训练。训练过程有一个设定提升PSNR阈值THs以及一个预测提升PSNR阈值THp。前者依据经验作为固定参数，后者通过冷启动的训练数据实时预测最大提升值。系统通过浅层神经网络预测数据结果。网络输入层和输出层的数据个数都为1。中间的隐藏层有3层，每个隐藏层有20个神经元，每层使用sigmoid作为激活函数。这样的网络结构能够较好地拟合在闭区间上连续的大部分函数。同时训练损失函数为MSE，使用了L2正则化防止数据过拟合。最后选择提升阈值为：其中，α为小于1的比例系数。训练模型PSNR达到该阈值TH后即停止训练，以节省计算资源。附图说明图1为整体系统架构。图2为F-EDSR结构图。图3为TCSR结构图。图4为OFRnet结构图。图5为融合模块结构图。图6为重构模块结构图。图7为自适应训练策略的测试结果。图8为二级级联网络的测试结果。具体实施方式实施例：设实施例的环境推流端：ubuntu18.04操作系统；OSB推流工具；ffmpeg转码工具；python环境下基于pytorch的系统架构，GPU为GTX 1080ti。服务器端：ubuntu18.04操作系统；nginx架构；上传端协议RTMP；视频分发HLS；视频封装ffmpeg；python环境下基于pytorch的系统架构，GPU为GTX 1080ti。步骤1：推流端通过OBS工具向系统推送高清视频流进行直播；步骤2：推流端将视频流转码压缩为低分辨率视频流通过RTMP推送到服务器；步骤3：服务器端接收到视频流之后，将视频输入一级超分网络实时输出高清帧，再将高清帧输入二级超分网络挑帧处理，最后得到超分辨率视频输出；步骤4：推流端实时监测F-EDSR对当前视频流的处理效果。发生PSNR跳动时证明有场景转换，会让训练模块开始在线训练F-EDSR；步骤5：训练模块根据当前视频流开始训练F-EDSR。提升到一定阈值之后停止训练；步骤6：一旦上一个F-EDSR传输完毕，推流端就开始传输当前最佳F-EDSR；步骤7：服务器端接收到最新F-EDSR，将其替换到系统。并用该模型处理视频流。实验结果：如图7所示，在直播场景有变化时，本发明系统的自适应训练与单次训练对比，服务器端视频流质量有着显著提升；与持续训练对比，视频流质量相近。如图8所示，实验用7类真实的直播数据进行测试，TCSR的输入为F-EDSR的输出。可见二级级联网络中，TCSR网络的输出相比于F-EDSR在不同类别的直播视频上都会有相应提升。参考文献.Wang L,Guo Y,Liu L,et al.Deep video super-resolution using HRoptical flow estimation.IEEE Transactions on Image Processing,2020,29:4323-4336..Wang X,Chan KC K,Yu K,et al.Edvr:Video restoration with enhanceddeformable convolutional networks.Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition Workshops.2019..The Complete Guide to Live Streaming.https://www.wowza.com/wp-content/uploads/The-Complete-Guide-to-Live-Streaming.pdf.Dong C,Loy C C,He K,et al.Learning a deep convolutional networkfor image super-resolution.European Conference on Computer Vision.2014:184-199..Kim J,Lee J K,Lee KM.Accurate image super-resolution using verydeep convolutional networks.Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition.2016:1646-1654..Ledig C,Theis L,Huszár F,et al.Photo-realistic single imagesuper-resolution using a generative adversarial network.Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition.2017:4681-4690..Lim B,Son S,Kim H,et al.Enhanced deep residual networks forsingle image super-resolution.Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition Workshops.2017:136-144..Liu H,Ruan Z,Zhao P,et al.Video super resolution based on deeplearning:A comprehensive survey.arXiv preprint arXiv:2007.12928,2020..Chu M,Xie Y,Mayer J,et al.Learning temporal coherence via self-supervision for GAN-based video generation.ACM Transactions on Graphics,2020,39:75:1-75:13.。
