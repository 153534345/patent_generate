标题title
一种超级计算机的网络拓扑构建方法及网络拓扑结构
摘要abst
本发明涉及超级计算机技术领域，公开了一种超级计算机的网络拓扑构建方法及网络拓扑结构，该方法，将计算结点划分成组、将I/O转发结点分组放置，并改变I/O转发方式，使得计算结点只能共享使用本组内的I/O转发结点。本发明解决了现有技术存在的以下问题：在二维网络中产生大量聚集的通信数据包，造成通信阻塞，严重影响了互连通信系统的运行稳定性，并影响了I/O效率等。
权利要求书clms
1.一种超级计算机的网络拓扑构建方法，其特征在于，将计算结点划分成组、将I/O转发结点分组放置，并改变I/O转发方式，使得计算结点只能共享使用本组内的I/O转发结点。2.根据权利要求1所述的一种超级计算机的网络拓扑构建方法，其特征在于，在同一组中，I/O转发结点轮询服务相邻计算结点。3.根据权利要求1或2所述的一种超级计算机的网络拓扑构建方法，其特征在于，包括以下步骤：S1，将计算结点与I/O转发结点进行分组放置；S2，将计算结点与I/O转发结点进行分组轮询映射。4.根据权利要求3所述的一种超级计算机的网络拓扑构建方法，其特征在于，步骤S1包括以下步骤：S11，将超级计算机的计算结点按行或按列分组：计算超级计算机二维网络拓扑的行数与列数/＞，比较两值并取较小者，将较小值记为/＞；若/＞，则按行将计算结点分组为/＞组；若/＞，则按列将计算结点分组为/＞组；S12，计算I/O转发结点数量,根据步骤S11得出的分组数量/＞，计算每组平均分配的I/O转发结点数量/＞；S13，在二维网络的每一组中，根据机框与存储系统的物理位置与二者之间的布线距离，在距存储系统最近的机框中放置个I/O转发结点；S14，将映射方式设置成局部的分组映射模式：每组计算结点无法使用所有的I/O转发结点，只能使用组内的I/O转发结点。5.根据权利要求4所述的一种超级计算机的网络拓扑构建方法，其特征在于，步骤S14中，若是采用Lustre LNET router的I/O转发架构，则更改router设置，将每个计算结点的router设置成计算结点所在组的I/O转发结点，并重启I/O转发服务。6.根据权利要求5所述的一种超级计算机的网络拓扑构建方法，其特征在于，步骤S2包括以下步骤：S21，根据计算结点的分组结果，计算每一组中计算结点的数量，将计算结点记为；计算每一组中I/O转发结点的数量，将I/O转发结点记为；其中，i表示计算结点序号，m表示计算结点数量，0≤i≤m，j表示I/O转发结点序号，n表示I/O转发结点数量，0≤j≤n；S22，对于每一组中的计算结点，将/＞映射至I/O转发结点/＞，以使计算结点/＞的所有I/O请求将交由该I/O转发结点处理。7.一种超级计算机的网络拓扑结构，其特征在于，采用权利要求1至6任意项所述的一种超级计算机的网络拓扑构建方法构建，包括：行数为、列数为/＞的机框，机框中包括计算结点、I/O转发结点，计算结点与I/O转发结点进行分组放置；在二维网络的每一组中，根据机框与存储系统的物理位置与二者之间的布线距离，在距存储系统最近的机框中放置个I/O转发结点；映射方式设置成局部的分组映射模式：每一组中计算结点无法使用所有的I/O转发结点，只能使用组内的I/O转发结点。8.根据权利要求7所述的一种超级计算机的网络拓扑结构，其特征在于，对于每一组中的计算结点，将/＞映射至I/O转发结点/＞，以使计算结点/＞的所有I/O请求将交由该I/O转发结点处理。9.根据权利要求7或8所述的一种超级计算机的网络拓扑结构，其特征在于，还包括存储网络、存储系统，每一组中I/O转发结点分别通过存储网络与存储系统通信连接。10.根据权利要求9所述的一种超级计算机的网络拓扑结构，其特征在于，还包括行交换机、列交换机，同一行中的机框共用一个行交换机，同一列中的机框共用一个列交换机。
说明书desc
技术领域本发明涉及超级计算机技术领域，具体是一种超级计算机的网络拓扑构建方法及网络拓扑结构。背景技术为了简化超级计算机系统的网络拓扑结构，提高超级计算机系统的运行稳定性，当前大型超级计算机普遍采用了I/O转发架构。如图1所示，计算结点与I/O转发结点通过计算网络互连，I/O转发结点与存储结点通过存储网络互连。作业运行在计算结点上，当作业需要读写数据时，其I/O请求从计算结点发送给I/O转发结点，由I/O转发结点转发给存储结点。计算结点的数据读写请求须经I/O转发结点转发后才可发送给存储结点。在采用二维网络拓扑的超级计算机中，计算网络采用二维网络结构。如图2所示，网络中的结点被分组放置在机框中，机框内的结点间只需通过框内交换板即可互连通信。在网络逻辑拓扑中，机框是二维网络中的最小连接单位，机框以行列形式呈二维排列。同一行的机框连接着同一个交换机，并且同一列的机框连接着同一个交换机，即每个机框同时连接着一个行交换机与一个列交换机。同行的机框之间只需通过该行的1个交换机即可互连，同列的机框之间只需通过该列的1个交换机即可互连。不同行且不同列的机框必须通过两个交换机才能互连，例如机框F00中的计算结点与机框F11中的计算结点通信，须先经过行交换机R0跳转至该两机框的行列交点处的机框F03，再经过列交换机C3跳转至目的机框F11。可见，在采用二维网络拓扑的超级计算机中，框内结点间通信的网络距离最短，同行或同列的框间通信网络距离次之，不同行且不同列的框间通信网络距离最长。在超级计算机生产系统中，跨行跨列的通信不但会增加通信延迟，影响通信效率，更会造成二维网络中的通信泛洪，易造成通信阻塞，影响通信系统运行稳定性。因此采用二维网络拓扑之后，超级计算机必须尽量减少跨行跨列的通信行为，以提升通信效率与网络稳定性。然而，目前在采用二维胖树网络拓扑的超级计算机中，I/O转发架构设计仍存在一定优化空间。在超级计算机的实际部署中，考虑到设计与维护的便利性，通常将I/O转发结点集中放置在一个或少数几个机框中。例如，在图2所示的超级计算机中，I/O转发结点全部位于机框F15中，该种I/O转发架构存在以下两个方面的问题：有3个机框的计算结点与I/O转发结点处于同一列中，通信需经过1台列交换机；有3个机框的计算结点与I/O转发结点处于同一行中，通信需经过1台行交换机；有9个机框的计算结点与I/O转发结点既不在同一行中，也不在同一列中，通信需经过2台交换机。上述现象将会产生两个方面的性能影响，一是由于大部分机框的计算结点需要经过跨行跨列通信才能与I/O转发结点通信，严重影响了这些计算结点的数据读写效率，并造成二维网络上的通信泛洪，造成网络互连系统运行不稳定；二是集群系统中所有的I/O流量都将发往机框F15中的I/O转发结点，造成该机框的通信负载较大，该机框的运行稳定性也将受到影响。当前I/O转发构架多采用集中映射的方式，即同一个或相邻几个机框里的计算结点共享使用一个I/O转发结点。例如，在图2中机框F00中计算结点的I/O请求全部交由机框F15中的I/O转发结点A处理，类似的，机框F01中计算结点与I/O转发结点B对应。据统计，一个作业更容易被分配到相邻的计算结点，因此相邻的计算结点更容易同时读写数据，这将造成一小部分I/O转发结点过载，导致负载不均。由于I/O转发结点是连接计算结点与存储服务器的桥梁，I/O转发结点承担了超级计算机系统上所有的I/O流量，在以上存储转发架构下，几乎所有的I/O流量都必须跨行跨列与一小部分I/O转发结点通信，在二维网络中产生大量聚集的通信数据包，造成通信阻塞，严重影响了互连通信系统的运行稳定性，并影响了I/O效率。当前采用二维网络拓扑的超级计算机中，其存储转发架构并没有针对以上“跨行跨列通信问题”进行特殊设计，于是便产生了两个方面的问题。当前的I/O转发结点一般集中地连接在一个或少数几个机框交换板上，因此除了与这些I/O转发结点所在机框同行同列的机框外，绝大部分机框内的计算结点都与I/O转发结点既不同行也不同列，计算结点与I/O转发结点的通信必须经过两台交换机；当前I/O转发构架多采用集中映射的方式，即同一个或相邻几个机框里的计算结点共享使用一个I/O转发结点，据统计，一个作业更容易被分配到相邻的计算结点，因此相邻的计算结点更容易同时读写数据，这将造成一小部分I/O转发结点过载，导致负载不均。由于I/O转发结点是连接计算结点与存储服务器的桥梁，I/O转发结点承担了超级计算机系统上所有的I/O流量，在以上存储转发架构下，几乎所有的I/O流量都必须跨行跨列与一小部分I/O转发结点通信，在二维网络中产生大量聚集的通信数据包，造成通信阻塞，严重影响了互连通信系统的运行稳定性，并影响了I/O效率。发明内容为克服现有技术的不足，本发明提供了一种超级计算机的网络拓扑构建方法及网络拓扑结构，解决现有技术存在的以下问题：在二维网络中产生大量聚集的通信数据包，造成通信阻塞，严重影响了互连通信系统的运行稳定性，并影响了I/O效率等。本发明解决上述问题所采用的技术方案是：一种超级计算机的网络拓扑构建方法，将计算结点划分成组、将I/O转发结点分组放置，并改变I/O转发方式，使得计算结点只能共享使用本组内的I/O转发结点。作为一种优选的技术方案，在同一组中，I/O转发结点轮询服务相邻计算结点。作为一种优选的技术方案，包括以下步骤：S1，将计算结点与I/O转发结点进行分组放置；S2，将计算结点与I/O转发结点进行分组轮询映射。作为一种优选的技术方案，步骤S1包括以下步骤：S11，将超级计算机的计算结点按行或按列分组：计算超级计算机二维网络拓扑的行数与列数/＞，比较两值并取较小者，将较小值记为/＞；若/＞，则按行将计算结点分组为/＞组；若/＞，则按列将计算结点分组为/＞组；S12，计算I/O转发结点数量,根据步骤S11得出的分组数量/＞，计算每组平均分配的I/O转发结点数量/＞；S13，在二维网络的每一组中，根据机框与存储系统的物理位置与二者之间的布线距离，在距存储系统最近的机框中放置个I/O转发结点；S14，将映射方式设置成局部的分组映射模式：每组计算结点无法使用所有的I/O转发结点，只能使用组内的I/O转发结点。作为一种优选的技术方案，步骤S14中，若是采用Lustre LNET router的I/O转发架构，则更改router设置，将每个计算结点的router设置成计算结点所在组的I/O转发结点，并重启I/O转发服务。作为一种优选的技术方案，步骤S2包括以下步骤：S21，根据计算结点的分组结果，计算每一组中计算结点的数量，将计算结点记为；计算每一组中I/O转发结点的数量，将I/O转发结点记为；其中，i表示计算结点序号，m表示计算结点数量，0≤i≤m，j表示I/O转发结点序号，n表示I/O转发结点数量，0≤j≤n；S22，对于每一组中的计算结点，将/＞映射至I/O转发结点/＞，以使计算结点/＞的所有I/O请求将交由该I/O转发结点处理。一种超级计算机的网络拓扑结构，采用所述的一种超级计算机的网络拓扑构建方法构建，包括：行数为、列数为/＞的机框，机框中包括计算结点、I/O转发结点，计算结点与I/O转发结点进行分组放置；在二维网络的每一组中，根据机框与存储系统的物理位置与二者之间的布线距离，在距存储系统最近的机框中放置个I/O转发结点；映射方式设置成局部的分组映射模式：每一组中计算结点无法使用所有的I/O转发结点，只能使用组内的I/O转发结点。作为一种优选的技术方案，对于每一组中的计算结点，将/＞映射至I/O转发结点/＞，以使计算结点/＞的所有I/O请求将交由该I/O转发结点处理。作为一种优选的技术方案，还包括存储网络、存储系统，每一组中I/O转发结点分别通过存储网络与存储系统通信连接。作为一种优选的技术方案，还包括行交换机、列交换机，同一行中的机框共用一个行交换机，同一列中的机框共用一个列交换机。本发明相比于现有技术，具有以下有益效果：本发明可缩短高性能计算机上运行的计算作业读写数据时的网络通信距离；本发明增加高性能计算机上运行的计算作业可利用的I/O转发结点数量；本发明减少二维通信网络中的跨行跨列通信数量，从而提高计算作业的数据读写效率、提升网络通信系统的稳定性。附图说明图1为现有技术的一种超级计算机的I/O转发架构的示意图；图2为现有技术的一种采用二维网络拓扑的超级计算机的计算网络的示意图；图3为本发明提出的二维网络拓扑超级计算机的I/O转发架构的示意图。具体实施方式下面结合实施例及附图，对本发明作进一步的详细说明，但本发明的实施方式不限于此。实施例1如图1至图3所示，本发明针对二维网络中计算结点按行按列分组的网络拓扑结构，提出了一种新的I/O转发结点的网络位置设计方法，并针对新的网络位置，设计了一种分组轮询的计算结点与I/O转发结点的映射方式。本发明提出的二维网络拓扑超级计算机的I/O转发架构包含2个部分，第一部分涉及一种将I/O转发结点根据二维网络拓扑特点分组放置的技术方案，第二部分涉及一种将计算结点与I/O转发结点分组轮询映射的技术方案。下面将分别介绍此两部分技术方案。当前采用二维网络拓扑的超级计算机网络架构如图2所示。超级计算机的计算网络采用二维网络拓扑，其中机框中包含若干个相同数量的计算结点，机框F15中包含若干个I/O转发结点；超级计算机的存储网络采用通用网络拓扑。其中I/O转发结点同时连接着计算网络与存储网络，作为计算结点与存储系统通信的桥梁。在二维网络拓扑中，同一行的机框通过行交换机互连，同一列的机框通过列交换机互连。若两个既不同行也不同列的机框之间通信，则机框F00需通过先通过行交换机跳转至机框F01，再通过列交换机跳转至机框F05。本发明提出的第一部分技术方案，拟将I/O转发结点根据二维网络拓扑特点分组放置，其体系架构如图3所示：第一步骤，将超级计算机的计算结点按行或按列分组。由于每组中的I/O转发结点数量越多，就越能够均摊组内的I/O负载。在I/O转发结点数量固定的情况下，应当选择数量较小的行或列进行分组，如此每组分得的I/O转发结点数量会更多。计算超级计算机二维网络拓扑的行数与列数/＞，比较两值并取较小者。在本发明的描述中，均假设行数比列数小，计算结点选择按行分组。第二步骤，计算I/O转发结点数量,根据第一步骤得出的分组数量，计算每组平均分配的I/O转发结点数量/＞。第三步骤，在二维网络的每一行中，根据机框与存储系统的物理位置与二者之间的布线距离，选择一个距存储系统最近的机框，放置个I/O转发结点。第四步骤，改变原本的计算结点与I/O转发结点的全局映射方式，将映射方式设置成局部的分组映射模式。每组计算结点无法使用所有的I/O转发结点，只能使用组内的I/O转发结点。改变映射模式的方式根据采用的I/O转发架构技术不同而不同，对于采用LustreLNET router的I/O转发架构，需更改router设置，将每个计算结点的router设置成计算结点所在组的I/O转发结点，并重启I/O转发服务。本发明提出的第二部分技术方案，拟将计算结点与I/O转发结点进行分组轮询映射：第一步骤，根据计算结点的分组结果，计算每一组中计算结点的数量，将计算结点编号为；计算每一组中I/O转发结点的数量，将I/O转发结点编号为。第二步骤，对于每一个计算结点组中的计算结点，将其映射至I/O转发结点，即该计算结点的所有I/O请求将交由该I/O转发结点处理。改变映射的具体实现方式与第一部分技术方案中的第四步骤相同。经过以上两个部分的具体实施步骤，实现了以下两个方面的功能：每一个计算结点与I/O转发结点都在二维网络的同一行或同一列中，仅需通过一个行交换机通信，而原来的I/O转发架构中需通过两个交换机才能通信；每一组中相邻的计算结点都使用了不同的I/O转发结点，由于相邻的计算结点同属于同一个作业的概率更大，同时进行I/O的可能性较高，此种方式可以显著提升调动的I/O转发结点数量，均衡I/O转发结点负载。在采用计算网络与存储网络分离、且计算网络采用二维网络拓扑的超级计算机中，I/O转发架构需要特殊设计以适应二维网络拓扑的特点，本发明具有如下关键创新：1.将计算结点划分成组、将I/O转发结点分组放置，并改变I/O转发方式，使得计算结点只能共享使用本组内的I/O转发结点。该方式可以有效缩短计算结点至I/O转发结点的网络距离，提升网络通信效率与稳定性。2.对于每一个计算结点组，改变原来组内单个I/O转发结点服务整个机框中所有计算结点的I/O转发方式，本发明使I/O转发结点轮询服务相邻计算结点，即相邻计算结点能够使用不同I/O转发结点，实现I/O转发结点负载均衡。本发明通过设计并部署一种超级计算机上的I/O转发架构，其可部署在采用了二维通信网络拓扑的超级计算机上，部署后可优化超级计算机上运行的计算作业，具体地：可缩短作业读写数据时的网络通信距离；增加作业可利用的I/O转发结点数量；减少二维通信网络中的跨行跨列通信数量，从而提高计算作业的数据读写效率、提升网络通信系统的稳定性。如上所述，可较好地实现本发明。本说明书中所有实施例公开的所有特征，或隐含公开的所有方法或过程中的步骤，除了互相排斥的特征和/或步骤以外，均可以以任何方式组合和/或扩展、替换。以上所述，仅是本发明的较佳实施例而已，并非对本发明作任何形式上的限制，依据本发明的技术实质，在本发明的精神和原则之内，对以上实施例所作的任何简单的修改、等同替换与改进等，均仍属于本发明技术方案的保护范围之内。
