标题title
基于深度强化学习的网络业务接入和切片资源配置方法
摘要abst
本发明公开了一种基于深度强化学习的网络业务接入和切片资源配置方法，包括：将定义好的网络流量切片与事先分类的业务相关联；根据业务的时延和能耗分配切片的通信、计算和存储资源，建立优化目标；将优化目标建模成马尔科夫决策过程，利用训练好的Q‑Learning模型求解得到最优切片接入策略；基于最优切片接入策略，将切片资源分配优化问题建模成马尔科夫决策过程，利用训练好的DDPG模型得到切片的最优资源配置策略。本发明所述方法，比较全面地考虑了各类业务的时延差异，实现了最大化业务的满意度；根据不同的业务资源需求，灵活动态地进行定制化网络资源分配，降低资源浪费，保障用户服务体验和提高网络资源利用率。
权利要求书clms
1.一种基于深度强化学习的网络业务接入和切片资源配置方法，其特征在于，包括：将定义好的网络流量切片与事先分类的业务相关联；根据业务的时延和能耗分配切片的通信、计算和存储资源，建立优化目标；将优化目标建模成马尔科夫决策过程，利用训练好的Q-Learning模型求解得到最优切片接入策略；基于最优切片接入策略，将切片资源分配优化问题建模成马尔科夫决策过程，利用训练好的DDPG模型得到切片的最优资源配置策略。2.根据权利要求1所述的基于深度强化学习的网络业务接入和切片资源配置方法，其特征在于，所述将定义好的网络流量切片与事先分类的业务相关联，包括：以时延敏感度、带宽和可靠性为参数指标，按照不同需求将业务分为四类：自动化和远程控制类、高清视频类、实时采集类、车联网类；根据流量特征，定义六类网络流量切片，其中，切片一至切片四与四类业务关联，切片五为非地面通信，包括无法建设基站和基站损坏时的应急通信，或者在其他切片业务饱和时，为优先级不高且时延要求很低的业务提供通信资源；切片六为其他，用于接入基于场景不确定性出现的新型业务类型。3.根据权利要求1所述的基于深度强化学习的网络业务接入和切片资源配置方法，其特征在于，所述根据业务的时延和能耗分配切片的通信、计算和存储资源，建立优化目标，包括：在智慧工厂场景下，定义各切片的效用，以及切片业务的优先级，每个请求有i个业务，在某一时刻有M个请求，请求业务集合为，表示请求m是否存在第i类业务，其中，0表示不存在，1表示存在；共有/＞个i类业务资源，请求m的每个业务需要执行计算任务/＞，传输大小为/＞的业务内容，时延最低要求为/＞；计算业务的时延为：；计算业务的能量消耗为：；其中： 为第i类业务分配的带宽资源，/＞为第i类业务分配的计算资源，/＞为第i类业务分配的存储资源；/＞，为第i类业务对应内容的缓存指示变量，取值为1时，表示对应请求的内容已经存储在基站中，无需处理可直接传输获得；反之则表示没有存储在基站中，则需要计算业务时延和能量消耗；P为DU基站发射功率，/＞为路径损耗；/＞表示噪声功率；/＞因此，优化目标为min ;s.t C1:;C2: ;C3: ;C4: ;C5: ;其中，和/＞分别是不同类型业务时延和能耗对网络性能的影响因子，C1、C2、C3分别是存储空间、带宽、计算资源分配的约束条件，C4是每一类业务的缓存存储的约束，C5是不同业务类型的时延约束条件。4.根据权利要求1所述的基于深度强化学习的网络业务接入和切片资源配置方法，其特征在于，所述将优化目标建模成马尔科夫决策过程，包括：agent：需要请求接入基站的新用户；t时刻，第k个代理的状态空间表示为：；其中：表示t时刻切片l中基站的剩余可用资源，/＞表示业务请求的资源；代理采取的动作为：；其中：表示将业务接入到几个切片中，/＞，其中/＞表示是否将业务接入到切片i中；代理获得的奖励为：；其中：和/＞分别是不同类型业务时延和能耗对网络性能的影响因子，/＞和/＞分别表示业务i的时延和能耗。5.根据权利要求4所述的基于深度强化学习的网络业务接入和切片资源配置方法，其特征在于，所述Q-Learning模型的训练方法，包括：a. 根据划分的状态情况及可选取动作建立Reward-Table，从动作空间中选取一种动作；b. 初始化Q值表，Q值表与Reward-Table同阶，Q←0；c. 对应Reward-Table，根据不同状态任意选取动作，更新Q值表，Q值的更新公式如下：；/＞其中：为节点在状态/＞下采取动作/＞后可获得的期望最大收益， R为立即获得的收益，从Reward-Table中获得；/＞，/＞为学习率，用于决定本次的误差有多少是要被学习的； /＞为折扣因子，用于确定延迟回报与即时回报的相对比例；d. 继续从出发，若未达到目标状态，则进行下一步更新；e. 如果算法未达到周期数，则转入步骤a进行下一个场景，否则结束训练，得到训练完毕的收敛Q表。6.根据权利要求1所述的基于深度强化学习的网络业务接入和切片资源配置方法，其特征在于，所述基于最优切片接入策略，将切片资源分配优化问题建模为马尔科夫决策过程，利用训练好的DDPG模型得到切片的最优资源配置策略，包括：获取网络中每个切片的资源状态信息，初始化系统的资源环境，构成环境的状态参数；构建切片资源分配优化问题的马尔科夫决策过程，对DDPG模型进行离线训练；利用训练好的DDPG模型为每个切片提供最优资源分配方案。7.根据权利要求6所述的基于深度强化学习的网络业务接入和切片资源配置方法，其特征在于，所述获取网络中每个切片的资源状态信息，初始化系统的资源环境，构成环境的状态参数，包括：根据最优切片接入策略，获取系统的资源状态信息，包括业务需求信息和切片资源信息；对于任意第i类切片下：业务数量为，业务所需的资源为：；其中，为带宽资源，/＞为计算资源，为存储资源；切片中的基站数量为L，切片上各基站可分配的带宽资源为/＞，计算资源为/＞，存储资源为/＞。8.根据权利要求7所述的基于深度强化学习的网络业务接入和切片资源配置方法，其特征在于，所述切片资源分配优化问题的马尔科夫决策过程，包括：定义深度强化学习的状态空间为切片的资源状态和业务的资源需求，代理为切片本身；t时刻，第k个代理的状态空间表示为：；其中：表示切片l中DU基站剩余可用资源，/＞表示业务的资源需求，L表示DU基站数量；定义t时刻，第k个代理采取的动作空间为：，/＞表示t时刻对切片做出的动作，包括接受和拒绝业务请求，对基站的扩容、缩容，增加基站的数量；定义t时刻，第k个代理对应获得的奖励为执行动作时带来的系统的总效用，所述总效用为M个切片的效用总合，综合切片的资源利用率和切片的业务接受率作为切片的效用表示为：/＞；；其中：为切片的效用，/＞、/＞均表示权重，/＞、/＞、/＞；/＞表示第i类切片分配给业务n的资源，/＞表示切片接受的业务数，N表示总的业务请求数，/＞表示每个基站的资源。9.根据权利要求6所述的基于深度强化学习的网络业务接入和切片资源配置方法，其特征在于，所述DDPG模型包括Actor当前网络、Actor目标网络、Critic当前网络、Critic目标网络；所述Actor当前网络：实现深度强化学习模型参数的更新，其根据当前状态/＞选择动作/＞，与环境交互后得到下一个状态/＞和环境反馈的奖励值/＞；所述Actor目标网络：从经验回收池中采样下一个状态，选择下一个最优动作/＞，并且定期将Actor当前网络参数/＞复制给/＞；所述Critic当前网络：负责价值网络参数的迭代更新，负责计算当前Q值/＞；所述Critic目标网络：负责协助计算目标Q值，并且定期将Critic当前网络参数/＞复制给/＞。10.根据权利要求9所述的基于深度强化学习的网络业务接入和切片资源配置方法，其特征在于，所述DDPG模型的训练方法包括：初始化Actor当前网络和Critic当前网络/＞，权重分别为/＞和/＞；初始化Actor目标网络和Critic目标网络/＞，并复制权重/＞，；初始化经验缓冲池D的大小为N；对于每个回合：a. 初始化动作探索的随机过程和状态/＞；b. 对于每个步长：根据当前策略和和噪声随机选择一个动作；c. 执行动作，环境反馈即时奖励/＞和下一时刻状态/＞；d. 添加到经验缓冲池D中；e. 从D中随机抽取一个小批量的；f. 计算实际的Q值：，/＞为衰减因子；g. 使用损失函数，更新Critic当前网络参数/＞；h. 使用，更新Actor当前网络参数/＞；/＞i. 平稳更新Critic目标网络和Actor目标网络参数：；； j. 如果当前状态为终止状态，则当前回合迭代完毕，否则转到步骤b。
说明书desc
技术领域本发明涉及一种基于深度强化学习的网络业务接入和切片资源配置方法，属于网络资源分配技术领域。背景技术欧美国家最早针对工业提出了“智慧工厂”的概念。“智慧工厂”的发展，是智能工业发展的新方向。特征在制造生产上体现为：系统的自主能力：分析采集的外界及自身的特征，判断及规划自身行为；整体的可视化技术：结合信号处理、预测、仿真及多媒体技术，展示设计与制造过程；自我学习及维护能力：透过系统自我学习功能，在制造过程中落实资料库补充、更新，及自动执行故障诊断，并具备对故障排除与维护，或通知对的系统执行的能力；人机共存的系统：人机之间具备互相协调合作关系，各自在不同层次之间相辅相成。近年来随着5G技术的发展，工业物联网、蜂窝车联网、增强/虚拟现实等垂直行业应用蓬勃发展，迥异的应用带来了差异化的性能需求，如车联网对时延、可靠性要求极高，增强/虚拟现实要求高速率、低时延等等。迥异的垂直应用场景促使网络在同一张物理网络上提供差异化的服务，并对网络的可扩展性、可用性、成本提出了更高的要求。网络切片技术在3GPP中广泛研究并被认为是在同一张物理网络上提供灵活、可定制异构服务的颠覆性技术。网络切片作为未来通信的一个关键技术，其核心在于针对不同应用场景的差异执行定制化服务。网络切片在通用的物理网络基础设施之上，通过无线资源的虚拟化、隔离和共享，使多个网络切片能够完成不同的传输任务。目前标准组织，如3GPP，也在广泛讨论如何设计无线接入网络以支持网络切片， 5G RAN架构考虑采用中央单元和分布单元独立部署的方式，以更好地满足各场景和应用的需求。CU设备主要包括非实时的无线高层协议栈功能，同时也支持部分核心网功能下沉和边缘应用业务的部署，而DU设备主要处理物理层功能和实时性需求的功能，功能分割可配置能够满足不同应用场景的需求。空天地一体化网络作为一个新兴的网络架构，具有很高的研究价值。SAGIN架构的设计思想是地基网络为基础，天基网络和空基网络作为补充和延伸，为广域空间范围内的各种网络应用提供泛在、智能、协同、高效的信息保障，因此被广泛认为是未来无线通信系统的发展方向。现有技术中都在一定方面上对网络切片资源分配进行了某种性能指标的改进和提升，但是大多数研究利用强化学习方法单一的解决业务是否接入问题或是整个网络的资源配置问题，没有 将业务接入和切片资源配置两方面联合考虑，针对性的为业务精准高效的选择基站接入并分配资源，灵活配置整个网络，减少资源浪费。发明内容本发明的目的在于克服现有技术中的不足，提供一种基于深度强化学习的网络业务接入和切片资源配置方法，比较全面地考虑了各类业务的时延差异，有效提高了用户服务质量和网络吞吐量，改善了资源严重浪费的问题。为达到上述目的，本发明是采用下述技术方案实现的：本发明提供了一种基于深度强化学习的网络业务接入和切片资源配置方法，包括：S1：将定义好的网络流量切片与事先分类的业务相关联；S2：根据业务的时延和能耗分配切片的通信、计算和存储资源，建立优化目标；S3：将优化目标建模成马尔科夫决策过程，利用训练好的Q-Learning模型求解得到最优切片接入策略；S4：基于最优切片接入策略，将切片资源分配优化问题建模为马尔科夫决策过程，利用训练好的DDPG模型得到切片的最优资源配置策略。进一步的，所述S1具体为：初始化网络资源和用户业务请求，由于不同的业务的QoS、优先级、对时延敏感度、可靠性、吞吐量以及通信、计算、存储资源等等需求不同，以时延敏感度、带宽、可靠性为参数指标，按照不同需求划分四类业务：自动化和远程控制类、高清视频业务类、实时采集类、车联网类。为满足对平台资源、链路带宽以及网络时延和可靠性等指标的差异化需求，根据流量特征，定义六类网络流量切片：切片一：工业自动化、远程控制、应急系统、告警业务等需要低延时/高可靠连接的业务，包括物料自动领用、半成品自动周转、成品自动入库，打造无人分拣、智能搬运的智慧仓储作业系统以及对设备停机、传送带卡料、产品积压、员工离岗等异常情况的预警推送。切片二：3D/超高清视频等大流量增强移动宽带业务，包括机器显示器，管理员监控高清视频业务等。切片三：对延迟敏感度较低且吞吐量要求相对较低的应用程序。例如手机终端、可穿戴设备、环境监测、物流过程管控，对物流车辆的实时地理位置与行车轨迹数据进行实时采集等。切片四：无人驾驶、包括无人小车，车与车之间、车与路边设施、车与互联网之间的相互通信业务。切片五：非地面通信，无法建设基站和基站损坏时的应急通信，或者在其他切片业务饱和时，为优先级不高且时延要求很低的业务提供通信资源。切片六：其他，用于接入基于场景不确定性出现的新型业务类型。切片一至切片四分别关联智慧工厂中的四类业务。进一步的，所述S2为根据请求的业务数据，不同业务所需的计算任务量和数据传输量大小以及每类切片的资源多少，计算每个业务的时延和能耗，建立以网络全局请求满意度为目标的优化函数，该满意度定义为业务请求接入的时延和能耗的加权和。具体包括：在智慧工厂场景下，定义各切片的效用，以及切片业务的优先级，每个请求有i个业务，在某一时刻有M个请求，请求业务集合为，表示请求m是否存在第i类业务，其中，0表示不存在，1表示存在；共有/＞个i类业务资源，请求m的每个业务需要执行计算任务/＞，传输大小为/＞的业务内容，时延最低要求为/＞；计算业务的时延为：；计算业务的能量消耗为：；其中： 为第i类业务分配的带宽资源，/＞为第i类业务分配的计算资源，/＞为第i类业务分配的存储资源；/＞，为第i类业务对应内容的缓存指示变量，取值为1时，表示对应请求的内容已经存储在基站中，无需处理可直接传输获得；反之则表示没有存储在基站中，则需要计算业务时延和能量消耗；P为DU基站发射功率，/＞为路径损耗；/＞表示噪声功率；因此，优化目标为min ；s.t C1: ；C2: ；C3: ；C4: ；C5: ；其中，和/＞分别是不同类型业务时延和能耗对网络性能的影响因子，C1、C2、C3分别是存储空间、带宽、计算资源分配的约束条件，C4是每一类业务的缓存存储的约束，C5是不同业务类型的时延约束条件。进一步的，所述S3具体为：将优化目标建模成马尔科夫决策过程，具体包括：agent：需要请求接入基站的新用户；t时刻，第k个代理的状态空间表示为：；其中：表示t时刻切片l中基站的剩余可用资源，/＞表示业务请求的资源；/＞代理采取的动作为：；其中：表示将业务接入到几个切片中，/＞，其中表示是否将业务接入到切片i中；代理获得的奖励为：；其中：和/＞分别是不同类型业务时延和能耗对网络性能的影响因子，/＞和/＞分别表示业务i的时延和能耗。根据状态、动作和奖励，利用训练好的Q-Learning模型求得最优的业务选择切片接入方案。所述Q-Learning模型的训练方法具体过程为：a. 根据划分的状态情况及可选取动作建立Reward-Table，从动作空间中选取一种动作；b. 初始化Q值表，Q值表与Reward-Table同阶，Q←0；c. 对应Reward-Table，根据不同状态任意选取动作，更新Q值表，Q值的更新公式如下：；其中：为节点在状态/＞下采取动作/＞后可获得的期望最大收益， R为立即获得的收益，从Reward-Table中获得；/＞，/＞为学习率，用于决定本次的误差有多少是要被学习的； /＞为折扣因子，用于确定延迟回报与即时回报的相对比例；d. 继续从出发，若未达到目标状态，则进行下一步更新；e. 如果算法未达到周期数，则转入步骤a进行下一个场景，否则结束训练，得到训练完毕的收敛Q表。进一步的，所述S4具体包括以下步骤：S41：获取网络中每个切片的资源状态信息，初始化系统的资源环境，构成环境的状态参数；S42：构建切片资源分配优化问题的马尔科夫决策过程，对DDPG模型进行离线训练；S43：利用训练好的DDPG模型为每个切片提供最优资源分配方案。进一步地，所述的S41具体为：根据最优切片接入策略，获取系统的资源状态信息，包括业务需求信息和切片资源信息；对于任意第i类切片下：业务数量为，业务所需的资源为：；/＞其中，为带宽资源，/＞为计算资源，为存储资源；切片中的基站数量为L，切片上各基站可分配的带宽资源为/＞，计算资源为/＞，存储资源为/＞。联合考虑业务的资源需求以及切片内基站的可分配通信、计算、存储资源，以实现系统的最佳资源配置方法。进一步地，所述的S42中，构建切片资源分配优化问题的马尔科夫决策过程，包括：定义深度强化学习的状态空间为切片的资源状态和业务的资源需求，代理为切片本身；t时刻，第k个代理的状态空间表示为：；其中：表示切片l中DU基站剩余可用资源，/＞表示业务的资源需求，L表示DU基站数量；定义t时刻，第k个代理采取的动作空间为：，/＞表示t时刻对切片做出的动作，包括接受和拒绝业务请求，对基站的扩容、缩容，增加基站的数量；定义t时刻，第k个代理对应获得的奖励为执行动作时带来的系统的总效用，即M个切片的效用总合，综合切片的资源利用率和切片的业务接受率作为切片的效用表示为：；；其中：为切片i的效用，/＞、/＞均表示权重，/＞、/＞、/＞；/＞表示第i类切片分配给业务n的资源，/＞表示切片接受的业务数，N表示总的业务请求数，/＞表示每个基站的资源。进一步地，所述DDPG模型包括Actor当前网络、Actor目标网络、Critic当前网络、Critic目标网络；所述Actor当前网络：实现深度强化学习模型参数的更新，其根据当前状态/＞选择动作/＞，与环境交互后得到下一个状态/＞和环境反馈的奖励值/＞；所述Actor目标网络：从经验回收池中采样下一个状态，选择下一个最优动作，并且定期将Actor当前网络参数/＞复制给/＞；所述Critic当前网络：负责价值网络参数的迭代更新，负责计算当前Q值；所述Critic目标网络：负责协助计算目标Q值，并且定期将Critic当前网络参数/＞复制给/＞。再进一步地，所述的DDPG从当前网络到目标网络的复制是每次参数只更新一点：；；其中，表示学习速率。进一步地，所述DDPG模型的训练方法包括：初始化Actor当前网络和Critic当前网络/＞，权重分别为/＞和/＞；初始化Actor目标网络和Critic目标网络/＞，并复制权重/＞，；初始化经验缓冲池D的大小为N；对于每个回合：a. 初始化动作探索的随机过程和状态/＞；b. 对于每个步长：根据当前策略和和噪声随机选择一个动作；c. 执行动作，环境反馈即时奖励/＞和下一时刻状态/＞；d. 添加到经验缓冲池D中；e. 从D中随机抽取一个小批量的；f. 计算实际的Q值：，/＞为衰减因子；g. 使用损失函数，更新Critic当前网络参数/＞；h. 使用，更新Actor当前网络参数/＞；i. 平稳更新Critic目标网络和Actor目标网络参数：；；j. 如果当前状态为终止状态，则当前回合迭代完毕，否则转到步骤b。与现有技术相比，本发明所达到的有益效果：本发明所述方法，根据所应用的场景的不同业务之间的特征以及对资源需求的差异性对业务进行分类，提出一种利用Q-Learning方法将不同的业务接入到已经定义的合适的无线网络切片中，定义状态空间为业务的资源需求和切片中基站剩余可用资源，动作空间为切片的选择，包括接入几类切片和具体接入哪几类切片，奖励函数综合了接入该业务所需的时延和能耗，实现了最大化业务的满意度；同时，针对业务请求的不断变化带来的每类切片上资源的需求也相应动态变化，难以保证用户的服务体验的问题，提出一种基于DDPG方法的动态资源配置方案，定义的状态空间包括基站的数量、基站剩余可用资源以及新的业务请求的资源需求，动作空间为对切片中基站的动作，包括基站数量的增减和基站容量的扩缩操作，奖励函数为执行动作的总效用，即切片资源利用率和业务接受率的综合，根据不同的业务资源需求，灵活动态地进行定制化网络资源分配，降低资源浪费，保障用户服务体验和提高网络资源利用率。附图说明图1是本发明实施例提供的一种基于深度强化学习的网络业务接入和切片资源配置方法的基本流程示意图；图2是本发明实施例提供的一种基于深度强化学习的网络业务接入和切片资源配置方法的详细流程示意图；图3是本发明实施例提供的智慧工厂中的无线接入网切片CU-DU资源调度策略架构；图4是本发明实施例所述的Q-Learning模型网络训练的流程图；图5是本发明实施例所述的DDPG模型网络训练的流程图。实施方式下面通过附图以及具体实施例对本发明技术方案做详细的说明，应当理解本申请实施例以及实施例中的具体特征是对本申请技术方案的详细的说明，而不是对本申请技术方案的限定，在不冲突的情况下，本申请实施例以及实施例中的技术特征可以相互组合。实施例本发明的目的是提供一种基于深度强化学习的网络切片业务接入策略和资源配置方法，比较全面地考虑了各类业务的时延差异，有效提高了用户服务质量和网络吞吐量，改善了资源严重浪费的问题。在该模型中对智慧工厂中的无线接入网中的业务进行切片划分，为不同的业务提供基于Q学习的最优切片无线接入网算法，并为不同的切片根据业务资源的变化提供基于DDPG的最佳的切片资源分配算法，其基本流程如图1和图2所示。本流程图仅仅示出了本实施例所述方法的逻辑顺序，在互不冲突的前提下，在本发明其它可能的实施例中，可以以不同于图1所示的顺序完成所示出或描述的步骤。本实施例提供一种结合地面和非地面通信的基于深度强化学习的智慧工厂网络业务接入和切片资源配置方法，参见图1和图2，本实施例所述方法具体包括如下步骤：S1: 将定义好的网络流量切片与事先分类的业务相关联初始化网络资源和用户业务请求，由于不同的业务的QoS、优先级、对时延敏感度、可靠性、吞吐量以及通信、计算、存储资源等等需求不同，以时延敏感度、带宽、可靠性为参数指标，按照不同需求划分四类业务：自动化和远程控制类、高清视频业务类、实时采集类、车联网类。为满足对平台资源、链路带宽以及网络时延和可靠性等指标的差异化需求，根据流量特征，定义六类网络流量切片，如图3所示：切片一：工业自动化、远程控制、应急系统、告警业务等需要低延时/高可靠连接的业务，包括物料自动领用、半成品自动周转、成品自动入库，打造无人分拣、智能搬运的智慧仓储作业系统以及对设备停机、传送带卡料、产品积压、员工离岗等异常情况的预警推送。切片二：3D/超高清视频等大流量增强移动宽带业务，包括机器显示器，管理员监控高清视频业务等。切片三：对延迟敏感度较低且吞吐量要求相对较低的应用程序。例如手机终端、可穿戴设备、环境监测、物流过程管控，对物流车辆的实时地理位置与行车轨迹数据进行实时采集等。切片四：无人驾驶、包括无人小车，车与车之间、车与路边设施、车与互联网之间的相互通信业务。切片五：非地面通信，无法建设基站和基站损坏时的应急通信，或者在其他切片业务饱和时，为优先级不高且时延要求很低的业务提供通信资源。切片六：其他，用于接入基于场景不确定性出现的新型业务类型。切片一至切片四分别关联智慧工厂中的四类业务。S2：根据业务的时延和能耗分配切片的通信、计算和存储资源，建立优化目标根据请求的业务数据，不同业务所需的计算任务量和数据传输量大小以及每类切片的资源多少，计算每个业务的时延和能耗，建立以网络全局请求满意度为目标的优化函数，该满意度定义为业务请求接入的时延和能耗的加权和。具体包括：在智慧工厂场景下，定义各切片的效用，以及切片业务的优先级，每个请求有i个业务，在某一时刻有M个请求，请求业务集合为，表示请求m是否存在第i类业务，其中，0表示不存在，1表示存在；共有/＞个i类业务资源，请求m的每个业务需要执行计算任务/＞，传输大小为/＞的业务内容，时延最低要求为/＞；计算业务的时延为：；计算业务的能量消耗为：；其中： 为第i类业务分配的带宽资源，/＞为第i类业务分配的计算资源，/＞为第i类业务分配的存储资源；/＞，为第i类业务对应内容的缓存指示变量，取值为1时，表示对应请求的内容已经存储在基站中，无需处理可直接传输获得；反之则表示没有存储在基站中，则需要计算业务时延和能量消耗；P为DU基站发射功率，/＞为路径损耗；/＞表示噪声功率；因此，优化目标为min ；s.t C1: ；/＞C2: ；C3: ；C4: ；C5: ；其中，和/＞分别是不同类型业务时延和能耗对网络性能的影响因子，C1、C2、C3分别是存储空间、带宽、计算资源分配的约束条件，C4是每一类业务的缓存存储的约束，C5是不同业务类型的时延约束条件。S3：将优化目标建模成马尔科夫决策过程，利用训练好的Q-Learning模型求解得到最优切片接入策略将业务的切片接入选择问题建模成马尔可夫决策问题，马尔科夫决策过程包括：agent：需要请求接入基站的新用户；t时刻，第k个代理的状态空间表示为：；其中：表示t时刻切片l中基站的剩余可用资源，/＞表示业务请求的资源；代理采取的动作为：；其中：表示将业务接入到几个切片中，/＞，其中表示是否将业务接入到切片中；代理获得的奖励为：；其中：和/＞分别是不同类型业务时延和能耗对网络性能的影响因子，/＞和/＞分别表示业务i的时延和能耗。根据状态、动作和奖励，利用训练好的Q-Learning模型求得最优的业务选择切片接入方案。Q-Learning模型的训练方法如图4所示，具体过程为：a. 根据划分的状态情况及可选取动作建立Reward-Table，从动作空间中选取一种动作；b. 初始化Q值表，Q值表与Reward-Table同阶，Q←0；c. 对应Reward-Table，根据不同状态任意选取动作，更新Q值表，Q值的更新公式如下：；其中：为节点在状态/＞下采取动作/＞后可获得的期望最大收益， R为立即获得的收益，从Reward-Table中获得；/＞，/＞为学习率，用于决定本次的误差有多少是要被学习的； /＞为折扣因子，用于确定延迟回报与即时回报的相对比例；d. 继续从出发，若未达到目标状态，则进行下一步更新；e. 如果算法未达到周期数，则转入步骤a进行下一个场景，否则结束训练，得到训练完毕的收敛Q表。本发明在网络切片的业务接入模块，设计了动作空间、状态空间以及奖励函数，当新用户的请求到达时，观察当前业务所处环境，根据业务类型、切片资源状态环境，以及已经训练好的Q表自主选择最优切片，即新用户的每一个业务具体接入到哪些切片中，选择的切片能够满足业务的时延要求，且综合能耗最低，最终输出当前动作，即获得新用户各业务的最佳切片接入策略，降低了能量损耗，又满足了时延要求。S4：基于最优切片接入策略，将切片资源分配优化问题建模为马尔科夫决策过程，利用训练好的DDPG模型得到切片的最优资源配置策略S41：获取网络中每个切片的资源状态信息，初始化系统的资源环境，构成环境的状态参数根据第一方面最终确定的切片接入策略，获取系统的资源状态信息，包括业务需求信息和切片资源信息；对于任意第i类切片下：业务数量为，业务所需的资源为：；其中，为带宽资源，/＞为计算资源，为存储资源；切片中的基站数量为L，切片上各基站可分配的带宽资源为/＞，计算资源为/＞，存储资源为/＞。联合考虑业务的资源需求以及切片内基站的可分配通信、计算、存储资源，以实现系统的最佳资源配置方法。S42：构建切片资源分配优化问题的马尔科夫决策过程，对DDPG模型进行离线训练定义深度强化学习的状态空间为切片的资源状态和业务的资源需求，代理为切片本身；t时刻，第k个代理的状态空间表示为：；其中：表示切片l中DU基站剩余可用资源，/＞表示业务的资源需求，L表示DU基站数量；定义t时刻，第k个代理采取的动作空间为：，/＞表示t时刻对切片做出的动作，包括接受和拒绝业务请求，对基站的扩容、缩容，增加基站的数量；定义t时刻，第k个代理对应获得的奖励为执行动作时带来的系统的总效用，即M个切片的效用总合，综合切片的资源利用率和切片的业务接受率作为切片的效用表示为：；/＞；其中：为切片i的效用，/＞、/＞均表示权重，/＞、/＞、/＞；/＞表示第i类切片分配给业务n的资源，/＞表示切片接受的业务数，N表示总的业务请求数，/＞表示每个基站的资源。S43：利用训练好的DDPG模型为每个切片提供最优资源分配方案所述DDPG模型包括Actor当前网络、Actor目标网络、Critic当前网络、Critic目标网络；所述Actor当前网络：实现深度强化学习模型参数的更新，其根据当前状态/＞选择动作/＞，与环境交互后得到下一个状态/＞和环境反馈的奖励值/＞；所述Actor目标网络：从经验回收池中采样下一个状态，选择下一个最优动作，并且定期将Actor当前网络参数/＞复制给/＞；所述Critic当前网络：负责价值网络参数的迭代更新，负责计算当前Q值；所述Critic目标网络：负责协助计算目标Q值，并且定期将Critic当前网络参数/＞复制给/＞。所述的DDPG从当前网络到目标网络的复制是每次参数只更新一点：；；其中，表示学习速率。所述DDPG模型的训练方法如图5所示，具体步骤包括：初始化Actor当前网络和Critic当前网络/＞，权重分别为/＞和/＞；初始化Actor目标网络和Critic目标网络/＞，并复制权重/＞，；初始化经验缓冲池D的大小为N；对于每个回合：a. 初始化动作探索的随机过程和状态/＞；b.对于每个步长：根据当前策略和和噪声随机选择一个动作；c. 执行动作，环境反馈即时奖励/＞和下一时刻状态/＞；d. 添加到经验缓冲池D中；e. 从D中随机抽取一个小批量的；f. 计算实际的Q值：，/＞为衰减因子；/＞g. 使用损失函数，更新Critic当前网络参数/＞；h. 使用，更新Actor当前网络参数/＞；i. 平稳更新Critic目标网络和Actor目标网络参数：；；j. 如果当前状态为终止状态，则当前回合迭代完毕，否则转到步骤b。本发明在切片资源分配模块，以业务接受率和资源利用率综合最大化为目标，设计了状态空间、动作空间和奖励函数，当新的业务请求到达，切片内基站的资源配置需要动态变化，观察当前切片环境，根据网络切片资源状态和业务的资源需求，以及已经训练好的DDPG模型，保障全局业务接受率的基础上，输出当前网络切片的最优资源配置策略，即针对每一个切片，对基站做出扩容缩容获关闭基站等动作，提高资源利用率。本领域内的技术人员应明白，本申请的实施例可提供为方法、系统、或计算机程序产品。因此，本申请可采用完全硬件实施例、完全软件实施例、或结合软件和硬件方面的实施例的形式。而且，本申请可采用在一个或多个其中包含有计算机可用程序代码的计算机可用存储介质上实施的计算机程序产品的形式。本申请是参照根据本申请实施例的方法、设备、和计算机程序产品的流程图和／或方框图来描述的。应理解可由计算机程序指令实现流程图和／或方框图中的每一流程和／或方框、以及流程图和／或方框图中的流程和／或方框的结合。可提供这些计算机程序指令到通用计算机、专用计算机、嵌入式处理机或其他可编程数据处理设备的处理器以产生一个机器，使得通过计算机或其他可编程数据处理设备的处理器执行的指令产生用于实现在流程图一个流程或多个流程和／或方框图一个方框或多个方框中指定的功能的装置。这些计算机程序指令也可存储在能引导计算机或其他可编程数据处理设备以特定方式工作的计算机可读存储器中，使得存储在该计算机可读存储器中的指令产生包括指令装置的制造品，该指令装置实现在流程图一个流程或多个流程和／或方框图一个方框或多个方框中指定的功能。这些计算机程序指令也可装载到计算机或其他可编程数据处理设备上，使得在计算机或其他可编程设备上执行一系列操作步骤以产生计算机实现的处理，从而在计算机或其他可编程设备上执行的指令提供用于实现在流程图一个流程或多个流程和／或方框图一个方框或多个方框中指定的功能的步骤。以上所述仅是本发明的优选实施方式，应当指出，对于本技术领域的普通技术人员来说，在不脱离本发明技术原理的前提下，还可以做出若干改进和变形，这些改进和变形也应视为本发明的保护范围。
