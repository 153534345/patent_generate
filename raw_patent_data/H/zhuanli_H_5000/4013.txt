标题title
一种基于多策略强化学习的多目标内容存储方法
摘要abst
本发明公开一种基于多策略强化学习的多目标内容存储方法，包括以下步骤：步骤S1：提出基于视频文件传输的协同缓存无线网络体系结构，定义了异构无线基站的状态空间和动作空间；步骤S2：动态内容缓存更新算法应用于各智能体，实现计算资源利用率最大化。步骤S3：利用权重映射网络来确定对于卸载流量的偏好权重，以帮助智能体实现策略选择过程，同时在协调器中引入混合网络来捕获各个智能体的信息，训练全局策略更新参数，并将结果反馈给各个无线基站进行各智能体的局部策略更新。本技术方案能够通过多策略方法学习一系列备选策略，为网络实现当前流量和视频质量之间的权衡。
权利要求书clms
1.一种基于多策略强化学习的多目标内容存储方法，其特征在于，包括如下步骤：步骤S1：提出一种半分布式协同缓存的视频传输系统，在视频传输系统中定义各个功能不同的基站各自的状态空间和动作空间，联合状态空间和联合动作空间，以及基于优化目标设计的奖励函数，实现卸载后的视频流量与用户视频体验之间的帕累托最优性；步骤S2：设计符合应用场景的多智能体深度强化学习算法，将其应用于每个无线基站的缓存策略决策中；提出一种基于D3QN的动态内容缓存更新算法应用于各智能体，每个智能体利用两个D3QN网络实现对卸载流量和用户体验质量的估计，并且最终通过协调器接收全局策略更新参数来迭代更新神经网络的参数以使全局收敛；其中D3QN网络采用优势函数，通过目标Q值选择的动作来选择目标Q值；基于D3QN的动态内容缓存更新算法如算法一；其中，算法一：初始化所有智能体中的D3QN网络；初始化强化学习中的所有参数；在时隙t＝0；在智能体m＝1；从环境中得到状态，并观察；以1-ε的概率使用基于Hypervolume的策略选择动作，或以ε的概率随机选择动作；将当前状态、最优动作以及Q值，目标Q值传输给边缘服务器；计算当前的奖励；存储当前时刻全局状态、动作、奖励以及下一时刻的全局状态；将当前时刻的全局状态输入到协调器中，并得到一个总Q值；从经验回放库中采集样本，并与得到的总Q值计算损失函数，然后作梯度下降；将梯度结果回传给各个智能体中来更新它们的评估网络；每过一段时隙用所有评估网络的参数更新其相应的目标网络；步骤S3：构建基于QMIX结构的半分布式多智能体系统，QMIX结构包含有一个混合网络，这个混合网络部署在协调器来汇总全局信息，其中混合网络中包含有超网络生成网络中间层神经元的权重和偏置，协调器首先计算系统奖励，然后将每个智能体的动作价值函数作为输入，利用混合网络计算全局策略更新参数，最后将结果反馈给各个无线基站进行各智能体的局部策略更新。2.根据权利要求1所述的一种基于多策略强化学习的多目标内容存储方法，其特征在于：首先，表示为一个簇内的基站集合，其中0代表边缘服务器即协调器，表示归属于无线基站m的用户集，文件集合质量最高层数L，二进制变量δmvl表示无线基站m是否缓存第v个视频的第l层；另外还给出单位接入时延ω1，ω2，ω3和ω4分别表示本不同传输路径的传输损耗，同时，还定义了用户请求视频变量和服务质量变量puv。3.根据权利要求2所述的一种基于多策略强化学习的多目标内容存储方法，其特征在于：构建缓存模型的性能指标，包括所减少的视频传输损耗o1和用户体验质量o2；并基于这两个目标优化问题构建最终优化目标，即奖励函数，还定义用户请求变量，用户请求质量还有无线基站缓存变量为状态空间，不同的基站由于设备功能性的差异状态空间会不一样，同时定义下一个时刻的无线缓存策略为个个智能体的动作。4.根据权利要求1所述的一种基于多策略强化学习的多目标内容存储方法，其特征在于：利用基于多目标的两个D3QN网络来分析关于用户的请求信息或当前缓存信息，将其作为深度强化学习算法的状态空间，来评估当前各缓存决策对于两个目标的性能表现，每个无线基站在下一个时间周期对缓存内容和对用户服务质量的决策，然后将所有信息打包发送给协调器进行统一的汇总，并获取新的网络更新参数。5.根据权利要求4所述的一种基于多策略强化学习的多目标内容存储方法，其特征在于：获取新的网络更新参数具体可以通过如下方式获取：1)在无线网络中针对每个异构的无线基站定义出它们各自的状态空间及动作空间，采用深度强化学习的的方法，通过定义动作价值函数，进行网络参数的不断迭代，最终每个无线基站能够得到各个不同状态下的最优缓存策略的帕累托集合；2)D3QN网络额外使用双网络机制来稳定，双网络机制采取一个延迟更新的结构完全一致的神经网络来提升算法稳定性；确保在不同的用户视频点播需求下都有最优的缓存决策实现，通过利用多个独立的D3QN网络来实现对视频传输系统不同目标的评估，不同于传统的单个网络的动作选择，额外提出一种基于Hypervolume的动作选择机制来选取多目标下的执行动作；这种动作选择机制通过评估Hypervolume的值来比较每个不同缓存决策对于帕累托前沿是否具有正向的贡献，从而筛选出对于前沿贡献最大的动作，执行该动作，并依靠这样的迭代实现最终对于帕累托前沿的逼近和收敛。6.根据权利要求5所述的一种基于多策略强化学习的多目标内容存储方法，其特征在于：D3QN网络分别进行对两个目标的动作价值函数评估，其最后输出的动作值函数向量表示为协调器首先收集所有智能体的状态和奖励构建联合状态和联合动作并从中然后计算出整个系统的奖励值，此外，协调器还通过混合网络计算全局动作价值函数其中elu表示激活函数，μ,η分别表示混合网络产生的权重和偏置，同理，也易于获得。7.根据权利要求6所述的一种基于多策略强化学习的多目标内容存储方法，其特征在于：集合奖励函数构建一个损失函数以计算全局策略更新参数，训练得到的全局策略更新参数可以反向传递回无线基站群内的各个无线基站，以便于他们针对自身的神经网络用进行更新，得到更好的策略。8.根据权利要求6所述的一种基于多策略强化学习的多目标内容存储方法，其特征在于：引入一个协调器，用来指挥智能体进行策略选取，并收集不同无线基站的状态和动作，以此计算出整个视频传输系统的奖励，协调器中部署有混合网络，其包含有超网络生成网络中间层神经元的权重和偏差，同时收集各个无线基站策略执行之后的动作价值函数，进而构建损失函数计算出全局策略更新参数，并将全局策略更新参数返回给各个无线基站进行决策网络的更新。9.根据权利要求5所述的一种基于多策略强化学习的多目标内容存储方法，其特征在于：引入基于hypervolumn的动作选择机制，其中定义动作Q值向量中非支配的Q向量为帕累托集合，其中表示非支配关系，并且我们针对这种非支配Q向量组成的帕累托集合，定义它的hypervolumn指标为其中Λ应表示勒贝格测度，因此最优动作选择是选择让hypervolume指标最大的动作，在状态s'下，我们有10.根据权利要求8所述的一种基于多策略强化学习的多目标内容存储方法，其特征在于：在协调器中引入一个权重映射网络来反应当前流量下对第一个目标，即卸载传输流量的偏好，该网络以当前系统流量为输入，以第一个目标的权重为输出，其中其中k和k'分别表示当前系统流量和网络流量阈值。
说明书desc
技术领域本发明涉及无线通信领域和计算机技术领域，特别是一种基于多策略强化学习的多目标内容存储方法。背景技术随着5G的商业化，移动数据流量会迎来急剧增长，尤其是视频数据流量随着移动无线通信、视频数据需求呈指数级增长，而边缘计算服务器性能的不断提高，也使得实时视频点播服务服务在5G通信网络中逐渐成为主要业务。为了满足不同用户对视频质量的要求，快速适应无线网络的波动，可伸缩视频编码作为H.265标准的一部分已经成视频编码的有力候选，在无线基站中缓存用户所需的视频文件，被认为是另一种流量解决方案，更有效地利用有限的缓存存储来满足用户的各种视频需求。为了捕捉用户请求内容和无线环境的动态特性，策略决策算法框架被引入无线缓存领域，其中深度强化学习结合了深度神经网络和强化学习学习，在解决复杂控制问题方面表现出了优异的性能，此外，由于大规模无线基站的布局，如何通过多个无线基站间的协作提高无线网络的整体服务性能得到了越来越多的关注。发明内容有鉴于此，本发明的目的在于提供一种基于多策略强化学习的多目标内容存储方法，提出利用无线网络的协调器嵌入混合网络计算的全局策略更新参数，传递给各个无线基站进行各智能体的局部策略更新；通过神经网络参数的不断迭代更新，来使得所生成的帕累托前沿的策略集合越来越准确，从而通过权重映射网络来得到全局最佳的缓存策略。为实现上述目的，本发明采用如下技术方案：一种基于多策略强化学习的多目标内容存储方法，包括如下步骤：步骤S1：提出一种半分布式协同缓存的视频传输系统，在视频传输系统中定义各个功能不同的基站各自的状态空间和动作空间，联合状态空间和联合动作空间，以及基于优化目标设计的奖励函数，实现卸载后的视频流量与用户视频体验之间的帕累托最优性；步骤S2：设计符合应用场景的多智能体深度强化学习算法，将其应用于每个无线基站的缓存策略决策中；提出一种基于D3QN的动态内容缓存更新算法应用于各智能体，每个智能体利用两个D3QN网络实现对卸载流量和用户体验质量的估计，并且最终通过协调器接收全局策略更新参数来迭代更新神经网络的参数以使全局收敛；其中D3QN网络采用优势函数，通过目标Q值选择的动作来选择目标Q值；基于D3QN的动态内容缓存更新算法如算法一；其中，算法一：初始化所有智能体中的D3QN网络；初始化强化学习中的所有参数；在时隙t＝0；在智能体m＝1；从环境中得到状态，并观察；以1-ε的概率使用基于Hypervolume的策略选择动作，或以ε的概率随机选择动作；将当前状态、最优动作以及Q值，目标Q值传输给边缘服务器；计算当前的奖励；存储当前时刻全局状态、动作、奖励以及下一时刻的全局状态；将当前时刻的全局状态输入到协调器中，并得到一个总Q值；从经验回放库中采集样本，并与得到的总Q值计算损失函数，然后作梯度下降；将梯度结果回传给各个智能体中来更新它们的评估网络；每过一段时隙用所有评估网络的参数更新其相应的目标网络。步骤S3：构建基于QMIX结构的半分布式多智能体系统，QMIX结构包含有一个混合网络，这个混合网络部署在协调器来汇总全局信息，其中混合网络中包含有超网络生成网络中间层神经元的权重和偏置，协调器首先计算系统奖励，然后将每个智能体的动作价值函数作为输入，利用混合网络计算全局策略更新参数，最后将结果反馈给各个无线基站进行各智能体的局部策略更新。在一较佳的实施例中：首先，表示为一个簇内的基站集合，其中0代表边缘服务器即协调器，表示归属于无线基站m的用户集，文件集合质量最高层数L，二进制变量δmvl表示无线基站m是否缓存第v个视频的第l层；另外还给出单位接入时延ω1，ω2，ω3和ω4分别表示本不同传输路径的传输损耗，同时，还定义了用户请求视频变量和服务质量变量puv。在一较佳的实施例中：构建缓存模型的性能指标，包括所减少的视频传输损耗o1和用户体验质量o2；并基于这两个目标优化问题构建最终优化目标，即奖励函数，还定义用户请求变量，用户请求质量还有无线基站缓存变量为状态空间，不同的基站由于设备功能性的差异状态空间会不一样，同时定义下一个时刻的无线缓存策略为个个智能体的动作。在一较佳的实施例中：利用基于多目标的两个D3QN网络来分析关于用户的请求信息或当前缓存信息，将其作为深度强化学习算法的状态空间，来评估当前各缓存决策对于两个目标的性能表现，每个无线基站在下一个时间周期对缓存内容和对用户服务质量的决策，然后将所有信息打包发送给协调器进行统一的汇总，并获取新的网络更新参数。在一较佳的实施例中：获取新的网络更新参数具体可以通过如下方式获取：1)在无线网络中针对每个异构的无线基站定义出它们各自的状态空间及动作空间，采用深度强化学习的的方法，通过定义动作价值函数，进行网络参数的不断迭代，最终每个无线基站能够得到各个不同状态下的最优缓存策略的帕累托集合；2)D3QN网络额外使用双网络机制来稳定，双网络机制采取一个延迟更新的结构完全一致的神经网络来提升算法稳定性；确保在不同的用户视频点播需求下都有最优的缓存决策实现，通过利用多个独立的D3QN网络来实现对视频传输系统不同目标的评估，不同于传统的单个网络的动作选择，额外提出一种基于Hypervolume的动作选择机制来选取多目标下的执行动作；这种动作选择机制通过评估Hypervolume的值来比较每个不同缓存决策对于帕累托前沿是否具有正向的贡献，从而筛选出对于前沿贡献最大的动作，执行该动作，并依靠这样的迭代实现最终对于帕累托前沿的逼近和收敛。在一较佳的实施例中：D3QN网络分别进行对两个目标的动作价值函数评估，其最后输出的动作值函数向量表示为协调器首先收集所有智能体的状态和奖励构建联合状态和联合动作并从中然后计算出整个系统的奖励值，此外，协调器还通过混合网络计算全局动作价值函数其中elu表示激活函数，μ,η分别表示混合网络产生的权重和偏置，同理，也易于获得。在一较佳的实施例中：集合奖励函数构建一个损失函数以计算全局策略更新参数，训练得到的全局策略更新参数可以反向传递回无线基站群内的各个无线基站，以便于他们针对自身的神经网络用进行更新，得到更好的策略。在一较佳的实施例中：引入一个协调器，用来指挥智能体进行策略选取，并收集不同无线基站的状态和动作，以此计算出整个视频传输系统的奖励，协调器中部署有混合网络，其包含有超网络生成网络中间层神经元的权重和偏差，同时收集各个无线基站策略执行之后的动作价值函数，进而构建损失函数计算出全局策略更新参数，并将全局策略更新参数返回给各个无线基站进行决策网络的更新。在一较佳的实施例中：引入基于hypervolumn的动作选择机制，其中定义动作Q值向量中非支配的Q向量为帕累托集合，其中表示非支配关系，并且我们针对这种非支配Q向量组成的帕累托集合，定义它的hypervolumn指标为其中Λ应表示勒贝格测度，因此最优动作选择是选择让hypervolume指标最大的动作，在状态s'下，我们有在一较佳的实施例中：在协调器中引入一个权重映射网络来反应当前流量下对第一个目标，即卸载传输流量的偏好，该网络以当前系统流量为输入，以第一个目标的权重为输出，其中其中k和k'分别表示当前系统流量和网络流量阈值。与现有技术相比，本发明具有以下有益效果：本发明提出的一种基于多策略强化学习的多目标内容存储方法，利用QMIX结构能够促进多智能体间的协作，同时利用动作选择机制和权重映射网络，解决了在真实无线网络环境中的多目标决策问题，从而提高了移动无线边缘缓存的服务能力。附图说明图1是本发明优选实施例中协同缓存无线网络体系结构示意图；图2是本发明优选实施例中视频传输系统缓存决策过程的示意图；图3是本发明优选实施例中不同算法的hypervolume对比图；图4是本发明优选实施例中的算法在不同文件数下的帕累托前沿对比图；图5是本发明优选实施例中的的算法在不同层数下的帕累托前沿对比图。具体实施方式下面结合附图及实施例对本发明做进一步说明。应该指出，以下详细说明都是例示性的，旨在对本申请提供进一步的说明。除非另有指明，本文使用的所有技术和科学术语具有与本申请所属技术领域的普通技术人员通常理解的相同含义。需要注意的是，这里所使用的术语仅是为了描述具体实施方式，而非意图限制根据本申请的示例性实施方式；如在这里所使用的，除非上下文另外明确指出，否则单数形式也意图包括复数形式，此外，还应当理解的是，当在本说明书中使用术语“包含”和/或“包括”时，其指明存在特征、步骤、操作、器件、组件和/或它们的组合。一种基于多策略强化学习的多目标内容存储方法，具体按照如下步骤实现，步骤S1：提出协同缓存无线网络体系结构，定义了各个智能体状态空间、动作空间和基于卸载流量和用户体验设计的奖励函数，目的是以最大限度地提升本地无线基站的服务质量；1)首先，表示为一个簇内的基站集合，其中0代表边缘服务器即协调器，表示归属于无线基站m的用户集，文件集合质量最高层数L，二进制变量δmvl表示无线基站m是否缓存了第v个视频的第l层。另外还给出了单位接入时延ω1，ω2，ω3和ω4分别表示本不同传输路径的传输损耗，同时，为了构造优化问题我们还定义了用户请求视频变量和服务质量变量puv。2)构建缓存模型的性能指标，包括所减少的视频传输损耗o1和用户体验质量o2如下，并基于这两个目标优化问题构建最终优化目标，也就是我们的奖励函数，此外我们还定义用户请求变量，用户请求质量还有无线基站缓存变量为状态空间，不同的基站由于设备功能性的差异状态空间会不一样，同时我们定义下一个时刻的无线缓存策略为个个智能体的动作。步骤S2：我们通过构建基于D3QN和QMIX结构的模型来做缓存决策并协调各个无线基站的协作，在协调器中，我们收集和分析关于各个无线基站的信息，并将智能体的动作价值函数作为混合网络的输入，输出为整个系统的全局动作价值函数以及全局策略更新参数，并将结果反馈回整个半分布式系统中，提升无线边缘缓存的协作性能；其中D3QN网络采用优势函数，使其在收集离散动作的数据后，能够更加准确的去估算Q值，选择更加合适的动作，并通过目标Q值选择的动作来选择目标Q值，从而消除Q值过高估计的问题。具体的基于D3QN的动态内容缓存更新算法如算法一；其中，算法一：基于QMIX的半分步式协作缓存的传输算法初始化所有智能体中的D3QN网络；初始化强化学习中的所有参数；在时隙t＝0；在智能体m＝1；从环境中得到状态，并观察；以1-ε的概率使用基于Hypervolume的策略选择动作，或以ε的概率随机选择动作；将当前状态、最优动作以及Q值，目标Q值传输给边缘服务器；计算当前的奖励；存储当前时刻全局状态、动作、奖励以及下一时刻的全局状态；将当前时刻的全局状态输入到协调器中，并得到一个总Q值；从经验回放库中采集样本，并与得到的总Q值计算损失函数，然后作梯度下降；将梯度结果回传给各个智能体中来更新它们的评估网络；每过一段时隙用所有评估网络的参数更新其相应的目标网络。1)我们利用两个独立的D3QN网络分别进行对两个目标的动作价值函数评估，其最后输出的动作值函数向量可以表示为为了无线基站间能够更好的协作，我们在协调器中引入了一个新的模块叫混合网络，协调器首先能收集所有智能体的状态和奖励构建联合状态和联合动作并从中然后计算出整个系统的奖励值，此外，协调器还能通过混合网络计算全局动作价值函数其中elu表示激活函数，μ,η分别表示混合网络产生的权重和偏置，同理，也易于获得。2)根据前面所计算出来的整个系统的全局动作价值函数，我们集合奖励函数构建了一个损失函数以计算全局策略更新参数，训练得到的全局策略更新参数可以反向传递回无线基站群内的各个无线基站，以便于他们针对自身的神经网络用进行更新，得到更好的策略，更新后的策略在协作性上和预测性上都会有更好的性能表现。步骤S3：考虑到无线网络的动态性，我们希望智能体能够在各种不同的网络状态下都选取最优策略，因此我们利用权重映射网络和动作选择机制来额外辅助智能体实现决策过程。由于引入双D3QN网络，我们的动作Q值变为以向量的形式存在，但是智能体训练过程中又需要保证选取其中一个最优动作，因此我们引入了基于hypervolumn的动作选择机制，其中我们定义动作Q值向量中非支配的Q向量为帕累托集合，其中表示非支配关系，并且我们针对这种非支配Q向量组成的帕累托集合，我们定义它的hypervolumn指标为其中Λ应表示勒贝格测度，因此我们的最优动作选择便是选择能让hypervolume指标最大的动作，例如在状态s'下，我们有有了训练过程中的动作选择机制，我们还需要对训练结束后的智能体的决策做出定义，因为每个时刻无线网络的环境都有可能发生变化，因此我们在协调器中引入一个权重映射网络来反应当前流量下对第一个目标，即卸载传输流量的偏好，该网络以当前系统流量为输入，以第一个目标的权重为输出，其中其中k和k'分别表示当前系统流量和网络流量阈值。为了让本领域技术人员进一步了解本发明所提出的一种基于多策略强化学习的多目标内容存储方法，下面结合具体实施例作详细说明。本实施例以本发明技术方案为前提进行实施。如图1所示，为协同缓存无线网络体系结构示意图。该模型主要有无线基站、协调器、远程视频服务器、核心网等组成，介绍了无线基站下的用户缓存模型，以及用户间协作模型，每个无线基站都可以通过回程链路在源服务器上下载文件，并在本地缓存文件，直接为小区内的用户服务。如图2所示，是视频传输系统缓存决策过程的示意图。由于无线网络的动态特性，流量和视频质量之间的权衡是不断波动的。因此，在动态网络中，传统的单一策略方法在没有事先权值知识的情况下无法很好地解决这一问题。因此，我们提出了一种多策略方法来学习一系列最优解决方案，然后根据当前的网络环境执行决策。如图3所示，是本发明实施例中不同算法的hypervolume对比图。我们将不同算法中多策略方法所逼近的帕累托策略集合所形成的hypervolume指标进行了对比，其中hypervolume指标表示帕累托前沿的所有点到参考点之间形成的超体积。实现展示我们的算法对比传统的VDN方法能够更加逼近帕累托前沿，形成更好的策略合集。如图4所示，是本发明实施例中的算法在不同文件数下的帕累托前沿对比图。通过实验结果我们可以得知，我们的算法不仅能够在不同的复杂度下都能够很好的找到一个全局策略集合，同时随着视频文件数的降低，我们的帕累托前沿合集能够获得更高的卸载流量和更高的用户体验质量。如图5所示，是本发明实例中的的算法在不同层数下的帕累托前沿对比图。对比于传统的多智能体算法，我们引入的多策略多目标强化学习方法在3视频层，4视频层和5视频层的情况下都能收敛到最优策略集合，这表示我们的算法能够在不同场景下都迅速定位一个全局最优策略。以上是本发明的较佳实施例，凡依本发明技术方案所作的改变，所产生的功能作用未超出本发明技术方案的范围时，均属于本发明的保护范围。
