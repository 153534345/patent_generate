标题title
一种基于多目标深度强化学习的主动配电网重构方法
摘要abst
本发明公开一种基于多目标深度强化学习的主动配电网重构方法，方法包括：步骤一、以整个重构周期内三相不平衡度最小化和开关动作总次数最少化为构建多目标优化模型；步骤二、基于各时段各开关的状态矩阵、各时段各节点各相有功功率矩阵和无功功率矩阵构造配电网重构智能体的状态空间与动作空间；步骤三、将三相电流总体不平衡度指标和开关总次数指标映射到归一化二维目标空间，并基于逼近理想解排序法的设计多目标奖励函数；步骤四、采用基于最大熵的软行动器‑评判器深度强化算法实现可均衡协调三相平衡效果和开关动作总次数的主动配电网多目标优化动态重构在线决策。填补了DRL方法应用在主动配电网多目标优化重构的空缺。
权利要求书clms
1.一种基于多目标深度强化学习的主动配电网重构方法，其特征在于，步骤如下：步骤一、以整个重构周期内三相不平衡度最小化和开关动作总次数最少化为目标函数构建多目标优化模型；步骤二、基于各时段各开关的状态矩阵、各时段各节点各相有功功率矩阵和无功功率矩阵构造配电网重构智能体的状态空间与动作空间，其中配电网重构智能体的状态空间定义为：，式中，为在t时段的各开关的状态矩阵，为在t时段各节点各相有功功率矩阵和，为在t时段各节点各相无功功率矩阵；配电网重构智能体的动作空间定义为：；步骤三、将三相电流总体不平衡度指标和开关总次数指标映射到归一化二维目标空间，并基于逼近理想解排序法设计多目标奖励函数；步骤四、采用基于最大熵的软行动器-评判器深度强化算法实现可均衡协调三相平衡效果和开关动作总次数的主动配电网多目标优化动态重构在线决策。2.根据权利要求1所述的一种基于多目标深度强化学习的主动配电网重构方法，其特征在于，在步骤一中，以重构周期内三相不平衡度最小化的的目标函数的表达式为：，式中，为配电变压器在t时刻的a相电流不平衡度，为配电变压器在t时刻的b相电流不平衡度，为配电变压器在t时刻的c相电流不平衡度，为三相电流总体不平衡度，为时间周期；以重构周期内开关动作总次数最少化的目标函数的表达式为：，式中，为开关在t时段的状态，为开关在t-1时段的状态，为配网中的可用开关总数。3.根据权利要求1所述的一种基于多目标深度强化学习的主动配电网重构方法，其特征在于，在步骤一中，所述多目标优化模型的约束包括节点电压约束、支路传输功率约束以及网络不满足辐射状网络拓扑约束，所述多目标优化模型的表达式为：，式中，为约束校核后的三相电流总体不平衡度，为约束校核后的开关动作总次数，为节点电压约束和支路传输功率不满足约束条件的越限惩罚项，为网络不满足辐射状网络拓扑约束条件的惩罚项，为三相电流总体不平衡度，为开关动作总次数；其中，构造节点电压约束和支路传输功率不满足约束条件的越限惩罚项为：，式中，和分别为网络中节点n的电压上下限，为网络中支路的传输功率上限，为t时刻网络中节点n的电压值，为t时刻网络中支路的传输功率，为网络节点总数，为支路总数，和均为惩罚因子，为时间周期；构造网络不满足辐射状网络拓扑约束条件的惩罚项为：，式中，为惩罚因子，rank为求矩阵的秩，Lp为矩阵。4.根据权利要求3所述的一种基于多目标深度强化学习的主动配电网重构方法，其特征在于，其中所述节点电压约束以及所述支路传输功率约束的表达式为：，式中，和分别为网络中节点n的电压上下限，为网络中支路的传输功率上限，为t时刻网络中节点n的电压值，为t时刻网络中支路的传输功率。5.根据权利要求1所述的一种基于多目标深度强化学习的主动配电网重构方法，其特征在于，在步骤三中，所述将三相电流总体不平衡度指标和开关总次数指标映射到归一化二维目标空间，并基于逼近理想解排序法设计多目标奖励函数包括：对动作的目标值进行归一化处理，并根据归一化处理结果分别计算动作的目标值在二维目标空间中与理想最优解的第一相对距离以及动作的目标值在二维目标空间中与理想最劣解的第二相对距离，其中，所述动作的目标值包括三相电流总体不平衡度以及开关动作总次数；基于所述第一相对距离以及所述第二相对距离构造与所述动作相对应的多目标奖励函数。6.根据权利要求5所述的一种基于多目标深度强化学习的主动配电网重构方法，其特征在于，所述动作在二维目标空间中的坐标为，所述理想最优解和所述理想最劣解在二维目标空间中的坐标分别为和，其中，所述二维目标空间中包含坐标系，所述坐标系的横坐标表示开关动作总次数，所述坐标系的纵坐标表示三相电流总体不平衡度；计算所述动作的目标值在二维目标空间中与理想最优解的第一相对距离的表达式为：，，式中，为归一化后的三相电流总体不平衡度，为归一化后的开关动作总次数，为三相电流总体不平衡度的理想最劣解，为三相电流总体不平衡度的理想最优解，为开关动作总次数的理想最劣解，为开关动作总次数的理想最优解；计算所述动作的目标值在二维目标空间中与理想最劣解的第二相对距离的表达式为：。7.根据权利要求1所述的一种基于多目标深度强化学习的主动配电网重构方法，其特征在于，在步骤三中，与所述动作相对应的多目标奖励函数的表达式为：，式中，为动作的目标值在二维目标空间中与理想最优解的第一相对距离，为动作的目标值在二维目标空间中与理想最劣解的第二相对距离。8.根据权利要求1所述的一种基于多目标深度强化学习的主动配电网重构方法，在步骤四中，最大熵的软行动器-评判器深度强化算法包括价值网络、动作-价值网络以及策略网络，其中所述价值网络的目标函数为：，式中，为t时刻下Q网络的目标函数，Q网络为价值网络，为Q网络的参数，为括号内的期望，为t时刻下Q网络的动作价值函数，折扣因子，为t+1时刻的价值函数，为系统t时刻的状态值，为系统t+1时刻的状态值，为t时刻交互的收益值。
说明书desc
技术领域本发明属于配电技术领域，尤其涉及一种基于多目标深度强化学习的主动配电网重构方法。背景技术配网重构作为配电网络优化运行的重要内容，可起到平衡负荷、消除过载和降低网损的作用。以往大多数配电网重构方面的文献是以最小化网络有功损耗为优化目标进行研究的，由于配网网损总量一般并不大，加之重构过程还会增加开关操作费用和人工费用等，因此通过配网重构降低网损可带来的经济效益往往非常有限。事实上，配电网运行中三相不平衡是长期以来亟待解决的突出问题。近年来随着单相或两相分布电源并入主动配电网的数量逐渐增多，这会加剧配电线路中三相不平衡。含分布式电源的配网三相不平衡常会引发三相电压不对称、中性线电流超限、配电变压器出力下降、线路损耗增加等诸多问题，而且在一定程度上会影响电网的安全稳定运行。因此，针对含分布式发电的主动配电网，通过配网重构以平衡三相负荷作用显得更为突出。配网重构一般可分为静态重构和动态重构，其中静态重构只是基于单个时段的配网负荷数据进行优化重构，而动态重构则是需要考虑多个连续时段内配网负荷数据的变化进行全局性优化重构。配网动态重构由于不仅考虑了网络的负荷波动特性，而且还需考虑供电可靠性、开关操作次数等实际运行中的约束条件，可实现有计划地进行网络重构，因此更具实用价值和研究意义。在实际应用中开关的最大动作次数不应主观设为定值，而应考虑开关操作的性价比和实际情况，综合降损效果、操作费用、使用寿命和线路供电可靠性要求等因素来灵活确定。因此，如何考虑其三相平衡效果和开关动作总次数的协调关系，实现多目标优化重构成为了目前亟需解决的问题。发明内容本发明提供一种基于多目标深度强化学习的主动配电网重构方法，用于解决如何考虑三相平衡效果和开关动作总次数的协调关系，实现多目标优化重构的技术问题。本发明提供一种基于多目标深度强化学习的主动配电网重构方法，步骤如下：步骤一、以整个重构周期内三相不平衡度最小化和开关动作总次数最少化为目标函数构建多目标优化模型；步骤二、基于各时段各开关的状态矩阵、各时段各节点各相有功功率矩阵和无功功率矩阵构造配电网重构智能体的状态空间与动作空间，其中配电网重构智能体的状态空间定义为：，式中，为在t时段的各开关的状态矩阵，为在t时段各节点各相有功功率矩阵和，为在t时段各节点各相无功功率矩阵；配电网重构智能体的动作空间定义为：；步骤三、将三相电流总体不平衡度指标和开关总次数指标映射到归一化二维目标空间，并基于逼近理想解排序法设计多目标奖励函数；步骤四、采用基于最大熵的软行动器-评判器深度强化算法实现可均衡协调三相平衡效果和开关动作总次数的主动配电网多目标优化动态重构在线决策。进一步地，在步骤一中，以重构周期内三相不平衡度最小化的的目标函数的表达式为：，式中，为配电变压器在t时刻的a相电流不平衡度，为配电变压器在t时刻的b相电流不平衡度，为配电变压器在t时刻的c相电流不平衡度，为三相电流总体不平衡度，为时间周期；以重构周期内开关动作总次数最少化的目标函数的表达式为：，式中，为开关在t时段的状态，为开关在t-1时段的状态，为配网中的可用开关总数。进一步地，在步骤一中，所述多目标优化模型的约束包括节点电压约束、支路传输功率约束以及网络不满足辐射状网络拓扑约束，所述多目标优化模型的表达式为：，式中，为约束校核后的三相电流总体不平衡度，为约束校核后的开关动作总次数，为节点电压约束和支路传输功率不满足约束条件的越限惩罚项，为网络不满足辐射状网络拓扑约束条件的惩罚项，为三相电流总体不平衡度，为开关动作总次数；其中，构造节点电压约束和支路传输功率不满足约束条件的越限惩罚项为：，式中，和分别为网络中节点n的电压上下限，为网络中支路的传输功率上限，为t时刻网络中节点n的电压值，为t时刻网络中支路的传输功率，为网络节点总数，为支路总数，和均为惩罚因子，为时间周期；构造网络不满足辐射状网络拓扑约束条件的惩罚项为：，式中，为惩罚因子，rank为求矩阵的秩，Lp为矩阵。进一步地，其中所述节点电压约束以及所述支路传输功率约束的表达式为：，式中，和分别为网络中节点n的电压上下限，为网络中支路的传输功率上限，为t时刻网络中节点n的电压值，为t时刻网络中支路的传输功率。进一步地，在步骤三中，所述将三相电流总体不平衡度指标和开关总次数指标映射到归一化二维目标空间，并基于逼近理想解排序法设计多目标奖励函数包括：对动作的目标值进行归一化处理，并根据归一化处理结果分别计算动作的目标值在二维目标空间中与理想最优解的第一相对距离以及动作的目标值在二维目标空间中与理想最劣解的第二相对距离，其中，所述动作的目标值包括三相电流总体不平衡度以及开关动作总次数；基于所述第一相对距离以及所述第二相对距离构造与所述动作相对应的多目标奖励函数。进一步地，所述动作在二维目标空间中的坐标为，所述理想最优解和所述理想最劣解在二维目标空间中的坐标分别为和，其中，所述二维目标空间中包含坐标系，所述坐标系的横坐标表示开关动作总次数，所述坐标系的纵坐标表示三相电流总体不平衡度；计算所述动作的目标值在二维目标空间中与理想最优解的第一相对距离的表达式为：，，式中，为归一化后的三相电流总体不平衡度，为归一化后的开关动作总次数， 为三相电流总体不平衡度的理想最劣解，为三相电流总体不平衡度的理想最优解，为开关动作总次数的理想最劣解，为开关动作总次数的理想最优解。计算所述动作的目标值在二维目标空间中与理想最劣解的第二相对距离的表达式为：。进一步地，在步骤三中，与所述动作相对应的多目标奖励函数的表达式为：，式中，为动作的目标值在二维目标空间中与理想最优解的第一相对距离，为动作的目标值在二维目标空间中与理想最劣解的第二相对距离。进一步地，在步骤四中，最大熵的软行动器-评判器深度强化算法包括价值网络、动作-价值网络以及策略网络，其中所述价值网络的目标函数为：，式中，为t时刻下Q网络的目标函数，Q网络为价值网络，为Q网络的参数，为括号内的期望，为t时刻下Q网络的动作价值函数，折扣因子，为t+1时刻的价值函数，为系统t时刻的状态值，为系统t+1时刻的状态值，为t时刻交互的收益值。本申请的基于多目标深度强化学习的主动配电网重构方法，基于逼近理想解排序法建立多目标奖励函数，并采用基于最大熵的软行动器-评判器，设计一种多目标强化学习算法，实现可均衡协调考虑三相平衡效果和开关动作总次数的主动配电网多目标优化动态重构。附图说明为了更清楚地说明本发明实施例的技术方案，下面将对实施例描述中所需要使用的附图作一简单地介绍，显而易见地，下面描述中的附图是本发明的一些实施例，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。图1为本发明一实施例提供的一种基于多目标深度强化学习的主动配电网重构方法的流程图；图2为本发明一具体实施例提供的归一化二维目标空间示意图；图3为本发明一具体实施例提供的一种基于多目标深度强化学习的主动配电网重构方法。具体实施方式为使本发明实施例的目的、技术方案和优点更加清楚，下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有作出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。请参阅图1，其示出了本申请的一种基于多目标深度强化学习的主动配电网重构方法的流程图。如图1所示，基于多目标深度强化学习的主动配电网重构方法具体包括以下步骤：步骤一、以整个重构周期内三相不平衡度最小化和开关动作总次数最少化为目标函数构建多目标优化模型。在本实施例中，在三相潮流计算中，一般可将含分布式电源的各节点类型都可处理成PQ节点，这样，单相、两相或者三相的分布式电源各类型节点可简化处理为对应相线上方向相反的单相、两相或者三相负荷。之后采用式所示的三相潮流计算获得配电网络的三相支路电流和三相节点电压，该算法处理三相不平衡能力强且运算速度快。，式中，为换相周期内的采样时段，、、分别为t时刻节点n的第相负荷功率共轭、t时刻节点n的第相电压共轭和t时刻节点n的第相注入电流，为t时刻支路的第相电流，为节点注入电流—支路电流关联矩阵，为支路电流—节点电压关联矩阵，为支路始端电压矩阵，为支路终端电压矩阵；需要说明的是，主动配电网动态重构需要考虑多个连续时段内各节点负荷数据的变化进行全局性优化，本发明以整个重构周期内三相不平衡度最小化和开关动作总次数最少化为构建多目标优化模型。以负荷曲线在t时刻时，变压器出口侧的三相不平衡电流作为衡量三相不平衡度的指标。三相电流不平衡度的计算公式如式所示：，式中，为配电变压器在t时刻的i相电流不平衡度，为t时刻变压器出口侧i相电流幅值，为t时刻变压器出口侧三相电流幅值的平均值，由该时刻每相电流相加后取平均可得到；因而配网重构的三相电流总体不平衡度的目标函数如式所示：，式中，为配电变压器在t时刻的a相电流不平衡度，为配电变压器在t时刻的b相电流不平衡度，为配电变压器在t时刻的c相电流不平衡度，为三相电流总体不平衡度，为时间周期；进一步地，若以表示开关k在t时段的状态，其值为0是代表打开，为1时代表闭合，则重构周期内开关动作总次数N最少化目标函数可表达如下：，式中，为开关在t时段的状态，为开关在t-1时段的状态，为配网中的可用开关总数。需要说明的是，为了保证配网安全运行，所得到的网络重构后t时段的各节点电压和各支路传输功率应满足如下约束条件：，式中，和分别为网络中节点n的电压上下限，为网络中支路的传输功率上限，为t时刻网络中节点n的电压值，为t时刻网络中支路的传输功率；上述节点电压和支路传输功率约束可通过施加越限惩罚进行处理，越限惩罚项构造如式所示：，式中，为网络节点总数，为支路总数，和均为惩罚因子；此外，重构后的网络还应满足辐射状网络拓扑约束条件。网络呈辐射状的一个简单前提条件为网络节点数必须正好等于支路数加1，即：，考虑到当网络重构后所包含的孤岛和环网数量正好相同时式也同样会满足，因此若要保证重构后网络为辐射状，在上述前提下同时还必须满足网络中所有节点都连通，即不能存在孤岛，这种情况下自然也不会有环网存在，否则将不满足式；在此应用图论中代数连通度来对网络节点的连通性进行判断。将配电网络看作一个简单图G，图中的点集V包含M个节点，边集E包含L条支路，图G可表示为：，式中，、分别代表点集数组中的元素、边集数组中的元素，下标分别代表第M个节点和第L条支路;根据各节点的邻接关系可构造图G的邻接矩阵A如式，当节点与相邻连接时，则矩阵中元素为1，否则为0。，式中，为邻接矩阵A中第M行第1列的元素， 为邻接矩阵A中第1行第M列的元素，为邻接矩阵A中第M行第M列的元素；进一步可构造出矩阵Lp：，式中，sum为求和，diag为提取对角矩阵。若配电网是连通的，则矩阵Lp只能有一个零特性值，即有：，式中，rank为求矩阵的秩。结合式和式即可构造网络不满足辐射状网络拓扑约束条件的惩罚项如式所示：， 式中，为惩罚因子；融合上述各式最终建立配电网多目标动态重构模型如下：， 式中，为约束校核后的三相电流总体不平衡度，为约束校核后的开关动作总次数，为节点电压约束和支路传输功率不满足约束条件的越限惩罚项，为网络不满足辐射状网络拓扑约束条件的惩罚项；由于添加了越限惩罚项，在寻优过程中，一旦解不满足约束条件，必然会导致其目标函数值显著增大，从而使得该解适应度极差，很快便会被淘汰。具体地，在将获取的当前时刻状态和当前重构策略输入至多目标优化模型中，使多目标优化模型输出当前时刻的动作的过程中，采用强化学习对多目标优化模型进行训练，其中强化学习的核心思想在于训练一个智能体，使它能在与系统的交互过程中不断学习，最终能够获得一个策略，这个策略能让智能体在交互过程中所获得的总收益值最大化。结合控制理论，智能体即执行机构，它执行算法生成的策略，同时，系统受智能体的动作影响，发生状态的转移，并反馈给智能体一个奖励信号，形成一条闭环的控制链路。假设智能体在的t时刻的动作为，系统t时刻状态值和系统t+1时刻的状态值分别为和，交互过程中的收益值为。步骤二、基于各时段各开关的状态矩阵、各时段各节点各相有功功率矩阵和无功功率矩阵构造配电网重构智能体的状态空间与动作空间。在本实施例中配电网重构智能体的状态空间定义为：， 式中，为在t时段的各开关的状态矩阵，为在t时段各节点各相有功功率矩阵和，为在t时段各节点各相无功功率矩阵；配电网重构智能体的动作空间定义为：。 步骤三、将三相电流总体不平衡度指标和开关总次数指标映射到归一化二维目标空间，并基于逼近理想解排序法设计多目标奖励函数。在本实施例中，配电网重构中的目标为最小化三相电流总体不平衡度和最少化开关次数，这两个优化目标无法做到单位上的统一，本发明在此提出基于TOPSIS法的多目标奖励函数设计。TOPSIS法通过评估各指标与最优、劣解的接近程度以实现求解多目标最优折衷解的目的。本发明中共有2个评估指标，三相电流总体不平衡度最小指标和开关总次数最小指标。则TOPSIS法的基本步骤如下：1）归一化处理：重构方案的两个目标值的量纲不同，需要首先按式进行归一化处理，然后可将各重构方案映射到如图2所示的归一化二维目标空间中。， 式中，为归一化后的三相电流总体不平衡度，为归一化后的开关动作总次数， 为三相电流总体不平衡度的理想最劣解，为三相电流总体不平衡度的理想最优解，为开关动作总次数的理想最劣解，为开关动作总次数的理想最优解；基于多目标优化模型的表达式，可得归一化后的最优解为和最劣解为，如图2所示。进一步可根据式计算出所得方案在二维目标空间中与理想最优解和理想最劣解的相对距离。，式中，为动作的目标值在二维目标空间中与理想最优解的第一相对距离，为动作的目标值在二维目标空间中与理想最劣解的第二相对距离；在二维目标空间中距离理想最优解越近，距离理想最劣解越远，说明所得方案越优，据此可基于式构造多目标奖励函数。用相对距离关系构造奖励函数，奖励函数是在强化学习算法中对智能体进行训练时用于计算其奖励值的。，式中，为奖励值，，r越接近1表明此方案越优。步骤四、采用基于最大熵的软行动器-评判器深度强化算法实现可均衡协调三相平衡效果和开关动作总次数的主动配电网多目标优化动态重构在线决策。在本实施例中，采用最大熵的SAC算法对多目标优化模型的网络参数进行配置。具体为：SAC算法的框架主要由3个网络组成，分别是价值网络、动作-价值网络、策略网络。其中，C网络和Q网络负责计算价值函数值和动作-价值函数值，而策略网络则输出策略值，用于指导智能体行动。具体地，在Q网络中，SAC算法与其他类型强化学习算法的唯一不同之处在于，引入了最大熵项。对于Q网络，SAC算法采用了均方差形式的目标函数：， 式中，为t时刻下Q网络的目标函数，为Q网络的参数，为括号内的期望，为t时刻下Q网络的动作价值函数，折扣因子，为t+1时刻的价值函数，为系统t时刻状态值，为系统t+1时刻的状态值，为t时刻交互的收益值。而对于C网络，SAC算法采用了类似的做法，其目标函数定义为：， 式中，为t时刻下C网络的目标函数，为C网络的参数；为在t时刻策略下对应动作Y时内的期望，为温度系数，为在系统t时刻状态值和动作的策略，为C网络的价值函数。对于网络，其目标函数定义为：， 式中，为t时刻下网络的目标函数，为网络的参数；为括号内前一项对后一项的相对熵，为使分布归一化的算子，为在系统t时刻状态值和动作的策略。在获得了以上三个网络的目标函数、和后，即可利用梯度下降的方式来更新其对应的网络参数。综上，本实施例的方法，基于逼近理想解排序法建立多目标奖励函数，并采用基于最大熵的软行动器-评判器，设计一种多目标强化学习算法，实现可均衡协调考虑三相平衡效果和开关动作总次数的主动配电网多目标优化动态重构。在一个具体的实施例中，如图3所示，一种基于多目标深度强化学习的主动配电网重构方法具体流程如下：1）收集数据；收集某个区域一个月的分布式电源出力历史数据。2）处理数据；将分布式电源出力进行PQ负荷节点等效。3）导入配电网基本数据；导入配电网网络参数、各节点负荷数据、各时段负荷需求等。4) 设置环境参数、算法参数和训练总次数Z。5) 当前训练次数z = 0，t = 0。6) 初始化状态，即获取0时刻各开关的开断状态和各节点各相的有功功率和无功功率。7) 将此时刻的状态输入到深度强化学习智能体中。8) 智能体根据当前状态和当前策略选择动作。9) 智能体将动作输入到环境中。10) 基于TOPSIS法进行多目标奖励函数计算。11) 获取下一个时刻的状态。12) 将奖励和下一个时刻的状态反馈给智能体，智能体由此更新策略。13) 若t＜24，重复步骤7）-步骤11)。否则训练次数z+1。14) 若训练次数＜训练总次数，重复步骤7）-步骤12)。否则终止训练。15) 训练完成，保存训练好的智能体。16) 实际应用过程中，输入当前时刻的状态到智能体中，可得到实时的均衡协调考虑三相平衡效果和开关动作总次数的主动配电网多目标优化动态重构方案。最后应说明的是：以上实施例仅用以说明本发明的技术方案，而非对其限制；尽管参照前述实施例对本发明进行了详细的说明，本领域的普通技术人员应当理解：其依然可以对前述各实施例所记载的技术方案进行修改，或者对其中部分技术特征进行等同替换；而这些修改或者替换，并不使相应技术方案的本质脱离本发明各实施例技术方案的精神和范围。
