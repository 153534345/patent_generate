标题title
一种基于神经网络的细粒度视频流自适应调节系统及方法
摘要abst
本发明涉及视频流播放调节领域，公开了一种基于神经网络的细粒度视频流自适应调节系统及方法，系统包括六个独立的神经网络和一个经验缓冲区；神经网络包括策略网络、目标策略网络、两个评价网络和两个目标评价网络；策略网络即ABR模型，用于从环境中获取状态，并输出动作，这些作为经验数据被保存到经验缓冲区中；评价网络用于评价网络用于评估策略网络做出的动作的价值；目标策略网络用于稳定训练性能；目标评价网络用于在训练过程更新评价网络。本发明通过新的基于学习的训练方法，使得训练出的ABR模型能够进行细粒度的比特率适应和长期决策规划，可以在整个视频会话中保持稳定的性能。
权利要求书clms
1.一种基于神经网络的细粒度视频流自适应调节系统，其特征在于，该系统即BETA系统，包括六个独立的神经网络和一个经验缓冲区；神经网络包括策略网络、目标策略网络、两个评价网络和两个目标评价网络；在这六个网络中，只有策略网络执行自适应比特率决策，其余五个网络都是训练辅助工具；所述策略网络即ABR模型，用于从环境中获取状态，并输出动作，这些作为经验数据被保存到经验缓冲区中；所述评价网络的输入包括环境状态和策略网络输出的动作，输出Q值，用于评价网络用于评估策略网络做出的动作的价值，采用两个评价网络用于避免对动作价值的高估；所述目标策略网络的输入为环境状态，输出目标动作，用于稳定训练性能；所述目标评价网络的输入包括环境状态和目标策略网络输出的目标动作，其输出用于计算目标Q值，用于在训练过程更新评价网络；所述经验缓冲区用于存放经验数据。2.根据权利要求1所述的一种基于神经网络的细粒度视频流自适应调节系统，其特征在于，所述策略网络包括输入层、隐藏层和输出层；所述输入层的输入包含五个环境状态，用以量化网络条件和流媒体上下文，分别为带宽、视频块下载时间、上一个选择的比特率、缓冲区大小和剩余视频块数量；所述隐藏层的第一层由两个128个神经元的卷积层和三个128个神经元的稠密层构成，用于从输入层接收所有环境状态，其中，带宽和视频块下载时间分别传递到两个卷积层，其余三个环境状态分别传递到三个稠密层；然后，隐藏层第一层的输出将被输入到隐藏层第二层，其为一个由256个神经元组成的稠密层；最后，由一个采用Tanh激活函数的稠密层构成的输出层输出动作，表示为at，其为连续值，范围从-1到+1，其中，t为视频块序列号。3.根据权利要求2所述的一种基于神经网络的细粒度视频流自适应调节系统，其特征在于，所述评价网络包括输入层、隐藏层和输出层，所述输入层的输入除了包含与策略网络相同的五个环境状态外，还有一个额外的输入，即由策略网络输出的动作at；所述隐藏层的第一层包含两个128个神经元的卷积层和四个128个神经元的稠密层，用于从输入层接收所有环境状态和动作，其中，带宽和视频块下载时间分别传递到两个卷积层，其余三个环境状态和动作分别传递到四个稠密层；然后，他们的输出被输入到隐藏层的第二层，一个由256个神经元构成的稠密层；最后，由一个线性的稠密层构成的输出层输出Q值，用于评价策略网络所作动作的价值，以在训练中促进神经网络更新。4.一种基于神经网络的细粒度视频流自适应调节方法，采用如权利要求1所述的自适应调节系统，其特征在于，包括如下步骤：步骤1，由BETA系统对ABR模型进行训练；步骤2，将训练好的模型部署在服务器上；步骤3，开始时对用户请求的第一个视频块选择一个默认比特率，并将此视频块通过网络传送给用户播放器，用户播放器播放此视频块；步骤4，传输完成后，收集环境中的状态信息，形成环境状态；步骤5，将环境状态输入给ABR模型，模型根据状态输出动作at，并将该动作映射为针对下一个视频块的比特率；步骤6，服务器根据模型决策的比特率，将原始视频在线转码成相应比特率的视频块，随后通过网络将其传送给用户播放器播放，再返回步骤4。5.根据权利要求4所述的一种基于神经网络的细粒度视频流自适应调节方法，其特征在于，步骤1中，ABR模型的训练过程如下：经验积累与随机经验采样：训练前，系统对所有超参数进行初始化，在训练中的每个轮次，对于视频会话中的视频块t，策略网络首先根据环境状态st输出动作at，然后视频块t在一个虚拟网络环境中进行服务器端到用户端的传输；传输完成后，系统收集奖励rt和视频结束信号dt，组成一个四元组，存储在经验缓冲区D中；随后，系统从经验缓冲区D中随机采样一小批次经验用于后续计算过程；计算目标Q值和Q值：目标Q值计算公式如下：；其中，为目标Q值，γ是折扣因子，rt’+k是第k步的奖励，Qtar1和Qtar2分别是两个目标评价网络输出的第n步Q值；关于Q值，需要将状态st和动作at同时输入到两个评价网络中，分别得到两个评价网络的输出Q1和Q2，即评价网络对当前状态st下策略网络做出动作at的价值分数；更新评价网络：基于Qtar 、Q1和Q2，通过计算均方时序差分误差来更新两个评价网络；通过调整两个评价网络的神经元权重θi=1,2，使评价网络输出的Q值接近目标Q；更新策略网络：策略网络通过梯度下降进行更新，以最大化期望的Q值，记为EΩ，其中，st’是视频块t’下的状态，πϕ是将st’代入策略网络中产生的动作值，Qθ1为评价网络；更新目标网络：基于最新的策略网络和评价网络，目标策略网络和两个目标评价网络通过EWMA策略进行更新；模型保存与训练结束：每经过设定数量的训练轮次，系统对模型进行保存并同时进行性能验证，当性能达到设定阈值后，模型训练完成。6.根据权利要求5所述的一种基于神经网络的细粒度视频流自适应调节方法，其特征在于，系统从经验缓冲区D中随机采样一小批次经验，记为Ω，其中包含j个元组序列，每个序列中有n个连续的元组：；；…；；其中，i表示n个连续元组的第i个，t’表示视频块序列号。7.根据权利要求5所述的一种基于神经网络的细粒度视频流自适应调节方法，其特征在于，更新评价网络时，系统通过执行确定性策略梯度来实现这一步，以最小化以下损失函数：；其中，Ω为随机采样的小批次经验，其中包含j个元组序列，每个序列中有n个连续的元组，函数EΩ为平均期望值。8.根据权利要求4所述的一种基于神经网络的细粒度视频流自适应调节方法，其特征在于，步骤4中，从环境中收集的状态信息包括：带宽、视频块下载时间、上一个选择的比特率、缓冲区大小和剩余视频块数量。9.根据权利要求4所述的一种基于神经网络的细粒度视频流自适应调节方法，其特征在于，步骤5中，为了将模型输出动作at映射到可用的编码比特率范围内，即，定义了一个线性动作映射策略：；其中，at’为视频块t的最终比特率决策，at为策略网络输出的动作，范围为。
说明书desc
技术领域本发明涉及视频流播放调节领域，特别涉及一种基于神经网络的细粒度视频流自适应调节系统及方法。背景技术近年来，移动视频流媒体技术经历了快速的发展，现已成为互联网上的主要应用之一。Cisco报告显示，从2017到2022年，全球流媒体数据流量增长了15倍，到2022年底，已占到互联网总流量的82%。但是由于无线网络信号的不稳定性，移动网络带宽时常发生较大波动，这对视频传输造成了巨大挑战。因此，流媒体服务商重视自适应比特率流系统的开发，以避免带宽波动造成的性能损失。ABR系统基于DASH协议，其核心是智能的ABR模型，根据历史环境测量结果动态调整视频画质，最终的目标是最大化用户体验质量。我们应用了大规模的移动网络跟踪数据来评估当前业界比较先进的ABR系统的性能。结果发现，实际达到的QoE远非最优，尤其是在网络条件较差且带宽剧烈波动的情况下。通过进一步的调查发现，此问题主要源于在有限的视频编码选择下进行粗粒度的比特率决策，比特率未能良好的匹配带宽波动，导致QoE显著下降。解决上述问题的直观方法是通过提供更多的候选比特率版本，执行细粒度的比特率自适应。然而，将这种方法应用于现有的ABR模型，结果不尽人意。具体来说，对于启发式算法，例如PSQA，随着候选比特率版本数量的增加，求解QoE最大化问题的计算复杂性呈指数级增长，这极大地增加了ABR决策所需的时间，从而导致更多的播放卡顿；对于基于机器学习的算法，例如Pensieve，在神经网络的输出端增加端口数似乎是可行方案。然而，基于离散动作的深度强化学习方法无法支持较大的动作空间，从而阻碍学习代理探索最优的ABR解决方案，导致训练结果性能不佳。综上，传统的ABR算法只能实现有限数量比特率的自适应，此类方法在网络状况较差或者带宽波动剧烈时，所决策比特率通常是次优的，因此严重限制了QoE性能。发明内容为解决上述技术问题，本发明提供了一种基于神经网络的细粒度视频流自适应调节系统及方法，通过基于连续动作控制的深度强化学习训练方法，训练出的ABR模型能够进行细粒度的比特率自适应并进行长远决策规划，使整个视频会话实现高用户体验质量并保持不同网络条件下的高鲁棒性。为达到上述目的，本发明的技术方案如下：一种基于神经网络的细粒度视频流自适应调节系统，该系统即BETA系统，包括六个独立的神经网络和一个经验缓冲区；神经网络包括策略网络、目标策略网络、两个评价网络和两个目标评价网络；在这六个网络中，只有策略网络执行自适应比特率决策，其余五个网络都是训练辅助工具；所述策略网络即ABR模型，用于从环境中获取状态，并输出动作，这些作为经验数据被保存到经验缓冲区中；所述评价网络的输入包括环境状态和策略网络输出的动作，输出Q值，用于评价网络用于评估策略网络做出的动作的价值，采用两个评价网络用于避免对动作价值的高估；所述目标策略网络的输入为环境状态，输出目标动作，用于稳定训练性能；所述目标评价网络的输入包括环境状态和目标策略网络输出的目标动作，其输出用于计算目标Q值，用于在训练过程更新评价网络；所述经验缓冲区用于存放经验数据。上述方案中，所述策略网络包括输入层、隐藏层和输出层；所述输入层的输入包含五个环境状态，用以量化网络条件和流媒体上下文，分别为带宽、视频块下载时间、上一个选择的比特率、缓冲区大小和剩余视频块数量；所述隐藏层的第一层由两个128个神经元的卷积层和三个128个神经元的稠密层构成，用于从输入层接收所有环境状态，其中，带宽和视频块下载时间分别传递到两个卷积层，其余三个环境状态分别传递到三个稠密层；然后，隐藏层第一层的输出将被输入到隐藏层第二层，其为一个由256个神经元组成的稠密层；最后，由一个采用Tanh激活函数的稠密层构成的输出层输出动作，表示为at，其为连续值，范围从-1到+1，其中，t为视频块序列号。上述方案中，所述评价网络包括输入层、隐藏层和输出层，所述输入层的输入除了包含与策略网络相同的五个环境状态外，还有一个额外的输入，即由策略网络输出的动作at；所述隐藏层的第一层包含两个128个神经元的卷积层和四个128个神经元的稠密层，用于从输入层接收所有环境状态和动作，其中，带宽和视频块下载时间分别传递到两个卷积层，其余三个环境状态和动作分别传递到四个稠密层；然后，他们的输出被输入到隐藏层的第二层，一个由256个神经元构成的稠密层；最后，由一个线性的稠密层构成的输出层输出Q值，用于评价策略网络所作动作的价值，以在训练中促进神经网络更新。一种基于神经网络的细粒度视频流自适应调节方法，采用如上所述的自适应调节系统，包括如下步骤：步骤1，由BETA系统对ABR模型进行训练；步骤2，将训练好的模型部署在服务器上；步骤3，开始时对用户请求的第一个视频块选择一个默认比特率，并将此视频块通过网络传送给用户播放器，用户播放器播放此视频块；步骤4，传输完成后，收集环境中的状态信息，形成环境状态；步骤5，将环境状态输入给ABR模型，模型根据状态输出动作at，并将该动作映射为针对下一个视频块的比特率；步骤6，服务器根据模型决策的比特率，将原始视频在线转码成相应比特率的视频块，随后通过网络将其传送给用户播放器播放，再返回步骤4。上述方案中，步骤1中，ABR模型的训练过程如下：经验积累与随机经验采样：训练前，系统对所有超参数进行初始化，在训练中的每个轮次，对于视频会话中的视频块t，策略网络首先根据环境状态st输出动作at，然后视频块t在一个虚拟网络环境中进行服务器端到用户端的传输；传输完成后，系统收集奖励rt和视频结束信号dt，组成一个四元组，存储在经验缓冲区D中；随后，系统从经验缓冲区D中随机采样一小批次经验用于后续计算过程；计算目标Q值和Q值：目标Q值计算公式如下：；其中，为目标Q值，γ是折扣因子，rt’+k是第k步的奖励，Qtar1和Qtar2分别是两个目标评价网络输出的第n步Q值。关于Q值，需要将状态st和动作at同时输入到两个评价网络中，分别得到两个评价网络的输出Q1和Q2，即评价网络对当前状态st下策略网络做出动作at的价值分数。更新评价网络：基于Qtar 、Q1和Q2，通过计算均方时序差分误差来更新两个评价网络；通过调整两个评价网络的神经元权重θi=1,2，使评价网络输出的Q值接近目标Q；更新策略网络：策略网络通过梯度下降进行更新，以最大化期望的Q值，记为EΩ，其中，st’是视频块t’下的状态，πϕ是将st’代入策略网络中产生的动作值，Qθ1为评价网络。更新目标网络：基于最新的策略网络和评价网络，目标策略网络和两个目标评价网络通过EWMA策略进行更新。模型保存与训练结束：每经过设定数量的训练轮次，系统对模型进行保存并同时进行性能验证，当性能达到设定阈值后，模型训练完成。上述方案中，系统从经验缓冲区D中随机采样一小批次经验，记为Ω，其中包含j个元组序列，每个序列中有n个连续的元组：；；…；；其中，i表示n个连续元组的第i个，t’表示视频块序列号。上述方案中，更新评价网络时，系统通过执行确定性策略梯度来实现，以最小化以下损失函数：；其中，Ω为随机采样的小批次经验，其中包含j个元组序列，每个序列中有n个连续的元组，函数EΩ为平均期望值。上述方案中，步骤4中，从环境中收集的状态信息包括：带宽、视频块下载时间、上一个选择的比特率、缓冲区大小和剩余视频块数量。上述方案中，步骤5中，为了将模型输出动作at映射到可用的编码比特率范围内，即，定义了一个线性动作映射策略：；其中，at’为视频块t的最终比特率决策，at为策略网络输出的动作，范围为。通过上述技术方案，本发明提供的一种基于神经网络的细粒度视频流自适应调节系统及方法具有如下有益效果：本发明提出了BETA系统，通过采用新型连续动作控制的深度强化学习训练方法，从一个全新的角度重新审视ABR模型的设计，即把比特率决策定义在神经网络的连续值动作域，因此，训练出的ABR模型能够进行细粒度的比特率调整。然后在流式传输过程中，服务器根据模型决策的比特率，将原始视频在线转码成相应比特率的视频块进行传输，使视频比特率可以最大限度地匹配高度变化的网络带宽。BETA系统极大地释放了 ABR 模型的性能潜力，实现了使整个视频会话的高用户体验质量并在不同网络条件下保持高鲁棒性。本发明公开了自适应比特率算法设计细节、模型训练过程、模型实施方法和结果评估，根据评估结果，BETA系统所决策的比特率可以很好的匹配高波动下的网络带宽并做出长远决策，从而在高波动网络条件下显著提高用户体验。附图说明为了更清楚地说明本发明实施例或现有技术中的技术方案，下面将对实施例或现有技术描述中所需要使用的附图作简单地介绍。图1为本发明的六个网络之间以及与经验缓冲区之间的关系；图2为本发明的策略网络和评价网络的结构细节；图3为模型训练流程图；图4为QoE各项指标的比较；其中低、中、高表示三个具有不同平均吞吐量的网络带宽集；为描绘用户体验质量QoE；为描绘视频画质，画质越高QoE越高；为描绘卡顿时间，此项为惩罚项，卡顿时间越长QoE越低，为描绘画质波动，此项为惩罚项，画质波动越大QoE越低；图5为在三种网络条件下QoE的累积分布函数图，为低带宽；为中带宽；为高带宽；图6为BETA、DDPG和TD3的训练QoE性能；图7为Pensieve所采用的A3C训练方法在不同输出端口数下的QoE性能。具体实施方式下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述。本发明提供了一种基于神经网络的细粒度视频流自适应调节系统，该系统即BETA系统，如图1所示，包括六个独立的神经网络和一个经验缓冲区；神经网络包括策略网络、目标策略网络、两个评价网络和两个目标评价网络。在这六个网络中，只有策略网络执行自适应比特率决策，即策略网络为ABR模型，其余五个网络都是训练辅助工具。其中，两个评价网络的输入包括策略网络的输出。两个目标评价网络的输入包括目标策略网络的输出。策略网络和目标策略网络的结构完全相同，评价网络和目标评价网络的结构完全相同。评价网络的存在是为了评估策略网络做出的动作的价值，采用双评价网络可以有效避免对动作价值的高估。目标策略网络和两个目标评价网络的存在是为了稳定训练性能。在训练中的每个轮次，策略网络不断从环境中获取状态，并输出动作。这些均被称为经验数据被保存到经验缓冲区中。随着训练的进行，经验缓冲区中的数据逐渐增多。随后，系统会从经验缓冲区中采样一小批次经验，其中包括连续n步的状态、动作、奖励值和视频结束信号，其中奖励值被用来计算n步折扣奖励。第n步的状态信息被输入到目标策略网络中产生目标动作，进而目标动作与第n步状态被输入到目标评价网络，输出第n步Q值。第1步状态与动作被输入到评价网络，输出第1步Q值。n步折扣奖励与第n步Q值的加权和即为目标Q值，其将与第1步Q值进行对比并计算损失值以更新评价网络。策略网络的更新仅基于第1步Q值。如图2所示，策略网络包括输入层、隐藏层和输出层；输入层包含五个环境状态，用以量化网络条件和流媒体环境，分别如下：带宽：下载过去m个视频块的TCP吞吐量。由向量＜t=0,…m-1＞表示，其中每个内部因子ct表示在下载视频块t期间的平均吞吐量；视频块下载时间：下载过去m个视频块所消耗的时间，由向量＜t=0,…m-1＞表示，其中每个内部因子dt表示下载视频块t的时间消耗；上一个选择的比特率：上一个下载的视频块的决策比特率值，该状态用于衡量相邻视频块的比特率波动程度；缓冲区大小：传输每个视频块之前时刻的客户端播放器缓冲区占用率，此状态可预警未来播放卡顿；剩余视频块数量：在当前视频会话中尚未下载的视频块数量。隐藏层的第一层由两个128个神经元的卷积层和三个128个神经元的稠密层构成，用于从输入层接收所有环境状态，其中，带宽和视频块下载时间分别传递到两个卷积层，其余三个环境状态分别传递到三个稠密层。然后，隐藏层第一层的输出将被输入到隐藏层第二层，其为一个256个神经元的稠密层。最后，由一个采用Tanh激活函数的稠密层构成的输出层输出动作，表示为at，其为连续值，范围从-1到+1，其中t为视频块序列号。策略网络和评价网络之间的主要区别在于输入层和输出层。如图2所示，评价网络包括输入层、隐藏层和输出层，所述输入层的输入除了包含与策略网络相同的五个环境状态外，还有一个额外的输入，即由策略网络输出的动作at；所述隐藏层的第一层包含两个128个神经元的卷积层和四个128个神经元的稠密层，用于从输入层接收所有环境状态和动作，其中，带宽和视频块下载时间分别传递到两个卷积层，其余三个环境状态和动作分别传递到四个稠密层；然后，他们输出被输入到隐藏层的第二层，一个由256个神经元构成的稠密层；最后，由一个线性的稠密层构成的输出层输出Q值，用于评价策略网络所作动作的价值，以在训练中促进神经网络更新。一种基于神经网络的细粒度视频流自适应调节方法，基于如上的自适应调节系统，包括如下步骤：步骤1，由BETA系统对ABR模型进行训练；如图3所示，ABR模型训练过程如下：经验积累与随机经验采样：训练前，系统对所有超参数进行初始化，包括两个评价网络、策略网络、三个目标网络、经验缓冲区容量等。在训练中的每个轮次，对于视频会话中的视频块t，策略网络首先根据环境状态st输出动作at，然后视频块t将在一个虚拟网络环境中进行服务器端到用户端的传输。传输完成后，系统收集奖励rt和视频结束信号dt，其中奖励rt即为用户体验质量，视频结束信号dt用于判断当前视频会话是否完成。这些指标组成一个四元组，被称为经验数据，存储在经验缓冲区D中。随着训练的进行，经验缓冲区中的数据逐渐增多。随后，系统会从缓冲区D中随机采样一小批次经验，记为Ω，其中包含j个元组序列，每个序列中有n个连续的元组：；；…；；其中，i表示n个连续元组的第i个，t’表示视频块序列号。计算目标Q值和Q值：目标Q值计算分两种情况：1、采样的n个连续的元组中没有结束视频块，即m≥n，m是当前视频会话中剩余视频块的数量；2、n个连续元组中有结束片，即m＜n。若是前者，则需要计算n步累积奖励和目标评价网络输出的第n步Q值的加权和，作为目标Q值；若是后者，则直接将m步累积奖励作为目标Q值。目标Q值计算公式如下：；其中，为目标Q值，γ是折扣因子，rt’+k是第k步的奖励，Qtar1和Qtar2分别是两个目标评价网络输出的第n步Q值；关于Q值，需要将状态st和动作at同时输入到两个评价网络中，分别得到两个评价网络的输出Q1和Q2，即评价网络对当前状态st下策略网络做出动作at的价值分数。更新评价网络：基于Qtar 、Q1和Q2，通过计算均方时序差分误差来更新两个评价网络。通过调整两个评价网络的神经元权重θi=1,2，使评价网络输出的Q值接近目标Q。系统通过执行确定性策略梯度来实现这一步，以最小化以下损失函数：；其中，Ω为随机采样的小批次经验，其中包含j个元组序列，每个序列中有n个连续的元组，函数EΩ为平均期望值。更新策略网络：策略网络通过梯度下降进行更新，目的是最大化期望的Q值，记为EΩ，其中st’是视频块t’下的状态，πϕ是将st’代入策略网络中产生的动作值，Ω为随机采样的小批次经验，其中包含j个元组序列，每个序列中有n个连续的元组，函数EΩ为平均期望值，Qθ1为评价网络。更新目标网络：基于最新的策略网络和评价网络，目标策略网络和两个目标评价网络通过EWMA策略进行更新。模型保存与训练结束：每经过一定数量的训练轮次，系统对模型进行保存并同时进行性能验证，当性能达到一定阈值后，即可视为模型训练完成。训练是在服务器上离线执行的。本发明采用了一个现有的流媒体仿真器。网络状况是通过吞吐量追踪来模拟，追踪数据集包含超过300,000个视频会话，覆盖了蜂窝网络与Wi-Fi网络。经验缓冲区的容量被设置为400,000，随机采样批次为64个元组序列，其中每个序列包含连续的120个元组。对于策略网络和目标策略网络的更新，均采用Adam优化器，其中策略网络的学习率设定为，评价网络的学习率设定为。步骤3，开始时对用户请求的第一个视频块选择一个默认比特率，并将此视频块通过网络传送给用户播放器，用户播放器播放此视频块。步骤4，传输完成后，收集环境中的状态信息，形成环境状态。从环境中收集的状态信息包括：下载上一个视频块的网络带宽、下载上一个视频块所用时间、上一个视频块的比特率值、当前用户播放器缓冲区大小和剩余视频块数量。使用新收集到的状态形成总体环境状态，即前10个视频块下载带宽、前10个视频块的下载时间、上一个视频块的比特率值、当前用户播放器缓冲区大小、和剩余视频块数量。因为前9个视频块的带宽和下载时间已在先前几轮迭代中被收集，所以只需要收集当前最近下载视频块的下载带宽和下载时间即可。步骤5，将环境状态输入给ABR模型，模型根据状态输出动作at，并将该动作映射为针对下一个视频块比特率。将收集的环境状态输入到训练好的ABR模型中，ABR模型根据状态输出动作at。该过程类似函数y=f，其中函数f类比于ABR模型，x类比于环境状态，y类比于动作at。直观来讲，ABR模型描述了环境状态和动作at间的映射关系。为了将模型输出动作at映射到可用的编码比特率范围内，即，定义了一个线性动作映射策略：；其中，at’为视频块t的最终比特率决策，at为策略网络输出的动作，范围为。步骤6，服务器根据模型决策的比特率，将原始视频在线转码成相应比特率的视频块，随后通过网络将其传送给用户播放器播放，再返回步骤4。关于视频转码，随着近年来硬件水平的不断提升，在线实时转码已经不再是挑战，当前许多云转码平台已经可以实现非常快速的在线转码。本发明初衷在于BETA系统通过在线实时转码可以向用户播放器传输以任意比特率编码的视频块，以恰当匹配网络带宽，提高用户体验。结果评估：本评估将BETA系统与当前业界比较先进的三种ABR系统进行性能对比：1）Pensieve：基于深度强化学习A3C；2）PSQA：基于启发式流媒体参数动态调整策略；3）EAS：基于机器学习优化的网络感知流传输系统。在本评估中，用户体验质量QoE通过如下函数量化：；其中，zt是下载视频片段t 期间的播放卡顿时长，是比特率，函数q将比特率映射为视频画质，上式中最后一项量化视频画质的波动程度。图4比较了QoE和三个流媒体指标。根据平均吞吐量将网络环境划分为三个子集，即低，中，高。图4中显示，与比较算法相比，BETA平均提高了11.4%～17.3%的QoE，尤其在吞吐量波动剧烈的低带宽网络中，QoE的提高达到29.3%~244.1%。QoE的大幅提升主要由于BETA可以显著减少播放卡顿，参见图4中。无论在哪个网络中，播放卡顿均趋于零，这是因为BETA的ABR决策不仅是细粒度的，而且是长期的决策规划。图4中中，BETA在中、高带宽网络下的画质都要优于对比算法，在低带宽下也可以实现较高画质。图4中显示，BETA的画质波动也是四个算法中最低的。此外，BETA在不同视频会话中取得的QoE分布在图5中CDF分布展示。图5中、和分别对应低、中、高带宽网络。从图5中可以看出，BETA在众多视频会话中表现出更加稳定且优越的QoE性能。相比较而言，三个对比算法均表现出了更加显著的QoE波动，尤其在低带宽网络环境下。为了对比不同训练方法的性能，本发明将BETA与两种现存方法，即TD3和DDPG，进行比较。从图6中我们观察到BETA训练过程中的QoE更加稳定，而TD3和DDPG在训练过程中都呈现出剧烈的QoE波动，尤其是训练后期。这是因为BETA的ABR决策在长期Q值估计下更具远见，而TD3和DDPG均是短视的单步优化。在图7中，评估了A3C在不同数量的输出端口数下的训练结果，以查看其在不同级别粒度下离散动作的表现。观察发现，A3C的QoE从6端口到9端口的变化区间内实现了增长，但随着端口数进一步增加，QoE剧烈下滑。这是因为对动作域的过度细粒度离散化使A3C对最优策略的探索难度显著增加，因而限制了训练性能。对所公开的实施例的上述说明，使本领域专业技术人员能够实现或使用本发明。对这些实施例的多种修改对本领域的专业技术人员来说将是显而易见的，本文中所定义的一般原理可以在不脱离本发明的精神或范围的情况下，在其它实施例中实现。因此，本发明将不会被限制于本文所示的这些实施例，而是要符合与本文所公开的原理和新颖特点相一致的最宽的范围。
